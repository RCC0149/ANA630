{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30b80330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basic operations\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# for visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib_venn import venn3\n",
    "from itertools import combinations\n",
    "\n",
    "# for modeling\n",
    "from sklearn.feature_selection import SelectKBest, f_classif \n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import confusion_matrix, f1_score, roc_auc_score, classification_report   \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.stats import chi2_contingency, pointbiserialr, skew\n",
    "\n",
    "# to avoid warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd082742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the required data for the first part of assigment.\n",
    "SECOM_df = pd.read_csv('secom_data.csv')\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6c4aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Time</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 11:55:00</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>100.0</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>1.5005</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>202.4396</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9558</td>\n",
       "      <td>414.8710</td>\n",
       "      <td>10.0433</td>\n",
       "      <td>0.9680</td>\n",
       "      <td>192.3963</td>\n",
       "      <td>12.5190</td>\n",
       "      <td>1.4026</td>\n",
       "      <td>-5419.00</td>\n",
       "      <td>2916.50</td>\n",
       "      <td>-4043.75</td>\n",
       "      <td>751.00</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>1.7730</td>\n",
       "      <td>3.0490</td>\n",
       "      <td>64.2333</td>\n",
       "      <td>2.0222</td>\n",
       "      <td>0.1632</td>\n",
       "      <td>3.5191</td>\n",
       "      <td>83.3971</td>\n",
       "      <td>9.5126</td>\n",
       "      <td>50.6170</td>\n",
       "      <td>64.2588</td>\n",
       "      <td>49.3830</td>\n",
       "      <td>66.3141</td>\n",
       "      <td>86.9555</td>\n",
       "      <td>117.5132</td>\n",
       "      <td>61.29</td>\n",
       "      <td>4.515</td>\n",
       "      <td>70.0</td>\n",
       "      <td>352.7173</td>\n",
       "      <td>10.1841</td>\n",
       "      <td>130.3691</td>\n",
       "      <td>723.3092</td>\n",
       "      <td>1.3072</td>\n",
       "      <td>141.2282</td>\n",
       "      <td>1.0</td>\n",
       "      <td>624.3145</td>\n",
       "      <td>218.3174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.592</td>\n",
       "      <td>4.841</td>\n",
       "      <td>2834.0</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.9484</td>\n",
       "      <td>4.7057</td>\n",
       "      <td>-1.7264</td>\n",
       "      <td>350.9264</td>\n",
       "      <td>10.6231</td>\n",
       "      <td>108.6427</td>\n",
       "      <td>16.1445</td>\n",
       "      <td>21.7264</td>\n",
       "      <td>29.5367</td>\n",
       "      <td>693.7724</td>\n",
       "      <td>0.9226</td>\n",
       "      <td>148.6009</td>\n",
       "      <td>1.0</td>\n",
       "      <td>608.1700</td>\n",
       "      <td>84.0793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>-0.0206</td>\n",
       "      <td>0.0141</td>\n",
       "      <td>-0.0307</td>\n",
       "      <td>-0.0083</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0567</td>\n",
       "      <td>-0.0044</td>\n",
       "      <td>7.2163</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.3895</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>1747.6049</td>\n",
       "      <td>0.1841</td>\n",
       "      <td>8671.9301</td>\n",
       "      <td>-0.3274</td>\n",
       "      <td>-0.0055</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>-0.2786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3974</td>\n",
       "      <td>-0.0251</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>-0.0042</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.2468</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>0.0</td>\n",
       "      <td>748.6115</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>58.4306</td>\n",
       "      <td>0.6002</td>\n",
       "      <td>0.9804</td>\n",
       "      <td>6.3788</td>\n",
       "      <td>15.88</td>\n",
       "      <td>2.639</td>\n",
       "      <td>15.94</td>\n",
       "      <td>15.93</td>\n",
       "      <td>0.8656</td>\n",
       "      <td>3.353</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>3.188</td>\n",
       "      <td>-0.0473</td>\n",
       "      <td>0.7243</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>2.2967</td>\n",
       "      <td>1000.7263</td>\n",
       "      <td>39.2373</td>\n",
       "      <td>123.0</td>\n",
       "      <td>111.3</td>\n",
       "      <td>75.2</td>\n",
       "      <td>46.2000</td>\n",
       "      <td>350.6710</td>\n",
       "      <td>0.3948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.78</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0898</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.0358</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>12.2566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.271</td>\n",
       "      <td>10.284</td>\n",
       "      <td>0.4734</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>11.8901</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1017.0</td>\n",
       "      <td>967.0</td>\n",
       "      <td>1066.0</td>\n",
       "      <td>368.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.095</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.7250</td>\n",
       "      <td>0.1139</td>\n",
       "      <td>0.3183</td>\n",
       "      <td>0.5888</td>\n",
       "      <td>0.3184</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.3979</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.95</td>\n",
       "      <td>0.333</td>\n",
       "      <td>12.49</td>\n",
       "      <td>16.713</td>\n",
       "      <td>0.0803</td>\n",
       "      <td>5.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.19</td>\n",
       "      <td>65.363</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.292</td>\n",
       "      <td>5.38</td>\n",
       "      <td>20.10</td>\n",
       "      <td>0.296</td>\n",
       "      <td>10.62</td>\n",
       "      <td>10.30</td>\n",
       "      <td>5.38</td>\n",
       "      <td>4.040</td>\n",
       "      <td>16.230</td>\n",
       "      <td>0.2951</td>\n",
       "      <td>8.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.30</td>\n",
       "      <td>97.314</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.0599</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0704</td>\n",
       "      <td>0.0520</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>3.4789</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0707</td>\n",
       "      <td>0.0211</td>\n",
       "      <td>175.2173</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>1940.3994</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0744</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>219.9453</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>2.8374</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.4269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0472</td>\n",
       "      <td>40.855</td>\n",
       "      <td>4.5152</td>\n",
       "      <td>30.9815</td>\n",
       "      <td>33.9606</td>\n",
       "      <td>22.9057</td>\n",
       "      <td>15.9525</td>\n",
       "      <td>110.2144</td>\n",
       "      <td>0.1310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5883</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.0197</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>3.9321</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5123</td>\n",
       "      <td>3.5811</td>\n",
       "      <td>0.1337</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>3.8447</td>\n",
       "      <td>0.1077</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>418.1363</td>\n",
       "      <td>398.3185</td>\n",
       "      <td>496.1582</td>\n",
       "      <td>158.3330</td>\n",
       "      <td>0.0373</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0462</td>\n",
       "      <td>0.6083</td>\n",
       "      <td>0.3032</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.0434</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.1343</td>\n",
       "      <td>0.3670</td>\n",
       "      <td>0.1431</td>\n",
       "      <td>0.0610</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2698</td>\n",
       "      <td>0.1181</td>\n",
       "      <td>3.8208</td>\n",
       "      <td>5.3737</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>1.6252</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2461</td>\n",
       "      <td>18.0118</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0752</td>\n",
       "      <td>1.5989</td>\n",
       "      <td>6.5893</td>\n",
       "      <td>0.0913</td>\n",
       "      <td>3.0911</td>\n",
       "      <td>8.4654</td>\n",
       "      <td>1.5989</td>\n",
       "      <td>1.2293</td>\n",
       "      <td>5.3406</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>2.8551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.9971</td>\n",
       "      <td>31.8843</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>0.0315</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0206</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>0.0491</td>\n",
       "      <td>1.2708</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0229</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>55.2039</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>560.2658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>0.0124</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.5932</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.9967</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>0.1437</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>14.2396</td>\n",
       "      <td>1.4392</td>\n",
       "      <td>5.6188</td>\n",
       "      <td>3.6721</td>\n",
       "      <td>2.9329</td>\n",
       "      <td>2.1118</td>\n",
       "      <td>24.8504</td>\n",
       "      <td>29.0271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.9458</td>\n",
       "      <td>2.7380</td>\n",
       "      <td>5.9846</td>\n",
       "      <td>525.0965</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3.4641</td>\n",
       "      <td>6.0544</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.6840</td>\n",
       "      <td>2.4788</td>\n",
       "      <td>4.7141</td>\n",
       "      <td>1.7275</td>\n",
       "      <td>6.1800</td>\n",
       "      <td>3.2750</td>\n",
       "      <td>3.6084</td>\n",
       "      <td>18.7673</td>\n",
       "      <td>33.1562</td>\n",
       "      <td>26.3617</td>\n",
       "      <td>49.0013</td>\n",
       "      <td>10.0503</td>\n",
       "      <td>2.7073</td>\n",
       "      <td>3.1158</td>\n",
       "      <td>3.1136</td>\n",
       "      <td>44.5055</td>\n",
       "      <td>42.2737</td>\n",
       "      <td>1.3071</td>\n",
       "      <td>0.8693</td>\n",
       "      <td>1.1975</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.9163</td>\n",
       "      <td>0.6448</td>\n",
       "      <td>1.4324</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.1362</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.9396</td>\n",
       "      <td>3.2698</td>\n",
       "      <td>9.5805</td>\n",
       "      <td>2.3106</td>\n",
       "      <td>6.1463</td>\n",
       "      <td>4.0502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7924</td>\n",
       "      <td>29.9394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2052</td>\n",
       "      <td>311.6377</td>\n",
       "      <td>5.7277</td>\n",
       "      <td>2.7864</td>\n",
       "      <td>9.7752</td>\n",
       "      <td>63.7987</td>\n",
       "      <td>24.7625</td>\n",
       "      <td>13.6778</td>\n",
       "      <td>2.3394</td>\n",
       "      <td>31.9893</td>\n",
       "      <td>5.8142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.6936</td>\n",
       "      <td>115.7408</td>\n",
       "      <td>0.0</td>\n",
       "      <td>613.3069</td>\n",
       "      <td>291.4842</td>\n",
       "      <td>494.6996</td>\n",
       "      <td>178.1759</td>\n",
       "      <td>843.1138</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>53.1098</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>48.2091</td>\n",
       "      <td>0.7578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.9570</td>\n",
       "      <td>2.1739</td>\n",
       "      <td>10.0261</td>\n",
       "      <td>17.1202</td>\n",
       "      <td>22.3756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.6707</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.9864</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.3804</td>\n",
       "      <td>0.1094</td>\n",
       "      <td>4.8560</td>\n",
       "      <td>3.1406</td>\n",
       "      <td>0.5064</td>\n",
       "      <td>6.6926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0570</td>\n",
       "      <td>4.0825</td>\n",
       "      <td>11.5074</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>7.116</td>\n",
       "      <td>1.0616</td>\n",
       "      <td>395.570</td>\n",
       "      <td>75.752</td>\n",
       "      <td>0.4234</td>\n",
       "      <td>12.93</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.1827</td>\n",
       "      <td>5.7349</td>\n",
       "      <td>0.3363</td>\n",
       "      <td>39.8842</td>\n",
       "      <td>3.2687</td>\n",
       "      <td>1.0297</td>\n",
       "      <td>1.0344</td>\n",
       "      <td>0.4385</td>\n",
       "      <td>0.1039</td>\n",
       "      <td>42.3877</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>533.8500</td>\n",
       "      <td>2.1113</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>0.1026</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5005</td>\n",
       "      <td>0.0118</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 12:32:00</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>100.0</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>0.1247</td>\n",
       "      <td>1.4966</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0148</td>\n",
       "      <td>0.9627</td>\n",
       "      <td>200.5470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.1548</td>\n",
       "      <td>414.7347</td>\n",
       "      <td>9.2599</td>\n",
       "      <td>0.9701</td>\n",
       "      <td>191.2872</td>\n",
       "      <td>12.4608</td>\n",
       "      <td>1.3825</td>\n",
       "      <td>-5441.50</td>\n",
       "      <td>2604.25</td>\n",
       "      <td>-3498.75</td>\n",
       "      <td>-1640.25</td>\n",
       "      <td>1.2973</td>\n",
       "      <td>2.0143</td>\n",
       "      <td>7.3900</td>\n",
       "      <td>68.4222</td>\n",
       "      <td>2.2667</td>\n",
       "      <td>0.2102</td>\n",
       "      <td>3.4171</td>\n",
       "      <td>84.9052</td>\n",
       "      <td>9.7997</td>\n",
       "      <td>50.6596</td>\n",
       "      <td>64.2828</td>\n",
       "      <td>49.3404</td>\n",
       "      <td>64.9193</td>\n",
       "      <td>87.5241</td>\n",
       "      <td>118.1188</td>\n",
       "      <td>78.25</td>\n",
       "      <td>2.773</td>\n",
       "      <td>70.0</td>\n",
       "      <td>352.2445</td>\n",
       "      <td>10.0373</td>\n",
       "      <td>133.1727</td>\n",
       "      <td>724.8264</td>\n",
       "      <td>1.2887</td>\n",
       "      <td>145.8445</td>\n",
       "      <td>1.0</td>\n",
       "      <td>631.2618</td>\n",
       "      <td>205.1695</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.590</td>\n",
       "      <td>4.842</td>\n",
       "      <td>2853.0</td>\n",
       "      <td>0.9324</td>\n",
       "      <td>0.9479</td>\n",
       "      <td>4.6820</td>\n",
       "      <td>0.8073</td>\n",
       "      <td>352.0073</td>\n",
       "      <td>10.3092</td>\n",
       "      <td>113.9800</td>\n",
       "      <td>10.9036</td>\n",
       "      <td>19.1927</td>\n",
       "      <td>27.6301</td>\n",
       "      <td>697.1964</td>\n",
       "      <td>1.1598</td>\n",
       "      <td>154.3709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>620.3582</td>\n",
       "      <td>82.3494</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0039</td>\n",
       "      <td>-0.0198</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0440</td>\n",
       "      <td>-0.0358</td>\n",
       "      <td>-0.0120</td>\n",
       "      <td>-0.0377</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>6.8043</td>\n",
       "      <td>0.1358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.3754</td>\n",
       "      <td>0.9894</td>\n",
       "      <td>1931.6464</td>\n",
       "      <td>0.1874</td>\n",
       "      <td>8407.0299</td>\n",
       "      <td>0.1455</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.5854</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.9353</td>\n",
       "      <td>-0.0158</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0752</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>-0.0903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>731.2517</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>58.6680</td>\n",
       "      <td>0.5958</td>\n",
       "      <td>0.9731</td>\n",
       "      <td>6.5061</td>\n",
       "      <td>15.88</td>\n",
       "      <td>2.541</td>\n",
       "      <td>15.91</td>\n",
       "      <td>15.88</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>2.771</td>\n",
       "      <td>0.4138</td>\n",
       "      <td>3.272</td>\n",
       "      <td>-0.0946</td>\n",
       "      <td>0.8122</td>\n",
       "      <td>0.9985</td>\n",
       "      <td>2.2932</td>\n",
       "      <td>998.1081</td>\n",
       "      <td>37.9213</td>\n",
       "      <td>98.0</td>\n",
       "      <td>80.3</td>\n",
       "      <td>81.0</td>\n",
       "      <td>56.2000</td>\n",
       "      <td>219.7679</td>\n",
       "      <td>0.2301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.70</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>0.0204</td>\n",
       "      <td>12.3319</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.285</td>\n",
       "      <td>13.077</td>\n",
       "      <td>0.5666</td>\n",
       "      <td>0.0144</td>\n",
       "      <td>11.8428</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>568.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>3277.0</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.124</td>\n",
       "      <td>2.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.561</td>\n",
       "      <td>1.0498</td>\n",
       "      <td>0.1917</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>0.6582</td>\n",
       "      <td>0.4115</td>\n",
       "      <td>1.0181</td>\n",
       "      <td>0.2315</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>0.439</td>\n",
       "      <td>10.14</td>\n",
       "      <td>16.358</td>\n",
       "      <td>0.0892</td>\n",
       "      <td>6.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.05</td>\n",
       "      <td>82.986</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.222</td>\n",
       "      <td>3.74</td>\n",
       "      <td>19.59</td>\n",
       "      <td>0.316</td>\n",
       "      <td>11.65</td>\n",
       "      <td>8.02</td>\n",
       "      <td>3.74</td>\n",
       "      <td>3.659</td>\n",
       "      <td>15.078</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>8.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.02</td>\n",
       "      <td>134.250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0566</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>0.1651</td>\n",
       "      <td>0.1578</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.0734</td>\n",
       "      <td>0.0747</td>\n",
       "      <td>3.9578</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0761</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>128.4285</td>\n",
       "      <td>0.0238</td>\n",
       "      <td>1988.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0203</td>\n",
       "      <td>0.0236</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0064</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.0287</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>3.8999</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.5749</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>29.743</td>\n",
       "      <td>3.6327</td>\n",
       "      <td>29.0598</td>\n",
       "      <td>28.9862</td>\n",
       "      <td>22.3163</td>\n",
       "      <td>17.4008</td>\n",
       "      <td>83.5542</td>\n",
       "      <td>0.0767</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.8459</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0440</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>3.9011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1016</td>\n",
       "      <td>3.9483</td>\n",
       "      <td>0.1662</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>3.7836</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>233.9865</td>\n",
       "      <td>26.5879</td>\n",
       "      <td>139.2082</td>\n",
       "      <td>1529.7622</td>\n",
       "      <td>0.0502</td>\n",
       "      <td>0.0561</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.8151</td>\n",
       "      <td>0.3464</td>\n",
       "      <td>0.0291</td>\n",
       "      <td>0.1822</td>\n",
       "      <td>0.3814</td>\n",
       "      <td>0.0715</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>0.0856</td>\n",
       "      <td>0.1214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6522</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>2.9939</td>\n",
       "      <td>5.2445</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>1.8045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.7661</td>\n",
       "      <td>23.6230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0778</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>5.9247</td>\n",
       "      <td>0.0878</td>\n",
       "      <td>3.3604</td>\n",
       "      <td>7.7421</td>\n",
       "      <td>1.1506</td>\n",
       "      <td>1.1265</td>\n",
       "      <td>5.0108</td>\n",
       "      <td>0.1013</td>\n",
       "      <td>2.4278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.4890</td>\n",
       "      <td>41.7080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0142</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>0.0768</td>\n",
       "      <td>0.0729</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>0.0399</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>1.2474</td>\n",
       "      <td>0.0017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>46.3453</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>677.1873</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0033</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.0999</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>1.1655</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.1921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>10.5837</td>\n",
       "      <td>1.0323</td>\n",
       "      <td>4.3465</td>\n",
       "      <td>2.5939</td>\n",
       "      <td>3.2858</td>\n",
       "      <td>2.5197</td>\n",
       "      <td>15.0150</td>\n",
       "      <td>27.7464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.5695</td>\n",
       "      <td>3.9300</td>\n",
       "      <td>9.0604</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>368.9713</td>\n",
       "      <td>2.1196</td>\n",
       "      <td>6.1491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.8918</td>\n",
       "      <td>3.1531</td>\n",
       "      <td>6.1188</td>\n",
       "      <td>1.4857</td>\n",
       "      <td>6.1911</td>\n",
       "      <td>2.8088</td>\n",
       "      <td>3.1595</td>\n",
       "      <td>10.4383</td>\n",
       "      <td>2.2655</td>\n",
       "      <td>8.4887</td>\n",
       "      <td>199.7866</td>\n",
       "      <td>8.6336</td>\n",
       "      <td>5.7093</td>\n",
       "      <td>1.6779</td>\n",
       "      <td>3.2153</td>\n",
       "      <td>48.5294</td>\n",
       "      <td>37.5793</td>\n",
       "      <td>16.4174</td>\n",
       "      <td>1.2364</td>\n",
       "      <td>1.9562</td>\n",
       "      <td>0.8123</td>\n",
       "      <td>1.0239</td>\n",
       "      <td>0.8340</td>\n",
       "      <td>1.5683</td>\n",
       "      <td>0.2645</td>\n",
       "      <td>0.2751</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.1072</td>\n",
       "      <td>4.3737</td>\n",
       "      <td>7.6142</td>\n",
       "      <td>2.2568</td>\n",
       "      <td>6.9233</td>\n",
       "      <td>4.7448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4336</td>\n",
       "      <td>40.4475</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.7415</td>\n",
       "      <td>463.2883</td>\n",
       "      <td>5.5652</td>\n",
       "      <td>3.0652</td>\n",
       "      <td>10.2211</td>\n",
       "      <td>73.5536</td>\n",
       "      <td>19.4865</td>\n",
       "      <td>13.2430</td>\n",
       "      <td>2.1627</td>\n",
       "      <td>30.8643</td>\n",
       "      <td>5.8042</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.2928</td>\n",
       "      <td>163.0249</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>246.7762</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>359.0444</td>\n",
       "      <td>130.6350</td>\n",
       "      <td>820.7900</td>\n",
       "      <td>194.4371</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>58.1666</td>\n",
       "      <td>3.6822</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2029</td>\n",
       "      <td>0.1441</td>\n",
       "      <td>6.6487</td>\n",
       "      <td>12.6788</td>\n",
       "      <td>23.6469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.4365</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.6292</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.3970</td>\n",
       "      <td>0.0673</td>\n",
       "      <td>6.6475</td>\n",
       "      <td>3.1310</td>\n",
       "      <td>0.8832</td>\n",
       "      <td>8.8370</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.7910</td>\n",
       "      <td>2.9799</td>\n",
       "      <td>9.5796</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>7.116</td>\n",
       "      <td>1.3526</td>\n",
       "      <td>408.798</td>\n",
       "      <td>74.640</td>\n",
       "      <td>0.7193</td>\n",
       "      <td>16.00</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>7.1196</td>\n",
       "      <td>0.4989</td>\n",
       "      <td>53.1836</td>\n",
       "      <td>3.9139</td>\n",
       "      <td>1.7819</td>\n",
       "      <td>0.9634</td>\n",
       "      <td>0.1745</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>18.1087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>535.0164</td>\n",
       "      <td>2.4335</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.2653</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0055</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>0.0096</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0060</td>\n",
       "      <td>208.2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>19/07/2008 13:17:00</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>100.0</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>0.1241</td>\n",
       "      <td>1.4436</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.9615</td>\n",
       "      <td>202.0179</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5157</td>\n",
       "      <td>416.7075</td>\n",
       "      <td>9.3144</td>\n",
       "      <td>0.9674</td>\n",
       "      <td>192.7035</td>\n",
       "      <td>12.5404</td>\n",
       "      <td>1.4123</td>\n",
       "      <td>-5447.75</td>\n",
       "      <td>2701.75</td>\n",
       "      <td>-4047.00</td>\n",
       "      <td>-1916.50</td>\n",
       "      <td>1.3122</td>\n",
       "      <td>2.0295</td>\n",
       "      <td>7.5788</td>\n",
       "      <td>67.1333</td>\n",
       "      <td>2.3333</td>\n",
       "      <td>0.1734</td>\n",
       "      <td>3.5986</td>\n",
       "      <td>84.7569</td>\n",
       "      <td>8.6590</td>\n",
       "      <td>50.1530</td>\n",
       "      <td>64.1114</td>\n",
       "      <td>49.8470</td>\n",
       "      <td>65.8389</td>\n",
       "      <td>84.7327</td>\n",
       "      <td>118.6128</td>\n",
       "      <td>14.37</td>\n",
       "      <td>5.434</td>\n",
       "      <td>70.0</td>\n",
       "      <td>364.3782</td>\n",
       "      <td>9.8783</td>\n",
       "      <td>131.8027</td>\n",
       "      <td>734.7924</td>\n",
       "      <td>1.2992</td>\n",
       "      <td>141.0845</td>\n",
       "      <td>1.0</td>\n",
       "      <td>637.2655</td>\n",
       "      <td>185.7574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.486</td>\n",
       "      <td>4.748</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>4.5873</td>\n",
       "      <td>23.8245</td>\n",
       "      <td>364.5364</td>\n",
       "      <td>10.1685</td>\n",
       "      <td>115.6273</td>\n",
       "      <td>11.3019</td>\n",
       "      <td>16.1755</td>\n",
       "      <td>24.2829</td>\n",
       "      <td>710.5095</td>\n",
       "      <td>0.8694</td>\n",
       "      <td>145.8000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625.9636</td>\n",
       "      <td>84.7681</td>\n",
       "      <td>140.6972</td>\n",
       "      <td>485.2665</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0078</td>\n",
       "      <td>-0.0326</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>0.0213</td>\n",
       "      <td>-0.0054</td>\n",
       "      <td>-0.1134</td>\n",
       "      <td>-0.0182</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>7.1041</td>\n",
       "      <td>0.1362</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4532</td>\n",
       "      <td>0.9880</td>\n",
       "      <td>1685.8514</td>\n",
       "      <td>0.1497</td>\n",
       "      <td>9317.1698</td>\n",
       "      <td>0.0553</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-0.0013</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.1343</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1427</td>\n",
       "      <td>0.1218</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0134</td>\n",
       "      <td>-0.0026</td>\n",
       "      <td>-0.0016</td>\n",
       "      <td>-0.0006</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>-0.0301</td>\n",
       "      <td>-0.0728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4684</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.0</td>\n",
       "      <td>718.5777</td>\n",
       "      <td>0.9899</td>\n",
       "      <td>58.4808</td>\n",
       "      <td>0.6015</td>\n",
       "      <td>0.9772</td>\n",
       "      <td>6.4527</td>\n",
       "      <td>15.90</td>\n",
       "      <td>2.882</td>\n",
       "      <td>15.94</td>\n",
       "      <td>15.95</td>\n",
       "      <td>0.8798</td>\n",
       "      <td>3.094</td>\n",
       "      <td>0.4777</td>\n",
       "      <td>3.272</td>\n",
       "      <td>-0.1892</td>\n",
       "      <td>0.8194</td>\n",
       "      <td>0.9978</td>\n",
       "      <td>2.2592</td>\n",
       "      <td>998.4440</td>\n",
       "      <td>42.0579</td>\n",
       "      <td>89.0</td>\n",
       "      <td>126.4</td>\n",
       "      <td>96.5</td>\n",
       "      <td>45.1001</td>\n",
       "      <td>306.0380</td>\n",
       "      <td>0.3263</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.33</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0754</td>\n",
       "      <td>0.0483</td>\n",
       "      <td>0.0619</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>8.2660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.819</td>\n",
       "      <td>8.443</td>\n",
       "      <td>0.4909</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>8.2054</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>562.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>759.0</td>\n",
       "      <td>2100.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.068</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.319</td>\n",
       "      <td>1.0824</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.5753</td>\n",
       "      <td>0.3141</td>\n",
       "      <td>0.9677</td>\n",
       "      <td>0.2706</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.78</td>\n",
       "      <td>0.745</td>\n",
       "      <td>13.31</td>\n",
       "      <td>22.912</td>\n",
       "      <td>0.1959</td>\n",
       "      <td>9.21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.87</td>\n",
       "      <td>60.110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139</td>\n",
       "      <td>5.09</td>\n",
       "      <td>19.75</td>\n",
       "      <td>0.949</td>\n",
       "      <td>9.71</td>\n",
       "      <td>16.73</td>\n",
       "      <td>5.09</td>\n",
       "      <td>11.059</td>\n",
       "      <td>22.624</td>\n",
       "      <td>0.1164</td>\n",
       "      <td>13.30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.73</td>\n",
       "      <td>79.618</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>0.0494</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.0840</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>0.0718</td>\n",
       "      <td>2.4266</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0963</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>182.4956</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>839.6006</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0192</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1729</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>104.4042</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>4.1446</td>\n",
       "      <td>0.0733</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.4166</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0487</td>\n",
       "      <td>29.621</td>\n",
       "      <td>3.9133</td>\n",
       "      <td>23.5510</td>\n",
       "      <td>41.3837</td>\n",
       "      <td>32.6256</td>\n",
       "      <td>15.7716</td>\n",
       "      <td>97.3868</td>\n",
       "      <td>0.1117</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.5274</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>2.8705</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5306</td>\n",
       "      <td>2.5493</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>2.8046</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>251.4536</td>\n",
       "      <td>329.6406</td>\n",
       "      <td>325.0672</td>\n",
       "      <td>902.4576</td>\n",
       "      <td>0.0800</td>\n",
       "      <td>0.0583</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>0.6964</td>\n",
       "      <td>0.4031</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.1041</td>\n",
       "      <td>0.3846</td>\n",
       "      <td>0.0151</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.1288</td>\n",
       "      <td>0.3677</td>\n",
       "      <td>0.1175</td>\n",
       "      <td>0.1261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.7247</td>\n",
       "      <td>0.2682</td>\n",
       "      <td>3.8541</td>\n",
       "      <td>6.1797</td>\n",
       "      <td>0.0546</td>\n",
       "      <td>2.5680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.6067</td>\n",
       "      <td>16.0104</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>1.5481</td>\n",
       "      <td>5.9453</td>\n",
       "      <td>0.2777</td>\n",
       "      <td>3.1600</td>\n",
       "      <td>8.9855</td>\n",
       "      <td>1.5481</td>\n",
       "      <td>2.9844</td>\n",
       "      <td>6.2277</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>3.7663</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.6983</td>\n",
       "      <td>24.7959</td>\n",
       "      <td>13.5664</td>\n",
       "      <td>15.4488</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0383</td>\n",
       "      <td>0.7786</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>58.0575</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>283.6616</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.7334</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1.2356</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.1375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>11.4871</td>\n",
       "      <td>1.1798</td>\n",
       "      <td>4.0782</td>\n",
       "      <td>4.3102</td>\n",
       "      <td>3.7696</td>\n",
       "      <td>2.0627</td>\n",
       "      <td>18.0233</td>\n",
       "      <td>21.6062</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.7236</td>\n",
       "      <td>3.0609</td>\n",
       "      <td>5.2231</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2.2943</td>\n",
       "      <td>4.0917</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.6425</td>\n",
       "      <td>2.0261</td>\n",
       "      <td>5.2707</td>\n",
       "      <td>1.8268</td>\n",
       "      <td>4.2581</td>\n",
       "      <td>3.7479</td>\n",
       "      <td>3.5220</td>\n",
       "      <td>10.3162</td>\n",
       "      <td>29.1663</td>\n",
       "      <td>18.7546</td>\n",
       "      <td>109.5747</td>\n",
       "      <td>14.2503</td>\n",
       "      <td>5.7650</td>\n",
       "      <td>0.8972</td>\n",
       "      <td>3.1281</td>\n",
       "      <td>60.0000</td>\n",
       "      <td>70.9161</td>\n",
       "      <td>8.8647</td>\n",
       "      <td>1.2771</td>\n",
       "      <td>0.4264</td>\n",
       "      <td>0.6263</td>\n",
       "      <td>0.8973</td>\n",
       "      <td>0.6301</td>\n",
       "      <td>1.4698</td>\n",
       "      <td>0.3194</td>\n",
       "      <td>0.2748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8795</td>\n",
       "      <td>7.5418</td>\n",
       "      <td>10.0984</td>\n",
       "      <td>3.1182</td>\n",
       "      <td>15.0790</td>\n",
       "      <td>6.5280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8042</td>\n",
       "      <td>32.3594</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0301</td>\n",
       "      <td>21.3645</td>\n",
       "      <td>5.4178</td>\n",
       "      <td>9.3327</td>\n",
       "      <td>8.3977</td>\n",
       "      <td>148.0287</td>\n",
       "      <td>31.4674</td>\n",
       "      <td>45.5423</td>\n",
       "      <td>3.1842</td>\n",
       "      <td>13.3923</td>\n",
       "      <td>9.1221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.6727</td>\n",
       "      <td>93.9245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>434.2674</td>\n",
       "      <td>151.7665</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>190.3869</td>\n",
       "      <td>746.9150</td>\n",
       "      <td>74.0741</td>\n",
       "      <td>191.7582</td>\n",
       "      <td>250.1742</td>\n",
       "      <td>34.1573</td>\n",
       "      <td>1.0281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.9238</td>\n",
       "      <td>1.5357</td>\n",
       "      <td>10.8251</td>\n",
       "      <td>18.9849</td>\n",
       "      <td>9.0113</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>240.7767</td>\n",
       "      <td>244.2748</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>36.9067</td>\n",
       "      <td>2.9626</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.5293</td>\n",
       "      <td>0.0751</td>\n",
       "      <td>7.0870</td>\n",
       "      <td>12.1831</td>\n",
       "      <td>0.6451</td>\n",
       "      <td>6.4568</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1538</td>\n",
       "      <td>2.9667</td>\n",
       "      <td>9.3046</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.7942</td>\n",
       "      <td>411.136</td>\n",
       "      <td>74.654</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>16.16</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.0857</td>\n",
       "      <td>7.1619</td>\n",
       "      <td>0.3752</td>\n",
       "      <td>23.0713</td>\n",
       "      <td>3.9306</td>\n",
       "      <td>1.1386</td>\n",
       "      <td>1.5021</td>\n",
       "      <td>0.3718</td>\n",
       "      <td>0.1233</td>\n",
       "      <td>24.7524</td>\n",
       "      <td>267.064</td>\n",
       "      <td>0.9032</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.4122</td>\n",
       "      <td>0.2562</td>\n",
       "      <td>0.4119</td>\n",
       "      <td>68.8489</td>\n",
       "      <td>535.0245</td>\n",
       "      <td>2.0293</td>\n",
       "      <td>11.21</td>\n",
       "      <td>0.1882</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>0.0640</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>0.4958</td>\n",
       "      <td>0.0157</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>0.0584</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0148</td>\n",
       "      <td>82.8602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 14:43:00</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>100.0</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>0.1217</td>\n",
       "      <td>1.4882</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>-0.0033</td>\n",
       "      <td>0.9629</td>\n",
       "      <td>201.8482</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>422.2894</td>\n",
       "      <td>9.6924</td>\n",
       "      <td>0.9687</td>\n",
       "      <td>192.1557</td>\n",
       "      <td>12.4782</td>\n",
       "      <td>1.4011</td>\n",
       "      <td>-5468.25</td>\n",
       "      <td>2648.25</td>\n",
       "      <td>-4515.00</td>\n",
       "      <td>-1657.25</td>\n",
       "      <td>1.3137</td>\n",
       "      <td>2.0038</td>\n",
       "      <td>7.3145</td>\n",
       "      <td>62.9333</td>\n",
       "      <td>2.6444</td>\n",
       "      <td>0.2071</td>\n",
       "      <td>3.3813</td>\n",
       "      <td>84.9105</td>\n",
       "      <td>8.6789</td>\n",
       "      <td>50.5100</td>\n",
       "      <td>64.1125</td>\n",
       "      <td>49.4900</td>\n",
       "      <td>65.1951</td>\n",
       "      <td>86.6867</td>\n",
       "      <td>117.0442</td>\n",
       "      <td>76.90</td>\n",
       "      <td>1.279</td>\n",
       "      <td>70.0</td>\n",
       "      <td>363.0273</td>\n",
       "      <td>9.9305</td>\n",
       "      <td>131.8027</td>\n",
       "      <td>733.8778</td>\n",
       "      <td>1.3027</td>\n",
       "      <td>142.5427</td>\n",
       "      <td>1.0</td>\n",
       "      <td>637.3727</td>\n",
       "      <td>189.9079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.486</td>\n",
       "      <td>4.748</td>\n",
       "      <td>2936.0</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9447</td>\n",
       "      <td>4.5873</td>\n",
       "      <td>24.3791</td>\n",
       "      <td>361.4582</td>\n",
       "      <td>10.2112</td>\n",
       "      <td>116.1818</td>\n",
       "      <td>13.5597</td>\n",
       "      <td>15.6209</td>\n",
       "      <td>23.4736</td>\n",
       "      <td>710.4043</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>147.6545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>625.2945</td>\n",
       "      <td>70.2289</td>\n",
       "      <td>160.3210</td>\n",
       "      <td>464.9735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0555</td>\n",
       "      <td>-0.0461</td>\n",
       "      <td>-0.0400</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.0676</td>\n",
       "      <td>-0.1051</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>7.5925</td>\n",
       "      <td>0.1302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4004</td>\n",
       "      <td>0.9904</td>\n",
       "      <td>1752.0968</td>\n",
       "      <td>0.1958</td>\n",
       "      <td>8205.7000</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>-0.0021</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0411</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>-0.0195</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0699</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>-0.0483</td>\n",
       "      <td>-0.1180</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4647</td>\n",
       "      <td>0.9564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>709.0867</td>\n",
       "      <td>0.9906</td>\n",
       "      <td>58.6635</td>\n",
       "      <td>0.6016</td>\n",
       "      <td>0.9761</td>\n",
       "      <td>6.4935</td>\n",
       "      <td>15.55</td>\n",
       "      <td>3.132</td>\n",
       "      <td>15.61</td>\n",
       "      <td>15.59</td>\n",
       "      <td>1.3660</td>\n",
       "      <td>2.480</td>\n",
       "      <td>0.5176</td>\n",
       "      <td>3.119</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>0.7244</td>\n",
       "      <td>0.9961</td>\n",
       "      <td>2.3802</td>\n",
       "      <td>980.4510</td>\n",
       "      <td>41.1025</td>\n",
       "      <td>127.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>123.7</td>\n",
       "      <td>47.8000</td>\n",
       "      <td>162.4320</td>\n",
       "      <td>0.1915</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.51</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.0190</td>\n",
       "      <td>13.2651</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.073</td>\n",
       "      <td>15.241</td>\n",
       "      <td>1.3029</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>11.9738</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.0699</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>859.0</td>\n",
       "      <td>355.0</td>\n",
       "      <td>3433.0</td>\n",
       "      <td>3004.0</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.0356</td>\n",
       "      <td>0.2618</td>\n",
       "      <td>0.4391</td>\n",
       "      <td>0.2618</td>\n",
       "      <td>0.8567</td>\n",
       "      <td>0.2452</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.22</td>\n",
       "      <td>0.693</td>\n",
       "      <td>14.67</td>\n",
       "      <td>22.562</td>\n",
       "      <td>0.1786</td>\n",
       "      <td>5.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.20</td>\n",
       "      <td>52.571</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.139</td>\n",
       "      <td>5.92</td>\n",
       "      <td>23.60</td>\n",
       "      <td>1.264</td>\n",
       "      <td>10.63</td>\n",
       "      <td>13.56</td>\n",
       "      <td>5.92</td>\n",
       "      <td>11.382</td>\n",
       "      <td>24.320</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>9.56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.97</td>\n",
       "      <td>104.950</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1248</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>0.1223</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>0.0708</td>\n",
       "      <td>0.0754</td>\n",
       "      <td>0.0643</td>\n",
       "      <td>0.0932</td>\n",
       "      <td>5.5398</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0764</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>152.0885</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>820.3999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0152</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.0954</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>3.2119</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.4212</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0513</td>\n",
       "      <td>31.830</td>\n",
       "      <td>3.1959</td>\n",
       "      <td>33.8960</td>\n",
       "      <td>37.8477</td>\n",
       "      <td>44.3906</td>\n",
       "      <td>16.9347</td>\n",
       "      <td>50.3631</td>\n",
       "      <td>0.0581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1775</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0417</td>\n",
       "      <td>0.0115</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>4.2154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8960</td>\n",
       "      <td>4.0526</td>\n",
       "      <td>0.3882</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>3.9403</td>\n",
       "      <td>0.0916</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>415.5048</td>\n",
       "      <td>157.0889</td>\n",
       "      <td>1572.6896</td>\n",
       "      <td>1377.4276</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0445</td>\n",
       "      <td>0.0465</td>\n",
       "      <td>0.6305</td>\n",
       "      <td>0.3046</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0824</td>\n",
       "      <td>0.3483</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.1701</td>\n",
       "      <td>0.1004</td>\n",
       "      <td>0.3465</td>\n",
       "      <td>0.0973</td>\n",
       "      <td>0.1675</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.4440</td>\n",
       "      <td>0.2004</td>\n",
       "      <td>4.1900</td>\n",
       "      <td>6.3329</td>\n",
       "      <td>0.0479</td>\n",
       "      <td>1.7339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.9660</td>\n",
       "      <td>15.7375</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0243</td>\n",
       "      <td>1.7317</td>\n",
       "      <td>6.6262</td>\n",
       "      <td>0.3512</td>\n",
       "      <td>3.2699</td>\n",
       "      <td>9.4020</td>\n",
       "      <td>1.7317</td>\n",
       "      <td>3.0672</td>\n",
       "      <td>6.6839</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>3.0229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3292</td>\n",
       "      <td>29.0339</td>\n",
       "      <td>8.4026</td>\n",
       "      <td>4.8851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>0.0198</td>\n",
       "      <td>0.0531</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0224</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.0273</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>1.8222</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0252</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>45.7058</td>\n",
       "      <td>0.0188</td>\n",
       "      <td>309.8492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.4228</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>1.1135</td>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.1348</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>13.3972</td>\n",
       "      <td>1.1907</td>\n",
       "      <td>5.6363</td>\n",
       "      <td>3.9482</td>\n",
       "      <td>4.9881</td>\n",
       "      <td>2.1737</td>\n",
       "      <td>17.8537</td>\n",
       "      <td>14.5054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.2860</td>\n",
       "      <td>2.4643</td>\n",
       "      <td>7.6602</td>\n",
       "      <td>317.7362</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.9689</td>\n",
       "      <td>6.5718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>94.4594</td>\n",
       "      <td>3.6091</td>\n",
       "      <td>13.4420</td>\n",
       "      <td>1.5441</td>\n",
       "      <td>6.2313</td>\n",
       "      <td>2.8049</td>\n",
       "      <td>4.9898</td>\n",
       "      <td>15.7089</td>\n",
       "      <td>13.4051</td>\n",
       "      <td>76.0354</td>\n",
       "      <td>181.2641</td>\n",
       "      <td>5.1760</td>\n",
       "      <td>5.3899</td>\n",
       "      <td>1.3671</td>\n",
       "      <td>2.7013</td>\n",
       "      <td>34.0336</td>\n",
       "      <td>41.5236</td>\n",
       "      <td>7.1274</td>\n",
       "      <td>1.1054</td>\n",
       "      <td>0.4097</td>\n",
       "      <td>0.5183</td>\n",
       "      <td>0.6849</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>1.3141</td>\n",
       "      <td>0.2829</td>\n",
       "      <td>0.3332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.4680</td>\n",
       "      <td>6.9785</td>\n",
       "      <td>11.1303</td>\n",
       "      <td>3.0744</td>\n",
       "      <td>13.7105</td>\n",
       "      <td>3.9918</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.8555</td>\n",
       "      <td>27.6824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0301</td>\n",
       "      <td>24.2831</td>\n",
       "      <td>6.5291</td>\n",
       "      <td>12.3786</td>\n",
       "      <td>9.1494</td>\n",
       "      <td>100.0021</td>\n",
       "      <td>37.8979</td>\n",
       "      <td>48.4887</td>\n",
       "      <td>3.4234</td>\n",
       "      <td>35.4323</td>\n",
       "      <td>6.4746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.5135</td>\n",
       "      <td>149.4399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>225.0169</td>\n",
       "      <td>100.4883</td>\n",
       "      <td>305.7500</td>\n",
       "      <td>88.5553</td>\n",
       "      <td>104.6660</td>\n",
       "      <td>71.7583</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>336.7660</td>\n",
       "      <td>72.9635</td>\n",
       "      <td>1.7670</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.1817</td>\n",
       "      <td>0.1488</td>\n",
       "      <td>8.6804</td>\n",
       "      <td>29.2542</td>\n",
       "      <td>9.9979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>711.6418</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>113.5593</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.1200</td>\n",
       "      <td>2.4416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.2699</td>\n",
       "      <td>0.0977</td>\n",
       "      <td>5.4751</td>\n",
       "      <td>6.7553</td>\n",
       "      <td>0.7404</td>\n",
       "      <td>6.4865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.1565</td>\n",
       "      <td>3.2465</td>\n",
       "      <td>7.7754</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>7.116</td>\n",
       "      <td>1.1650</td>\n",
       "      <td>372.822</td>\n",
       "      <td>72.442</td>\n",
       "      <td>1.8804</td>\n",
       "      <td>131.68</td>\n",
       "      <td>39.33</td>\n",
       "      <td>0.6812</td>\n",
       "      <td>56.9303</td>\n",
       "      <td>17.4781</td>\n",
       "      <td>161.4081</td>\n",
       "      <td>35.3198</td>\n",
       "      <td>54.2917</td>\n",
       "      <td>1.1613</td>\n",
       "      <td>0.7288</td>\n",
       "      <td>0.2710</td>\n",
       "      <td>62.7572</td>\n",
       "      <td>268.228</td>\n",
       "      <td>0.6511</td>\n",
       "      <td>7.32</td>\n",
       "      <td>0.1630</td>\n",
       "      <td>3.5611</td>\n",
       "      <td>0.0670</td>\n",
       "      <td>2.7290</td>\n",
       "      <td>25.0363</td>\n",
       "      <td>530.5682</td>\n",
       "      <td>2.0253</td>\n",
       "      <td>9.33</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>0.4990</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>19/07/2008 15:22:00</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>0.1235</td>\n",
       "      <td>1.5031</td>\n",
       "      <td>-0.0031</td>\n",
       "      <td>-0.0072</td>\n",
       "      <td>0.9569</td>\n",
       "      <td>201.9424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5661</td>\n",
       "      <td>420.5925</td>\n",
       "      <td>10.3387</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>191.6037</td>\n",
       "      <td>12.4735</td>\n",
       "      <td>1.3888</td>\n",
       "      <td>-5476.25</td>\n",
       "      <td>2635.25</td>\n",
       "      <td>-3987.50</td>\n",
       "      <td>117.00</td>\n",
       "      <td>1.2887</td>\n",
       "      <td>1.9912</td>\n",
       "      <td>7.2748</td>\n",
       "      <td>62.8333</td>\n",
       "      <td>3.1556</td>\n",
       "      <td>0.2696</td>\n",
       "      <td>3.2728</td>\n",
       "      <td>86.3269</td>\n",
       "      <td>8.7677</td>\n",
       "      <td>50.2480</td>\n",
       "      <td>64.1511</td>\n",
       "      <td>49.7520</td>\n",
       "      <td>66.1542</td>\n",
       "      <td>86.1468</td>\n",
       "      <td>121.4364</td>\n",
       "      <td>76.39</td>\n",
       "      <td>2.209</td>\n",
       "      <td>70.0</td>\n",
       "      <td>353.3400</td>\n",
       "      <td>10.4091</td>\n",
       "      <td>176.3136</td>\n",
       "      <td>789.7523</td>\n",
       "      <td>1.0341</td>\n",
       "      <td>138.0882</td>\n",
       "      <td>1.0</td>\n",
       "      <td>667.7418</td>\n",
       "      <td>233.5491</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.624</td>\n",
       "      <td>4.894</td>\n",
       "      <td>2865.0</td>\n",
       "      <td>0.9298</td>\n",
       "      <td>0.9449</td>\n",
       "      <td>4.6414</td>\n",
       "      <td>-12.2945</td>\n",
       "      <td>355.0809</td>\n",
       "      <td>9.7948</td>\n",
       "      <td>144.0191</td>\n",
       "      <td>21.9782</td>\n",
       "      <td>32.2945</td>\n",
       "      <td>44.1498</td>\n",
       "      <td>745.6025</td>\n",
       "      <td>0.9256</td>\n",
       "      <td>146.6636</td>\n",
       "      <td>1.0</td>\n",
       "      <td>645.7636</td>\n",
       "      <td>65.8417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0534</td>\n",
       "      <td>0.0183</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>-0.0449</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.0178</td>\n",
       "      <td>-0.0123</td>\n",
       "      <td>-0.0048</td>\n",
       "      <td>7.5017</td>\n",
       "      <td>0.1342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4530</td>\n",
       "      <td>0.9902</td>\n",
       "      <td>1828.3846</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>9014.4600</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>-0.0077</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>-0.0001</td>\n",
       "      <td>0.2189</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.6704</td>\n",
       "      <td>-0.0167</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>0.0696</td>\n",
       "      <td>-0.0045</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0799</td>\n",
       "      <td>-0.2038</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.9424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>796.5950</td>\n",
       "      <td>0.9908</td>\n",
       "      <td>58.3858</td>\n",
       "      <td>0.5913</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>6.3551</td>\n",
       "      <td>15.75</td>\n",
       "      <td>3.148</td>\n",
       "      <td>15.73</td>\n",
       "      <td>15.71</td>\n",
       "      <td>0.9460</td>\n",
       "      <td>3.027</td>\n",
       "      <td>0.5328</td>\n",
       "      <td>3.299</td>\n",
       "      <td>-0.5677</td>\n",
       "      <td>0.7780</td>\n",
       "      <td>1.0010</td>\n",
       "      <td>2.3715</td>\n",
       "      <td>993.1274</td>\n",
       "      <td>38.1448</td>\n",
       "      <td>119.0</td>\n",
       "      <td>143.2</td>\n",
       "      <td>123.1</td>\n",
       "      <td>48.8000</td>\n",
       "      <td>296.3030</td>\n",
       "      <td>0.3744</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.64</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0634</td>\n",
       "      <td>0.0451</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>14.2354</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.005</td>\n",
       "      <td>12.506</td>\n",
       "      <td>0.4434</td>\n",
       "      <td>0.0126</td>\n",
       "      <td>13.9047</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.0538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>699.0</td>\n",
       "      <td>283.0</td>\n",
       "      <td>1747.0</td>\n",
       "      <td>1443.0</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.113</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.499</td>\n",
       "      <td>0.5760</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>0.5830</td>\n",
       "      <td>0.3053</td>\n",
       "      <td>0.8285</td>\n",
       "      <td>0.1308</td>\n",
       "      <td>0.922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.24</td>\n",
       "      <td>0.282</td>\n",
       "      <td>10.85</td>\n",
       "      <td>37.715</td>\n",
       "      <td>0.1189</td>\n",
       "      <td>3.98</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.54</td>\n",
       "      <td>72.149</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>5.52</td>\n",
       "      <td>15.76</td>\n",
       "      <td>0.519</td>\n",
       "      <td>10.71</td>\n",
       "      <td>19.77</td>\n",
       "      <td>5.52</td>\n",
       "      <td>8.446</td>\n",
       "      <td>33.832</td>\n",
       "      <td>0.3951</td>\n",
       "      <td>9.09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.77</td>\n",
       "      <td>92.307</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0915</td>\n",
       "      <td>0.0506</td>\n",
       "      <td>0.0769</td>\n",
       "      <td>0.1079</td>\n",
       "      <td>0.0797</td>\n",
       "      <td>0.1047</td>\n",
       "      <td>0.0924</td>\n",
       "      <td>0.1015</td>\n",
       "      <td>4.1338</td>\n",
       "      <td>0.0030</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0802</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>69.1510</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>1406.4004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0067</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0240</td>\n",
       "      <td>0.0</td>\n",
       "      <td>149.2172</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>2.5775</td>\n",
       "      <td>0.0177</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>0.4051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0488</td>\n",
       "      <td>19.862</td>\n",
       "      <td>3.6163</td>\n",
       "      <td>34.1250</td>\n",
       "      <td>55.9626</td>\n",
       "      <td>53.0876</td>\n",
       "      <td>17.4864</td>\n",
       "      <td>88.7672</td>\n",
       "      <td>0.1092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0929</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>0.0116</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>0.0080</td>\n",
       "      <td>4.4239</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.2376</td>\n",
       "      <td>3.6536</td>\n",
       "      <td>0.1293</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>4.3474</td>\n",
       "      <td>0.1275</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>319.1252</td>\n",
       "      <td>128.0296</td>\n",
       "      <td>799.5884</td>\n",
       "      <td>628.3083</td>\n",
       "      <td>0.0755</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>1.3500</td>\n",
       "      <td>0.2698</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>0.1541</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.0310</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.2194</td>\n",
       "      <td>0.1354</td>\n",
       "      <td>0.3072</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.3574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.8956</td>\n",
       "      <td>0.0766</td>\n",
       "      <td>2.9130</td>\n",
       "      <td>11.0583</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>1.1229</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.3296</td>\n",
       "      <td>23.1160</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0822</td>\n",
       "      <td>1.6216</td>\n",
       "      <td>4.7279</td>\n",
       "      <td>0.1773</td>\n",
       "      <td>3.1550</td>\n",
       "      <td>9.7777</td>\n",
       "      <td>1.6216</td>\n",
       "      <td>2.5923</td>\n",
       "      <td>10.5352</td>\n",
       "      <td>0.1301</td>\n",
       "      <td>3.0939</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.3767</td>\n",
       "      <td>32.0537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>0.0221</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>0.0522</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>0.0545</td>\n",
       "      <td>0.0476</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>1.5530</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>21.0312</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>494.7368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.2692</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0077</td>\n",
       "      <td>0.1356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>7.1493</td>\n",
       "      <td>1.1704</td>\n",
       "      <td>5.3823</td>\n",
       "      <td>4.7226</td>\n",
       "      <td>4.9184</td>\n",
       "      <td>2.1850</td>\n",
       "      <td>22.3369</td>\n",
       "      <td>24.4142</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6256</td>\n",
       "      <td>3.3208</td>\n",
       "      <td>4.2178</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>866.0295</td>\n",
       "      <td>2.5046</td>\n",
       "      <td>7.0492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.2255</td>\n",
       "      <td>2.9734</td>\n",
       "      <td>4.2892</td>\n",
       "      <td>1.2943</td>\n",
       "      <td>7.2570</td>\n",
       "      <td>3.4473</td>\n",
       "      <td>3.8754</td>\n",
       "      <td>12.7642</td>\n",
       "      <td>10.7390</td>\n",
       "      <td>43.8119</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.4064</td>\n",
       "      <td>2.0088</td>\n",
       "      <td>1.5533</td>\n",
       "      <td>6.2069</td>\n",
       "      <td>25.3521</td>\n",
       "      <td>37.4691</td>\n",
       "      <td>15.2470</td>\n",
       "      <td>0.6672</td>\n",
       "      <td>0.7198</td>\n",
       "      <td>0.6076</td>\n",
       "      <td>0.9088</td>\n",
       "      <td>0.6136</td>\n",
       "      <td>1.2524</td>\n",
       "      <td>0.1518</td>\n",
       "      <td>0.7592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.3131</td>\n",
       "      <td>2.7092</td>\n",
       "      <td>6.1538</td>\n",
       "      <td>4.7756</td>\n",
       "      <td>11.4945</td>\n",
       "      <td>2.8822</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.8248</td>\n",
       "      <td>30.8924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.3863</td>\n",
       "      <td>44.8980</td>\n",
       "      <td>4.4384</td>\n",
       "      <td>5.2987</td>\n",
       "      <td>7.4365</td>\n",
       "      <td>89.9529</td>\n",
       "      <td>17.0927</td>\n",
       "      <td>19.1303</td>\n",
       "      <td>4.5375</td>\n",
       "      <td>42.6838</td>\n",
       "      <td>6.1979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0615</td>\n",
       "      <td>140.1953</td>\n",
       "      <td>0.0</td>\n",
       "      <td>171.4486</td>\n",
       "      <td>276.8810</td>\n",
       "      <td>461.8619</td>\n",
       "      <td>240.1781</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>587.3773</td>\n",
       "      <td>748.1781</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>55.1057</td>\n",
       "      <td>2.2358</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.2712</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>3.7821</td>\n",
       "      <td>107.6905</td>\n",
       "      <td>15.6016</td>\n",
       "      <td>0.0</td>\n",
       "      <td>293.1396</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>148.0663</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.5512</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.7319</td>\n",
       "      <td>0.0616</td>\n",
       "      <td>4.4146</td>\n",
       "      <td>2.9954</td>\n",
       "      <td>2.2181</td>\n",
       "      <td>6.3745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0579</td>\n",
       "      <td>1.9999</td>\n",
       "      <td>9.4805</td>\n",
       "      <td>0.1096</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0.0026</td>\n",
       "      <td>7.116</td>\n",
       "      <td>1.4636</td>\n",
       "      <td>399.914</td>\n",
       "      <td>79.156</td>\n",
       "      <td>1.0388</td>\n",
       "      <td>19.63</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.4287</td>\n",
       "      <td>9.7608</td>\n",
       "      <td>0.8311</td>\n",
       "      <td>70.9706</td>\n",
       "      <td>4.9086</td>\n",
       "      <td>2.5014</td>\n",
       "      <td>0.9778</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.0461</td>\n",
       "      <td>22.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>532.0155</td>\n",
       "      <td>2.0275</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>0.0706</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4800</td>\n",
       "      <td>0.4766</td>\n",
       "      <td>0.1045</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>0.0202</td>\n",
       "      <td>0.0149</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>73.8432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Target                 Time        1        2          3          4  \\\n",
       "0      -1  19/07/2008 11:55:00  3030.93  2564.00  2187.7333  1411.1265   \n",
       "1      -1  19/07/2008 12:32:00  3095.78  2465.14  2230.4222  1463.6606   \n",
       "2       1  19/07/2008 13:17:00  2932.61  2559.94  2186.4111  1698.0172   \n",
       "3      -1  19/07/2008 14:43:00  2988.72  2479.90  2199.0333   909.7926   \n",
       "4      -1  19/07/2008 15:22:00  3032.24  2502.87  2233.3667  1326.5200   \n",
       "\n",
       "        5      6         7       8       9      10      11      12        13  \\\n",
       "0  1.3602  100.0   97.6133  0.1242  1.5005  0.0162 -0.0034  0.9455  202.4396   \n",
       "1  0.8294  100.0  102.3433  0.1247  1.4966 -0.0005 -0.0148  0.9627  200.5470   \n",
       "2  1.5102  100.0   95.4878  0.1241  1.4436  0.0041  0.0013  0.9615  202.0179   \n",
       "3  1.3204  100.0  104.2367  0.1217  1.4882 -0.0124 -0.0033  0.9629  201.8482   \n",
       "4  1.5334  100.0  100.3967  0.1235  1.5031 -0.0031 -0.0072  0.9569  201.9424   \n",
       "\n",
       "    14       15        16       17      18        19       20      21  \\\n",
       "0  0.0   7.9558  414.8710  10.0433  0.9680  192.3963  12.5190  1.4026   \n",
       "1  0.0  10.1548  414.7347   9.2599  0.9701  191.2872  12.4608  1.3825   \n",
       "2  0.0   9.5157  416.7075   9.3144  0.9674  192.7035  12.5404  1.4123   \n",
       "3  0.0   9.6052  422.2894   9.6924  0.9687  192.1557  12.4782  1.4011   \n",
       "4  0.0  10.5661  420.5925  10.3387  0.9735  191.6037  12.4735  1.3888   \n",
       "\n",
       "        22       23       24       25      26      27      28       29  \\\n",
       "0 -5419.00  2916.50 -4043.75   751.00  0.8955  1.7730  3.0490  64.2333   \n",
       "1 -5441.50  2604.25 -3498.75 -1640.25  1.2973  2.0143  7.3900  68.4222   \n",
       "2 -5447.75  2701.75 -4047.00 -1916.50  1.3122  2.0295  7.5788  67.1333   \n",
       "3 -5468.25  2648.25 -4515.00 -1657.25  1.3137  2.0038  7.3145  62.9333   \n",
       "4 -5476.25  2635.25 -3987.50   117.00  1.2887  1.9912  7.2748  62.8333   \n",
       "\n",
       "       30      31      32       33      34       35       36       37  \\\n",
       "0  2.0222  0.1632  3.5191  83.3971  9.5126  50.6170  64.2588  49.3830   \n",
       "1  2.2667  0.2102  3.4171  84.9052  9.7997  50.6596  64.2828  49.3404   \n",
       "2  2.3333  0.1734  3.5986  84.7569  8.6590  50.1530  64.1114  49.8470   \n",
       "3  2.6444  0.2071  3.3813  84.9105  8.6789  50.5100  64.1125  49.4900   \n",
       "4  3.1556  0.2696  3.2728  86.3269  8.7677  50.2480  64.1511  49.7520   \n",
       "\n",
       "        38       39        40     41     42    43        44       45  \\\n",
       "0  66.3141  86.9555  117.5132  61.29  4.515  70.0  352.7173  10.1841   \n",
       "1  64.9193  87.5241  118.1188  78.25  2.773  70.0  352.2445  10.0373   \n",
       "2  65.8389  84.7327  118.6128  14.37  5.434  70.0  364.3782   9.8783   \n",
       "3  65.1951  86.6867  117.0442  76.90  1.279  70.0  363.0273   9.9305   \n",
       "4  66.1542  86.1468  121.4364  76.39  2.209  70.0  353.3400  10.4091   \n",
       "\n",
       "         46        47      48        49   50        51        52   53     54  \\\n",
       "0  130.3691  723.3092  1.3072  141.2282  1.0  624.3145  218.3174  0.0  4.592   \n",
       "1  133.1727  724.8264  1.2887  145.8445  1.0  631.2618  205.1695  0.0  4.590   \n",
       "2  131.8027  734.7924  1.2992  141.0845  1.0  637.2655  185.7574  0.0  4.486   \n",
       "3  131.8027  733.8778  1.3027  142.5427  1.0  637.3727  189.9079  0.0  4.486   \n",
       "4  176.3136  789.7523  1.0341  138.0882  1.0  667.7418  233.5491  0.0  4.624   \n",
       "\n",
       "      55      56      57      58      59       60        61       62  \\\n",
       "0  4.841  2834.0  0.9317  0.9484  4.7057  -1.7264  350.9264  10.6231   \n",
       "1  4.842  2853.0  0.9324  0.9479  4.6820   0.8073  352.0073  10.3092   \n",
       "2  4.748  2936.0  0.9139  0.9447  4.5873  23.8245  364.5364  10.1685   \n",
       "3  4.748  2936.0  0.9139  0.9447  4.5873  24.3791  361.4582  10.2112   \n",
       "4  4.894  2865.0  0.9298  0.9449  4.6414 -12.2945  355.0809   9.7948   \n",
       "\n",
       "         63       64       65       66        67      68        69   70  \\\n",
       "0  108.6427  16.1445  21.7264  29.5367  693.7724  0.9226  148.6009  1.0   \n",
       "1  113.9800  10.9036  19.1927  27.6301  697.1964  1.1598  154.3709  1.0   \n",
       "2  115.6273  11.3019  16.1755  24.2829  710.5095  0.8694  145.8000  1.0   \n",
       "3  116.1818  13.5597  15.6209  23.4736  710.4043  0.9761  147.6545  1.0   \n",
       "4  144.0191  21.9782  32.2945  44.1498  745.6025  0.9256  146.6636  1.0   \n",
       "\n",
       "         71       72        73        74   75      76      77      78      79  \\\n",
       "0  608.1700  84.0793       NaN       NaN  0.0  0.0126 -0.0206  0.0141 -0.0307   \n",
       "1  620.3582  82.3494       NaN       NaN  0.0 -0.0039 -0.0198  0.0004 -0.0440   \n",
       "2  625.9636  84.7681  140.6972  485.2665  0.0 -0.0078 -0.0326 -0.0052  0.0213   \n",
       "3  625.2945  70.2289  160.3210  464.9735  0.0 -0.0555 -0.0461 -0.0400  0.0400   \n",
       "4  645.7636  65.8417       NaN       NaN  0.0 -0.0534  0.0183 -0.0167 -0.0449   \n",
       "\n",
       "       80      81      82      83      84      85  86      87      88  \\\n",
       "0 -0.0083 -0.0026 -0.0567 -0.0044  7.2163  0.1320 NaN  2.3895  0.9690   \n",
       "1 -0.0358 -0.0120 -0.0377  0.0017  6.8043  0.1358 NaN  2.3754  0.9894   \n",
       "2 -0.0054 -0.1134 -0.0182  0.0287  7.1041  0.1362 NaN  2.4532  0.9880   \n",
       "3  0.0676 -0.1051  0.0028  0.0277  7.5925  0.1302 NaN  2.4004  0.9904   \n",
       "4  0.0034 -0.0178 -0.0123 -0.0048  7.5017  0.1342 NaN  2.4530  0.9902   \n",
       "\n",
       "          89      90         91      92      93      94      95      96  \\\n",
       "0  1747.6049  0.1841  8671.9301 -0.3274 -0.0055 -0.0001  0.0001  0.0003   \n",
       "1  1931.6464  0.1874  8407.0299  0.1455 -0.0015  0.0000 -0.0005  0.0001   \n",
       "2  1685.8514  0.1497  9317.1698  0.0553  0.0006 -0.0013  0.0000  0.0002   \n",
       "3  1752.0968  0.1958  8205.7000  0.0697 -0.0003 -0.0021 -0.0001  0.0002   \n",
       "4  1828.3846  0.1829  9014.4600  0.0448 -0.0077 -0.0001 -0.0001 -0.0001   \n",
       "\n",
       "       97   98      99     100     101     102     103     104     105  \\\n",
       "0 -0.2786  0.0  0.3974 -0.0251  0.0002  0.0002  0.1350 -0.0042  0.0003   \n",
       "1  0.5854  0.0 -0.9353 -0.0158 -0.0004 -0.0004 -0.0752 -0.0045  0.0002   \n",
       "2 -0.1343  0.0 -0.1427  0.1218  0.0006 -0.0001  0.0134 -0.0026 -0.0016   \n",
       "3  0.0411  0.0  0.0177 -0.0195 -0.0002  0.0000 -0.0699 -0.0059  0.0003   \n",
       "4  0.2189  0.0 -0.6704 -0.0167  0.0004 -0.0003  0.0696 -0.0045  0.0002   \n",
       "\n",
       "      106     107     108     109  110  111  112     113     114  115  \\\n",
       "0  0.0056  0.0000 -0.2468  0.3196  NaN  NaN  NaN     NaN  0.9460  0.0   \n",
       "1  0.0015  0.0000  0.0772 -0.0903  NaN  NaN  NaN     NaN  0.9425  0.0   \n",
       "2 -0.0006  0.0013 -0.0301 -0.0728  NaN  NaN  NaN  0.4684  0.9231  0.0   \n",
       "3  0.0003  0.0021 -0.0483 -0.1180  NaN  NaN  NaN  0.4647  0.9564  0.0   \n",
       "4  0.0078  0.0000 -0.0799 -0.2038  NaN  NaN  NaN     NaN  0.9424  0.0   \n",
       "\n",
       "        116     117      118     119     120     121    122    123    124  \\\n",
       "0  748.6115  0.9908  58.4306  0.6002  0.9804  6.3788  15.88  2.639  15.94   \n",
       "1  731.2517  0.9902  58.6680  0.5958  0.9731  6.5061  15.88  2.541  15.91   \n",
       "2  718.5777  0.9899  58.4808  0.6015  0.9772  6.4527  15.90  2.882  15.94   \n",
       "3  709.0867  0.9906  58.6635  0.6016  0.9761  6.4935  15.55  3.132  15.61   \n",
       "4  796.5950  0.9908  58.3858  0.5913  0.9628  6.3551  15.75  3.148  15.73   \n",
       "\n",
       "     125     126    127     128    129     130     131     132     133  \\\n",
       "0  15.93  0.8656  3.353  0.4098  3.188 -0.0473  0.7243  0.9960  2.2967   \n",
       "1  15.88  0.8703  2.771  0.4138  3.272 -0.0946  0.8122  0.9985  2.2932   \n",
       "2  15.95  0.8798  3.094  0.4777  3.272 -0.1892  0.8194  0.9978  2.2592   \n",
       "3  15.59  1.3660  2.480  0.5176  3.119  0.2838  0.7244  0.9961  2.3802   \n",
       "4  15.71  0.9460  3.027  0.5328  3.299 -0.5677  0.7780  1.0010  2.3715   \n",
       "\n",
       "         134      135    136    137    138      139       140     141  142  \\\n",
       "0  1000.7263  39.2373  123.0  111.3   75.2  46.2000  350.6710  0.3948  0.0   \n",
       "1   998.1081  37.9213   98.0   80.3   81.0  56.2000  219.7679  0.2301  0.0   \n",
       "2   998.4440  42.0579   89.0  126.4   96.5  45.1001  306.0380  0.3263  0.0   \n",
       "3   980.4510  41.1025  127.0  118.0  123.7  47.8000  162.4320  0.1915  0.0   \n",
       "4   993.1274  38.1448  119.0  143.2  123.1  48.8000  296.3030  0.3744  0.0   \n",
       "\n",
       "    143     144     145     146     147     148      149  150    151     152  \\\n",
       "0  6.78  0.0034  0.0898  0.0850  0.0358  0.0328  12.2566  0.0  4.271  10.284   \n",
       "1  5.70  0.0049  0.1356  0.0600  0.0547  0.0204  12.3319  0.0  6.285  13.077   \n",
       "2  8.33  0.0038  0.0754  0.0483  0.0619  0.0221   8.2660  0.0  4.819   8.443   \n",
       "3  5.51  0.0030  0.1140  0.0393  0.0613  0.0190  13.2651  0.0  9.073  15.241   \n",
       "4  3.64  0.0041  0.0634  0.0451  0.0623  0.0240  14.2354  0.0  9.005  12.506   \n",
       "\n",
       "      153     154      155   156     157  158  159     160    161     162  \\\n",
       "0  0.4734  0.0167  11.8901  0.41  0.0506  NaN  NaN  1017.0  967.0  1066.0   \n",
       "1  0.5666  0.0144  11.8428  0.35  0.0437  NaN  NaN   568.0   59.0   297.0   \n",
       "2  0.4909  0.0177   8.2054  0.47  0.0497  NaN  NaN   562.0  788.0   759.0   \n",
       "3  1.3029  0.0150  11.9738  0.35  0.0699  NaN  NaN   859.0  355.0  3433.0   \n",
       "4  0.4434  0.0126  13.9047  0.43  0.0538  NaN  NaN   699.0  283.0  1747.0   \n",
       "\n",
       "      163    164    165    166  167  168    169    170     171     172  \\\n",
       "0   368.0  0.090  0.048  0.095  2.0  0.9  0.069  0.046  0.7250  0.1139   \n",
       "1  3277.0  0.112  0.115  0.124  2.2  1.1  0.079  0.561  1.0498  0.1917   \n",
       "2  2100.0  0.187  0.117  0.068  2.1  1.4  0.123  0.319  1.0824  0.0369   \n",
       "3  3004.0  0.068  0.108  0.100  1.7  0.9  0.086  0.241  0.9386  0.0356   \n",
       "4  1443.0  0.147  0.040  0.113  3.9  0.8  0.101  0.499  0.5760  0.0631   \n",
       "\n",
       "      173     174     175     176     177    178  179  180    181    182  \\\n",
       "0  0.3183  0.5888  0.3184  0.9499  0.3979  0.160  0.0  0.0  20.95  0.333   \n",
       "1  0.4115  0.6582  0.4115  1.0181  0.2315  0.325  0.0  0.0  17.99  0.439   \n",
       "2  0.3141  0.5753  0.3141  0.9677  0.2706  0.326  0.0  0.0  17.78  0.745   \n",
       "3  0.2618  0.4391  0.2618  0.8567  0.2452  0.390  0.0  0.0  16.22  0.693   \n",
       "4  0.3053  0.5830  0.3053  0.8285  0.1308  0.922  0.0  0.0  15.24  0.282   \n",
       "\n",
       "     183     184     185   186  187    188     189  190  191  192  193  194  \\\n",
       "0  12.49  16.713  0.0803  5.72  0.0  11.19  65.363  0.0  0.0  0.0  0.0  0.0   \n",
       "1  10.14  16.358  0.0892  6.92  0.0   9.05  82.986  0.0  0.0  0.0  0.0  0.0   \n",
       "2  13.31  22.912  0.1959  9.21  0.0  17.87  60.110  0.0  0.0  0.0  0.0  0.0   \n",
       "3  14.67  22.562  0.1786  5.69  0.0  18.20  52.571  0.0  0.0  0.0  0.0  0.0   \n",
       "4  10.85  37.715  0.1189  3.98  0.0  25.54  72.149  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   195    196   197    198    199    200    201   202     203     204     205  \\\n",
       "0  0.0  0.292  5.38  20.10  0.296  10.62  10.30  5.38   4.040  16.230  0.2951   \n",
       "1  0.0  0.222  3.74  19.59  0.316  11.65   8.02  3.74   3.659  15.078  0.3580   \n",
       "2  0.0  0.139  5.09  19.75  0.949   9.71  16.73  5.09  11.059  22.624  0.1164   \n",
       "3  0.0  0.139  5.92  23.60  1.264  10.63  13.56  5.92  11.382  24.320  0.3458   \n",
       "4  0.0  0.250  5.52  15.76  0.519  10.71  19.77  5.52   8.446  33.832  0.3951   \n",
       "\n",
       "     206  207    208      209  210     211     212     213     214     215  \\\n",
       "0   8.64  0.0  10.30   97.314  0.0  0.0772  0.0599  0.0700  0.0547  0.0704   \n",
       "1   8.96  0.0   8.02  134.250  0.0  0.0566  0.0488  0.1651  0.1578  0.0468   \n",
       "2  13.30  0.0  16.73   79.618  0.0  0.0339  0.0494  0.0696  0.0406  0.0401   \n",
       "3   9.56  0.0  21.97  104.950  0.0  0.1248  0.0463  0.1223  0.0354  0.0708   \n",
       "4   9.09  0.0  19.77   92.307  0.0  0.0915  0.0506  0.0769  0.1079  0.0797   \n",
       "\n",
       "      216     217     218     219     220  221     222     223       224  \\\n",
       "0  0.0520  0.0301  0.1135  3.4789  0.0010  NaN  0.0707  0.0211  175.2173   \n",
       "1  0.0987  0.0734  0.0747  3.9578  0.0050  NaN  0.0761  0.0014  128.4285   \n",
       "2  0.0840  0.0349  0.0718  2.4266  0.0014  NaN  0.0963  0.0152  182.4956   \n",
       "3  0.0754  0.0643  0.0932  5.5398  0.0023  NaN  0.0764  0.0015  152.0885   \n",
       "4  0.1047  0.0924  0.1015  4.1338  0.0030  NaN  0.0802  0.0004   69.1510   \n",
       "\n",
       "      225        226  227     228     229  230  231  232  233  234  235  236  \\\n",
       "0  0.0315  1940.3994  0.0  0.0744  0.0546  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0238  1988.0000  0.0  0.0203  0.0236  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0284   839.6006  0.0  0.0192  0.0170  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0573   820.3999  0.0  0.0152  0.0149  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.1970  1406.4004  0.0  0.0227  0.0272  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   237  238     239     240  241  242  243  244  245  246  247     248  \\\n",
       "0  0.0  0.0  0.0027  0.0040  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN   \n",
       "1  0.0  0.0  0.0064  0.0036  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN   \n",
       "2  0.0  0.0  0.0062  0.0040  0.0  0.0  0.0  0.0  NaN  NaN  NaN  0.1729   \n",
       "3  0.0  0.0  0.0067  0.0040  0.0  0.0  0.0  0.0  NaN  NaN  NaN  0.0191   \n",
       "4  0.0  0.0  0.0067  0.0031  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN   \n",
       "\n",
       "      249  250       251     252     253     254     255     256  257  258  \\\n",
       "0  0.0188  0.0  219.9453  0.0011  2.8374  0.0189  0.0050  0.4269  0.0  0.0   \n",
       "1  0.0154  0.0  193.0287  0.0007  3.8999  0.0187  0.0086  0.5749  0.0  0.0   \n",
       "2  0.0273  0.0  104.4042  0.0007  4.1446  0.0733  0.0063  0.4166  0.0  0.0   \n",
       "3  0.0234  0.0   94.0954  0.0010  3.2119  0.0406  0.0072  0.4212  0.0  0.0   \n",
       "4  0.0240  0.0  149.2172  0.0006  2.5775  0.0177  0.0214  0.4051  0.0  0.0   \n",
       "\n",
       "   259  260  261  262  263  264  265  266  267     268     269     270  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0472  40.855  4.5152   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0411  29.743  3.6327   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0487  29.621  3.9133   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0513  31.830  3.1959   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0488  19.862  3.6163   \n",
       "\n",
       "       271      272      273      274       275     276  277     278     279  \\\n",
       "0  30.9815  33.9606  22.9057  15.9525  110.2144  0.1310  0.0  2.5883  0.0010   \n",
       "1  29.0598  28.9862  22.3163  17.4008   83.5542  0.0767  0.0  1.8459  0.0012   \n",
       "2  23.5510  41.3837  32.6256  15.7716   97.3868  0.1117  0.0  2.5274  0.0012   \n",
       "3  33.8960  37.8477  44.3906  16.9347   50.3631  0.0581  0.0  2.1775  0.0007   \n",
       "4  34.1250  55.9626  53.0876  17.4864   88.7672  0.1092  0.0  1.0929  0.0013   \n",
       "\n",
       "      280     281     282     283     284  285     286     287     288  \\\n",
       "0  0.0319  0.0197  0.0120  0.0109  3.9321  0.0  1.5123  3.5811  0.1337   \n",
       "1  0.0440  0.0171  0.0154  0.0069  3.9011  0.0  2.1016  3.9483  0.1662   \n",
       "2  0.0249  0.0152  0.0157  0.0075  2.8705  0.0  1.5306  2.5493  0.1479   \n",
       "3  0.0417  0.0115  0.0172  0.0063  4.2154  0.0  2.8960  4.0526  0.3882   \n",
       "4  0.0257  0.0116  0.0163  0.0080  4.4239  0.0  3.2376  3.6536  0.1293   \n",
       "\n",
       "      289     290     291     292  293  294       295       296        297  \\\n",
       "0  0.0055  3.8447  0.1077  0.0167  NaN  NaN  418.1363  398.3185   496.1582   \n",
       "1  0.0049  3.7836  0.1000  0.0139  NaN  NaN  233.9865   26.5879   139.2082   \n",
       "2  0.0059  2.8046  0.1185  0.0167  NaN  NaN  251.4536  329.6406   325.0672   \n",
       "3  0.0049  3.9403  0.0916  0.0245  NaN  NaN  415.5048  157.0889  1572.6896   \n",
       "4  0.0040  4.3474  0.1275  0.0181  NaN  NaN  319.1252  128.0296   799.5884   \n",
       "\n",
       "         298     299     300     301     302     303     304     305     306  \\\n",
       "0   158.3330  0.0373  0.0202  0.0462  0.6083  0.3032  0.0200  0.0174  0.2827   \n",
       "1  1529.7622  0.0502  0.0561  0.0591  0.8151  0.3464  0.0291  0.1822  0.3814   \n",
       "2   902.4576  0.0800  0.0583  0.0326  0.6964  0.4031  0.0416  0.1041  0.3846   \n",
       "3  1377.4276  0.0285  0.0445  0.0465  0.6305  0.3046  0.0286  0.0824  0.3483   \n",
       "4   628.3083  0.0755  0.0181  0.0476  1.3500  0.2698  0.0320  0.1541  0.2155   \n",
       "\n",
       "      307     308     309     310     311     312     313  314  315  316  \\\n",
       "0  0.0434  0.1342  0.2419  0.1343  0.3670  0.1431  0.0610  0.0  0.0  0.0   \n",
       "1  0.0715  0.1667  0.2630  0.1667  0.3752  0.0856  0.1214  0.0  0.0  0.0   \n",
       "2  0.0151  0.1288  0.2268  0.1288  0.3677  0.1175  0.1261  0.0  0.0  0.0   \n",
       "3  0.0128  0.1004  0.1701  0.1004  0.3465  0.0973  0.1675  0.0  0.0  0.0   \n",
       "4  0.0310  0.1354  0.2194  0.1354  0.3072  0.0582  0.3574  0.0  0.0  0.0   \n",
       "\n",
       "      317     318     319      320     321     322  323     324      325  326  \\\n",
       "0  6.2698  0.1181  3.8208   5.3737  0.0254  1.6252  0.0  3.2461  18.0118  0.0   \n",
       "1  5.6522  0.1417  2.9939   5.2445  0.0264  1.8045  0.0  2.7661  23.6230  0.0   \n",
       "2  5.7247  0.2682  3.8541   6.1797  0.0546  2.5680  0.0  4.6067  16.0104  0.0   \n",
       "3  5.4440  0.2004  4.1900   6.3329  0.0479  1.7339  0.0  4.9660  15.7375  0.0   \n",
       "4  4.8956  0.0766  2.9130  11.0583  0.0327  1.1229  0.0  7.3296  23.1160  0.0   \n",
       "\n",
       "   327  328  329  330  331     332     333     334     335     336     337  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0752  1.5989  6.5893  0.0913  3.0911  8.4654   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0778  1.1506  5.9247  0.0878  3.3604  7.7421   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0243  1.5481  5.9453  0.2777  3.1600  8.9855   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0243  1.7317  6.6262  0.3512  3.2699  9.4020   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0822  1.6216  4.7279  0.1773  3.1550  9.7777   \n",
       "\n",
       "      338     339      340     341     342  343     344      345      346  \\\n",
       "0  1.5989  1.2293   5.3406  0.0867  2.8551  0.0  2.9971  31.8843      NaN   \n",
       "1  1.1506  1.1265   5.0108  0.1013  2.4278  0.0  2.4890  41.7080      NaN   \n",
       "2  1.5481  2.9844   6.2277  0.0353  3.7663  0.0  5.6983  24.7959  13.5664   \n",
       "3  1.7317  3.0672   6.6839  0.0928  3.0229  0.0  6.3292  29.0339   8.4026   \n",
       "4  1.6216  2.5923  10.5352  0.1301  3.0939  0.0  6.3767  32.0537      NaN   \n",
       "\n",
       "       347  348     349     350     351     352     353     354     355  \\\n",
       "0      NaN  0.0  0.0215  0.0274  0.0315  0.0238  0.0206  0.0238  0.0144   \n",
       "1      NaN  0.0  0.0142  0.0230  0.0768  0.0729  0.0143  0.0513  0.0399   \n",
       "2  15.4488  0.0  0.0105  0.0208  0.0327  0.0171  0.0116  0.0428  0.0154   \n",
       "3   4.8851  0.0  0.0407  0.0198  0.0531  0.0167  0.0224  0.0422  0.0273   \n",
       "4      NaN  0.0  0.0246  0.0221  0.0329  0.0522  0.0256  0.0545  0.0476   \n",
       "\n",
       "      356     357     358  359     360     361      362     363       364  \\\n",
       "0  0.0491  1.2708  0.0004  NaN  0.0229  0.0065  55.2039  0.0105  560.2658   \n",
       "1  0.0365  1.2474  0.0017  NaN  0.0248  0.0005  46.3453  0.0069  677.1873   \n",
       "2  0.0383  0.7786  0.0005  NaN  0.0302  0.0046  58.0575  0.0092  283.6616   \n",
       "3  0.0484  1.8222  0.0006  NaN  0.0252  0.0004  45.7058  0.0188  309.8492   \n",
       "4  0.0463  1.5530  0.0010  NaN  0.0286  0.0001  21.0312  0.0573  494.7368   \n",
       "\n",
       "   365     366     367     368     369  370  371  372  373  374  375  376  \\\n",
       "0  0.0  0.0170  0.0148  0.0124  0.0114  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0  0.0053  0.0059  0.0081  0.0033  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0  0.0054  0.0043  0.0030  0.0037  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0  0.0046  0.0049  0.0028  0.0034  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0  0.0063  0.0077  0.0052  0.0027  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "      377     378  379  380  381  382  383  384  385     386     387  388  \\\n",
       "0  0.0010  0.0013  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN  0.0055  0.0   \n",
       "1  0.0022  0.0013  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN  0.0049  0.0   \n",
       "2  0.0021  0.0015  0.0  0.0  0.0  0.0  NaN  NaN  NaN  0.0221  0.0100  0.0   \n",
       "3  0.0024  0.0014  0.0  0.0  0.0  0.0  NaN  NaN  NaN  0.0038  0.0068  0.0   \n",
       "4  0.0025  0.0012  0.0  0.0  0.0  0.0  NaN  NaN  NaN     NaN  0.0089  0.0   \n",
       "\n",
       "       389     390     391     392     393     394  395  396  397  398  399  \\\n",
       "0  61.5932  0.0003  0.9967  0.0082  0.0017  0.1437  0.0  0.0  0.0  0.0  0.0   \n",
       "1  65.0999  0.0002  1.1655  0.0068  0.0027  0.1921  0.0  0.0  0.0  0.0  0.0   \n",
       "2  28.7334  0.0003  1.2356  0.0190  0.0020  0.1375  0.0  0.0  0.0  0.0  0.0   \n",
       "3  32.4228  0.0003  1.1135  0.0132  0.0023  0.1348  0.0  0.0  0.0  0.0  0.0   \n",
       "4  57.2692  0.0002  0.8495  0.0065  0.0077  0.1356  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   400  401  402  403  404  405     406      407     408     409     410  \\\n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0151  14.2396  1.4392  5.6188  3.6721   \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0120  10.5837  1.0323  4.3465  2.5939   \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0190  11.4871  1.1798  4.0782  4.3102   \n",
       "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0155  13.3972  1.1907  5.6363  3.9482   \n",
       "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0165   7.1493  1.1704  5.3823  4.7226   \n",
       "\n",
       "      411     412      413      414  415     416     417     418       419  \\\n",
       "0  2.9329  2.1118  24.8504  29.0271  0.0  6.9458  2.7380  5.9846  525.0965   \n",
       "1  3.2858  2.5197  15.0150  27.7464  0.0  5.5695  3.9300  9.0604    0.0000   \n",
       "2  3.7696  2.0627  18.0233  21.6062  0.0  8.7236  3.0609  5.2231    0.0000   \n",
       "3  4.9881  2.1737  17.8537  14.5054  0.0  5.2860  2.4643  7.6602  317.7362   \n",
       "4  4.9184  2.1850  22.3369  24.4142  0.0  3.6256  3.3208  4.2178    0.0000   \n",
       "\n",
       "        420     421     422  423      424     425      426     427     428  \\\n",
       "0    0.0000  3.4641  6.0544  0.0  53.6840  2.4788   4.7141  1.7275  6.1800   \n",
       "1  368.9713  2.1196  6.1491  0.0  61.8918  3.1531   6.1188  1.4857  6.1911   \n",
       "2    0.0000  2.2943  4.0917  0.0  50.6425  2.0261   5.2707  1.8268  4.2581   \n",
       "3    0.0000  1.9689  6.5718  0.0  94.4594  3.6091  13.4420  1.5441  6.2313   \n",
       "4  866.0295  2.5046  7.0492  0.0  85.2255  2.9734   4.2892  1.2943  7.2570   \n",
       "\n",
       "      429     430      431      432      433       434      435     436  \\\n",
       "0  3.2750  3.6084  18.7673  33.1562  26.3617   49.0013  10.0503  2.7073   \n",
       "1  2.8088  3.1595  10.4383   2.2655   8.4887  199.7866   8.6336  5.7093   \n",
       "2  3.7479  3.5220  10.3162  29.1663  18.7546  109.5747  14.2503  5.7650   \n",
       "3  2.8049  4.9898  15.7089  13.4051  76.0354  181.2641   5.1760  5.3899   \n",
       "4  3.4473  3.8754  12.7642  10.7390  43.8119    0.0000  11.4064  2.0088   \n",
       "\n",
       "      437     438      439      440      441     442     443     444     445  \\\n",
       "0  3.1158  3.1136  44.5055  42.2737   1.3071  0.8693  1.1975  0.6288  0.9163   \n",
       "1  1.6779  3.2153  48.5294  37.5793  16.4174  1.2364  1.9562  0.8123  1.0239   \n",
       "2  0.8972  3.1281  60.0000  70.9161   8.8647  1.2771  0.4264  0.6263  0.8973   \n",
       "3  1.3671  2.7013  34.0336  41.5236   7.1274  1.1054  0.4097  0.5183  0.6849   \n",
       "4  1.5533  6.2069  25.3521  37.4691  15.2470  0.6672  0.7198  0.6076  0.9088   \n",
       "\n",
       "      446     447     448     449  450  451  452     453     454      455  \\\n",
       "0  0.6448  1.4324  0.4576  0.1362  0.0  0.0  0.0  5.9396  3.2698   9.5805   \n",
       "1  0.8340  1.5683  0.2645  0.2751  0.0  0.0  0.0  5.1072  4.3737   7.6142   \n",
       "2  0.6301  1.4698  0.3194  0.2748  0.0  0.0  0.0  4.8795  7.5418  10.0984   \n",
       "3  0.5290  1.3141  0.2829  0.3332  0.0  0.0  0.0  4.4680  6.9785  11.1303   \n",
       "4  0.6136  1.2524  0.1518  0.7592  0.0  0.0  0.0  4.3131  2.7092   6.1538   \n",
       "\n",
       "      456      457     458  459     460      461  462  463  464  465  466  \\\n",
       "0  2.3106   6.1463  4.0502  0.0  1.7924  29.9394  0.0  0.0  0.0  0.0  0.0   \n",
       "1  2.2568   6.9233  4.7448  0.0  1.4336  40.4475  0.0  0.0  0.0  0.0  0.0   \n",
       "2  3.1182  15.0790  6.5280  0.0  2.8042  32.3594  0.0  0.0  0.0  0.0  0.0   \n",
       "3  3.0744  13.7105  3.9918  0.0  2.8555  27.6824  0.0  0.0  0.0  0.0  0.0   \n",
       "4  4.7756  11.4945  2.8822  0.0  3.8248  30.8924  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   467     468       469     470      471      472       473      474  \\\n",
       "0  0.0  6.2052  311.6377  5.7277   2.7864   9.7752   63.7987  24.7625   \n",
       "1  0.0  4.7415  463.2883  5.5652   3.0652  10.2211   73.5536  19.4865   \n",
       "2  0.0  3.0301   21.3645  5.4178   9.3327   8.3977  148.0287  31.4674   \n",
       "3  0.0  3.0301   24.2831  6.5291  12.3786   9.1494  100.0021  37.8979   \n",
       "4  0.0  5.3863   44.8980  4.4384   5.2987   7.4365   89.9529  17.0927   \n",
       "\n",
       "       475     476      477     478  479     480       481  482       483  \\\n",
       "0  13.6778  2.3394  31.9893  5.8142  0.0  1.6936  115.7408  0.0  613.3069   \n",
       "1  13.2430  2.1627  30.8643  5.8042  0.0  1.2928  163.0249  0.0    0.0000   \n",
       "2  45.5423  3.1842  13.3923  9.1221  0.0  2.6727   93.9245  0.0  434.2674   \n",
       "3  48.4887  3.4234  35.4323  6.4746  0.0  3.5135  149.4399  0.0  225.0169   \n",
       "4  19.1303  4.5375  42.6838  6.1979  0.0  3.0615  140.1953  0.0  171.4486   \n",
       "\n",
       "        484       485       486       487       488       489       490  \\\n",
       "0  291.4842  494.6996  178.1759  843.1138    0.0000   53.1098    0.0000   \n",
       "1  246.7762    0.0000  359.0444  130.6350  820.7900  194.4371    0.0000   \n",
       "2  151.7665    0.0000  190.3869  746.9150   74.0741  191.7582  250.1742   \n",
       "3  100.4883  305.7500   88.5553  104.6660   71.7583    0.0000  336.7660   \n",
       "4  276.8810  461.8619  240.1781    0.0000  587.3773  748.1781    0.0000   \n",
       "\n",
       "       491     492  493     494     495      496       497      498  499  \\\n",
       "0  48.2091  0.7578  NaN  2.9570  2.1739  10.0261   17.1202  22.3756  0.0   \n",
       "1  58.1666  3.6822  NaN  3.2029  0.1441   6.6487   12.6788  23.6469  0.0   \n",
       "2  34.1573  1.0281  NaN  3.9238  1.5357  10.8251   18.9849   9.0113  0.0   \n",
       "3  72.9635  1.7670  NaN  3.1817  0.1488   8.6804   29.2542   9.9979  0.0   \n",
       "4  55.1057  2.2358  NaN  3.2712  0.0372   3.7821  107.6905  15.6016  0.0   \n",
       "\n",
       "        500       501  502  503  504  505  506  507  508  509  510       511  \\\n",
       "0    0.0000    0.0000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   64.6707   \n",
       "1    0.0000    0.0000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  141.4365   \n",
       "2    0.0000    0.0000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  240.7767   \n",
       "3    0.0000  711.6418  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  113.5593   \n",
       "4  293.1396    0.0000  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  148.0663   \n",
       "\n",
       "        512  513  514  515  516  517  518  519      520     521  522      523  \\\n",
       "0    0.0000  0.0  0.0  0.0  0.0  NaN  NaN  NaN      NaN  1.9864  0.0  29.3804   \n",
       "1    0.0000  0.0  0.0  0.0  0.0  NaN  NaN  NaN      NaN  1.6292  0.0  26.3970   \n",
       "2  244.2748  0.0  0.0  0.0  0.0  NaN  NaN  NaN  36.9067  2.9626  0.0  14.5293   \n",
       "3    0.0000  0.0  0.0  0.0  0.0  NaN  NaN  NaN   4.1200  2.4416  0.0  13.2699   \n",
       "4    0.0000  0.0  0.0  0.0  0.0  NaN  NaN  NaN      NaN  2.5512  0.0  18.7319   \n",
       "\n",
       "      524     525      526     527     528  529  530  531  532  533  534  535  \\\n",
       "0  0.1094  4.8560   3.1406  0.5064  6.6926  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "1  0.0673  6.6475   3.1310  0.8832  8.8370  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "2  0.0751  7.0870  12.1831  0.6451  6.4568  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "3  0.0977  5.4751   6.7553  0.7404  6.4865  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "4  0.0616  4.4146   2.9954  2.2181  6.3745  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "   536  537  538  539     540     541      542     543     544     545    546  \\\n",
       "0  0.0  0.0  0.0  0.0  2.0570  4.0825  11.5074  0.1096  0.0078  0.0026  7.116   \n",
       "1  0.0  0.0  0.0  0.0  1.7910  2.9799   9.5796  0.1096  0.0078  0.0026  7.116   \n",
       "2  0.0  0.0  0.0  0.0  2.1538  2.9667   9.3046  0.1096  0.0078  0.0026  7.116   \n",
       "3  0.0  0.0  0.0  0.0  2.1565  3.2465   7.7754  0.1096  0.0078  0.0026  7.116   \n",
       "4  0.0  0.0  0.0  0.0  2.0579  1.9999   9.4805  0.1096  0.0078  0.0026  7.116   \n",
       "\n",
       "      547      548     549     550     551    552     553      554      555  \\\n",
       "0  1.0616  395.570  75.752  0.4234   12.93   0.78  0.1827   5.7349   0.3363   \n",
       "1  1.3526  408.798  74.640  0.7193   16.00   1.33  0.2829   7.1196   0.4989   \n",
       "2  0.7942  411.136  74.654  0.1832   16.16   0.85  0.0857   7.1619   0.3752   \n",
       "3  1.1650  372.822  72.442  1.8804  131.68  39.33  0.6812  56.9303  17.4781   \n",
       "4  1.4636  399.914  79.156  1.0388   19.63   1.98  0.4287   9.7608   0.8311   \n",
       "\n",
       "        556      557      558     559     560     561      562      563  \\\n",
       "0   39.8842   3.2687   1.0297  1.0344  0.4385  0.1039  42.3877      NaN   \n",
       "1   53.1836   3.9139   1.7819  0.9634  0.1745  0.0375  18.1087      NaN   \n",
       "2   23.0713   3.9306   1.1386  1.5021  0.3718  0.1233  24.7524  267.064   \n",
       "3  161.4081  35.3198  54.2917  1.1613  0.7288  0.2710  62.7572  268.228   \n",
       "4   70.9706   4.9086   2.5014  0.9778  0.2156  0.0461  22.0500      NaN   \n",
       "\n",
       "      564   565     566     567     568     569      570       571     572  \\\n",
       "0     NaN   NaN     NaN     NaN     NaN     NaN      NaN  533.8500  2.1113   \n",
       "1     NaN   NaN     NaN     NaN     NaN     NaN      NaN  535.0164  2.4335   \n",
       "2  0.9032  1.10  0.6219  0.4122  0.2562  0.4119  68.8489  535.0245  2.0293   \n",
       "3  0.6511  7.32  0.1630  3.5611  0.0670  2.7290  25.0363  530.5682  2.0253   \n",
       "4     NaN   NaN     NaN     NaN     NaN     NaN      NaN  532.0155  2.0275   \n",
       "\n",
       "     573     574     575     576     577      578     579     580     581  \\\n",
       "0   8.95  0.3157  3.0624  0.1026  1.6765  14.9509     NaN     NaN     NaN   \n",
       "1   5.92  0.2653  2.0111  0.0772  1.1065  10.9003  0.0096  0.0201  0.0060   \n",
       "2  11.21  0.1882  4.0923  0.0640  2.0952   9.2721  0.0584  0.0484  0.0148   \n",
       "3   9.33  0.1738  2.8971  0.0525  1.7585   8.5831  0.0202  0.0149  0.0044   \n",
       "4   8.83  0.2224  3.1776  0.0706  1.6597  10.9698     NaN     NaN     NaN   \n",
       "\n",
       "        582     583     584     585      586     587     588     589       590  \n",
       "0       NaN  0.5005  0.0118  0.0035   2.3630     NaN     NaN     NaN       NaN  \n",
       "1  208.2045  0.5019  0.0223  0.0055   4.4447  0.0096  0.0201  0.0060  208.2045  \n",
       "2   82.8602  0.4958  0.0157  0.0039   3.1745  0.0584  0.0484  0.0148   82.8602  \n",
       "3   73.8432  0.4990  0.0103  0.0025   2.0544  0.0202  0.0149  0.0044   73.8432  \n",
       "4       NaN  0.4800  0.4766  0.1045  99.3032  0.0202  0.0149  0.0044   73.8432  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View some initial records.\n",
    "SECOM_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97007c9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 592)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate dataframe dimensions.\n",
    "SECOM_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5533c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>150</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>179</th>\n",
       "      <th>180</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>187</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>227</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>230</th>\n",
       "      <th>231</th>\n",
       "      <th>232</th>\n",
       "      <th>233</th>\n",
       "      <th>234</th>\n",
       "      <th>235</th>\n",
       "      <th>236</th>\n",
       "      <th>237</th>\n",
       "      <th>238</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>257</th>\n",
       "      <th>258</th>\n",
       "      <th>259</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>314</th>\n",
       "      <th>315</th>\n",
       "      <th>316</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>323</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>326</th>\n",
       "      <th>327</th>\n",
       "      <th>328</th>\n",
       "      <th>329</th>\n",
       "      <th>330</th>\n",
       "      <th>331</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>365</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>370</th>\n",
       "      <th>371</th>\n",
       "      <th>372</th>\n",
       "      <th>373</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>398</th>\n",
       "      <th>399</th>\n",
       "      <th>400</th>\n",
       "      <th>401</th>\n",
       "      <th>402</th>\n",
       "      <th>403</th>\n",
       "      <th>404</th>\n",
       "      <th>405</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>415</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>423</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>450</th>\n",
       "      <th>451</th>\n",
       "      <th>452</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>459</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>462</th>\n",
       "      <th>463</th>\n",
       "      <th>464</th>\n",
       "      <th>465</th>\n",
       "      <th>466</th>\n",
       "      <th>467</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>482</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>502</th>\n",
       "      <th>503</th>\n",
       "      <th>504</th>\n",
       "      <th>505</th>\n",
       "      <th>506</th>\n",
       "      <th>507</th>\n",
       "      <th>508</th>\n",
       "      <th>509</th>\n",
       "      <th>510</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>513</th>\n",
       "      <th>514</th>\n",
       "      <th>515</th>\n",
       "      <th>516</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>529</th>\n",
       "      <th>530</th>\n",
       "      <th>531</th>\n",
       "      <th>532</th>\n",
       "      <th>533</th>\n",
       "      <th>534</th>\n",
       "      <th>535</th>\n",
       "      <th>536</th>\n",
       "      <th>537</th>\n",
       "      <th>538</th>\n",
       "      <th>539</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.867</td>\n",
       "      <td>3,014.453</td>\n",
       "      <td>2,495.850</td>\n",
       "      <td>2,200.547</td>\n",
       "      <td>1,396.377</td>\n",
       "      <td>4.197</td>\n",
       "      <td>100.000</td>\n",
       "      <td>101.113</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.463</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>199.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.005</td>\n",
       "      <td>413.086</td>\n",
       "      <td>9.908</td>\n",
       "      <td>0.971</td>\n",
       "      <td>190.047</td>\n",
       "      <td>12.481</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-5,618.394</td>\n",
       "      <td>2,699.378</td>\n",
       "      <td>-3,806.300</td>\n",
       "      <td>-298.598</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.938</td>\n",
       "      <td>6.639</td>\n",
       "      <td>69.500</td>\n",
       "      <td>2.366</td>\n",
       "      <td>0.184</td>\n",
       "      <td>3.673</td>\n",
       "      <td>85.337</td>\n",
       "      <td>8.960</td>\n",
       "      <td>50.583</td>\n",
       "      <td>64.556</td>\n",
       "      <td>49.417</td>\n",
       "      <td>66.221</td>\n",
       "      <td>86.837</td>\n",
       "      <td>118.680</td>\n",
       "      <td>67.905</td>\n",
       "      <td>3.353</td>\n",
       "      <td>70.000</td>\n",
       "      <td>355.539</td>\n",
       "      <td>10.031</td>\n",
       "      <td>136.743</td>\n",
       "      <td>733.673</td>\n",
       "      <td>1.178</td>\n",
       "      <td>139.972</td>\n",
       "      <td>1.000</td>\n",
       "      <td>632.254</td>\n",
       "      <td>157.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.593</td>\n",
       "      <td>4.839</td>\n",
       "      <td>2,856.172</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.593</td>\n",
       "      <td>2.960</td>\n",
       "      <td>355.159</td>\n",
       "      <td>10.423</td>\n",
       "      <td>116.502</td>\n",
       "      <td>13.990</td>\n",
       "      <td>20.542</td>\n",
       "      <td>27.132</td>\n",
       "      <td>706.669</td>\n",
       "      <td>16.715</td>\n",
       "      <td>147.438</td>\n",
       "      <td>1.000</td>\n",
       "      <td>619.102</td>\n",
       "      <td>104.329</td>\n",
       "      <td>150.362</td>\n",
       "      <td>468.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7.452</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.402</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1,807.815</td>\n",
       "      <td>0.189</td>\n",
       "      <td>8,827.537</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.980</td>\n",
       "      <td>101.318</td>\n",
       "      <td>231.819</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>747.384</td>\n",
       "      <td>0.987</td>\n",
       "      <td>58.626</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.971</td>\n",
       "      <td>6.311</td>\n",
       "      <td>15.796</td>\n",
       "      <td>3.898</td>\n",
       "      <td>15.830</td>\n",
       "      <td>15.795</td>\n",
       "      <td>1.185</td>\n",
       "      <td>2.751</td>\n",
       "      <td>0.648</td>\n",
       "      <td>3.192</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1,004.043</td>\n",
       "      <td>39.392</td>\n",
       "      <td>117.961</td>\n",
       "      <td>138.195</td>\n",
       "      <td>122.693</td>\n",
       "      <td>57.603</td>\n",
       "      <td>416.767</td>\n",
       "      <td>26.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.642</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.017</td>\n",
       "      <td>8.471</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.814</td>\n",
       "      <td>14.047</td>\n",
       "      <td>1.197</td>\n",
       "      <td>0.012</td>\n",
       "      <td>7.698</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1,039.651</td>\n",
       "      <td>882.681</td>\n",
       "      <td>555.346</td>\n",
       "      <td>4,066.850</td>\n",
       "      <td>4,797.155</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.252</td>\n",
       "      <td>2.789</td>\n",
       "      <td>1.236</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.013</td>\n",
       "      <td>0.547</td>\n",
       "      <td>10.781</td>\n",
       "      <td>26.661</td>\n",
       "      <td>0.145</td>\n",
       "      <td>7.366</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.936</td>\n",
       "      <td>43.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.287</td>\n",
       "      <td>8.688</td>\n",
       "      <td>20.093</td>\n",
       "      <td>0.557</td>\n",
       "      <td>11.532</td>\n",
       "      <td>17.600</td>\n",
       "      <td>7.839</td>\n",
       "      <td>10.170</td>\n",
       "      <td>30.073</td>\n",
       "      <td>32.218</td>\n",
       "      <td>9.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20.376</td>\n",
       "      <td>73.264</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.072</td>\n",
       "      <td>3.771</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.009</td>\n",
       "      <td>122.847</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1,041.057</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.730</td>\n",
       "      <td>4.149</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>109.651</td>\n",
       "      <td>0.004</td>\n",
       "      <td>4.645</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>19.505</td>\n",
       "      <td>3.778</td>\n",
       "      <td>29.260</td>\n",
       "      <td>46.057</td>\n",
       "      <td>41.298</td>\n",
       "      <td>20.181</td>\n",
       "      <td>136.292</td>\n",
       "      <td>8.693</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.211</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.804</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.120</td>\n",
       "      <td>4.260</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.579</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>335.551</td>\n",
       "      <td>401.815</td>\n",
       "      <td>252.999</td>\n",
       "      <td>1,879.228</td>\n",
       "      <td>2,342.827</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.977</td>\n",
       "      <td>0.173</td>\n",
       "      <td>3.189</td>\n",
       "      <td>7.916</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.393</td>\n",
       "      <td>13.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>2.593</td>\n",
       "      <td>6.216</td>\n",
       "      <td>0.168</td>\n",
       "      <td>3.427</td>\n",
       "      <td>9.736</td>\n",
       "      <td>2.327</td>\n",
       "      <td>3.038</td>\n",
       "      <td>9.329</td>\n",
       "      <td>14.674</td>\n",
       "      <td>2.732</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.199</td>\n",
       "      <td>23.217</td>\n",
       "      <td>7.958</td>\n",
       "      <td>5.770</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>39.936</td>\n",
       "      <td>0.018</td>\n",
       "      <td>333.320</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.285</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.155</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6.731</td>\n",
       "      <td>1.232</td>\n",
       "      <td>5.341</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.929</td>\n",
       "      <td>2.616</td>\n",
       "      <td>30.911</td>\n",
       "      <td>25.613</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.631</td>\n",
       "      <td>3.404</td>\n",
       "      <td>8.191</td>\n",
       "      <td>320.259</td>\n",
       "      <td>309.061</td>\n",
       "      <td>1.821</td>\n",
       "      <td>4.175</td>\n",
       "      <td>0.000</td>\n",
       "      <td>77.660</td>\n",
       "      <td>3.315</td>\n",
       "      <td>6.796</td>\n",
       "      <td>1.234</td>\n",
       "      <td>4.059</td>\n",
       "      <td>4.221</td>\n",
       "      <td>4.172</td>\n",
       "      <td>18.422</td>\n",
       "      <td>22.358</td>\n",
       "      <td>99.368</td>\n",
       "      <td>205.519</td>\n",
       "      <td>14.734</td>\n",
       "      <td>9.371</td>\n",
       "      <td>7.513</td>\n",
       "      <td>4.017</td>\n",
       "      <td>54.701</td>\n",
       "      <td>70.644</td>\n",
       "      <td>11.527</td>\n",
       "      <td>0.802</td>\n",
       "      <td>1.345</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.347</td>\n",
       "      <td>5.461</td>\n",
       "      <td>7.884</td>\n",
       "      <td>3.637</td>\n",
       "      <td>12.326</td>\n",
       "      <td>5.264</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.838</td>\n",
       "      <td>29.197</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.252</td>\n",
       "      <td>224.173</td>\n",
       "      <td>5.662</td>\n",
       "      <td>5.368</td>\n",
       "      <td>9.639</td>\n",
       "      <td>137.888</td>\n",
       "      <td>39.427</td>\n",
       "      <td>37.637</td>\n",
       "      <td>4.263</td>\n",
       "      <td>20.132</td>\n",
       "      <td>6.258</td>\n",
       "      <td>0.128</td>\n",
       "      <td>3.283</td>\n",
       "      <td>75.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>318.418</td>\n",
       "      <td>206.564</td>\n",
       "      <td>215.289</td>\n",
       "      <td>201.112</td>\n",
       "      <td>302.506</td>\n",
       "      <td>239.455</td>\n",
       "      <td>352.616</td>\n",
       "      <td>272.170</td>\n",
       "      <td>51.354</td>\n",
       "      <td>2.443</td>\n",
       "      <td>8.171</td>\n",
       "      <td>2.530</td>\n",
       "      <td>0.956</td>\n",
       "      <td>6.808</td>\n",
       "      <td>29.866</td>\n",
       "      <td>11.821</td>\n",
       "      <td>0.000</td>\n",
       "      <td>263.196</td>\n",
       "      <td>240.981</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.764</td>\n",
       "      <td>275.979</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.679</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.806</td>\n",
       "      <td>11.728</td>\n",
       "      <td>2.696</td>\n",
       "      <td>11.610</td>\n",
       "      <td>14.729</td>\n",
       "      <td>0.454</td>\n",
       "      <td>5.688</td>\n",
       "      <td>5.560</td>\n",
       "      <td>1.443</td>\n",
       "      <td>6.396</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.034</td>\n",
       "      <td>1.943</td>\n",
       "      <td>9.612</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.611</td>\n",
       "      <td>1.040</td>\n",
       "      <td>403.546</td>\n",
       "      <td>75.680</td>\n",
       "      <td>0.663</td>\n",
       "      <td>17.013</td>\n",
       "      <td>1.231</td>\n",
       "      <td>0.277</td>\n",
       "      <td>7.704</td>\n",
       "      <td>0.504</td>\n",
       "      <td>57.747</td>\n",
       "      <td>4.217</td>\n",
       "      <td>1.623</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.072</td>\n",
       "      <td>32.285</td>\n",
       "      <td>262.730</td>\n",
       "      <td>0.680</td>\n",
       "      <td>6.445</td>\n",
       "      <td>0.146</td>\n",
       "      <td>2.611</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2.452</td>\n",
       "      <td>21.118</td>\n",
       "      <td>530.524</td>\n",
       "      <td>2.102</td>\n",
       "      <td>28.450</td>\n",
       "      <td>0.346</td>\n",
       "      <td>9.162</td>\n",
       "      <td>0.105</td>\n",
       "      <td>5.564</td>\n",
       "      <td>16.642</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>97.934</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.068</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>99.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.498</td>\n",
       "      <td>73.622</td>\n",
       "      <td>80.408</td>\n",
       "      <td>29.513</td>\n",
       "      <td>441.692</td>\n",
       "      <td>56.356</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.237</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3.257</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.797</td>\n",
       "      <td>17.221</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.781</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.017</td>\n",
       "      <td>626.822</td>\n",
       "      <td>295.499</td>\n",
       "      <td>1,380.162</td>\n",
       "      <td>2,902.690</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.244</td>\n",
       "      <td>3.461</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.535</td>\n",
       "      <td>2.027</td>\n",
       "      <td>1.344</td>\n",
       "      <td>1.183</td>\n",
       "      <td>2.575</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.807</td>\n",
       "      <td>24.063</td>\n",
       "      <td>2.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.235</td>\n",
       "      <td>0.175</td>\n",
       "      <td>7.849</td>\n",
       "      <td>12.170</td>\n",
       "      <td>0.190</td>\n",
       "      <td>4.524</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.644</td>\n",
       "      <td>60.925</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060</td>\n",
       "      <td>25.749</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.085</td>\n",
       "      <td>9.532</td>\n",
       "      <td>6.028</td>\n",
       "      <td>0.275</td>\n",
       "      <td>8.629</td>\n",
       "      <td>7.120</td>\n",
       "      <td>4.977</td>\n",
       "      <td>7.122</td>\n",
       "      <td>11.623</td>\n",
       "      <td>307.502</td>\n",
       "      <td>4.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.539</td>\n",
       "      <td>31.652</td>\n",
       "      <td>18.388</td>\n",
       "      <td>17.630</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.013</td>\n",
       "      <td>53.537</td>\n",
       "      <td>0.052</td>\n",
       "      <td>396.314</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.880</td>\n",
       "      <td>2.105</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>48.949</td>\n",
       "      <td>0.009</td>\n",
       "      <td>6.485</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.220</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.053</td>\n",
       "      <td>6.538</td>\n",
       "      <td>2.990</td>\n",
       "      <td>57.545</td>\n",
       "      <td>53.910</td>\n",
       "      <td>52.253</td>\n",
       "      <td>12.345</td>\n",
       "      <td>263.301</td>\n",
       "      <td>506.922</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.552</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>18.741</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.242</td>\n",
       "      <td>31.003</td>\n",
       "      <td>23.364</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5.239</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.040</td>\n",
       "      <td>406.849</td>\n",
       "      <td>983.043</td>\n",
       "      <td>574.809</td>\n",
       "      <td>4,239.245</td>\n",
       "      <td>6,553.569</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.407</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.312</td>\n",
       "      <td>0.224</td>\n",
       "      <td>4.164</td>\n",
       "      <td>6.836</td>\n",
       "      <td>0.110</td>\n",
       "      <td>7.189</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.610</td>\n",
       "      <td>21.712</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.395</td>\n",
       "      <td>15.721</td>\n",
       "      <td>10.552</td>\n",
       "      <td>0.538</td>\n",
       "      <td>16.446</td>\n",
       "      <td>8.691</td>\n",
       "      <td>5.104</td>\n",
       "      <td>14.623</td>\n",
       "      <td>17.462</td>\n",
       "      <td>565.101</td>\n",
       "      <td>11.541</td>\n",
       "      <td>0.051</td>\n",
       "      <td>17.498</td>\n",
       "      <td>28.067</td>\n",
       "      <td>1.168</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.056</td>\n",
       "      <td>55.156</td>\n",
       "      <td>0.071</td>\n",
       "      <td>433.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>4.336</td>\n",
       "      <td>10.045</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.016</td>\n",
       "      <td>54.597</td>\n",
       "      <td>0.037</td>\n",
       "      <td>64.355</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>7.344</td>\n",
       "      <td>1.152</td>\n",
       "      <td>8.402</td>\n",
       "      <td>17.866</td>\n",
       "      <td>17.738</td>\n",
       "      <td>3.830</td>\n",
       "      <td>85.608</td>\n",
       "      <td>168.949</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5.864</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.963</td>\n",
       "      <td>9.764</td>\n",
       "      <td>7.386</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.011</td>\n",
       "      <td>137.692</td>\n",
       "      <td>477.050</td>\n",
       "      <td>283.531</td>\n",
       "      <td>1,975.111</td>\n",
       "      <td>3,226.924</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.216</td>\n",
       "      <td>2.179</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2.117</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.519</td>\n",
       "      <td>6.616</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>5.645</td>\n",
       "      <td>3.403</td>\n",
       "      <td>0.173</td>\n",
       "      <td>5.782</td>\n",
       "      <td>7.556</td>\n",
       "      <td>1.699</td>\n",
       "      <td>5.645</td>\n",
       "      <td>6.075</td>\n",
       "      <td>261.738</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.372</td>\n",
       "      <td>8.895</td>\n",
       "      <td>17.513</td>\n",
       "      <td>17.077</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.020</td>\n",
       "      <td>17.056</td>\n",
       "      <td>0.022</td>\n",
       "      <td>138.802</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.341</td>\n",
       "      <td>3.168</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>17.227</td>\n",
       "      <td>0.012</td>\n",
       "      <td>20.326</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.830</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.578</td>\n",
       "      <td>1.777</td>\n",
       "      <td>2.123</td>\n",
       "      <td>0.551</td>\n",
       "      <td>18.414</td>\n",
       "      <td>47.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.958</td>\n",
       "      <td>1.035</td>\n",
       "      <td>4.055</td>\n",
       "      <td>287.704</td>\n",
       "      <td>325.448</td>\n",
       "      <td>3.058</td>\n",
       "      <td>6.914</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.597</td>\n",
       "      <td>6.325</td>\n",
       "      <td>23.258</td>\n",
       "      <td>0.996</td>\n",
       "      <td>3.042</td>\n",
       "      <td>10.633</td>\n",
       "      <td>6.435</td>\n",
       "      <td>36.060</td>\n",
       "      <td>36.395</td>\n",
       "      <td>126.189</td>\n",
       "      <td>225.779</td>\n",
       "      <td>34.109</td>\n",
       "      <td>34.370</td>\n",
       "      <td>34.558</td>\n",
       "      <td>1.611</td>\n",
       "      <td>34.108</td>\n",
       "      <td>38.376</td>\n",
       "      <td>6.169</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.919</td>\n",
       "      <td>2.251</td>\n",
       "      <td>3.060</td>\n",
       "      <td>0.938</td>\n",
       "      <td>8.126</td>\n",
       "      <td>4.538</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.346</td>\n",
       "      <td>13.335</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.674</td>\n",
       "      <td>230.767</td>\n",
       "      <td>3.152</td>\n",
       "      <td>4.983</td>\n",
       "      <td>10.174</td>\n",
       "      <td>47.698</td>\n",
       "      <td>22.457</td>\n",
       "      <td>24.823</td>\n",
       "      <td>2.611</td>\n",
       "      <td>14.940</td>\n",
       "      <td>10.185</td>\n",
       "      <td>5.062</td>\n",
       "      <td>2.639</td>\n",
       "      <td>35.752</td>\n",
       "      <td>0.000</td>\n",
       "      <td>281.011</td>\n",
       "      <td>192.864</td>\n",
       "      <td>213.127</td>\n",
       "      <td>218.690</td>\n",
       "      <td>287.364</td>\n",
       "      <td>263.838</td>\n",
       "      <td>252.044</td>\n",
       "      <td>228.047</td>\n",
       "      <td>18.049</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.759</td>\n",
       "      <td>0.974</td>\n",
       "      <td>6.615</td>\n",
       "      <td>3.260</td>\n",
       "      <td>24.622</td>\n",
       "      <td>4.957</td>\n",
       "      <td>0.000</td>\n",
       "      <td>324.771</td>\n",
       "      <td>323.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>37.692</td>\n",
       "      <td>329.665</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.784</td>\n",
       "      <td>4.891</td>\n",
       "      <td>4.716</td>\n",
       "      <td>15.814</td>\n",
       "      <td>5.702</td>\n",
       "      <td>103.123</td>\n",
       "      <td>7.104</td>\n",
       "      <td>4.148</td>\n",
       "      <td>20.663</td>\n",
       "      <td>3.920</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.253</td>\n",
       "      <td>0.732</td>\n",
       "      <td>2.896</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>5.064</td>\n",
       "      <td>3.391</td>\n",
       "      <td>0.673</td>\n",
       "      <td>4.967</td>\n",
       "      <td>1.361</td>\n",
       "      <td>0.276</td>\n",
       "      <td>2.193</td>\n",
       "      <td>0.599</td>\n",
       "      <td>35.208</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.870</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.052</td>\n",
       "      <td>19.026</td>\n",
       "      <td>7.631</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.634</td>\n",
       "      <td>0.081</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.997</td>\n",
       "      <td>10.213</td>\n",
       "      <td>17.500</td>\n",
       "      <td>0.275</td>\n",
       "      <td>86.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26.920</td>\n",
       "      <td>0.068</td>\n",
       "      <td>16.921</td>\n",
       "      <td>12.485</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>87.521</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.578</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>93.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>2,743.240</td>\n",
       "      <td>2,158.750</td>\n",
       "      <td>2,060.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>100.000</td>\n",
       "      <td>82.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.655</td>\n",
       "      <td>182.094</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.249</td>\n",
       "      <td>333.449</td>\n",
       "      <td>4.470</td>\n",
       "      <td>0.579</td>\n",
       "      <td>169.177</td>\n",
       "      <td>9.877</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-7,150.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9,986.750</td>\n",
       "      <td>-14,804.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.400</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.034</td>\n",
       "      <td>2.070</td>\n",
       "      <td>83.183</td>\n",
       "      <td>7.603</td>\n",
       "      <td>49.835</td>\n",
       "      <td>63.677</td>\n",
       "      <td>40.229</td>\n",
       "      <td>64.919</td>\n",
       "      <td>84.733</td>\n",
       "      <td>111.713</td>\n",
       "      <td>1.434</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>70.000</td>\n",
       "      <td>342.755</td>\n",
       "      <td>9.464</td>\n",
       "      <td>108.846</td>\n",
       "      <td>699.814</td>\n",
       "      <td>0.497</td>\n",
       "      <td>125.798</td>\n",
       "      <td>1.000</td>\n",
       "      <td>607.393</td>\n",
       "      <td>40.261</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.706</td>\n",
       "      <td>3.932</td>\n",
       "      <td>2,801.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.932</td>\n",
       "      <td>4.220</td>\n",
       "      <td>-28.988</td>\n",
       "      <td>324.714</td>\n",
       "      <td>9.461</td>\n",
       "      <td>81.490</td>\n",
       "      <td>1.659</td>\n",
       "      <td>6.448</td>\n",
       "      <td>4.308</td>\n",
       "      <td>632.423</td>\n",
       "      <td>0.414</td>\n",
       "      <td>87.025</td>\n",
       "      <td>1.000</td>\n",
       "      <td>581.777</td>\n",
       "      <td>21.433</td>\n",
       "      <td>-59.478</td>\n",
       "      <td>456.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>5.826</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.105</td>\n",
       "      <td>2.243</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1,627.471</td>\n",
       "      <td>0.111</td>\n",
       "      <td>7,397.310</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-5.272</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.785</td>\n",
       "      <td>88.194</td>\n",
       "      <td>213.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.000</td>\n",
       "      <td>544.025</td>\n",
       "      <td>0.890</td>\n",
       "      <td>52.807</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.841</td>\n",
       "      <td>5.126</td>\n",
       "      <td>15.460</td>\n",
       "      <td>1.671</td>\n",
       "      <td>15.170</td>\n",
       "      <td>15.430</td>\n",
       "      <td>0.312</td>\n",
       "      <td>2.340</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.779</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2.191</td>\n",
       "      <td>980.451</td>\n",
       "      <td>33.366</td>\n",
       "      <td>58.000</td>\n",
       "      <td>36.100</td>\n",
       "      <td>19.200</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.421</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.337</td>\n",
       "      <td>2.020</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>234.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>9.400</td>\n",
       "      <td>0.093</td>\n",
       "      <td>3.170</td>\n",
       "      <td>5.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.220</td>\n",
       "      <td>6.613</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.750</td>\n",
       "      <td>9.220</td>\n",
       "      <td>0.090</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.728</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.010</td>\n",
       "      <td>5.359</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.264</td>\n",
       "      <td>0.009</td>\n",
       "      <td>168.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6.098</td>\n",
       "      <td>1.302</td>\n",
       "      <td>15.547</td>\n",
       "      <td>10.402</td>\n",
       "      <td>6.943</td>\n",
       "      <td>8.651</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>82.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.788</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.657</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.710</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.537</td>\n",
       "      <td>2.837</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.790</td>\n",
       "      <td>5.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.200</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.256</td>\n",
       "      <td>2.056</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.720</td>\n",
       "      <td>0.003</td>\n",
       "      <td>60.988</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.424</td>\n",
       "      <td>2.738</td>\n",
       "      <td>1.216</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.020</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.467</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.121</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.671</td>\n",
       "      <td>0.904</td>\n",
       "      <td>2.329</td>\n",
       "      <td>0.695</td>\n",
       "      <td>3.049</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.991</td>\n",
       "      <td>7.953</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.833</td>\n",
       "      <td>2.403</td>\n",
       "      <td>11.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.687</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646</td>\n",
       "      <td>8.841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.723</td>\n",
       "      <td>0.556</td>\n",
       "      <td>4.888</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.772</td>\n",
       "      <td>4.814</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.681</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.171</td>\n",
       "      <td>2.170</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.614</td>\n",
       "      <td>3.276</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.429</td>\n",
       "      <td>0.444</td>\n",
       "      <td>372.822</td>\n",
       "      <td>71.038</td>\n",
       "      <td>0.045</td>\n",
       "      <td>6.110</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.019</td>\n",
       "      <td>2.786</td>\n",
       "      <td>0.052</td>\n",
       "      <td>4.827</td>\n",
       "      <td>1.497</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.018</td>\n",
       "      <td>7.237</td>\n",
       "      <td>242.286</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.371</td>\n",
       "      <td>3.250</td>\n",
       "      <td>317.196</td>\n",
       "      <td>0.980</td>\n",
       "      <td>3.540</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.664</td>\n",
       "      <td>4.582</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.198</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>2,966.260</td>\n",
       "      <td>2,452.248</td>\n",
       "      <td>2,181.044</td>\n",
       "      <td>1,081.876</td>\n",
       "      <td>1.018</td>\n",
       "      <td>100.000</td>\n",
       "      <td>97.920</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.411</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.958</td>\n",
       "      <td>198.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.095</td>\n",
       "      <td>406.127</td>\n",
       "      <td>9.568</td>\n",
       "      <td>0.968</td>\n",
       "      <td>188.300</td>\n",
       "      <td>12.460</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-5,933.250</td>\n",
       "      <td>2,578.000</td>\n",
       "      <td>-4,371.750</td>\n",
       "      <td>-1,476.000</td>\n",
       "      <td>1.095</td>\n",
       "      <td>1.907</td>\n",
       "      <td>5.264</td>\n",
       "      <td>67.378</td>\n",
       "      <td>2.089</td>\n",
       "      <td>0.162</td>\n",
       "      <td>3.363</td>\n",
       "      <td>84.490</td>\n",
       "      <td>8.580</td>\n",
       "      <td>50.252</td>\n",
       "      <td>64.025</td>\n",
       "      <td>49.421</td>\n",
       "      <td>66.041</td>\n",
       "      <td>86.578</td>\n",
       "      <td>118.016</td>\n",
       "      <td>74.800</td>\n",
       "      <td>2.690</td>\n",
       "      <td>70.000</td>\n",
       "      <td>350.802</td>\n",
       "      <td>9.925</td>\n",
       "      <td>130.729</td>\n",
       "      <td>724.442</td>\n",
       "      <td>0.985</td>\n",
       "      <td>136.927</td>\n",
       "      <td>1.000</td>\n",
       "      <td>625.928</td>\n",
       "      <td>115.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.816</td>\n",
       "      <td>2,836.000</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.947</td>\n",
       "      <td>4.532</td>\n",
       "      <td>-1.872</td>\n",
       "      <td>350.596</td>\n",
       "      <td>10.283</td>\n",
       "      <td>112.023</td>\n",
       "      <td>10.364</td>\n",
       "      <td>17.365</td>\n",
       "      <td>23.056</td>\n",
       "      <td>698.770</td>\n",
       "      <td>0.891</td>\n",
       "      <td>145.237</td>\n",
       "      <td>1.000</td>\n",
       "      <td>612.774</td>\n",
       "      <td>87.484</td>\n",
       "      <td>145.305</td>\n",
       "      <td>464.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>7.104</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.377</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1,777.470</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8,564.690</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.979</td>\n",
       "      <td>100.389</td>\n",
       "      <td>230.374</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>721.023</td>\n",
       "      <td>0.990</td>\n",
       "      <td>57.978</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.965</td>\n",
       "      <td>6.246</td>\n",
       "      <td>15.730</td>\n",
       "      <td>3.202</td>\n",
       "      <td>15.762</td>\n",
       "      <td>15.723</td>\n",
       "      <td>0.974</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.549</td>\n",
       "      <td>3.074</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.996</td>\n",
       "      <td>2.277</td>\n",
       "      <td>999.996</td>\n",
       "      <td>37.347</td>\n",
       "      <td>92.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>81.300</td>\n",
       "      <td>50.900</td>\n",
       "      <td>243.786</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.459</td>\n",
       "      <td>8.090</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.007</td>\n",
       "      <td>5.927</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.027</td>\n",
       "      <td>721.675</td>\n",
       "      <td>411.000</td>\n",
       "      <td>295.000</td>\n",
       "      <td>1,321.000</td>\n",
       "      <td>451.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.132</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.850</td>\n",
       "      <td>0.378</td>\n",
       "      <td>7.732</td>\n",
       "      <td>21.171</td>\n",
       "      <td>0.102</td>\n",
       "      <td>5.390</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.505</td>\n",
       "      <td>24.711</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>5.040</td>\n",
       "      <td>17.130</td>\n",
       "      <td>0.296</td>\n",
       "      <td>6.740</td>\n",
       "      <td>14.155</td>\n",
       "      <td>5.020</td>\n",
       "      <td>6.094</td>\n",
       "      <td>24.653</td>\n",
       "      <td>0.114</td>\n",
       "      <td>6.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.350</td>\n",
       "      <td>56.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.946</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>95.147</td>\n",
       "      <td>0.030</td>\n",
       "      <td>718.725</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.911</td>\n",
       "      <td>2.726</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.206</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>13.828</td>\n",
       "      <td>2.957</td>\n",
       "      <td>24.982</td>\n",
       "      <td>30.014</td>\n",
       "      <td>27.093</td>\n",
       "      <td>18.247</td>\n",
       "      <td>81.216</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.698</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.438</td>\n",
       "      <td>2.467</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.092</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>229.809</td>\n",
       "      <td>185.090</td>\n",
       "      <td>130.220</td>\n",
       "      <td>603.033</td>\n",
       "      <td>210.937</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.302</td>\n",
       "      <td>0.117</td>\n",
       "      <td>2.320</td>\n",
       "      <td>6.245</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.670</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.273</td>\n",
       "      <td>7.579</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.547</td>\n",
       "      <td>5.454</td>\n",
       "      <td>0.089</td>\n",
       "      <td>2.036</td>\n",
       "      <td>8.289</td>\n",
       "      <td>1.543</td>\n",
       "      <td>1.901</td>\n",
       "      <td>7.589</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.999</td>\n",
       "      <td>17.861</td>\n",
       "      <td>4.441</td>\n",
       "      <td>2.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.169</td>\n",
       "      <td>0.009</td>\n",
       "      <td>228.683</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.014</td>\n",
       "      <td>4.548</td>\n",
       "      <td>0.967</td>\n",
       "      <td>4.128</td>\n",
       "      <td>3.013</td>\n",
       "      <td>3.265</td>\n",
       "      <td>2.321</td>\n",
       "      <td>18.408</td>\n",
       "      <td>11.376</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.927</td>\n",
       "      <td>2.660</td>\n",
       "      <td>5.766</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.030</td>\n",
       "      <td>3.184</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.977</td>\n",
       "      <td>1.965</td>\n",
       "      <td>3.766</td>\n",
       "      <td>0.743</td>\n",
       "      <td>3.113</td>\n",
       "      <td>1.935</td>\n",
       "      <td>2.571</td>\n",
       "      <td>7.000</td>\n",
       "      <td>11.059</td>\n",
       "      <td>31.032</td>\n",
       "      <td>10.027</td>\n",
       "      <td>7.551</td>\n",
       "      <td>3.494</td>\n",
       "      <td>1.951</td>\n",
       "      <td>3.071</td>\n",
       "      <td>36.290</td>\n",
       "      <td>48.174</td>\n",
       "      <td>5.414</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.764</td>\n",
       "      <td>3.748</td>\n",
       "      <td>5.807</td>\n",
       "      <td>2.900</td>\n",
       "      <td>8.817</td>\n",
       "      <td>3.828</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.291</td>\n",
       "      <td>20.222</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.697</td>\n",
       "      <td>38.473</td>\n",
       "      <td>4.847</td>\n",
       "      <td>2.823</td>\n",
       "      <td>5.807</td>\n",
       "      <td>105.525</td>\n",
       "      <td>24.901</td>\n",
       "      <td>23.157</td>\n",
       "      <td>3.494</td>\n",
       "      <td>11.577</td>\n",
       "      <td>4.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.628</td>\n",
       "      <td>52.895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>81.316</td>\n",
       "      <td>76.455</td>\n",
       "      <td>50.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.555</td>\n",
       "      <td>139.914</td>\n",
       "      <td>112.859</td>\n",
       "      <td>38.391</td>\n",
       "      <td>1.747</td>\n",
       "      <td>6.925</td>\n",
       "      <td>1.664</td>\n",
       "      <td>0.139</td>\n",
       "      <td>5.275</td>\n",
       "      <td>16.342</td>\n",
       "      <td>8.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.890</td>\n",
       "      <td>1.171</td>\n",
       "      <td>4.160</td>\n",
       "      <td>1.552</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.183</td>\n",
       "      <td>0.073</td>\n",
       "      <td>3.770</td>\n",
       "      <td>4.101</td>\n",
       "      <td>0.484</td>\n",
       "      <td>4.895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.890</td>\n",
       "      <td>1.385</td>\n",
       "      <td>7.496</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.797</td>\n",
       "      <td>400.694</td>\n",
       "      <td>73.254</td>\n",
       "      <td>0.226</td>\n",
       "      <td>14.530</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.095</td>\n",
       "      <td>6.738</td>\n",
       "      <td>0.344</td>\n",
       "      <td>27.018</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.036</td>\n",
       "      <td>15.762</td>\n",
       "      <td>259.973</td>\n",
       "      <td>0.567</td>\n",
       "      <td>4.980</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.884</td>\n",
       "      <td>15.466</td>\n",
       "      <td>530.703</td>\n",
       "      <td>1.983</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.242</td>\n",
       "      <td>2.568</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.408</td>\n",
       "      <td>11.502</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>46.185</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.307</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>44.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>3,011.490</td>\n",
       "      <td>2,499.405</td>\n",
       "      <td>2,201.067</td>\n",
       "      <td>1,285.214</td>\n",
       "      <td>1.317</td>\n",
       "      <td>100.000</td>\n",
       "      <td>101.512</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.462</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.966</td>\n",
       "      <td>199.536</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.967</td>\n",
       "      <td>412.219</td>\n",
       "      <td>9.852</td>\n",
       "      <td>0.973</td>\n",
       "      <td>189.664</td>\n",
       "      <td>12.500</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-5,523.250</td>\n",
       "      <td>2,664.000</td>\n",
       "      <td>-3,820.750</td>\n",
       "      <td>-78.750</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.986</td>\n",
       "      <td>7.265</td>\n",
       "      <td>69.156</td>\n",
       "      <td>2.378</td>\n",
       "      <td>0.187</td>\n",
       "      <td>3.431</td>\n",
       "      <td>85.135</td>\n",
       "      <td>8.770</td>\n",
       "      <td>50.396</td>\n",
       "      <td>64.166</td>\n",
       "      <td>49.604</td>\n",
       "      <td>66.232</td>\n",
       "      <td>86.821</td>\n",
       "      <td>118.399</td>\n",
       "      <td>78.290</td>\n",
       "      <td>3.074</td>\n",
       "      <td>70.000</td>\n",
       "      <td>353.721</td>\n",
       "      <td>10.035</td>\n",
       "      <td>136.400</td>\n",
       "      <td>733.450</td>\n",
       "      <td>1.251</td>\n",
       "      <td>140.008</td>\n",
       "      <td>1.000</td>\n",
       "      <td>631.371</td>\n",
       "      <td>183.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.596</td>\n",
       "      <td>4.843</td>\n",
       "      <td>2,854.000</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.573</td>\n",
       "      <td>0.947</td>\n",
       "      <td>353.799</td>\n",
       "      <td>10.437</td>\n",
       "      <td>116.212</td>\n",
       "      <td>13.246</td>\n",
       "      <td>20.021</td>\n",
       "      <td>26.261</td>\n",
       "      <td>706.454</td>\n",
       "      <td>0.978</td>\n",
       "      <td>147.597</td>\n",
       "      <td>1.000</td>\n",
       "      <td>619.033</td>\n",
       "      <td>102.604</td>\n",
       "      <td>152.297</td>\n",
       "      <td>466.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>7.467</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1,809.249</td>\n",
       "      <td>0.190</td>\n",
       "      <td>8,825.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.981</td>\n",
       "      <td>101.482</td>\n",
       "      <td>231.201</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>750.861</td>\n",
       "      <td>0.991</td>\n",
       "      <td>58.549</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.969</td>\n",
       "      <td>6.314</td>\n",
       "      <td>15.790</td>\n",
       "      <td>3.877</td>\n",
       "      <td>15.830</td>\n",
       "      <td>15.780</td>\n",
       "      <td>1.144</td>\n",
       "      <td>2.735</td>\n",
       "      <td>0.654</td>\n",
       "      <td>3.195</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.312</td>\n",
       "      <td>1,004.050</td>\n",
       "      <td>38.903</td>\n",
       "      <td>109.000</td>\n",
       "      <td>134.600</td>\n",
       "      <td>117.700</td>\n",
       "      <td>55.900</td>\n",
       "      <td>339.561</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.260</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.016</td>\n",
       "      <td>7.917</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.951</td>\n",
       "      <td>10.994</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.011</td>\n",
       "      <td>7.513</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1,020.300</td>\n",
       "      <td>623.000</td>\n",
       "      <td>438.000</td>\n",
       "      <td>2,614.000</td>\n",
       "      <td>1,784.000</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.184</td>\n",
       "      <td>2.600</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.690</td>\n",
       "      <td>0.524</td>\n",
       "      <td>10.170</td>\n",
       "      <td>27.200</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.735</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.865</td>\n",
       "      <td>40.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.259</td>\n",
       "      <td>6.780</td>\n",
       "      <td>19.370</td>\n",
       "      <td>0.424</td>\n",
       "      <td>8.570</td>\n",
       "      <td>17.235</td>\n",
       "      <td>6.760</td>\n",
       "      <td>8.462</td>\n",
       "      <td>30.097</td>\n",
       "      <td>0.158</td>\n",
       "      <td>7.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.720</td>\n",
       "      <td>73.248</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.631</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.002</td>\n",
       "      <td>119.436</td>\n",
       "      <td>0.040</td>\n",
       "      <td>967.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.185</td>\n",
       "      <td>3.673</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103.094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.865</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.977</td>\n",
       "      <td>3.704</td>\n",
       "      <td>28.773</td>\n",
       "      <td>45.676</td>\n",
       "      <td>40.019</td>\n",
       "      <td>19.581</td>\n",
       "      <td>110.601</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.658</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.875</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>317.867</td>\n",
       "      <td>278.672</td>\n",
       "      <td>195.826</td>\n",
       "      <td>1,202.412</td>\n",
       "      <td>820.099</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.832</td>\n",
       "      <td>0.163</td>\n",
       "      <td>2.899</td>\n",
       "      <td>8.389</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.459</td>\n",
       "      <td>12.505</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.085</td>\n",
       "      <td>2.063</td>\n",
       "      <td>5.980</td>\n",
       "      <td>0.129</td>\n",
       "      <td>2.514</td>\n",
       "      <td>9.074</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.561</td>\n",
       "      <td>9.474</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.006</td>\n",
       "      <td>23.215</td>\n",
       "      <td>5.567</td>\n",
       "      <td>3.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>39.696</td>\n",
       "      <td>0.013</td>\n",
       "      <td>309.832</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.373</td>\n",
       "      <td>1.106</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.920</td>\n",
       "      <td>1.240</td>\n",
       "      <td>4.922</td>\n",
       "      <td>4.490</td>\n",
       "      <td>4.733</td>\n",
       "      <td>2.548</td>\n",
       "      <td>26.157</td>\n",
       "      <td>20.255</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.177</td>\n",
       "      <td>3.234</td>\n",
       "      <td>7.396</td>\n",
       "      <td>302.178</td>\n",
       "      <td>272.449</td>\n",
       "      <td>1.645</td>\n",
       "      <td>3.943</td>\n",
       "      <td>0.000</td>\n",
       "      <td>69.905</td>\n",
       "      <td>2.667</td>\n",
       "      <td>4.764</td>\n",
       "      <td>1.135</td>\n",
       "      <td>3.941</td>\n",
       "      <td>2.534</td>\n",
       "      <td>3.454</td>\n",
       "      <td>11.106</td>\n",
       "      <td>16.381</td>\n",
       "      <td>57.969</td>\n",
       "      <td>151.116</td>\n",
       "      <td>10.198</td>\n",
       "      <td>4.551</td>\n",
       "      <td>2.764</td>\n",
       "      <td>3.781</td>\n",
       "      <td>49.091</td>\n",
       "      <td>65.438</td>\n",
       "      <td>12.086</td>\n",
       "      <td>0.808</td>\n",
       "      <td>1.265</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.271</td>\n",
       "      <td>5.227</td>\n",
       "      <td>7.425</td>\n",
       "      <td>3.724</td>\n",
       "      <td>11.351</td>\n",
       "      <td>4.793</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.830</td>\n",
       "      <td>26.168</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.645</td>\n",
       "      <td>150.340</td>\n",
       "      <td>5.472</td>\n",
       "      <td>4.061</td>\n",
       "      <td>7.396</td>\n",
       "      <td>138.255</td>\n",
       "      <td>34.247</td>\n",
       "      <td>32.820</td>\n",
       "      <td>4.276</td>\n",
       "      <td>15.974</td>\n",
       "      <td>5.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.184</td>\n",
       "      <td>70.434</td>\n",
       "      <td>0.000</td>\n",
       "      <td>293.519</td>\n",
       "      <td>148.317</td>\n",
       "      <td>138.775</td>\n",
       "      <td>112.953</td>\n",
       "      <td>249.927</td>\n",
       "      <td>112.275</td>\n",
       "      <td>348.529</td>\n",
       "      <td>219.487</td>\n",
       "      <td>48.557</td>\n",
       "      <td>2.251</td>\n",
       "      <td>8.009</td>\n",
       "      <td>2.529</td>\n",
       "      <td>0.233</td>\n",
       "      <td>6.608</td>\n",
       "      <td>22.039</td>\n",
       "      <td>10.907</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>46.986</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.589</td>\n",
       "      <td>5.833</td>\n",
       "      <td>2.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.743</td>\n",
       "      <td>0.100</td>\n",
       "      <td>4.877</td>\n",
       "      <td>5.134</td>\n",
       "      <td>1.550</td>\n",
       "      <td>6.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.055</td>\n",
       "      <td>1.786</td>\n",
       "      <td>9.459</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.911</td>\n",
       "      <td>403.122</td>\n",
       "      <td>74.084</td>\n",
       "      <td>0.471</td>\n",
       "      <td>16.340</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.198</td>\n",
       "      <td>7.428</td>\n",
       "      <td>0.479</td>\n",
       "      <td>54.442</td>\n",
       "      <td>4.067</td>\n",
       "      <td>1.530</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.059</td>\n",
       "      <td>29.731</td>\n",
       "      <td>264.272</td>\n",
       "      <td>0.651</td>\n",
       "      <td>5.160</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.049</td>\n",
       "      <td>2.000</td>\n",
       "      <td>16.988</td>\n",
       "      <td>532.398</td>\n",
       "      <td>2.119</td>\n",
       "      <td>8.650</td>\n",
       "      <td>0.293</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.089</td>\n",
       "      <td>1.625</td>\n",
       "      <td>13.818</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>72.289</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>71.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-1.000</td>\n",
       "      <td>3,056.650</td>\n",
       "      <td>2,538.823</td>\n",
       "      <td>2,218.055</td>\n",
       "      <td>1,591.224</td>\n",
       "      <td>1.526</td>\n",
       "      <td>100.000</td>\n",
       "      <td>104.587</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.517</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.971</td>\n",
       "      <td>202.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.862</td>\n",
       "      <td>419.089</td>\n",
       "      <td>10.128</td>\n",
       "      <td>0.977</td>\n",
       "      <td>192.189</td>\n",
       "      <td>12.547</td>\n",
       "      <td>1.415</td>\n",
       "      <td>-5,356.250</td>\n",
       "      <td>2,841.750</td>\n",
       "      <td>-3,352.750</td>\n",
       "      <td>1,377.250</td>\n",
       "      <td>1.304</td>\n",
       "      <td>2.003</td>\n",
       "      <td>7.330</td>\n",
       "      <td>72.267</td>\n",
       "      <td>2.656</td>\n",
       "      <td>0.207</td>\n",
       "      <td>3.531</td>\n",
       "      <td>85.742</td>\n",
       "      <td>9.061</td>\n",
       "      <td>50.579</td>\n",
       "      <td>64.345</td>\n",
       "      <td>49.748</td>\n",
       "      <td>66.343</td>\n",
       "      <td>87.002</td>\n",
       "      <td>118.940</td>\n",
       "      <td>80.200</td>\n",
       "      <td>3.521</td>\n",
       "      <td>70.000</td>\n",
       "      <td>360.772</td>\n",
       "      <td>10.152</td>\n",
       "      <td>142.098</td>\n",
       "      <td>741.455</td>\n",
       "      <td>1.340</td>\n",
       "      <td>143.196</td>\n",
       "      <td>1.000</td>\n",
       "      <td>638.136</td>\n",
       "      <td>206.977</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.617</td>\n",
       "      <td>4.869</td>\n",
       "      <td>2,874.000</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.952</td>\n",
       "      <td>4.669</td>\n",
       "      <td>4.385</td>\n",
       "      <td>359.674</td>\n",
       "      <td>10.592</td>\n",
       "      <td>120.927</td>\n",
       "      <td>16.376</td>\n",
       "      <td>22.814</td>\n",
       "      <td>29.915</td>\n",
       "      <td>714.597</td>\n",
       "      <td>1.065</td>\n",
       "      <td>149.959</td>\n",
       "      <td>1.000</td>\n",
       "      <td>625.170</td>\n",
       "      <td>115.499</td>\n",
       "      <td>158.438</td>\n",
       "      <td>467.890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.027</td>\n",
       "      <td>7.808</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.115</td>\n",
       "      <td>2.429</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1,841.873</td>\n",
       "      <td>0.200</td>\n",
       "      <td>9,065.432</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.982</td>\n",
       "      <td>102.078</td>\n",
       "      <td>233.036</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>776.782</td>\n",
       "      <td>0.991</td>\n",
       "      <td>59.134</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.978</td>\n",
       "      <td>6.376</td>\n",
       "      <td>15.860</td>\n",
       "      <td>4.392</td>\n",
       "      <td>15.900</td>\n",
       "      <td>15.870</td>\n",
       "      <td>1.338</td>\n",
       "      <td>2.873</td>\n",
       "      <td>0.714</td>\n",
       "      <td>3.311</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.358</td>\n",
       "      <td>1,008.671</td>\n",
       "      <td>40.805</td>\n",
       "      <td>127.000</td>\n",
       "      <td>181.000</td>\n",
       "      <td>161.600</td>\n",
       "      <td>62.900</td>\n",
       "      <td>502.206</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.021</td>\n",
       "      <td>9.585</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.275</td>\n",
       "      <td>14.347</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.015</td>\n",
       "      <td>9.055</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>1,277.750</td>\n",
       "      <td>966.000</td>\n",
       "      <td>625.000</td>\n",
       "      <td>5,034.000</td>\n",
       "      <td>6,384.000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.255</td>\n",
       "      <td>3.200</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.973</td>\n",
       "      <td>0.689</td>\n",
       "      <td>13.338</td>\n",
       "      <td>31.687</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8.450</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.860</td>\n",
       "      <td>57.675</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.296</td>\n",
       "      <td>9.555</td>\n",
       "      <td>21.460</td>\n",
       "      <td>0.726</td>\n",
       "      <td>11.460</td>\n",
       "      <td>20.163</td>\n",
       "      <td>9.490</td>\n",
       "      <td>11.953</td>\n",
       "      <td>33.506</td>\n",
       "      <td>0.231</td>\n",
       "      <td>9.940</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.370</td>\n",
       "      <td>90.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.405</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.005</td>\n",
       "      <td>144.503</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1,261.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.762</td>\n",
       "      <td>4.480</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>131.758</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.795</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.092</td>\n",
       "      <td>24.653</td>\n",
       "      <td>4.379</td>\n",
       "      <td>31.702</td>\n",
       "      <td>59.595</td>\n",
       "      <td>54.277</td>\n",
       "      <td>22.097</td>\n",
       "      <td>162.038</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>3.146</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.607</td>\n",
       "      <td>4.311</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3.025</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>403.989</td>\n",
       "      <td>428.555</td>\n",
       "      <td>273.953</td>\n",
       "      <td>2,341.289</td>\n",
       "      <td>3,190.616</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.116</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.548</td>\n",
       "      <td>0.218</td>\n",
       "      <td>4.021</td>\n",
       "      <td>9.481</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.633</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.345</td>\n",
       "      <td>17.925</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "      <td>2.791</td>\n",
       "      <td>6.550</td>\n",
       "      <td>0.210</td>\n",
       "      <td>3.360</td>\n",
       "      <td>10.042</td>\n",
       "      <td>2.785</td>\n",
       "      <td>3.405</td>\n",
       "      <td>10.440</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.985</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.885</td>\n",
       "      <td>28.873</td>\n",
       "      <td>6.825</td>\n",
       "      <td>4.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>47.079</td>\n",
       "      <td>0.019</td>\n",
       "      <td>412.330</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.387</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.652</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.585</td>\n",
       "      <td>1.417</td>\n",
       "      <td>5.787</td>\n",
       "      <td>5.937</td>\n",
       "      <td>6.458</td>\n",
       "      <td>2.853</td>\n",
       "      <td>38.140</td>\n",
       "      <td>29.307</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.571</td>\n",
       "      <td>4.011</td>\n",
       "      <td>9.169</td>\n",
       "      <td>524.002</td>\n",
       "      <td>582.935</td>\n",
       "      <td>2.215</td>\n",
       "      <td>4.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>92.911</td>\n",
       "      <td>3.471</td>\n",
       "      <td>6.883</td>\n",
       "      <td>1.540</td>\n",
       "      <td>4.769</td>\n",
       "      <td>3.609</td>\n",
       "      <td>4.756</td>\n",
       "      <td>17.423</td>\n",
       "      <td>21.765</td>\n",
       "      <td>120.173</td>\n",
       "      <td>305.026</td>\n",
       "      <td>12.754</td>\n",
       "      <td>5.823</td>\n",
       "      <td>3.822</td>\n",
       "      <td>4.679</td>\n",
       "      <td>66.667</td>\n",
       "      <td>84.973</td>\n",
       "      <td>15.796</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.913</td>\n",
       "      <td>6.902</td>\n",
       "      <td>9.577</td>\n",
       "      <td>4.342</td>\n",
       "      <td>14.388</td>\n",
       "      <td>6.089</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.309</td>\n",
       "      <td>35.279</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.387</td>\n",
       "      <td>335.922</td>\n",
       "      <td>6.006</td>\n",
       "      <td>7.007</td>\n",
       "      <td>9.720</td>\n",
       "      <td>168.410</td>\n",
       "      <td>47.728</td>\n",
       "      <td>45.169</td>\n",
       "      <td>4.742</td>\n",
       "      <td>23.737</td>\n",
       "      <td>6.704</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.625</td>\n",
       "      <td>93.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>514.586</td>\n",
       "      <td>262.865</td>\n",
       "      <td>294.667</td>\n",
       "      <td>288.893</td>\n",
       "      <td>501.607</td>\n",
       "      <td>397.506</td>\n",
       "      <td>510.647</td>\n",
       "      <td>377.144</td>\n",
       "      <td>61.495</td>\n",
       "      <td>2.840</td>\n",
       "      <td>9.079</td>\n",
       "      <td>3.199</td>\n",
       "      <td>0.563</td>\n",
       "      <td>7.897</td>\n",
       "      <td>32.438</td>\n",
       "      <td>14.469</td>\n",
       "      <td>0.000</td>\n",
       "      <td>536.205</td>\n",
       "      <td>505.401</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>64.249</td>\n",
       "      <td>555.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.760</td>\n",
       "      <td>1.933</td>\n",
       "      <td>10.972</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.809</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.451</td>\n",
       "      <td>6.329</td>\n",
       "      <td>2.212</td>\n",
       "      <td>7.594</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.947</td>\n",
       "      <td>2.458</td>\n",
       "      <td>11.238</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>8.021</td>\n",
       "      <td>1.286</td>\n",
       "      <td>407.431</td>\n",
       "      <td>78.397</td>\n",
       "      <td>0.850</td>\n",
       "      <td>19.035</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.358</td>\n",
       "      <td>8.637</td>\n",
       "      <td>0.562</td>\n",
       "      <td>74.629</td>\n",
       "      <td>4.703</td>\n",
       "      <td>1.816</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.089</td>\n",
       "      <td>44.113</td>\n",
       "      <td>265.707</td>\n",
       "      <td>0.769</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.186</td>\n",
       "      <td>3.099</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.971</td>\n",
       "      <td>24.772</td>\n",
       "      <td>534.356</td>\n",
       "      <td>2.291</td>\n",
       "      <td>10.130</td>\n",
       "      <td>0.367</td>\n",
       "      <td>3.493</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1.902</td>\n",
       "      <td>17.081</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>116.539</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.295</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>114.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000</td>\n",
       "      <td>3,356.350</td>\n",
       "      <td>2,846.440</td>\n",
       "      <td>2,315.267</td>\n",
       "      <td>3,715.042</td>\n",
       "      <td>1,114.537</td>\n",
       "      <td>100.000</td>\n",
       "      <td>129.252</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1.656</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.985</td>\n",
       "      <td>272.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.547</td>\n",
       "      <td>824.927</td>\n",
       "      <td>102.868</td>\n",
       "      <td>0.985</td>\n",
       "      <td>215.598</td>\n",
       "      <td>12.990</td>\n",
       "      <td>1.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3,656.250</td>\n",
       "      <td>2,363.000</td>\n",
       "      <td>14,106.000</td>\n",
       "      <td>1.383</td>\n",
       "      <td>2.053</td>\n",
       "      <td>7.659</td>\n",
       "      <td>77.900</td>\n",
       "      <td>3.511</td>\n",
       "      <td>0.285</td>\n",
       "      <td>4.804</td>\n",
       "      <td>105.604</td>\n",
       "      <td>23.345</td>\n",
       "      <td>59.771</td>\n",
       "      <td>94.264</td>\n",
       "      <td>50.165</td>\n",
       "      <td>67.959</td>\n",
       "      <td>88.419</td>\n",
       "      <td>133.390</td>\n",
       "      <td>86.120</td>\n",
       "      <td>37.880</td>\n",
       "      <td>70.000</td>\n",
       "      <td>377.297</td>\n",
       "      <td>11.053</td>\n",
       "      <td>176.314</td>\n",
       "      <td>789.752</td>\n",
       "      <td>1.511</td>\n",
       "      <td>163.251</td>\n",
       "      <td>1.000</td>\n",
       "      <td>667.742</td>\n",
       "      <td>258.543</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.764</td>\n",
       "      <td>5.011</td>\n",
       "      <td>2,936.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.960</td>\n",
       "      <td>4.848</td>\n",
       "      <td>168.145</td>\n",
       "      <td>373.866</td>\n",
       "      <td>11.785</td>\n",
       "      <td>287.151</td>\n",
       "      <td>188.092</td>\n",
       "      <td>48.988</td>\n",
       "      <td>118.084</td>\n",
       "      <td>770.608</td>\n",
       "      <td>7,272.828</td>\n",
       "      <td>167.831</td>\n",
       "      <td>1.000</td>\n",
       "      <td>722.602</td>\n",
       "      <td>238.477</td>\n",
       "      <td>175.413</td>\n",
       "      <td>692.426</td>\n",
       "      <td>4.196</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.144</td>\n",
       "      <td>8.990</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.118</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2,105.182</td>\n",
       "      <td>1.473</td>\n",
       "      <td>10,746.600</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.509</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.570</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.984</td>\n",
       "      <td>106.923</td>\n",
       "      <td>236.955</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.041</td>\n",
       "      <td>924.532</td>\n",
       "      <td>0.992</td>\n",
       "      <td>311.734</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.983</td>\n",
       "      <td>7.522</td>\n",
       "      <td>16.070</td>\n",
       "      <td>6.889</td>\n",
       "      <td>16.100</td>\n",
       "      <td>16.100</td>\n",
       "      <td>2.465</td>\n",
       "      <td>3.991</td>\n",
       "      <td>1.175</td>\n",
       "      <td>3.895</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.019</td>\n",
       "      <td>2.472</td>\n",
       "      <td>1,020.994</td>\n",
       "      <td>64.129</td>\n",
       "      <td>994.000</td>\n",
       "      <td>295.800</td>\n",
       "      <td>334.700</td>\n",
       "      <td>141.800</td>\n",
       "      <td>1,770.691</td>\n",
       "      <td>9,998.894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103.390</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.978</td>\n",
       "      <td>742.942</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.318</td>\n",
       "      <td>536.564</td>\n",
       "      <td>924.378</td>\n",
       "      <td>0.239</td>\n",
       "      <td>191.548</td>\n",
       "      <td>12.710</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.288</td>\n",
       "      <td>2,505.300</td>\n",
       "      <td>7,791.000</td>\n",
       "      <td>4,170.000</td>\n",
       "      <td>37,943.000</td>\n",
       "      <td>36,871.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.817</td>\n",
       "      <td>3.286</td>\n",
       "      <td>21.100</td>\n",
       "      <td>16.300</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.143</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.858</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>48.670</td>\n",
       "      <td>3.573</td>\n",
       "      <td>55.000</td>\n",
       "      <td>72.947</td>\n",
       "      <td>3.228</td>\n",
       "      <td>267.910</td>\n",
       "      <td>0.000</td>\n",
       "      <td>307.930</td>\n",
       "      <td>191.830</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.838</td>\n",
       "      <td>396.110</td>\n",
       "      <td>252.870</td>\n",
       "      <td>10.017</td>\n",
       "      <td>390.120</td>\n",
       "      <td>199.620</td>\n",
       "      <td>126.530</td>\n",
       "      <td>490.561</td>\n",
       "      <td>500.349</td>\n",
       "      <td>9,998.448</td>\n",
       "      <td>320.050</td>\n",
       "      <td>2.000</td>\n",
       "      <td>457.650</td>\n",
       "      <td>172.349</td>\n",
       "      <td>46.150</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.594</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.674</td>\n",
       "      <td>8.802</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1,768.880</td>\n",
       "      <td>1.436</td>\n",
       "      <td>3,601.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.984</td>\n",
       "      <td>99.902</td>\n",
       "      <td>237.184</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1,119.704</td>\n",
       "      <td>0.991</td>\n",
       "      <td>2,549.988</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.158</td>\n",
       "      <td>40.855</td>\n",
       "      <td>10.153</td>\n",
       "      <td>158.526</td>\n",
       "      <td>132.648</td>\n",
       "      <td>122.117</td>\n",
       "      <td>43.574</td>\n",
       "      <td>659.170</td>\n",
       "      <td>3,332.596</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.171</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.308</td>\n",
       "      <td>232.805</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.870</td>\n",
       "      <td>207.016</td>\n",
       "      <td>292.227</td>\n",
       "      <td>0.075</td>\n",
       "      <td>59.519</td>\n",
       "      <td>4.420</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.083</td>\n",
       "      <td>879.226</td>\n",
       "      <td>3,933.755</td>\n",
       "      <td>2,005.874</td>\n",
       "      <td>15,559.952</td>\n",
       "      <td>18,520.468</td>\n",
       "      <td>0.526</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.812</td>\n",
       "      <td>5.711</td>\n",
       "      <td>5.155</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.755</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.096</td>\n",
       "      <td>1.003</td>\n",
       "      <td>15.893</td>\n",
       "      <td>20.046</td>\n",
       "      <td>0.947</td>\n",
       "      <td>79.151</td>\n",
       "      <td>0.000</td>\n",
       "      <td>89.192</td>\n",
       "      <td>51.868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.096</td>\n",
       "      <td>174.894</td>\n",
       "      <td>90.516</td>\n",
       "      <td>3.413</td>\n",
       "      <td>172.712</td>\n",
       "      <td>214.863</td>\n",
       "      <td>38.900</td>\n",
       "      <td>196.688</td>\n",
       "      <td>197.499</td>\n",
       "      <td>5,043.879</td>\n",
       "      <td>97.709</td>\n",
       "      <td>0.447</td>\n",
       "      <td>156.336</td>\n",
       "      <td>59.324</td>\n",
       "      <td>257.011</td>\n",
       "      <td>187.759</td>\n",
       "      <td>13.915</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.279</td>\n",
       "      <td>2.835</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.409</td>\n",
       "      <td>547.172</td>\n",
       "      <td>0.416</td>\n",
       "      <td>1,072.203</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.627</td>\n",
       "      <td>30.998</td>\n",
       "      <td>74.844</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.131</td>\n",
       "      <td>348.829</td>\n",
       "      <td>0.313</td>\n",
       "      <td>805.394</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.051</td>\n",
       "      <td>14.728</td>\n",
       "      <td>3.313</td>\n",
       "      <td>44.310</td>\n",
       "      <td>9.576</td>\n",
       "      <td>13.807</td>\n",
       "      <td>6.215</td>\n",
       "      <td>128.282</td>\n",
       "      <td>899.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>116.862</td>\n",
       "      <td>9.690</td>\n",
       "      <td>39.038</td>\n",
       "      <td>999.316</td>\n",
       "      <td>998.681</td>\n",
       "      <td>111.496</td>\n",
       "      <td>273.095</td>\n",
       "      <td>0.000</td>\n",
       "      <td>424.215</td>\n",
       "      <td>103.181</td>\n",
       "      <td>898.609</td>\n",
       "      <td>24.990</td>\n",
       "      <td>113.223</td>\n",
       "      <td>118.753</td>\n",
       "      <td>186.616</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>994.286</td>\n",
       "      <td>995.745</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>32.274</td>\n",
       "      <td>851.613</td>\n",
       "      <td>657.762</td>\n",
       "      <td>33.058</td>\n",
       "      <td>1.277</td>\n",
       "      <td>5.132</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.109</td>\n",
       "      <td>1.764</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.475</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.978</td>\n",
       "      <td>34.490</td>\n",
       "      <td>42.070</td>\n",
       "      <td>10.184</td>\n",
       "      <td>232.126</td>\n",
       "      <td>164.109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>47.777</td>\n",
       "      <td>149.385</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>109.007</td>\n",
       "      <td>999.877</td>\n",
       "      <td>77.801</td>\n",
       "      <td>87.135</td>\n",
       "      <td>212.656</td>\n",
       "      <td>492.772</td>\n",
       "      <td>358.950</td>\n",
       "      <td>415.435</td>\n",
       "      <td>79.116</td>\n",
       "      <td>274.887</td>\n",
       "      <td>289.826</td>\n",
       "      <td>200.000</td>\n",
       "      <td>63.334</td>\n",
       "      <td>221.975</td>\n",
       "      <td>0.000</td>\n",
       "      <td>999.413</td>\n",
       "      <td>989.474</td>\n",
       "      <td>996.859</td>\n",
       "      <td>994.000</td>\n",
       "      <td>999.491</td>\n",
       "      <td>995.745</td>\n",
       "      <td>997.519</td>\n",
       "      <td>994.004</td>\n",
       "      <td>142.844</td>\n",
       "      <td>12.770</td>\n",
       "      <td>21.044</td>\n",
       "      <td>9.402</td>\n",
       "      <td>127.573</td>\n",
       "      <td>107.693</td>\n",
       "      <td>219.644</td>\n",
       "      <td>40.282</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>999.234</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>451.485</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>252.860</td>\n",
       "      <td>113.276</td>\n",
       "      <td>111.350</td>\n",
       "      <td>184.349</td>\n",
       "      <td>111.737</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>137.984</td>\n",
       "      <td>111.333</td>\n",
       "      <td>818.000</td>\n",
       "      <td>80.041</td>\n",
       "      <td>8.204</td>\n",
       "      <td>14.448</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.580</td>\n",
       "      <td>4.082</td>\n",
       "      <td>25.779</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21.044</td>\n",
       "      <td>3.979</td>\n",
       "      <td>421.702</td>\n",
       "      <td>83.720</td>\n",
       "      <td>7.066</td>\n",
       "      <td>131.680</td>\n",
       "      <td>39.330</td>\n",
       "      <td>2.718</td>\n",
       "      <td>56.930</td>\n",
       "      <td>17.478</td>\n",
       "      <td>303.550</td>\n",
       "      <td>35.320</td>\n",
       "      <td>54.292</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.446</td>\n",
       "      <td>101.115</td>\n",
       "      <td>311.404</td>\n",
       "      <td>1.299</td>\n",
       "      <td>32.580</td>\n",
       "      <td>0.689</td>\n",
       "      <td>14.014</td>\n",
       "      <td>0.293</td>\n",
       "      <td>12.746</td>\n",
       "      <td>84.802</td>\n",
       "      <td>589.508</td>\n",
       "      <td>2.740</td>\n",
       "      <td>454.560</td>\n",
       "      <td>2.197</td>\n",
       "      <td>170.020</td>\n",
       "      <td>0.550</td>\n",
       "      <td>90.424</td>\n",
       "      <td>96.960</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.104</td>\n",
       "      <td>99.303</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Target          1          2          3          4          5  \\\n",
       "count  1,567.000  1,561.000  1,560.000  1,553.000  1,553.000  1,553.000   \n",
       "mean      -0.867  3,014.453  2,495.850  2,200.547  1,396.377      4.197   \n",
       "std        0.498     73.622     80.408     29.513    441.692     56.356   \n",
       "min       -1.000  2,743.240  2,158.750  2,060.660      0.000      0.681   \n",
       "25%       -1.000  2,966.260  2,452.248  2,181.044  1,081.876      1.018   \n",
       "50%       -1.000  3,011.490  2,499.405  2,201.067  1,285.214      1.317   \n",
       "75%       -1.000  3,056.650  2,538.823  2,218.055  1,591.224      1.526   \n",
       "max        1.000  3,356.350  2,846.440  2,315.267  3,715.042  1,114.537   \n",
       "\n",
       "               6          7          8          9         10         11  \\\n",
       "count  1,553.000  1,553.000  1,558.000  1,565.000  1,565.000  1,565.000   \n",
       "mean     100.000    101.113      0.122      1.463     -0.001      0.000   \n",
       "std        0.000      6.237      0.009      0.074      0.015      0.009   \n",
       "min      100.000     82.131      0.000      1.191     -0.053     -0.035   \n",
       "25%      100.000     97.920      0.121      1.411     -0.011     -0.006   \n",
       "50%      100.000    101.512      0.122      1.462     -0.001      0.000   \n",
       "75%      100.000    104.587      0.124      1.517      0.008      0.006   \n",
       "max      100.000    129.252      0.129      1.656      0.075      0.053   \n",
       "\n",
       "              12         13         14         15         16         17  \\\n",
       "count  1,565.000  1,565.000  1,564.000  1,564.000  1,564.000  1,564.000   \n",
       "mean       0.964    199.957      0.000      9.005    413.086      9.908   \n",
       "std        0.012      3.257      0.000      2.797     17.221      2.404   \n",
       "min        0.655    182.094      0.000      2.249    333.449      4.470   \n",
       "25%        0.958    198.131      0.000      7.095    406.127      9.568   \n",
       "50%        0.966    199.536      0.000      8.967    412.219      9.852   \n",
       "75%        0.971    202.007      0.000     10.862    419.089     10.128   \n",
       "max        0.985    272.045      0.000     19.547    824.927    102.868   \n",
       "\n",
       "              18         19         20         21          22         23  \\\n",
       "count  1,564.000  1,564.000  1,557.000  1,567.000   1,565.000  1,565.000   \n",
       "mean       0.971    190.047     12.481      1.405  -5,618.394  2,699.378   \n",
       "std        0.012      2.781      0.218      0.017     626.822    295.499   \n",
       "min        0.579    169.177      9.877      1.180  -7,150.250      0.000   \n",
       "25%        0.968    188.300     12.460      1.397  -5,933.250  2,578.000   \n",
       "50%        0.973    189.664     12.500      1.406  -5,523.250  2,664.000   \n",
       "75%        0.977    192.189     12.547      1.415  -5,356.250  2,841.750   \n",
       "max        0.985    215.598     12.990      1.453       0.000  3,656.250   \n",
       "\n",
       "               24           25         26         27         28         29  \\\n",
       "count   1,565.000    1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean   -3,806.300     -298.598      1.204      1.938      6.639     69.500   \n",
       "std     1,380.162    2,902.690      0.178      0.189      1.244      3.461   \n",
       "min    -9,986.750  -14,804.500      0.000      0.000      0.000     59.400   \n",
       "25%    -4,371.750   -1,476.000      1.095      1.907      5.264     67.378   \n",
       "50%    -3,820.750      -78.750      1.283      1.986      7.265     69.156   \n",
       "75%    -3,352.750    1,377.250      1.304      2.003      7.330     72.267   \n",
       "max     2,363.000   14,106.000      1.383      2.053      7.659     77.900   \n",
       "\n",
       "              30         31         32         33         34         35  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       2.366      0.184      3.673     85.337      8.960     50.583   \n",
       "std        0.409      0.033      0.535      2.027      1.344      1.183   \n",
       "min        0.667      0.034      2.070     83.183      7.603     49.835   \n",
       "25%        2.089      0.162      3.363     84.490      8.580     50.252   \n",
       "50%        2.378      0.187      3.431     85.135      8.770     50.396   \n",
       "75%        2.656      0.207      3.531     85.742      9.061     50.579   \n",
       "max        3.511      0.285      4.804    105.604     23.345     59.771   \n",
       "\n",
       "              36         37         38         39         40         41  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,543.000   \n",
       "mean      64.556     49.417     66.221     86.837    118.680     67.905   \n",
       "std        2.575      1.183      0.304      0.447      1.807     24.063   \n",
       "min       63.677     40.229     64.919     84.733    111.713      1.434   \n",
       "25%       64.025     49.421     66.041     86.578    118.016     74.800   \n",
       "50%       64.166     49.604     66.232     86.821    118.399     78.290   \n",
       "75%       64.345     49.748     66.343     87.002    118.940     80.200   \n",
       "max       94.264     50.165     67.959     88.419    133.390     86.120   \n",
       "\n",
       "              42         43         44         45         46         47  \\\n",
       "count  1,543.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       3.353     70.000    355.539     10.031    136.743    733.673   \n",
       "std        2.360      0.000      6.235      0.175      7.849     12.170   \n",
       "min       -0.076     70.000    342.755      9.464    108.846    699.814   \n",
       "25%        2.690     70.000    350.802      9.925    130.729    724.442   \n",
       "50%        3.074     70.000    353.721     10.035    136.400    733.450   \n",
       "75%        3.521     70.000    360.772     10.152    142.098    741.455   \n",
       "max       37.880     70.000    377.297     11.053    176.314    789.752   \n",
       "\n",
       "              48         49         50         51         52         53  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       1.178    139.972      1.000    632.254    157.421      0.000   \n",
       "std        0.190      4.524      0.000      8.644     60.925      0.000   \n",
       "min        0.497    125.798      1.000    607.393     40.261      0.000   \n",
       "25%        0.985    136.927      1.000    625.928    115.509      0.000   \n",
       "50%        1.251    140.008      1.000    631.371    183.318      0.000   \n",
       "75%        1.340    143.196      1.000    638.136    206.977      0.000   \n",
       "max        1.511    163.251      1.000    667.742    258.543      0.000   \n",
       "\n",
       "              54         55         56         57         58         59  \\\n",
       "count  1,563.000  1,563.000  1,563.000  1,563.000  1,563.000  1,563.000   \n",
       "mean       4.593      4.839  2,856.172      0.929      0.949      4.593   \n",
       "std        0.055      0.060     25.749      0.007      0.004      0.085   \n",
       "min        3.706      3.932  2,801.000      0.875      0.932      4.220   \n",
       "25%        4.574      4.816  2,836.000      0.925      0.947      4.532   \n",
       "50%        4.596      4.843  2,854.000      0.931      0.949      4.573   \n",
       "75%        4.617      4.869  2,874.000      0.933      0.952      4.669   \n",
       "max        4.764      5.011  2,936.000      0.938      0.960      4.848   \n",
       "\n",
       "              60         61         62         63         64         65  \\\n",
       "count  1,560.000  1,561.000  1,561.000  1,561.000  1,560.000  1,560.000   \n",
       "mean       2.960    355.159     10.423    116.502     13.990     20.542   \n",
       "std        9.532      6.028      0.275      8.629      7.120      4.977   \n",
       "min      -28.988    324.714      9.461     81.490      1.659      6.448   \n",
       "25%       -1.872    350.596     10.283    112.023     10.364     17.365   \n",
       "50%        0.947    353.799     10.437    116.212     13.246     20.021   \n",
       "75%        4.385    359.674     10.592    120.927     16.376     22.814   \n",
       "max      168.145    373.866     11.785    287.151    188.092     48.988   \n",
       "\n",
       "              66         67         68         69         70         71  \\\n",
       "count  1,560.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean      27.132    706.669     16.715    147.438      1.000    619.102   \n",
       "std        7.122     11.623    307.502      4.240      0.000      9.539   \n",
       "min        4.308    632.423      0.414     87.025      1.000    581.777   \n",
       "25%       23.056    698.770      0.891    145.237      1.000    612.774   \n",
       "50%       26.261    706.454      0.978    147.597      1.000    619.033   \n",
       "75%       29.915    714.597      1.065    149.959      1.000    625.170   \n",
       "max      118.084    770.608  7,272.828    167.831      1.000    722.602   \n",
       "\n",
       "              72       73       74         75         76         77  \\\n",
       "count  1,561.000  773.000  773.000  1,561.000  1,543.000  1,543.000   \n",
       "mean     104.329  150.362  468.020      0.003     -0.007     -0.029   \n",
       "std       31.652   18.388   17.630      0.106      0.022      0.033   \n",
       "min       21.433  -59.478  456.045      0.000     -0.105     -0.186   \n",
       "25%       87.484  145.305  464.458      0.000     -0.020     -0.052   \n",
       "50%      102.604  152.297  466.082      0.000     -0.006     -0.029   \n",
       "75%      115.499  158.438  467.890      0.000      0.007     -0.006   \n",
       "max      238.477  175.413  692.426      4.196      0.232      0.072   \n",
       "\n",
       "              78         79         80         81         82         83  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean      -0.007     -0.014      0.003     -0.019     -0.021      0.006   \n",
       "std        0.031      0.048      0.023      0.049      0.017      0.036   \n",
       "min       -0.105     -0.348     -0.057     -0.144     -0.098     -0.213   \n",
       "25%       -0.029     -0.048     -0.011     -0.044     -0.027     -0.018   \n",
       "50%       -0.010     -0.013      0.001     -0.009     -0.020      0.008   \n",
       "75%        0.009      0.012      0.013      0.009     -0.012      0.027   \n",
       "max        0.133      0.249      0.101      0.119      0.058      0.144   \n",
       "\n",
       "              84         85       86         87         88         89  \\\n",
       "count  1,566.000  1,555.000  226.000  1,567.000  1,567.000  1,567.000   \n",
       "mean       7.452      0.133    0.113      2.402      0.982  1,807.815   \n",
       "std        0.516      0.005    0.003      0.037      0.013     53.537   \n",
       "min        5.826      0.117    0.105      2.243      0.775  1,627.471   \n",
       "25%        7.104      0.130    0.111      2.377      0.976  1,777.470   \n",
       "50%        7.467      0.133    0.114      2.404      0.987  1,809.249   \n",
       "75%        7.808      0.136    0.115      2.429      0.990  1,841.873   \n",
       "max        8.990      0.150    0.118      2.555      0.994  2,105.182   \n",
       "\n",
       "              90          91         92         93         94         95  \\\n",
       "count  1,516.000   1,516.000  1,561.000  1,565.000  1,565.000  1,561.000   \n",
       "mean       0.189   8,827.537      0.002      0.001     -0.001     -0.000   \n",
       "std        0.052     396.314      0.088      0.003      0.003      0.000   \n",
       "min        0.111   7,397.310     -0.357     -0.013     -0.017     -0.002   \n",
       "25%        0.169   8,564.690     -0.043     -0.001     -0.002     -0.000   \n",
       "50%        0.190   8,825.435      0.000      0.000     -0.000      0.000   \n",
       "75%        0.200   9,065.432      0.051      0.002      0.001      0.000   \n",
       "max        1.473  10,746.600      0.363      0.028      0.013      0.001   \n",
       "\n",
       "              96         97         98         99        100        101  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       0.000      0.017      0.000     -0.018      0.002     -0.000   \n",
       "std        0.000      0.220      0.000      0.427      0.063      0.000   \n",
       "min       -0.001     -1.480      0.000     -5.272     -0.528     -0.003   \n",
       "25%        0.000     -0.089      0.000     -0.219     -0.030     -0.000   \n",
       "50%        0.000      0.004      0.000      0.000      0.000      0.000   \n",
       "75%        0.000      0.122      0.000      0.189      0.030      0.000   \n",
       "max        0.001      2.509      0.000      2.570      0.885      0.002   \n",
       "\n",
       "             102        103        104        105        106        107  \\\n",
       "count  1,561.000  1,561.000  1,565.000  1,565.000  1,561.000  1,561.000   \n",
       "mean      -0.000      0.001     -0.010     -0.000     -0.000      0.001   \n",
       "std        0.000      0.063      0.003      0.001      0.003      0.003   \n",
       "min       -0.002     -0.535     -0.033     -0.012     -0.028     -0.013   \n",
       "25%       -0.000     -0.036     -0.012     -0.000     -0.002     -0.001   \n",
       "50%        0.000      0.000     -0.010      0.000     -0.000      0.000   \n",
       "75%        0.000      0.034     -0.008      0.000      0.001      0.002   \n",
       "max        0.002      0.298      0.020      0.007      0.013      0.017   \n",
       "\n",
       "             108        109      110      111      112      113        114  \\\n",
       "count  1,561.000  1,561.000  549.000  549.000  549.000  852.000  1,567.000   \n",
       "mean      -0.002     -0.011    0.980  101.318  231.819    0.458      0.945   \n",
       "std        0.087      0.087    0.009    1.880    2.105    0.049      0.012   \n",
       "min       -0.523     -0.345    0.785   88.194  213.008    0.000      0.853   \n",
       "25%       -0.049     -0.065    0.979  100.389  230.374    0.459      0.939   \n",
       "50%        0.000     -0.011    0.981  101.482  231.201    0.463      0.946   \n",
       "75%        0.049      0.038    0.982  102.078  233.036    0.466      0.952   \n",
       "max        0.486      0.394    0.984  106.923  236.955    0.488      0.976   \n",
       "\n",
       "             115        116        117        118        119        120  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,567.000  1,543.000  1,567.000   \n",
       "mean       0.000    747.384      0.987     58.626      0.598      0.971   \n",
       "std        0.002     48.949      0.009      6.485      0.008      0.009   \n",
       "min        0.000    544.025      0.890     52.807      0.527      0.841   \n",
       "25%        0.000    721.023      0.990     57.978      0.594      0.965   \n",
       "50%        0.000    750.861      0.991     58.549      0.599      0.969   \n",
       "75%        0.000    776.782      0.991     59.134      0.603      0.978   \n",
       "max        0.041    924.532      0.992    311.734      0.625      0.983   \n",
       "\n",
       "             121        122        123        124        125        126  \\\n",
       "count  1,567.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       6.311     15.796      3.898     15.830     15.795      1.185   \n",
       "std        0.124      0.100      0.904      0.108      0.114      0.281   \n",
       "min        5.126     15.460      1.671     15.170     15.430      0.312   \n",
       "25%        6.246     15.730      3.202     15.762     15.723      0.974   \n",
       "50%        6.314     15.790      3.877     15.830     15.780      1.144   \n",
       "75%        6.376     15.860      4.392     15.900     15.870      1.338   \n",
       "max        7.522     16.070      6.889     16.100     16.100      2.465   \n",
       "\n",
       "             127        128        129        130        131        132  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       2.751      0.648      3.192     -0.554      0.745      0.998   \n",
       "std        0.253      0.135      0.264      1.220      0.083      0.002   \n",
       "min        2.340      0.316      0.000     -3.779      0.420      0.994   \n",
       "25%        2.572      0.549      3.074     -0.899      0.689      0.996   \n",
       "50%        2.735      0.654      3.195     -0.142      0.759      0.998   \n",
       "75%        2.873      0.714      3.311      0.047      0.815      0.999   \n",
       "max        3.991      1.175      3.895      2.458      0.888      1.019   \n",
       "\n",
       "             133        134        135        136        137        138  \\\n",
       "count  1,559.000  1,559.000  1,559.000  1,562.000  1,561.000  1,560.000   \n",
       "mean       2.319  1,004.043     39.392    117.961    138.195    122.693   \n",
       "std        0.053      6.538      2.990     57.545     53.910     52.253   \n",
       "min        2.191    980.451     33.366     58.000     36.100     19.200   \n",
       "25%        2.277    999.996     37.347     92.000     90.000     81.300   \n",
       "50%        2.312  1,004.050     38.903    109.000    134.600    117.700   \n",
       "75%        2.358  1,008.671     40.805    127.000    181.000    161.600   \n",
       "max        2.472  1,020.994     64.129    994.000    295.800    334.700   \n",
       "\n",
       "             139        140        141        142        143        144  \\\n",
       "count  1,553.000  1,553.000  1,553.000  1,553.000  1,553.000  1,558.000   \n",
       "mean      57.603    416.767     26.078      0.000      6.642      0.004   \n",
       "std       12.345    263.301    506.922      0.000      3.552      0.001   \n",
       "min       19.800      0.000      0.032      0.000      1.740      0.000   \n",
       "25%       50.900    243.786      0.132      0.000      5.110      0.003   \n",
       "50%       55.900    339.561      0.236      0.000      6.260      0.004   \n",
       "75%       62.900    502.206      0.439      0.000      7.500      0.005   \n",
       "max      141.800  1,770.691  9,998.894      0.000    103.390      0.012   \n",
       "\n",
       "             145        146        147        148        149        150  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,564.000   \n",
       "mean       0.120      0.064      0.055      0.017      8.471      0.000   \n",
       "std        0.061      0.027      0.022      0.027     18.741      0.000   \n",
       "min        0.032      0.021      0.023      0.004      1.421      0.000   \n",
       "25%        0.084      0.048      0.042      0.010      6.360      0.000   \n",
       "50%        0.107      0.059      0.050      0.016      7.917      0.000   \n",
       "75%        0.133      0.072      0.061      0.021      9.585      0.000   \n",
       "max        0.625      0.251      0.248      0.978    742.942      0.000   \n",
       "\n",
       "             151        152        153        154        155        156  \\\n",
       "count  1,564.000  1,564.000  1,564.000  1,564.000  1,564.000  1,557.000   \n",
       "mean       6.814     14.047      1.197      0.012      7.698      0.507   \n",
       "std        3.242     31.003     23.364      0.009      5.239      1.122   \n",
       "min        1.337      2.020      0.154      0.004      1.244      0.140   \n",
       "25%        4.459      8.090      0.374      0.007      5.927      0.240   \n",
       "50%        5.951     10.994      0.469      0.011      7.513      0.320   \n",
       "75%        8.275     14.347      0.680      0.015      9.055      0.450   \n",
       "max       22.318    536.564    924.378      0.239    191.548     12.710   \n",
       "\n",
       "             157      158        159        160        161         162  \\\n",
       "count  1,567.000  138.000    138.000  1,565.000  1,565.000   1,565.000   \n",
       "mean       0.058    0.047  1,039.651    882.681    555.346   4,066.850   \n",
       "std        0.079    0.040    406.849    983.043    574.809   4,239.245   \n",
       "min        0.011    0.012    234.100      0.000      0.000       0.000   \n",
       "25%        0.036    0.027    721.675    411.000    295.000   1,321.000   \n",
       "50%        0.049    0.035  1,020.300    623.000    438.000   2,614.000   \n",
       "75%        0.067    0.049  1,277.750    966.000    625.000   5,034.000   \n",
       "max        2.202    0.288  2,505.300  7,791.000  4,170.000  37,943.000   \n",
       "\n",
       "              163        164        165        166        167        168  \\\n",
       "count   1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean    4,797.155      0.140      0.128      0.252      2.789      1.236   \n",
       "std     6,553.569      0.122      0.243      0.407      1.120      0.633   \n",
       "min         0.000      0.000      0.000      0.000      0.800      0.300   \n",
       "25%       451.000      0.091      0.068      0.132      2.100      0.900   \n",
       "50%     1,784.000      0.120      0.089      0.184      2.600      1.200   \n",
       "75%     6,384.000      0.154      0.116      0.255      3.200      1.500   \n",
       "max    36,871.000      0.957      1.817      3.286     21.100     16.300   \n",
       "\n",
       "             169        170        171        172        173        174  \\\n",
       "count  1,565.000  1,565.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.124      0.400      0.684      0.120      0.320      0.576   \n",
       "std        0.048      0.198      0.157      0.061      0.071      0.096   \n",
       "min        0.033      0.046      0.298      0.009      0.129      0.254   \n",
       "25%        0.090      0.230      0.576      0.080      0.277      0.517   \n",
       "50%        0.119      0.412      0.686      0.113      0.324      0.578   \n",
       "75%        0.151      0.536      0.797      0.140      0.370      0.634   \n",
       "max        0.725      1.143      1.153      0.494      0.548      0.864   \n",
       "\n",
       "             175        176        177        178        179        180  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,543.000  1,566.000   \n",
       "mean       0.320      0.778      0.245      0.395      0.000      0.000   \n",
       "std        0.071      0.116      0.075      0.283      0.000      0.000   \n",
       "min        0.129      0.462      0.073      0.047      0.000      0.000   \n",
       "25%        0.277      0.692      0.196      0.222      0.000      0.000   \n",
       "50%        0.324      0.768      0.243      0.299      0.000      0.000   \n",
       "75%        0.370      0.844      0.294      0.423      0.000      0.000   \n",
       "max        0.548      1.172      0.441      1.858      0.000      0.000   \n",
       "\n",
       "             181        182        183        184        185        186  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean      19.013      0.547     10.781     26.661      0.145      7.366   \n",
       "std        3.312      0.224      4.164      6.836      0.110      7.189   \n",
       "min        9.400      0.093      3.170      5.014      0.030      1.940   \n",
       "25%       16.850      0.378      7.732     21.171      0.102      5.390   \n",
       "50%       18.690      0.524     10.170     27.200      0.133      6.735   \n",
       "75%       20.973      0.689     13.338     31.687      0.169      8.450   \n",
       "max       48.670      3.573     55.000     72.947      3.228    267.910   \n",
       "\n",
       "             187        188        189        190        191        192  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,563.000  1,563.000   \n",
       "mean       0.000     17.936     43.211      0.000      0.000      0.000   \n",
       "std        0.000      8.610     21.712      0.000      0.000      0.000   \n",
       "min        0.000      6.220      6.613      0.000      0.000      0.000   \n",
       "25%        0.000     14.505     24.711      0.000      0.000      0.000   \n",
       "50%        0.000     17.865     40.210      0.000      0.000      0.000   \n",
       "75%        0.000     20.860     57.675      0.000      0.000      0.000   \n",
       "max        0.000    307.930    191.830      0.000      0.000      0.000   \n",
       "\n",
       "             193        194        195        196        197        198  \\\n",
       "count  1,563.000  1,563.000  1,563.000  1,563.000  1,560.000  1,561.000   \n",
       "mean       0.000      0.000      0.000      0.287      8.688     20.093   \n",
       "std        0.000      0.000      0.000      0.395     15.721     10.552   \n",
       "min        0.000      0.000      0.000      0.080      1.750      9.220   \n",
       "25%        0.000      0.000      0.000      0.218      5.040     17.130   \n",
       "50%        0.000      0.000      0.000      0.259      6.780     19.370   \n",
       "75%        0.000      0.000      0.000      0.296      9.555     21.460   \n",
       "max        0.000      0.000      0.000      4.838    396.110    252.870   \n",
       "\n",
       "             199        200        201        202        203        204  \\\n",
       "count  1,561.000  1,561.000  1,560.000  1,560.000  1,560.000  1,561.000   \n",
       "mean       0.557     11.532     17.600      7.839     10.170     30.073   \n",
       "std        0.538     16.446      8.691      5.104     14.623     17.462   \n",
       "min        0.090      2.770      3.210      0.000      0.000      7.728   \n",
       "25%        0.296      6.740     14.155      5.020      6.094     24.653   \n",
       "50%        0.424      8.570     17.235      6.760      8.462     30.097   \n",
       "75%        0.726     11.460     20.163      9.490     11.953     33.506   \n",
       "max       10.017    390.120    199.620    126.530    490.561    500.349   \n",
       "\n",
       "             205        206        207        208        209        210  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean      32.218      9.050      0.001     20.376     73.264      0.030   \n",
       "std      565.101     11.541      0.051     17.498     28.067      1.168   \n",
       "min        0.043      2.300      0.000      4.010      5.359      0.000   \n",
       "25%        0.114      6.040      0.000     16.350     56.158      0.000   \n",
       "50%        0.158      7.740      0.000     19.720     73.248      0.000   \n",
       "75%        0.231      9.940      0.000     22.370     90.515      0.000   \n",
       "max    9,998.448    320.050      2.000    457.650    172.349     46.150   \n",
       "\n",
       "             211        212        213        214        215        216  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean       0.089      0.057      0.051      0.060      0.083      0.081   \n",
       "std        0.042      0.025      0.032      0.053      0.056      0.030   \n",
       "min        0.032      0.002      0.007      0.004      0.019      0.006   \n",
       "25%        0.066      0.044      0.033      0.036      0.057      0.063   \n",
       "50%        0.080      0.053      0.042      0.056      0.075      0.083   \n",
       "75%        0.099      0.064      0.062      0.074      0.094      0.098   \n",
       "max        0.516      0.323      0.594      1.284      0.761      0.343   \n",
       "\n",
       "             217        218        219        220      221        222  \\\n",
       "count  1,543.000  1,543.000  1,566.000  1,555.000  226.000  1,567.000   \n",
       "mean       0.083      0.072      3.771      0.003    0.009      0.061   \n",
       "std        0.026      0.046      1.170      0.002    0.002      0.023   \n",
       "min        0.010      0.008      1.034      0.001    0.006      0.020   \n",
       "25%        0.070      0.046      2.946      0.002    0.008      0.040   \n",
       "50%        0.085      0.062      3.631      0.003    0.009      0.061   \n",
       "75%        0.098      0.086      4.405      0.004    0.010      0.076   \n",
       "max        0.283      0.674      8.802      0.016    0.024      0.231   \n",
       "\n",
       "             223        224        225        226        227        228  \\\n",
       "count  1,567.000  1,567.000  1,516.000  1,516.000  1,561.000  1,565.000   \n",
       "mean       0.009    122.847      0.059  1,041.057      0.000      0.019   \n",
       "std        0.056     55.156      0.071    433.170      0.000      0.011   \n",
       "min        0.000     32.264      0.009    168.800      0.000      0.006   \n",
       "25%        0.001     95.147      0.030    718.725      0.000      0.013   \n",
       "50%        0.002    119.436      0.040    967.300      0.000      0.017   \n",
       "75%        0.005    144.503      0.061  1,261.300      0.000      0.021   \n",
       "max        0.991  1,768.880      1.436  3,601.300      0.000      0.154   \n",
       "\n",
       "             229        230        231        232        233        234  \\\n",
       "count  1,565.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       0.018      0.000      0.000      0.000      0.000      0.000   \n",
       "std        0.011      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.007      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.013      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.015      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.020      0.000      0.000      0.000      0.000      0.000   \n",
       "max        0.213      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             235        236        237        238        239        240  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,565.000  1,565.000   \n",
       "mean       0.000      0.000      0.000      0.000      0.005      0.005   \n",
       "std        0.000      0.000      0.000      0.000      0.002      0.001   \n",
       "min        0.000      0.000      0.000      0.000      0.001      0.001   \n",
       "25%        0.000      0.000      0.000      0.000      0.004      0.004   \n",
       "50%        0.000      0.000      0.000      0.000      0.005      0.004   \n",
       "75%        0.000      0.000      0.000      0.000      0.006      0.005   \n",
       "max        0.000      0.000      0.000      0.000      0.024      0.024   \n",
       "\n",
       "             241        242        243        244      245      246      247  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  549.000  549.000  549.000   \n",
       "mean       0.000      0.000      0.000      0.000    0.006    1.730    4.149   \n",
       "std        0.000      0.000      0.000      0.000    0.085    4.336   10.045   \n",
       "min        0.000      0.000      0.000      0.000    0.000    0.291    1.102   \n",
       "25%        0.000      0.000      0.000      0.000    0.001    0.911    2.726   \n",
       "50%        0.000      0.000      0.000      0.000    0.002    1.185    3.673   \n",
       "75%        0.000      0.000      0.000      0.000    0.003    1.762    4.480   \n",
       "max        0.000      0.000      0.000      0.000    1.984   99.902  237.184   \n",
       "\n",
       "           248        249        250        251        252        253  \\\n",
       "count  852.000  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000   \n",
       "mean     0.053      0.025      0.001    109.651      0.004      4.645   \n",
       "std      0.067      0.049      0.016     54.597      0.037     64.355   \n",
       "min      0.000      0.003      0.000     21.011      0.000      0.767   \n",
       "25%      0.019      0.015      0.000     76.132      0.001      2.206   \n",
       "50%      0.027      0.021      0.000    103.094      0.001      2.865   \n",
       "75%      0.051      0.027      0.000    131.758      0.001      3.795   \n",
       "max      0.491      0.973      0.414  1,119.704      0.991  2,549.988   \n",
       "\n",
       "             254        255        256        257        258        259  \\\n",
       "count  1,543.000  1,567.000  1,567.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       0.033      0.014      0.404      0.000      0.000      0.000   \n",
       "std        0.022      0.009      0.120      0.000      0.000      0.000   \n",
       "min        0.009      0.002      0.127      0.000      0.000      0.000   \n",
       "25%        0.025      0.005      0.308      0.000      0.000      0.000   \n",
       "50%        0.031      0.015      0.405      0.000      0.000      0.000   \n",
       "75%        0.038      0.021      0.481      0.000      0.000      0.000   \n",
       "max        0.452      0.079      0.925      0.000      0.000      0.000   \n",
       "\n",
       "             260        261        262        263        264        265  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "std        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "max        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             266        267        268        269        270        271  \\\n",
       "count  1,558.000  1,558.000  1,559.000  1,559.000  1,559.000  1,562.000   \n",
       "mean       0.000      0.000      0.071     19.505      3.778     29.260   \n",
       "std        0.000      0.000      0.030      7.344      1.152      8.402   \n",
       "min        0.000      0.000      0.020      6.098      1.302     15.547   \n",
       "25%        0.000      0.000      0.044     13.828      2.957     24.982   \n",
       "50%        0.000      0.000      0.071     17.977      3.704     28.773   \n",
       "75%        0.000      0.000      0.092     24.653      4.379     31.702   \n",
       "max        0.000      0.000      0.158     40.855     10.153    158.526   \n",
       "\n",
       "             272        273        274        275        276        277  \\\n",
       "count  1,561.000  1,560.000  1,553.000  1,553.000  1,553.000  1,553.000   \n",
       "mean      46.057     41.298     20.181    136.292      8.693      0.000   \n",
       "std       17.866     17.738      3.830     85.608    168.949      0.000   \n",
       "min       10.402      6.943      8.651      0.000      0.011      0.000   \n",
       "25%       30.014     27.093     18.247     81.216      0.045      0.000   \n",
       "50%       45.676     40.019     19.581    110.601      0.078      0.000   \n",
       "75%       59.595     54.277     22.097    162.038      0.145      0.000   \n",
       "max      132.648    122.117     43.574    659.170  3,332.596      0.000   \n",
       "\n",
       "             278        279        280        281        282        283  \\\n",
       "count  1,553.000  1,558.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       2.211      0.001      0.041      0.018      0.015      0.006   \n",
       "std        1.196      0.000      0.020      0.006      0.006      0.009   \n",
       "min        0.561      0.000      0.011      0.007      0.007      0.002   \n",
       "25%        1.698      0.001      0.028      0.014      0.012      0.003   \n",
       "50%        2.083      0.001      0.037      0.017      0.014      0.005   \n",
       "75%        2.514      0.001      0.046      0.021      0.017      0.007   \n",
       "max       32.171      0.003      0.188      0.075      0.060      0.308   \n",
       "\n",
       "             284        285        286        287        288        289  \\\n",
       "count  1,565.000  1,564.000  1,564.000  1,564.000  1,564.000  1,564.000   \n",
       "mean       2.804      0.000      2.120      4.260      0.368      0.004   \n",
       "std        5.864      0.000      0.963      9.764      7.386      0.003   \n",
       "min        0.505      0.000      0.461      0.728      0.051      0.001   \n",
       "25%        2.210      0.000      1.438      2.467      0.115      0.002   \n",
       "50%        2.658      0.000      1.875      3.360      0.139      0.004   \n",
       "75%        3.146      0.000      2.607      4.311      0.198      0.005   \n",
       "max      232.805      0.000      6.870    207.016    292.227      0.075   \n",
       "\n",
       "             290        291        292      293      294        295  \\\n",
       "count  1,564.000  1,557.000  1,567.000  138.000  138.000  1,565.000   \n",
       "mean       2.579      0.123      0.020    0.014  335.551    401.815   \n",
       "std        1.617      0.271      0.026    0.011  137.692    477.050   \n",
       "min        0.396      0.042      0.004    0.004   82.323      0.000   \n",
       "25%        2.092      0.065      0.013    0.009  229.809    185.090   \n",
       "50%        2.549      0.083      0.017    0.011  317.867    278.672   \n",
       "75%        3.025      0.118      0.024    0.015  403.989    428.555   \n",
       "max       59.519      4.420      0.692    0.083  879.226  3,933.755   \n",
       "\n",
       "             296         297         298        299        300        301  \\\n",
       "count  1,565.000   1,565.000   1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean     252.999   1,879.228   2,342.827      0.064      0.060      0.118   \n",
       "std      283.531   1,975.111   3,226.924      0.064      0.131      0.219   \n",
       "min        0.000       0.000       0.000      0.000      0.000      0.000   \n",
       "25%      130.220     603.033     210.937      0.041      0.030      0.059   \n",
       "50%      195.826   1,202.412     820.099      0.053      0.040      0.083   \n",
       "75%      273.953   2,341.289   3,190.616      0.069      0.052      0.116   \n",
       "max    2,005.874  15,559.952  18,520.468      0.526      1.031      1.812   \n",
       "\n",
       "             302        303        304        305        306        307  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,566.000  1,566.000   \n",
       "mean       0.910      0.403      0.040      0.132      0.265      0.049   \n",
       "std        0.332      0.198      0.015      0.065      0.057      0.025   \n",
       "min        0.310      0.112      0.011      0.014      0.117      0.003   \n",
       "25%        0.717      0.296      0.030      0.073      0.225      0.033   \n",
       "50%        0.860      0.381      0.039      0.137      0.264      0.045   \n",
       "75%        1.046      0.477      0.049      0.178      0.307      0.055   \n",
       "max        5.711      5.155      0.226      0.334      0.475      0.225   \n",
       "\n",
       "             308        309        310        311        312        313  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.129      0.218      0.129      0.305      0.097      0.160   \n",
       "std        0.027      0.034      0.027      0.043      0.029      0.117   \n",
       "min        0.055      0.091      0.055      0.181      0.033      0.022   \n",
       "25%        0.114      0.198      0.114      0.279      0.078      0.091   \n",
       "50%        0.130      0.219      0.130      0.303      0.098      0.121   \n",
       "75%        0.148      0.238      0.148      0.332      0.116      0.160   \n",
       "max        0.211      0.324      0.211      0.444      0.178      0.755   \n",
       "\n",
       "             314        315        316        317        318        319  \\\n",
       "count  1,543.000  1,543.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.000      0.000      0.000      5.977      0.173      3.189   \n",
       "std        0.000      0.000      0.000      1.019      0.072      1.216   \n",
       "min        0.000      0.000      0.000      2.788      0.028      0.985   \n",
       "25%        0.000      0.000      0.000      5.302      0.117      2.320   \n",
       "50%        0.000      0.000      0.000      5.832      0.163      2.899   \n",
       "75%        0.000      0.000      0.000      6.548      0.218      4.021   \n",
       "max        0.000      0.000      0.000     13.096      1.003     15.893   \n",
       "\n",
       "             320        321        322        323        324        325  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       7.916      0.043      2.264      0.000      5.393     13.332   \n",
       "std        2.179      0.032      2.117      0.000      2.519      6.616   \n",
       "min        1.657      0.008      0.611      0.000      1.710      2.235   \n",
       "25%        6.245      0.031      1.670      0.000      4.273      7.579   \n",
       "50%        8.389      0.040      2.078      0.000      5.459     12.505   \n",
       "75%        9.481      0.050      2.633      0.000      6.345     17.925   \n",
       "max       20.046      0.947     79.151      0.000     89.192     51.868   \n",
       "\n",
       "             326        327        328        329        330        331  \\\n",
       "count  1,566.000  1,563.000  1,563.000  1,563.000  1,563.000  1,563.000   \n",
       "mean       0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "std        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "max        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             332        333        334        335        336        337  \\\n",
       "count  1,563.000  1,560.000  1,561.000  1,561.000  1,561.000  1,560.000   \n",
       "mean       0.083      2.593      6.216      0.168      3.427      9.736   \n",
       "std        0.063      5.645      3.403      0.173      5.782      7.556   \n",
       "min        0.022      0.537      2.837      0.028      0.790      5.215   \n",
       "25%        0.069      1.547      5.454      0.089      2.036      8.289   \n",
       "50%        0.085      2.063      5.980      0.129      2.514      9.074   \n",
       "75%        0.096      2.791      6.550      0.210      3.360     10.042   \n",
       "max        1.096    174.894     90.516      3.413    172.712    214.863   \n",
       "\n",
       "             338        339        340        341        342        343  \\\n",
       "count  1,560.000  1,560.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       2.327      3.038      9.329     14.674      2.732      0.000   \n",
       "std        1.699      5.645      6.075    261.738      3.668      0.011   \n",
       "min        0.000      0.000      2.200      0.013      0.574      0.000   \n",
       "25%        1.543      1.901      7.589      0.035      1.912      0.000   \n",
       "50%        2.054      2.561      9.474      0.046      2.377      0.000   \n",
       "75%        2.785      3.405     10.440      0.067      2.985      0.000   \n",
       "max       38.900    196.688    197.499  5,043.879     97.709      0.447   \n",
       "\n",
       "             344        345      346      347        348        349  \\\n",
       "count  1,561.000  1,561.000  773.000  773.000  1,561.000  1,543.000   \n",
       "mean       6.199     23.217    7.958    5.770      0.009      0.025   \n",
       "std        5.372      8.895   17.513   17.077      0.352      0.012   \n",
       "min        1.256      2.056    1.769    1.018      0.000      0.010   \n",
       "25%        4.999     17.861    4.441    2.533      0.000      0.018   \n",
       "50%        6.006     23.215    5.567    3.046      0.000      0.023   \n",
       "75%        6.885     28.873    6.825    4.086      0.000      0.027   \n",
       "max      156.336     59.324  257.011  187.759     13.915      0.220   \n",
       "\n",
       "             350        351        352        353        354        355  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean       0.025      0.023      0.028      0.023      0.040      0.042   \n",
       "std        0.011      0.014      0.025      0.013      0.015      0.013   \n",
       "min        0.001      0.003      0.002      0.006      0.003      0.004   \n",
       "25%        0.020      0.015      0.017      0.016      0.030      0.035   \n",
       "50%        0.024      0.019      0.025      0.022      0.042      0.044   \n",
       "75%        0.029      0.029      0.034      0.027      0.050      0.050   \n",
       "max        0.134      0.291      0.619      0.143      0.153      0.134   \n",
       "\n",
       "             356        357        358      359        360        361  \\\n",
       "count  1,543.000  1,566.000  1,555.000  226.000  1,567.000  1,567.000   \n",
       "mean       0.035      1.299      0.001    0.002      0.020      0.003   \n",
       "std        0.022      0.387      0.001    0.000      0.007      0.020   \n",
       "min        0.004      0.380      0.000    0.002      0.008      0.000   \n",
       "25%        0.021      1.025      0.001    0.002      0.014      0.000   \n",
       "50%        0.029      1.255      0.001    0.002      0.020      0.001   \n",
       "75%        0.042      1.533      0.001    0.003      0.025      0.002   \n",
       "max        0.279      2.835      0.005    0.005      0.089      0.409   \n",
       "\n",
       "             362        363        364        365        366        367  \\\n",
       "count  1,567.000  1,516.000  1,516.000  1,561.000  1,565.000  1,565.000   \n",
       "mean      39.936      0.018    333.320      0.000      0.005      0.005   \n",
       "std       17.056      0.022    138.802      0.000      0.003      0.002   \n",
       "min       10.720      0.003     60.988      0.000      0.002      0.002   \n",
       "25%       32.169      0.009    228.683      0.000      0.004      0.004   \n",
       "50%       39.696      0.013    309.832      0.000      0.005      0.004   \n",
       "75%       47.079      0.019    412.330      0.000      0.006      0.005   \n",
       "max      547.172      0.416  1,072.203      0.000      0.037      0.039   \n",
       "\n",
       "             368        369        370        371        372        373  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       0.004      0.003      0.000      0.000      0.000      0.000   \n",
       "std        0.003      0.002      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.003      0.002      0.000      0.000      0.000      0.000   \n",
       "50%        0.003      0.003      0.000      0.000      0.000      0.000   \n",
       "75%        0.004      0.004      0.000      0.000      0.000      0.000   \n",
       "max        0.036      0.033      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             374        375        376        377        378        379  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,565.000  1,565.000  1,561.000   \n",
       "mean       0.000      0.000      0.000      0.002      0.002      0.000   \n",
       "std        0.000      0.000      0.000      0.001      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.001      0.001      0.000   \n",
       "50%        0.000      0.000      0.000      0.002      0.002      0.000   \n",
       "75%        0.000      0.000      0.000      0.002      0.002      0.000   \n",
       "max        0.000      0.000      0.000      0.008      0.008      0.000   \n",
       "\n",
       "             380        381        382      383      384      385      386  \\\n",
       "count  1,561.000  1,561.000  1,561.000  549.000  549.000  549.000  852.000   \n",
       "mean       0.000      0.000      0.000    0.002    0.541    1.285    0.011   \n",
       "std        0.000      0.000      0.000    0.027    1.341    3.168    0.014   \n",
       "min        0.000      0.000      0.000    0.000    0.087    0.338    0.000   \n",
       "25%        0.000      0.000      0.000    0.000    0.295    0.842    0.005   \n",
       "50%        0.000      0.000      0.000    0.001    0.373    1.106    0.007   \n",
       "75%        0.000      0.000      0.000    0.001    0.541    1.387    0.011   \n",
       "max        0.000      0.000      0.000    0.627   30.998   74.844    0.207   \n",
       "\n",
       "             387        388        389        390        391        392  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000  1,543.000   \n",
       "mean       0.008      0.000     35.155      0.001      1.432      0.011   \n",
       "std        0.015      0.005     17.227      0.012     20.326      0.007   \n",
       "min        0.001      0.000      6.310      0.000      0.305      0.003   \n",
       "25%        0.005      0.000     24.387      0.000      0.675      0.008   \n",
       "50%        0.007      0.000     32.531      0.000      0.877      0.010   \n",
       "75%        0.009      0.000     42.652      0.000      1.148      0.012   \n",
       "max        0.307      0.131    348.829      0.313    805.394      0.138   \n",
       "\n",
       "             393        394        395        396        397        398  \\\n",
       "count  1,567.000  1,567.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       0.005      0.134      0.000      0.000      0.000      0.000   \n",
       "std        0.003      0.038      0.000      0.000      0.000      0.000   \n",
       "min        0.001      0.034      0.000      0.000      0.000      0.000   \n",
       "25%        0.002      0.104      0.000      0.000      0.000      0.000   \n",
       "50%        0.005      0.134      0.000      0.000      0.000      0.000   \n",
       "75%        0.007      0.160      0.000      0.000      0.000      0.000   \n",
       "max        0.023      0.299      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             399        400        401        402        403        404  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "std        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "max        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             405        406        407        408        409        410  \\\n",
       "count  1,558.000  1,559.000  1,559.000  1,559.000  1,562.000  1,561.000   \n",
       "mean       0.000      0.024      6.731      1.232      5.341      4.580   \n",
       "std        0.000      0.011      2.830      0.365      2.578      1.777   \n",
       "min        0.000      0.006      2.054      0.424      2.738      1.216   \n",
       "25%        0.000      0.014      4.548      0.967      4.128      3.013   \n",
       "50%        0.000      0.024      5.920      1.240      4.922      4.490   \n",
       "75%        0.000      0.032      8.585      1.417      5.787      5.937   \n",
       "max        0.000      0.051     14.728      3.313     44.310      9.576   \n",
       "\n",
       "             411        412        413        414        415        416  \\\n",
       "count  1,560.000  1,553.000  1,553.000  1,553.000  1,553.000  1,553.000   \n",
       "mean       4.929      2.616     30.911     25.613      0.000      6.631   \n",
       "std        2.123      0.551     18.414     47.308      0.000      3.958   \n",
       "min        0.734      0.961      0.000      4.042      0.000      1.534   \n",
       "25%        3.265      2.321     18.408     11.376      0.000      4.927   \n",
       "50%        4.733      2.548     26.157     20.255      0.000      6.177   \n",
       "75%        6.458      2.853     38.140     29.307      0.000      7.571   \n",
       "max       13.807      6.215    128.282    899.119      0.000    116.862   \n",
       "\n",
       "             417        418        419        420        421        422  \\\n",
       "count  1,558.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       3.404      8.191    320.259    309.061      1.821      4.175   \n",
       "std        1.035      4.055    287.704    325.448      3.058      6.914   \n",
       "min        0.000      2.153      0.000      0.000      0.441      0.722   \n",
       "25%        2.660      5.766      0.000      0.000      1.030      3.184   \n",
       "50%        3.234      7.396    302.178    272.449      1.645      3.943   \n",
       "75%        4.011      9.169    524.002    582.935      2.215      4.784   \n",
       "max        9.690     39.038    999.316    998.681    111.496    273.095   \n",
       "\n",
       "             423        424        425        426        427        428  \\\n",
       "count  1,564.000  1,564.000  1,564.000  1,564.000  1,564.000  1,564.000   \n",
       "mean       0.000     77.660      3.315      6.796      1.234      4.059   \n",
       "std        0.000     32.597      6.325     23.258      0.996      3.042   \n",
       "min        0.000     23.020      0.487      1.467      0.363      0.664   \n",
       "25%        0.000     55.977      1.965      3.766      0.743      3.113   \n",
       "50%        0.000     69.905      2.667      4.764      1.135      3.941   \n",
       "75%        0.000     92.911      3.471      6.883      1.540      4.769   \n",
       "max        0.000    424.215    103.181    898.609     24.990    113.223   \n",
       "\n",
       "             429        430        431        432        433        434  \\\n",
       "count  1,557.000  1,567.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       4.221      4.172     18.422     22.358     99.368    205.519   \n",
       "std       10.633      6.435     36.060     36.395    126.189    225.779   \n",
       "min        1.120      0.784      0.000      0.000      0.000      0.000   \n",
       "25%        1.935      2.571      7.000     11.059     31.032     10.027   \n",
       "50%        2.534      3.454     11.106     16.381     57.969    151.116   \n",
       "75%        3.609      4.756     17.423     21.765    120.173    305.026   \n",
       "max      118.753    186.616    400.000    400.000    994.286    995.745   \n",
       "\n",
       "             435        436        437        438        439        440  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean      14.734      9.371      7.513      4.017     54.701     70.644   \n",
       "std       34.109     34.370     34.558      1.611     34.108     38.376   \n",
       "min        0.000      0.000      0.000      1.157      0.000     14.121   \n",
       "25%        7.551      3.494      1.951      3.071     36.290     48.174   \n",
       "50%       10.198      4.551      2.764      3.781     49.091     65.438   \n",
       "75%       12.754      5.823      3.822      4.679     66.667     84.973   \n",
       "max      400.000    400.000    400.000     32.274    851.613    657.762   \n",
       "\n",
       "             441        442        443        444        445        446  \\\n",
       "count  1,565.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean      11.527      0.802      1.345      0.634      0.895      0.647   \n",
       "std        6.169      0.184      0.659      0.144      0.156      0.141   \n",
       "min        1.097      0.351      0.097      0.217      0.334      0.309   \n",
       "25%        5.414      0.680      0.908      0.550      0.805      0.556   \n",
       "50%       12.086      0.808      1.265      0.643      0.903      0.651   \n",
       "75%       15.796      0.928      1.578      0.733      0.989      0.748   \n",
       "max       33.058      1.277      5.132      1.085      1.351      1.109   \n",
       "\n",
       "             447        448        449        450        451        452  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,543.000  1,543.000  1,566.000   \n",
       "mean       1.175      0.282      0.332      0.000      0.000      0.000   \n",
       "std        0.176      0.086      0.236      0.000      0.000      0.000   \n",
       "min        0.697      0.085      0.040      0.000      0.000      0.000   \n",
       "25%        1.047      0.226      0.188      0.000      0.000      0.000   \n",
       "50%        1.164      0.280      0.251      0.000      0.000      0.000   \n",
       "75%        1.272      0.339      0.351      0.000      0.000      0.000   \n",
       "max        1.764      0.508      1.475      0.000      0.000      0.000   \n",
       "\n",
       "             453        454        455        456        457        458  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       5.347      5.461      7.884      3.637     12.326      5.264   \n",
       "std        0.919      2.251      3.060      0.938      8.126      4.538   \n",
       "min        2.671      0.904      2.329      0.695      3.049      1.443   \n",
       "25%        4.764      3.748      5.807      2.900      8.817      3.828   \n",
       "50%        5.271      5.227      7.425      3.724     11.351      4.793   \n",
       "75%        5.913      6.902      9.577      4.342     14.388      6.089   \n",
       "max       13.978     34.490     42.070     10.184    232.126    164.109   \n",
       "\n",
       "             459        460        461        462        463        464  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,563.000  1,563.000   \n",
       "mean       0.000      2.838     29.197      0.000      0.000      0.000   \n",
       "std        0.000      1.346     13.335      0.000      0.000      0.000   \n",
       "min        0.000      0.991      7.953      0.000      0.000      0.000   \n",
       "25%        0.000      2.291     20.222      0.000      0.000      0.000   \n",
       "50%        0.000      2.830     26.168      0.000      0.000      0.000   \n",
       "75%        0.000      3.309     35.279      0.000      0.000      0.000   \n",
       "max        0.000     47.777    149.385      0.000      0.000      0.000   \n",
       "\n",
       "             465        466        467        468        469        470  \\\n",
       "count  1,563.000  1,563.000  1,563.000  1,563.000  1,560.000  1,561.000   \n",
       "mean       0.000      0.000      0.000      6.252    224.173      5.662   \n",
       "std        0.000      0.000      0.000      8.674    230.767      3.152   \n",
       "min        0.000      0.000      0.000      1.716      0.000      2.601   \n",
       "25%        0.000      0.000      0.000      4.697     38.473      4.847   \n",
       "50%        0.000      0.000      0.000      5.645    150.340      5.472   \n",
       "75%        0.000      0.000      0.000      6.387    335.922      6.006   \n",
       "max        0.000      0.000      0.000    109.007    999.877     77.801   \n",
       "\n",
       "             471        472        473        474        475        476  \\\n",
       "count  1,561.000  1,561.000  1,560.000  1,560.000  1,560.000  1,561.000   \n",
       "mean       5.368      9.639    137.888     39.427     37.637      4.263   \n",
       "std        4.983     10.174     47.698     22.457     24.823      2.611   \n",
       "min        0.833      2.403     11.500      0.000      0.000      1.101   \n",
       "25%        2.823      5.807    105.525     24.901     23.157      3.494   \n",
       "50%        4.061      7.396    138.255     34.247     32.820      4.276   \n",
       "75%        7.007      9.720    168.410     47.728     45.169      4.742   \n",
       "max       87.135    212.656    492.772    358.950    415.435     79.116   \n",
       "\n",
       "             477        478        479        480        481        482  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean      20.132      6.258      0.128      3.283     75.538      0.000   \n",
       "std       14.940     10.185      5.062      2.639     35.752      0.000   \n",
       "min        0.000      1.687      0.000      0.646      8.841      0.000   \n",
       "25%       11.577      4.105      0.000      2.628     52.895      0.000   \n",
       "50%       15.974      5.242      0.000      3.184     70.434      0.000   \n",
       "75%       23.737      6.704      0.000      3.625     93.120      0.000   \n",
       "max      274.887    289.826    200.000     63.334    221.975      0.000   \n",
       "\n",
       "             483        484        485        486        487        488  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean     318.418    206.564    215.289    201.112    302.506    239.455   \n",
       "std      281.011    192.864    213.127    218.690    287.364    263.838   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000     81.316     76.455     50.384      0.000     55.555   \n",
       "50%      293.519    148.317    138.775    112.953    249.927    112.275   \n",
       "75%      514.586    262.865    294.667    288.893    501.607    397.506   \n",
       "max      999.413    989.474    996.859    994.000    999.491    995.745   \n",
       "\n",
       "             489        490        491        492      493        494  \\\n",
       "count  1,543.000  1,543.000  1,566.000  1,555.000  226.000  1,567.000   \n",
       "mean     352.616    272.170     51.354      2.443    8.171      2.530   \n",
       "std      252.044    228.047     18.049      1.224    1.759      0.974   \n",
       "min        0.000      0.000     13.723      0.556    4.888      0.833   \n",
       "25%      139.914    112.859     38.391      1.747    6.925      1.664   \n",
       "50%      348.529    219.487     48.557      2.251    8.009      2.529   \n",
       "75%      510.647    377.144     61.495      2.840    9.079      3.199   \n",
       "max      997.519    994.004    142.844     12.770   21.044      9.402   \n",
       "\n",
       "             495        496        497        498        499        500  \\\n",
       "count  1,567.000  1,567.000  1,516.000  1,516.000  1,561.000  1,565.000   \n",
       "mean       0.956      6.808     29.866     11.821      0.000    263.196   \n",
       "std        6.615      3.260     24.622      4.957      0.000    324.771   \n",
       "min        0.034      1.772      4.814      1.950      0.000      0.000   \n",
       "25%        0.139      5.275     16.342      8.150      0.000      0.000   \n",
       "50%        0.233      6.608     22.039     10.907      0.000      0.000   \n",
       "75%        0.563      7.897     32.438     14.469      0.000    536.205   \n",
       "max      127.573    107.693    219.644     40.282      0.000  1,000.000   \n",
       "\n",
       "             501        502        503        504        505        506  \\\n",
       "count  1,565.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean     240.981      0.000      0.000      0.000      0.000      0.000   \n",
       "std      323.003      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "75%      505.401      0.000      0.000      0.000      0.000      0.000   \n",
       "max      999.234      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             507        508        509        510        511        512  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,565.000  1,565.000   \n",
       "mean       0.000      0.000      0.000      0.000     55.764    275.979   \n",
       "std        0.000      0.000      0.000      0.000     37.692    329.665   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000     35.322      0.000   \n",
       "50%        0.000      0.000      0.000      0.000     46.986      0.000   \n",
       "75%        0.000      0.000      0.000      0.000     64.249    555.294   \n",
       "max        0.000      0.000      0.000      0.000    451.485  1,000.000   \n",
       "\n",
       "             513        514        515        516      517      518      519  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  549.000  549.000  549.000   \n",
       "mean       0.000      0.000      0.000      0.000    0.679    1.739    1.806   \n",
       "std        0.000      0.000      0.000      0.000   10.784    4.891    4.716   \n",
       "min        0.000      0.000      0.000      0.000    0.029    0.288    0.467   \n",
       "25%        0.000      0.000      0.000      0.000    0.121    0.890    1.171   \n",
       "50%        0.000      0.000      0.000      0.000    0.175    1.154    1.589   \n",
       "75%        0.000      0.000      0.000      0.000    0.265    1.760    1.933   \n",
       "max        0.000      0.000      0.000      0.000  252.860  113.276  111.350   \n",
       "\n",
       "           520        521        522        523        524        525  \\\n",
       "count  852.000  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000   \n",
       "mean    11.728      2.696     11.610     14.729      0.454      5.688   \n",
       "std     15.814      5.702    103.123      7.104      4.148     20.663   \n",
       "min      0.000      0.312      0.000      2.681      0.026      1.310   \n",
       "25%      4.160      1.552      0.000     10.183      0.073      3.770   \n",
       "50%      5.833      2.221      0.000     13.743      0.100      4.877   \n",
       "75%     10.972      2.904      0.000     17.809      0.133      6.451   \n",
       "max    184.349    111.737  1,000.000    137.984    111.333    818.000   \n",
       "\n",
       "             526        527        528        529        530        531  \\\n",
       "count  1,543.000  1,567.000  1,567.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       5.560      1.443      6.396      0.000      0.000      0.000   \n",
       "std        3.920      0.958      1.889      0.000      0.000      0.000   \n",
       "min        1.540      0.171      2.170      0.000      0.000      0.000   \n",
       "25%        4.101      0.484      4.895      0.000      0.000      0.000   \n",
       "50%        5.134      1.550      6.411      0.000      0.000      0.000   \n",
       "75%        6.329      2.212      7.594      0.000      0.000      0.000   \n",
       "max       80.041      8.204     14.448      0.000      0.000      0.000   \n",
       "\n",
       "             532        533        534        535        536        537  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "std        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "50%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "max        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "\n",
       "             538        539        540        541        542        543  \\\n",
       "count  1,558.000  1,558.000  1,559.000  1,559.000  1,559.000  1,565.000   \n",
       "mean       0.000      0.000      3.034      1.943      9.612      0.111   \n",
       "std        0.000      0.000      1.253      0.732      2.896      0.003   \n",
       "min        0.000      0.000      0.852      0.614      3.276      0.105   \n",
       "25%        0.000      0.000      1.890      1.385      7.496      0.110   \n",
       "50%        0.000      0.000      3.055      1.786      9.459      0.110   \n",
       "75%        0.000      0.000      3.947      2.458     11.238      0.113   \n",
       "max        0.000      0.000      6.580      4.082     25.779      0.118   \n",
       "\n",
       "             544        545        546        547        548        549  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,307.000  1,307.000  1,307.000   \n",
       "mean       0.008      0.003      7.611      1.040    403.546     75.680   \n",
       "std        0.002      0.000      1.316      0.389      5.064      3.391   \n",
       "min        0.005      0.002      4.429      0.444    372.822     71.038   \n",
       "25%        0.008      0.002      7.116      0.797    400.694     73.254   \n",
       "50%        0.008      0.003      7.116      0.911    403.122     74.084   \n",
       "75%        0.009      0.003      8.021      1.286    407.431     78.397   \n",
       "max        0.024      0.005     21.044      3.979    421.702     83.720   \n",
       "\n",
       "             550        551        552        553        554        555  \\\n",
       "count  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000   \n",
       "mean       0.663     17.013      1.231      0.277      7.704      0.504   \n",
       "std        0.673      4.967      1.361      0.276      2.193      0.599   \n",
       "min        0.045      6.110      0.120      0.019      2.786      0.052   \n",
       "25%        0.226     14.530      0.870      0.095      6.738      0.344   \n",
       "50%        0.471     16.340      1.150      0.198      7.428      0.479   \n",
       "75%        0.850     19.035      1.370      0.358      8.637      0.562   \n",
       "max        7.066    131.680     39.330      2.718     56.930     17.478   \n",
       "\n",
       "             556        557        558        559        560        561  \\\n",
       "count  1,307.000  1,307.000  1,307.000  1,566.000  1,566.000  1,566.000   \n",
       "mean      57.747      4.217      1.623      0.995      0.326      0.072   \n",
       "std       35.208      1.280      1.870      0.084      0.201      0.052   \n",
       "min        4.827      1.497      0.165      0.892      0.070      0.018   \n",
       "25%       27.018      3.625      1.183      0.955      0.150      0.036   \n",
       "50%       54.442      4.067      1.530      0.973      0.291      0.059   \n",
       "75%       74.629      4.703      1.816      1.001      0.444      0.089   \n",
       "max      303.550     35.320     54.292      1.512      1.074      0.446   \n",
       "\n",
       "             562        563        564        565        566        567  \\\n",
       "count  1,566.000  1,294.000  1,294.000  1,294.000  1,294.000  1,294.000   \n",
       "mean      32.285    262.730      0.680      6.445      0.146      2.611   \n",
       "std       19.026      7.631      0.122      2.634      0.081      1.033   \n",
       "min        7.237    242.286      0.305      0.970      0.022      0.412   \n",
       "25%       15.762    259.973      0.567      4.980      0.088      2.090   \n",
       "50%       29.731    264.272      0.651      5.160      0.120      2.150   \n",
       "75%       44.113    265.707      0.769      7.800      0.186      3.099   \n",
       "max      101.115    311.404      1.299     32.580      0.689     14.014   \n",
       "\n",
       "             568        569        570        571        572        573  \\\n",
       "count  1,294.000  1,294.000  1,294.000  1,567.000  1,567.000  1,567.000   \n",
       "mean       0.060      2.452     21.118    530.524      2.102     28.450   \n",
       "std        0.033      0.997     10.213     17.500      0.275     86.305   \n",
       "min        0.009      0.371      3.250    317.196      0.980      3.540   \n",
       "25%        0.038      1.884     15.466    530.703      1.983      7.500   \n",
       "50%        0.049      2.000     16.988    532.398      2.119      8.650   \n",
       "75%        0.075      2.971     24.772    534.356      2.291     10.130   \n",
       "max        0.293     12.746     84.802    589.508      2.740    454.560   \n",
       "\n",
       "             574        575        576        577        578      579  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000  618.000   \n",
       "mean       0.346      9.162      0.105      5.564     16.642    0.022   \n",
       "std        0.248     26.920      0.068     16.921     12.485    0.012   \n",
       "min        0.067      1.040      0.023      0.664      4.582   -0.017   \n",
       "25%        0.242      2.568      0.075      1.408     11.502    0.014   \n",
       "50%        0.293      2.976      0.089      1.625     13.818    0.020   \n",
       "75%        0.367      3.493      0.112      1.902     17.081    0.028   \n",
       "max        2.197    170.020      0.550     90.424     96.960    0.103   \n",
       "\n",
       "           580      581      582        583        584        585        586  \\\n",
       "count  618.000  618.000  618.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean     0.017    0.005   97.934      0.500      0.015      0.004      3.068   \n",
       "std      0.010    0.003   87.521      0.003      0.017      0.004      3.578   \n",
       "min      0.003    0.001    0.000      0.478      0.006      0.002      1.198   \n",
       "25%      0.011    0.003   46.185      0.498      0.012      0.003      2.307   \n",
       "50%      0.015    0.005   72.289      0.500      0.014      0.004      2.758   \n",
       "75%      0.020    0.006  116.539      0.502      0.017      0.004      3.295   \n",
       "max      0.080    0.029  737.305      0.510      0.477      0.104     99.303   \n",
       "\n",
       "             587        588        589        590  \n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  \n",
       "mean       0.021      0.016      0.005     99.670  \n",
       "std        0.012      0.009      0.003     93.892  \n",
       "min       -0.017      0.003      0.001      0.000  \n",
       "25%        0.013      0.011      0.003     44.369  \n",
       "50%        0.021      0.015      0.005     71.900  \n",
       "75%        0.028      0.020      0.006    114.750  \n",
       "max        0.103      0.080      0.029    737.305  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SECOM_df.describe().map('{:,.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7c48b0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Outcome and Explanatory Variables.\n",
    "\n",
    "Y = SECOM_df['Target']\n",
    "X = SECOM_df.drop(['Target','Time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "455de502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename Outcome Variables to Binary.\n",
    "Y = Y.replace({1: 1, -1: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e950d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per feature:\n",
      " 1         6\n",
      "2         7\n",
      "3        14\n",
      "4        14\n",
      "5        14\n",
      "6        14\n",
      "7        14\n",
      "8         9\n",
      "9         2\n",
      "10        2\n",
      "11        2\n",
      "12        2\n",
      "13        2\n",
      "14        3\n",
      "15        3\n",
      "16        3\n",
      "17        3\n",
      "18        3\n",
      "19        3\n",
      "20       10\n",
      "22        2\n",
      "23        2\n",
      "24        2\n",
      "25        2\n",
      "26        2\n",
      "27        2\n",
      "28        2\n",
      "29        2\n",
      "30        2\n",
      "31        2\n",
      "32        2\n",
      "33        1\n",
      "34        1\n",
      "35        1\n",
      "36        1\n",
      "37        1\n",
      "38        1\n",
      "39        1\n",
      "40        1\n",
      "41       24\n",
      "42       24\n",
      "43        1\n",
      "44        1\n",
      "45        1\n",
      "46        1\n",
      "47        1\n",
      "48        1\n",
      "49        1\n",
      "50        1\n",
      "51        1\n",
      "52        1\n",
      "53        1\n",
      "54        4\n",
      "55        4\n",
      "56        4\n",
      "57        4\n",
      "58        4\n",
      "59        4\n",
      "60        7\n",
      "61        6\n",
      "62        6\n",
      "63        6\n",
      "64        7\n",
      "65        7\n",
      "66        7\n",
      "67        6\n",
      "68        6\n",
      "69        6\n",
      "70        6\n",
      "71        6\n",
      "72        6\n",
      "73      794\n",
      "74      794\n",
      "75        6\n",
      "76       24\n",
      "77       24\n",
      "78       24\n",
      "79       24\n",
      "80       24\n",
      "81       24\n",
      "82       24\n",
      "83       24\n",
      "84        1\n",
      "85       12\n",
      "86     1341\n",
      "90       51\n",
      "91       51\n",
      "92        6\n",
      "93        2\n",
      "94        2\n",
      "95        6\n",
      "96        6\n",
      "97        6\n",
      "98        6\n",
      "99        6\n",
      "100       6\n",
      "101       6\n",
      "102       6\n",
      "103       6\n",
      "104       2\n",
      "105       2\n",
      "106       6\n",
      "107       6\n",
      "108       6\n",
      "109       6\n",
      "110    1018\n",
      "111    1018\n",
      "112    1018\n",
      "113     715\n",
      "119      24\n",
      "122       9\n",
      "123       9\n",
      "124       9\n",
      "125       9\n",
      "126       9\n",
      "127       9\n",
      "128       9\n",
      "129       9\n",
      "130       9\n",
      "131       9\n",
      "132       9\n",
      "133       8\n",
      "134       8\n",
      "135       8\n",
      "136       5\n",
      "137       6\n",
      "138       7\n",
      "139      14\n",
      "140      14\n",
      "141      14\n",
      "142      14\n",
      "143      14\n",
      "144       9\n",
      "145       2\n",
      "146       2\n",
      "147       2\n",
      "148       2\n",
      "149       2\n",
      "150       3\n",
      "151       3\n",
      "152       3\n",
      "153       3\n",
      "154       3\n",
      "155       3\n",
      "156      10\n",
      "158    1429\n",
      "159    1429\n",
      "160       2\n",
      "161       2\n",
      "162       2\n",
      "163       2\n",
      "164       2\n",
      "165       2\n",
      "166       2\n",
      "167       2\n",
      "168       2\n",
      "169       2\n",
      "170       2\n",
      "171       1\n",
      "172       1\n",
      "173       1\n",
      "174       1\n",
      "175       1\n",
      "176       1\n",
      "177       1\n",
      "178       1\n",
      "179      24\n",
      "180       1\n",
      "181       1\n",
      "182       1\n",
      "183       1\n",
      "184       1\n",
      "185       1\n",
      "186       1\n",
      "187       1\n",
      "188       1\n",
      "189       1\n",
      "190       1\n",
      "191       4\n",
      "192       4\n",
      "193       4\n",
      "194       4\n",
      "195       4\n",
      "196       4\n",
      "197       7\n",
      "198       6\n",
      "199       6\n",
      "200       6\n",
      "201       7\n",
      "202       7\n",
      "203       7\n",
      "204       6\n",
      "205       6\n",
      "206       6\n",
      "207       6\n",
      "208       6\n",
      "209       6\n",
      "210       6\n",
      "211      24\n",
      "212      24\n",
      "213      24\n",
      "214      24\n",
      "215      24\n",
      "216      24\n",
      "217      24\n",
      "218      24\n",
      "219       1\n",
      "220      12\n",
      "221    1341\n",
      "225      51\n",
      "226      51\n",
      "227       6\n",
      "228       2\n",
      "229       2\n",
      "230       6\n",
      "231       6\n",
      "232       6\n",
      "233       6\n",
      "234       6\n",
      "235       6\n",
      "236       6\n",
      "237       6\n",
      "238       6\n",
      "239       2\n",
      "240       2\n",
      "241       6\n",
      "242       6\n",
      "243       6\n",
      "244       6\n",
      "245    1018\n",
      "246    1018\n",
      "247    1018\n",
      "248     715\n",
      "254      24\n",
      "257       9\n",
      "258       9\n",
      "259       9\n",
      "260       9\n",
      "261       9\n",
      "262       9\n",
      "263       9\n",
      "264       9\n",
      "265       9\n",
      "266       9\n",
      "267       9\n",
      "268       8\n",
      "269       8\n",
      "270       8\n",
      "271       5\n",
      "272       6\n",
      "273       7\n",
      "274      14\n",
      "275      14\n",
      "276      14\n",
      "277      14\n",
      "278      14\n",
      "279       9\n",
      "280       2\n",
      "281       2\n",
      "282       2\n",
      "283       2\n",
      "284       2\n",
      "285       3\n",
      "286       3\n",
      "287       3\n",
      "288       3\n",
      "289       3\n",
      "290       3\n",
      "291      10\n",
      "293    1429\n",
      "294    1429\n",
      "295       2\n",
      "296       2\n",
      "297       2\n",
      "298       2\n",
      "299       2\n",
      "300       2\n",
      "301       2\n",
      "302       2\n",
      "303       2\n",
      "304       2\n",
      "305       2\n",
      "306       1\n",
      "307       1\n",
      "308       1\n",
      "309       1\n",
      "310       1\n",
      "311       1\n",
      "312       1\n",
      "313       1\n",
      "314      24\n",
      "315      24\n",
      "316       1\n",
      "317       1\n",
      "318       1\n",
      "319       1\n",
      "320       1\n",
      "321       1\n",
      "322       1\n",
      "323       1\n",
      "324       1\n",
      "325       1\n",
      "326       1\n",
      "327       4\n",
      "328       4\n",
      "329       4\n",
      "330       4\n",
      "331       4\n",
      "332       4\n",
      "333       7\n",
      "334       6\n",
      "335       6\n",
      "336       6\n",
      "337       7\n",
      "338       7\n",
      "339       7\n",
      "340       6\n",
      "341       6\n",
      "342       6\n",
      "343       6\n",
      "344       6\n",
      "345       6\n",
      "346     794\n",
      "347     794\n",
      "348       6\n",
      "349      24\n",
      "350      24\n",
      "351      24\n",
      "352      24\n",
      "353      24\n",
      "354      24\n",
      "355      24\n",
      "356      24\n",
      "357       1\n",
      "358      12\n",
      "359    1341\n",
      "363      51\n",
      "364      51\n",
      "365       6\n",
      "366       2\n",
      "367       2\n",
      "368       6\n",
      "369       6\n",
      "370       6\n",
      "371       6\n",
      "372       6\n",
      "373       6\n",
      "374       6\n",
      "375       6\n",
      "376       6\n",
      "377       2\n",
      "378       2\n",
      "379       6\n",
      "380       6\n",
      "381       6\n",
      "382       6\n",
      "383    1018\n",
      "384    1018\n",
      "385    1018\n",
      "386     715\n",
      "392      24\n",
      "395       9\n",
      "396       9\n",
      "397       9\n",
      "398       9\n",
      "399       9\n",
      "400       9\n",
      "401       9\n",
      "402       9\n",
      "403       9\n",
      "404       9\n",
      "405       9\n",
      "406       8\n",
      "407       8\n",
      "408       8\n",
      "409       5\n",
      "410       6\n",
      "411       7\n",
      "412      14\n",
      "413      14\n",
      "414      14\n",
      "415      14\n",
      "416      14\n",
      "417       9\n",
      "418       2\n",
      "419       2\n",
      "420       2\n",
      "421       2\n",
      "422       2\n",
      "423       3\n",
      "424       3\n",
      "425       3\n",
      "426       3\n",
      "427       3\n",
      "428       3\n",
      "429      10\n",
      "431       2\n",
      "432       2\n",
      "433       2\n",
      "434       2\n",
      "435       2\n",
      "436       2\n",
      "437       2\n",
      "438       2\n",
      "439       2\n",
      "440       2\n",
      "441       2\n",
      "442       1\n",
      "443       1\n",
      "444       1\n",
      "445       1\n",
      "446       1\n",
      "447       1\n",
      "448       1\n",
      "449       1\n",
      "450      24\n",
      "451      24\n",
      "452       1\n",
      "453       1\n",
      "454       1\n",
      "455       1\n",
      "456       1\n",
      "457       1\n",
      "458       1\n",
      "459       1\n",
      "460       1\n",
      "461       1\n",
      "462       1\n",
      "463       4\n",
      "464       4\n",
      "465       4\n",
      "466       4\n",
      "467       4\n",
      "468       4\n",
      "469       7\n",
      "470       6\n",
      "471       6\n",
      "472       6\n",
      "473       7\n",
      "474       7\n",
      "475       7\n",
      "476       6\n",
      "477       6\n",
      "478       6\n",
      "479       6\n",
      "480       6\n",
      "481       6\n",
      "482       6\n",
      "483      24\n",
      "484      24\n",
      "485      24\n",
      "486      24\n",
      "487      24\n",
      "488      24\n",
      "489      24\n",
      "490      24\n",
      "491       1\n",
      "492      12\n",
      "493    1341\n",
      "497      51\n",
      "498      51\n",
      "499       6\n",
      "500       2\n",
      "501       2\n",
      "502       6\n",
      "503       6\n",
      "504       6\n",
      "505       6\n",
      "506       6\n",
      "507       6\n",
      "508       6\n",
      "509       6\n",
      "510       6\n",
      "511       2\n",
      "512       2\n",
      "513       6\n",
      "514       6\n",
      "515       6\n",
      "516       6\n",
      "517    1018\n",
      "518    1018\n",
      "519    1018\n",
      "520     715\n",
      "526      24\n",
      "529       9\n",
      "530       9\n",
      "531       9\n",
      "532       9\n",
      "533       9\n",
      "534       9\n",
      "535       9\n",
      "536       9\n",
      "537       9\n",
      "538       9\n",
      "539       9\n",
      "540       8\n",
      "541       8\n",
      "542       8\n",
      "543       2\n",
      "544       2\n",
      "545       2\n",
      "546       2\n",
      "547     260\n",
      "548     260\n",
      "549     260\n",
      "550     260\n",
      "551     260\n",
      "552     260\n",
      "553     260\n",
      "554     260\n",
      "555     260\n",
      "556     260\n",
      "557     260\n",
      "558     260\n",
      "559       1\n",
      "560       1\n",
      "561       1\n",
      "562       1\n",
      "563     273\n",
      "564     273\n",
      "565     273\n",
      "566     273\n",
      "567     273\n",
      "568     273\n",
      "569     273\n",
      "570     273\n",
      "579     949\n",
      "580     949\n",
      "581     949\n",
      "582     949\n",
      "583       1\n",
      "584       1\n",
      "585       1\n",
      "586       1\n",
      "587       1\n",
      "588       1\n",
      "589       1\n",
      "590       1\n",
      "dtype: int64\n",
      "Total features with missing values: 538\n",
      "Total missing values: 41951\n",
      "Total proportion of missing values:  0.045 \n"
     ]
    }
   ],
   "source": [
    "# Assess Missing Values per Feature.\n",
    "missing_counts = X.isna().sum()\n",
    "\n",
    "print(\"Missing values per feature:\\n\", missing_counts[missing_counts>0])\n",
    "\n",
    "print(f\"Total features with missing values: {sum(missing_counts>0)}\")\n",
    "\n",
    "print(f\"Total missing values: {sum(missing_counts)}\")\n",
    "\n",
    "print(f\"Total proportion of missing values: {sum(missing_counts)/(1567*590): .3f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01dac6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features grouped by number of missing values:\n",
      "Missing 1 values: ['33', '34', '35', '36', '37', '38', '39', '40', '43', '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '84', '171', '172', '173', '174', '175', '176', '177', '178', '180', '181', '182', '183', '184', '185', '186', '187', '188', '189', '190', '219', '306', '307', '308', '309', '310', '311', '312', '313', '316', '317', '318', '319', '320', '321', '322', '323', '324', '325', '326', '357', '442', '443', '444', '445', '446', '447', '448', '449', '452', '453', '454', '455', '456', '457', '458', '459', '460', '461', '462', '491', '559', '560', '561', '562', '583', '584', '585', '586', '587', '588', '589', '590']\n",
      "Missing 2 values: ['9', '10', '11', '12', '13', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '93', '94', '104', '105', '145', '146', '147', '148', '149', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '228', '229', '239', '240', '280', '281', '282', '283', '284', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '366', '367', '377', '378', '418', '419', '420', '421', '422', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '500', '501', '511', '512', '543', '544', '545', '546']\n",
      "Missing 3 values: ['14', '15', '16', '17', '18', '19', '150', '151', '152', '153', '154', '155', '285', '286', '287', '288', '289', '290', '423', '424', '425', '426', '427', '428']\n",
      "Missing 4 values: ['54', '55', '56', '57', '58', '59', '191', '192', '193', '194', '195', '196', '327', '328', '329', '330', '331', '332', '463', '464', '465', '466', '467', '468']\n",
      "Missing 5 values: ['136', '271', '409']\n",
      "Missing 6 values: ['1', '61', '62', '63', '67', '68', '69', '70', '71', '72', '75', '92', '95', '96', '97', '98', '99', '100', '101', '102', '103', '106', '107', '108', '109', '137', '198', '199', '200', '204', '205', '206', '207', '208', '209', '210', '227', '230', '231', '232', '233', '234', '235', '236', '237', '238', '241', '242', '243', '244', '272', '334', '335', '336', '340', '341', '342', '343', '344', '345', '348', '365', '368', '369', '370', '371', '372', '373', '374', '375', '376', '379', '380', '381', '382', '410', '470', '471', '472', '476', '477', '478', '479', '480', '481', '482', '499', '502', '503', '504', '505', '506', '507', '508', '509', '510', '513', '514', '515', '516']\n",
      "Missing 7 values: ['2', '60', '64', '65', '66', '138', '197', '201', '202', '203', '273', '333', '337', '338', '339', '411', '469', '473', '474', '475']\n",
      "Missing 8 values: ['133', '134', '135', '268', '269', '270', '406', '407', '408', '540', '541', '542']\n",
      "Missing 9 values: ['8', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '144', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '279', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '417', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539']\n",
      "Missing 10 values: ['20', '156', '291', '429']\n",
      "Missing 12 values: ['85', '220', '358', '492']\n",
      "Missing 14 values: ['3', '4', '5', '6', '7', '139', '140', '141', '142', '143', '274', '275', '276', '277', '278', '412', '413', '414', '415', '416']\n",
      "Missing 24 values: ['41', '42', '76', '77', '78', '79', '80', '81', '82', '83', '119', '179', '211', '212', '213', '214', '215', '216', '217', '218', '254', '314', '315', '349', '350', '351', '352', '353', '354', '355', '356', '392', '450', '451', '483', '484', '485', '486', '487', '488', '489', '490', '526']\n",
      "Missing 51 values: ['90', '91', '225', '226', '363', '364', '497', '498']\n",
      "Missing 260 values: ['547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558']\n",
      "Missing 273 values: ['563', '564', '565', '566', '567', '568', '569', '570']\n",
      "Missing 715 values: ['113', '248', '386', '520']\n",
      "Missing 794 values: ['73', '74', '346', '347']\n",
      "Missing 949 values: ['579', '580', '581', '582']\n",
      "Missing 1018 values: ['110', '111', '112', '245', '246', '247', '383', '384', '385', '517', '518', '519']\n",
      "Missing 1341 values: ['86', '221', '359', '493']\n",
      "Missing 1429 values: ['158', '159', '293', '294']\n"
     ]
    }
   ],
   "source": [
    "# Group Features by Number of Missing Values to Explore Patterns.\n",
    "\n",
    "missing_groups = missing_counts.groupby(missing_counts).groups\n",
    "\n",
    "print(\"\\nFeatures grouped by number of missing values:\")\n",
    "for missing_value, features in missing_groups.items():\n",
    "    if missing_value > 0:\n",
    "        print(f\"Missing {missing_value} values: {list(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7d166310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Descriptive Statistics to Identify Constant Features.\n",
    "\n",
    "desc_stats = X.describe().T\n",
    "desc_stats['std'] = X.std()\n",
    "desc_stats['unique_count'] = X.nunique()\n",
    "desc_stats['missing_count'] = missing_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "929a87be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features with zero standard deviation (constant values):\n",
      " Index(['6', '14', '43', '50', '53', '70', '98', '142', '150', '179',\n",
      "       ...\n",
      "       '530', '531', '532', '533', '534', '535', '536', '537', '538', '539'],\n",
      "      dtype='object', length=116)\n"
     ]
    }
   ],
   "source": [
    "# Identify Features with Zero Standard Deviation (Constant Values).\n",
    "\n",
    "constant_features = desc_stats[desc_stats['std'] == 0].index\n",
    "\n",
    "print(f\"\\nFeatures with zero standard deviation (constant values):\\n\", constant_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c713903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confirmed constant features (min = max):\n",
      " Index(['6', '14', '43', '50', '53', '70', '98', '142', '150', '179',\n",
      "       ...\n",
      "       '530', '531', '532', '533', '534', '535', '536', '537', '538', '539'],\n",
      "      dtype='object', length=116)\n"
     ]
    }
   ],
   "source": [
    "# Cross-Check that Minimum equals Maximum Values for Confirmation.\n",
    "\n",
    "constant_features_confirmed = constant_features[(desc_stats.loc[constant_features, 'min'] == desc_stats.loc[constant_features, 'max'])]\n",
    "\n",
    "print(f\"Confirmed constant features (min = max):\\n\", constant_features_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0fb3437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6435\n",
      "Feature 6 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 14 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 43 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 50 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 53 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 70 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 98 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6435\n",
      "Feature 142 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 150 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  0.9659\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.82, p-value = 0.3665\n",
      "Feature 179 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 180 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 187 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 190 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 191 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 192 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 193 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 194 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 195 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 227 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 230 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 231 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 232 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 233 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 234 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 235 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 236 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 237 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 238 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 241 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 242 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 243 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 244 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 257 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 258 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 259 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 260 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 261 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 262 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 263 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 264 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 265 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 266 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 267 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6435\n",
      "Feature 277 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 285 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  0.9659\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.82, p-value = 0.3665\n",
      "Feature 314 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  0.9659\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.82, p-value = 0.3665\n",
      "Feature 315 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 316 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 323 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 326 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 327 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 328 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 329 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 330 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 331 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 365 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 370 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 371 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 372 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 373 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 374 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 375 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 376 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 379 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 380 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 381 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 382 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 395 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 396 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 397 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 398 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 399 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 400 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 401 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 402 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 403 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 404 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 405 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6435\n",
      "Feature 415 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 423 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  0.9659\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.82, p-value = 0.3665\n",
      "Feature 450 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  0.9659\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.82, p-value = 0.3665\n",
      "Feature 451 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 452 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 459 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 462 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 463 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 464 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 465 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 466 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.22, p-value = 0.6372\n",
      "Feature 467 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 482 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 499 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 502 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 503 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 504 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 505 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 506 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 507 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 508 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 509 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 510 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 513 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 514 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 515 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.00, p-value = 1.0000\n",
      "Feature 516 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 529 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 530 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 531 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 532 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 533 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 534 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 535 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 536 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 537 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 538 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Chi-square test for constant value vs. expected baseline: chi2 =  0.00, p-value =  1.0000\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.02, p-value = 0.8960\n",
      "Feature 539 has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\n",
      "Features to remove (constant and not predictive):\n",
      " ['6', '14', '43', '50', '53', '70', '98', '142', '150', '179', '180', '187', '190', '191', '192', '193', '194', '195', '227', '230', '231', '232', '233', '234', '235', '236', '237', '238', '241', '242', '243', '244', '257', '258', '259', '260', '261', '262', '263', '264', '265', '266', '267', '277', '285', '314', '315', '316', '323', '326', '327', '328', '329', '330', '331', '365', '370', '371', '372', '373', '374', '375', '376', '379', '380', '381', '382', '395', '396', '397', '398', '399', '400', '401', '402', '403', '404', '405', '415', '423', '450', '451', '452', '459', '462', '463', '464', '465', '466', '467', '482', '499', '502', '503', '504', '505', '506', '507', '508', '509', '510', '513', '514', '515', '516', '529', '530', '531', '532', '533', '534', '535', '536', '537', '538', '539']\n",
      "Features to retain (constant but potentially predictive):\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "# Reconcile Constant Features with the Outcome Variable.\n",
    "features_to_remove = []\n",
    "features_to_retain = []\n",
    "\n",
    "baseline_fail_rate = Y.mean()\n",
    "baseline_pass_rate = 1 - baseline_fail_rate\n",
    "total_samples = len(Y)\n",
    "expected_pass = baseline_pass_rate * total_samples\n",
    "expected_fail = baseline_fail_rate * total_samples\n",
    "\n",
    "for feature in constant_features_confirmed:\n",
    "    \n",
    "     # Check the Constant Value's Distribution Across Pass/Fail.\n",
    "     constant_value = X[feature].dropna().iloc[0]\n",
    "     non_missing_mask = X[feature].notna()\n",
    "     non_missing_outcomes = Y[non_missing_mask]\n",
    "\n",
    "     value_distribution = pd.crosstab(pd.Series(non_missing_outcomes, name='Outcome'), pd.Series(X[feature][non_missing_mask], name='Value'))\n",
    "\n",
    "     # Chi-Square Test for Constant Value Distribution vs. Expected Baseline.\n",
    "     observed_pass = value_distribution.iloc[0,0] if 0 in value_distribution.index else 0\n",
    "     observed_fail = value_distribution.iloc[1,0] if 1 in value_distribution.index else 0\n",
    "     non_missing_total = observed_pass + observed_fail\n",
    "     expected_pass_adjusted = baseline_pass_rate * non_missing_total\n",
    "     expected_fail_adjusted = baseline_fail_rate * non_missing_total\n",
    "\n",
    "     contingency_table_value = [[observed_pass, observed_fail],[expected_pass_adjusted, expected_fail_adjusted]]\n",
    "     chi2, p_value, _, _ = chi2_contingency(contingency_table_value)\n",
    "\n",
    "     print(f\"Chi-square test for constant value vs. expected baseline: chi2 = {chi2: .2f}, p-value = {p_value: .4f}\")\n",
    "\n",
    "     # Check Missingness Pattern vs. Outcome (Chi-Square Test).\n",
    "     missing_indicator = X[feature].isna().astype(int)\n",
    "     contingency_table_missing = pd.crosstab(missing_indicator, Y)\n",
    "     chi2_missing, p_value_missing, _, _ = chi2_contingency(contingency_table_missing)\n",
    "\n",
    "     print(f\"Chi-square test for missingness vs. outcome: chi2 = {chi2_missing:.2f}, p-value = {p_value_missing:.4f}\")\n",
    "\n",
    "     # Decide Whether to Remove or Retain.\n",
    "\n",
    "     if (p_value > 0.05) and (p_value_missing > 0.05):\n",
    "          print(f\"Feature {feature} has no predictive power (constant value and missingness not associated with outcome). Slotted for removal.\")\n",
    "\n",
    "          features_to_remove.append(feature)\n",
    "\n",
    "     else:\n",
    "          print(f\"Feature {feature} has potential predictive power (constant value or missingness associated with outcome). Retaining.\")\n",
    "\n",
    "          features_to_retain.append(feature)\n",
    "\n",
    "print(f\"Features to remove (constant and not predictive):\\n\", features_to_remove)\n",
    "print(f\"Features to retain (constant but potentially predictive):\\n\", features_to_retain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a12d178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 116 constant features. New feature count: 474\n"
     ]
    }
   ],
   "source": [
    "# Remove Constant Features With No Predictive Power From The X Dataset.\n",
    "X_reduced1 = X.drop(columns=constant_features_confirmed)\n",
    "\n",
    "print(f\"Removed {len(constant_features_confirmed)} constant features. New feature count: {X_reduced1.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f14b51b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per feature:\n",
      " 1         6\n",
      "2         7\n",
      "3        14\n",
      "4        14\n",
      "5        14\n",
      "7        14\n",
      "8         9\n",
      "9         2\n",
      "10        2\n",
      "11        2\n",
      "12        2\n",
      "13        2\n",
      "15        3\n",
      "16        3\n",
      "17        3\n",
      "18        3\n",
      "19        3\n",
      "20       10\n",
      "22        2\n",
      "23        2\n",
      "24        2\n",
      "25        2\n",
      "26        2\n",
      "27        2\n",
      "28        2\n",
      "29        2\n",
      "30        2\n",
      "31        2\n",
      "32        2\n",
      "33        1\n",
      "34        1\n",
      "35        1\n",
      "36        1\n",
      "37        1\n",
      "38        1\n",
      "39        1\n",
      "40        1\n",
      "41       24\n",
      "42       24\n",
      "44        1\n",
      "45        1\n",
      "46        1\n",
      "47        1\n",
      "48        1\n",
      "49        1\n",
      "51        1\n",
      "52        1\n",
      "54        4\n",
      "55        4\n",
      "56        4\n",
      "57        4\n",
      "58        4\n",
      "59        4\n",
      "60        7\n",
      "61        6\n",
      "62        6\n",
      "63        6\n",
      "64        7\n",
      "65        7\n",
      "66        7\n",
      "67        6\n",
      "68        6\n",
      "69        6\n",
      "71        6\n",
      "72        6\n",
      "73      794\n",
      "74      794\n",
      "75        6\n",
      "76       24\n",
      "77       24\n",
      "78       24\n",
      "79       24\n",
      "80       24\n",
      "81       24\n",
      "82       24\n",
      "83       24\n",
      "84        1\n",
      "85       12\n",
      "86     1341\n",
      "90       51\n",
      "91       51\n",
      "92        6\n",
      "93        2\n",
      "94        2\n",
      "95        6\n",
      "96        6\n",
      "97        6\n",
      "99        6\n",
      "100       6\n",
      "101       6\n",
      "102       6\n",
      "103       6\n",
      "104       2\n",
      "105       2\n",
      "106       6\n",
      "107       6\n",
      "108       6\n",
      "109       6\n",
      "110    1018\n",
      "111    1018\n",
      "112    1018\n",
      "113     715\n",
      "119      24\n",
      "122       9\n",
      "123       9\n",
      "124       9\n",
      "125       9\n",
      "126       9\n",
      "127       9\n",
      "128       9\n",
      "129       9\n",
      "130       9\n",
      "131       9\n",
      "132       9\n",
      "133       8\n",
      "134       8\n",
      "135       8\n",
      "136       5\n",
      "137       6\n",
      "138       7\n",
      "139      14\n",
      "140      14\n",
      "141      14\n",
      "143      14\n",
      "144       9\n",
      "145       2\n",
      "146       2\n",
      "147       2\n",
      "148       2\n",
      "149       2\n",
      "151       3\n",
      "152       3\n",
      "153       3\n",
      "154       3\n",
      "155       3\n",
      "156      10\n",
      "158    1429\n",
      "159    1429\n",
      "160       2\n",
      "161       2\n",
      "162       2\n",
      "163       2\n",
      "164       2\n",
      "165       2\n",
      "166       2\n",
      "167       2\n",
      "168       2\n",
      "169       2\n",
      "170       2\n",
      "171       1\n",
      "172       1\n",
      "173       1\n",
      "174       1\n",
      "175       1\n",
      "176       1\n",
      "177       1\n",
      "178       1\n",
      "181       1\n",
      "182       1\n",
      "183       1\n",
      "184       1\n",
      "185       1\n",
      "186       1\n",
      "188       1\n",
      "189       1\n",
      "196       4\n",
      "197       7\n",
      "198       6\n",
      "199       6\n",
      "200       6\n",
      "201       7\n",
      "202       7\n",
      "203       7\n",
      "204       6\n",
      "205       6\n",
      "206       6\n",
      "207       6\n",
      "208       6\n",
      "209       6\n",
      "210       6\n",
      "211      24\n",
      "212      24\n",
      "213      24\n",
      "214      24\n",
      "215      24\n",
      "216      24\n",
      "217      24\n",
      "218      24\n",
      "219       1\n",
      "220      12\n",
      "221    1341\n",
      "225      51\n",
      "226      51\n",
      "228       2\n",
      "229       2\n",
      "239       2\n",
      "240       2\n",
      "245    1018\n",
      "246    1018\n",
      "247    1018\n",
      "248     715\n",
      "254      24\n",
      "268       8\n",
      "269       8\n",
      "270       8\n",
      "271       5\n",
      "272       6\n",
      "273       7\n",
      "274      14\n",
      "275      14\n",
      "276      14\n",
      "278      14\n",
      "279       9\n",
      "280       2\n",
      "281       2\n",
      "282       2\n",
      "283       2\n",
      "284       2\n",
      "286       3\n",
      "287       3\n",
      "288       3\n",
      "289       3\n",
      "290       3\n",
      "291      10\n",
      "293    1429\n",
      "294    1429\n",
      "295       2\n",
      "296       2\n",
      "297       2\n",
      "298       2\n",
      "299       2\n",
      "300       2\n",
      "301       2\n",
      "302       2\n",
      "303       2\n",
      "304       2\n",
      "305       2\n",
      "306       1\n",
      "307       1\n",
      "308       1\n",
      "309       1\n",
      "310       1\n",
      "311       1\n",
      "312       1\n",
      "313       1\n",
      "317       1\n",
      "318       1\n",
      "319       1\n",
      "320       1\n",
      "321       1\n",
      "322       1\n",
      "324       1\n",
      "325       1\n",
      "332       4\n",
      "333       7\n",
      "334       6\n",
      "335       6\n",
      "336       6\n",
      "337       7\n",
      "338       7\n",
      "339       7\n",
      "340       6\n",
      "341       6\n",
      "342       6\n",
      "343       6\n",
      "344       6\n",
      "345       6\n",
      "346     794\n",
      "347     794\n",
      "348       6\n",
      "349      24\n",
      "350      24\n",
      "351      24\n",
      "352      24\n",
      "353      24\n",
      "354      24\n",
      "355      24\n",
      "356      24\n",
      "357       1\n",
      "358      12\n",
      "359    1341\n",
      "363      51\n",
      "364      51\n",
      "366       2\n",
      "367       2\n",
      "368       6\n",
      "369       6\n",
      "377       2\n",
      "378       2\n",
      "383    1018\n",
      "384    1018\n",
      "385    1018\n",
      "386     715\n",
      "392      24\n",
      "406       8\n",
      "407       8\n",
      "408       8\n",
      "409       5\n",
      "410       6\n",
      "411       7\n",
      "412      14\n",
      "413      14\n",
      "414      14\n",
      "416      14\n",
      "417       9\n",
      "418       2\n",
      "419       2\n",
      "420       2\n",
      "421       2\n",
      "422       2\n",
      "424       3\n",
      "425       3\n",
      "426       3\n",
      "427       3\n",
      "428       3\n",
      "429      10\n",
      "431       2\n",
      "432       2\n",
      "433       2\n",
      "434       2\n",
      "435       2\n",
      "436       2\n",
      "437       2\n",
      "438       2\n",
      "439       2\n",
      "440       2\n",
      "441       2\n",
      "442       1\n",
      "443       1\n",
      "444       1\n",
      "445       1\n",
      "446       1\n",
      "447       1\n",
      "448       1\n",
      "449       1\n",
      "453       1\n",
      "454       1\n",
      "455       1\n",
      "456       1\n",
      "457       1\n",
      "458       1\n",
      "460       1\n",
      "461       1\n",
      "468       4\n",
      "469       7\n",
      "470       6\n",
      "471       6\n",
      "472       6\n",
      "473       7\n",
      "474       7\n",
      "475       7\n",
      "476       6\n",
      "477       6\n",
      "478       6\n",
      "479       6\n",
      "480       6\n",
      "481       6\n",
      "483      24\n",
      "484      24\n",
      "485      24\n",
      "486      24\n",
      "487      24\n",
      "488      24\n",
      "489      24\n",
      "490      24\n",
      "491       1\n",
      "492      12\n",
      "493    1341\n",
      "497      51\n",
      "498      51\n",
      "500       2\n",
      "501       2\n",
      "511       2\n",
      "512       2\n",
      "517    1018\n",
      "518    1018\n",
      "519    1018\n",
      "520     715\n",
      "526      24\n",
      "540       8\n",
      "541       8\n",
      "542       8\n",
      "543       2\n",
      "544       2\n",
      "545       2\n",
      "546       2\n",
      "547     260\n",
      "548     260\n",
      "549     260\n",
      "550     260\n",
      "551     260\n",
      "552     260\n",
      "553     260\n",
      "554     260\n",
      "555     260\n",
      "556     260\n",
      "557     260\n",
      "558     260\n",
      "559       1\n",
      "560       1\n",
      "561       1\n",
      "562       1\n",
      "563     273\n",
      "564     273\n",
      "565     273\n",
      "566     273\n",
      "567     273\n",
      "568     273\n",
      "569     273\n",
      "570     273\n",
      "579     949\n",
      "580     949\n",
      "581     949\n",
      "582     949\n",
      "583       1\n",
      "584       1\n",
      "585       1\n",
      "586       1\n",
      "587       1\n",
      "588       1\n",
      "589       1\n",
      "590       1\n",
      "dtype: int64\n",
      "Total features with missing values:422\n"
     ]
    }
   ],
   "source": [
    "# Assess Missing Values per Feature.\n",
    "missing_counts = X_reduced1.isna().sum()\n",
    "\n",
    "print(\"Missing values per feature:\\n\", missing_counts[missing_counts>0])\n",
    "\n",
    "print(f\"Total features with missing values:{sum(missing_counts>0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "edfe144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features grouped by number of missing values:\n",
      "Missing 1 values: ['33', '34', '35', '36', '37', '38', '39', '40', '44', '45', '46', '47', '48', '49', '51', '52', '84', '171', '172', '173', '174', '175', '176', '177', '178', '181', '182', '183', '184', '185', '186', '188', '189', '219', '306', '307', '308', '309', '310', '311', '312', '313', '317', '318', '319', '320', '321', '322', '324', '325', '357', '442', '443', '444', '445', '446', '447', '448', '449', '453', '454', '455', '456', '457', '458', '460', '461', '491', '559', '560', '561', '562', '583', '584', '585', '586', '587', '588', '589', '590']\n",
      "Missing 2 values: ['9', '10', '11', '12', '13', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '93', '94', '104', '105', '145', '146', '147', '148', '149', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '228', '229', '239', '240', '280', '281', '282', '283', '284', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '366', '367', '377', '378', '418', '419', '420', '421', '422', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '500', '501', '511', '512', '543', '544', '545', '546']\n",
      "Missing 3 values: ['15', '16', '17', '18', '19', '151', '152', '153', '154', '155', '286', '287', '288', '289', '290', '424', '425', '426', '427', '428']\n",
      "Missing 4 values: ['54', '55', '56', '57', '58', '59', '196', '332', '468']\n",
      "Missing 5 values: ['136', '271', '409']\n",
      "Missing 6 values: ['1', '61', '62', '63', '67', '68', '69', '71', '72', '75', '92', '95', '96', '97', '99', '100', '101', '102', '103', '106', '107', '108', '109', '137', '198', '199', '200', '204', '205', '206', '207', '208', '209', '210', '272', '334', '335', '336', '340', '341', '342', '343', '344', '345', '348', '368', '369', '410', '470', '471', '472', '476', '477', '478', '479', '480', '481']\n",
      "Missing 7 values: ['2', '60', '64', '65', '66', '138', '197', '201', '202', '203', '273', '333', '337', '338', '339', '411', '469', '473', '474', '475']\n",
      "Missing 8 values: ['133', '134', '135', '268', '269', '270', '406', '407', '408', '540', '541', '542']\n",
      "Missing 9 values: ['8', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '144', '279', '417']\n",
      "Missing 10 values: ['20', '156', '291', '429']\n",
      "Missing 12 values: ['85', '220', '358', '492']\n",
      "Missing 14 values: ['3', '4', '5', '7', '139', '140', '141', '143', '274', '275', '276', '278', '412', '413', '414', '416']\n",
      "Missing 24 values: ['41', '42', '76', '77', '78', '79', '80', '81', '82', '83', '119', '211', '212', '213', '214', '215', '216', '217', '218', '254', '349', '350', '351', '352', '353', '354', '355', '356', '392', '483', '484', '485', '486', '487', '488', '489', '490', '526']\n",
      "Missing 51 values: ['90', '91', '225', '226', '363', '364', '497', '498']\n",
      "Missing 260 values: ['547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558']\n",
      "Missing 273 values: ['563', '564', '565', '566', '567', '568', '569', '570']\n",
      "Missing 715 values: ['113', '248', '386', '520']\n",
      "Missing 794 values: ['73', '74', '346', '347']\n",
      "Missing 949 values: ['579', '580', '581', '582']\n",
      "Missing 1018 values: ['110', '111', '112', '245', '246', '247', '383', '384', '385', '517', '518', '519']\n",
      "Missing 1341 values: ['86', '221', '359', '493']\n",
      "Missing 1429 values: ['158', '159', '293', '294']\n"
     ]
    }
   ],
   "source": [
    "# Regroup Features by Number of Missing Values to Explore Patterns.\n",
    "missing_groups = missing_counts.groupby(missing_counts).groups\n",
    "\n",
    "print(\"\\nFeatures grouped by number of missing values:\")\n",
    "for missing_value, features in missing_groups.items():\n",
    "    if missing_value > 0:\n",
    "        print(f\"Missing {missing_value} values: {list(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f14121d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of missingness in non-constant features:\n",
      "Total features: 474\n",
      "Features with no missing values: 52\n",
      "Features with missing values: 422\n",
      "Average missing values per feature: 86.78\n"
     ]
    }
   ],
   "source": [
    "# Summary Statistics of Missingness.\n",
    "\n",
    "print(\"\\nSummary of missingness in non-constant features:\")\n",
    "print(f\"Total features: {len(missing_counts)}\")\n",
    "print(f\"Features with no missing values: {(missing_counts == 0).sum()}\")\n",
    "print(f\"Features with missing values: {(missing_counts > 0).sum()}\")\n",
    "print(f\"Average missing values per feature: {missing_counts.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4fdb0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature 1 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 1 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 2 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.66, p-value = 0.4152\n",
      "Feature 2 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 3 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 3 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 4 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 4 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 5 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 5 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 7 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 7 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 8 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 8 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 9 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 9 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 10 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 10 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 11 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 11 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 12 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 12 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 13 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 13 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 15 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 15 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 16 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 16 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 17 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 17 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 18 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 18 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 19 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 19 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 20 (Missing 10 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.18, p-value = 0.6682\n",
      "Feature 20 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 22 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 22 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 23 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 23 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 24 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 24 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 25 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 25 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 26 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 26 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 27 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 27 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 28 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 28 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 29 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 29 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 30 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 30 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 31 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 31 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 32 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 32 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 33 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 33 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 34 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 34 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 35 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 35 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 36 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 36 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 37 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 37 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 38 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 38 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 39 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 39 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 40 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 40 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 41 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 41 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 42 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 42 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 44 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 44 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 45 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 45 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 46 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 46 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 47 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 47 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 48 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 48 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 49 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 49 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 51 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 51 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 52 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 52 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 54 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 54 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 55 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 55 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 56 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 56 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 57 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 57 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 58 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 58 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 59 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 59 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 60 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 60 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 61 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 61 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 62 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 62 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 63 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 63 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 64 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 64 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 65 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 65 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 66 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 66 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 67 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 67 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 68 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 68 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 69 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 69 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 71 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 71 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 72 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 72 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 73 (Missing 794 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 11.49, p-value = 0.0007\n",
      "Feature 73 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 74 (Missing 794 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 11.49, p-value = 0.0007\n",
      "Feature 74 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 75 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 75 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 76 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 76 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 77 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 77 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 78 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 78 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 79 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 79 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 80 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 80 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 81 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 81 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 82 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 82 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 83 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 83 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 84 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 84 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 85 (Missing 12 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.86, p-value = 0.3538\n",
      "Feature 85 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 86 (Missing 1341 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.33, p-value = 0.2480\n",
      "Feature 86 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 90 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 90 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 91 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 91 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 92 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 92 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 93 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 93 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 94 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 94 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 95 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 95 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 96 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 96 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 97 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 97 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 99 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 99 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 100 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 100 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 101 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 101 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 102 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 102 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 103 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 103 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 104 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 104 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 105 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 105 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 106 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 106 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 107 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 107 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 108 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 108 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 109 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 109 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 110 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 110 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 111 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 111 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 112 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 112 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 113 (Missing 715 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 8.67, p-value = 0.0032\n",
      "Feature 113 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 119 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 119 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 122 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 122 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 123 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 123 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 124 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 124 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 125 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 125 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 126 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 126 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 127 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 127 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 128 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 128 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 129 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 129 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 130 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 130 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 131 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 131 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 132 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 132 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 133 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 133 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 134 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 134 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 135 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 135 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 136 (Missing 5 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.36, p-value = 0.5504\n",
      "Feature 136 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 137 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 137 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 138 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.66, p-value = 0.4152\n",
      "Feature 138 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 139 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 139 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 140 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 140 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 141 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 141 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 143 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 143 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 144 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 144 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 145 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 145 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 146 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 146 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 147 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 147 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 148 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 148 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 149 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 149 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 151 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 151 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 152 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 152 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 153 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 153 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 154 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 154 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 155 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 155 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 156 (Missing 10 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.18, p-value = 0.6682\n",
      "Feature 156 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 158 (Missing 1429 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.17, p-value = 0.6781\n",
      "Feature 158 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 159 (Missing 1429 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.17, p-value = 0.6781\n",
      "Feature 159 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 160 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 160 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 161 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 161 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 162 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 162 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 163 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 163 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 164 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 164 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 165 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 165 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 166 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 166 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 167 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 167 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 168 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 168 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 169 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 169 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 170 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 170 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 171 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 171 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 172 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 172 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 173 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 173 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 174 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 174 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 175 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 175 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 176 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 176 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 177 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 177 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 178 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 178 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 181 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 181 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 182 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 182 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 183 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 183 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 184 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 184 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 185 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 185 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 186 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 186 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 188 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 188 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 189 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 189 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 196 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 196 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 197 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 197 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 198 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 198 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 199 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 199 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 200 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 200 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 201 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 201 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 202 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 202 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 203 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 203 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 204 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 204 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 205 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 205 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 206 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 206 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 207 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 207 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 208 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 208 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 209 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 209 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 210 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 210 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 211 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 211 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 212 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 212 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 213 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 213 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 214 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 214 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 215 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 215 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 216 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 216 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 217 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 217 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 218 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 218 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 219 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 219 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 220 (Missing 12 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.86, p-value = 0.3538\n",
      "Feature 220 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 221 (Missing 1341 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.33, p-value = 0.2480\n",
      "Feature 221 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 225 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 225 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 226 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 226 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 228 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 228 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 229 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 229 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 239 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 239 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 240 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 240 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 245 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 245 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 246 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 246 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 247 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 247 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 248 (Missing 715 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 8.67, p-value = 0.0032\n",
      "Feature 248 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 254 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 254 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 268 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 268 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 269 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 269 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 270 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 270 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 271 (Missing 5 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.36, p-value = 0.5504\n",
      "Feature 271 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 272 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 272 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 273 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.66, p-value = 0.4152\n",
      "Feature 273 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 274 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 274 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 275 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 275 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 276 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 276 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 278 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 278 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 279 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 279 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 280 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 280 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 281 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 281 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 282 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 282 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 283 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 283 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 284 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 284 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 286 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 286 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 287 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 287 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 288 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 288 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 289 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 289 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 290 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 290 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 291 (Missing 10 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.18, p-value = 0.6682\n",
      "Feature 291 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 293 (Missing 1429 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.17, p-value = 0.6781\n",
      "Feature 293 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 294 (Missing 1429 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.17, p-value = 0.6781\n",
      "Feature 294 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 295 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 295 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 296 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 296 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 297 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 297 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 298 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 298 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 299 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 299 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 300 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 300 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 301 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 301 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 302 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 302 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 303 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 303 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 304 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 304 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 305 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 305 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 306 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 306 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 307 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 307 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 308 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 308 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 309 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 309 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 310 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 310 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 311 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 311 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 312 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 312 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 313 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 313 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 317 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 317 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 318 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 318 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 319 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 319 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 320 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 320 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 321 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 321 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 322 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 322 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 324 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 324 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 325 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 325 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 332 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 332 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 333 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 333 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 334 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 334 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 335 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 335 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 336 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 336 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 337 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 337 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 338 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 338 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 339 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 339 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 340 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 340 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 341 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 341 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 342 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 342 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 343 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 343 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 344 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 344 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 345 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 345 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 346 (Missing 794 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 11.49, p-value = 0.0007\n",
      "Feature 346 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 347 (Missing 794 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 11.49, p-value = 0.0007\n",
      "Feature 347 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 348 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 348 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 349 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 349 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 350 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 350 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 351 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 351 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 352 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 352 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 353 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 353 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 354 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 354 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 355 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 355 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 356 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 356 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 357 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 357 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 358 (Missing 12 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.86, p-value = 0.3538\n",
      "Feature 358 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 359 (Missing 1341 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.33, p-value = 0.2480\n",
      "Feature 359 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 363 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 363 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 364 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 364 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 366 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 366 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 367 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 367 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 368 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 368 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 369 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 369 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 377 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 377 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 378 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 378 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 383 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 383 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 384 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 384 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 385 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 385 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 386 (Missing 715 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 8.67, p-value = 0.0032\n",
      "Feature 386 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 392 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 392 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 406 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 406 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 407 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 407 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 408 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 408 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 409 (Missing 5 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.36, p-value = 0.5504\n",
      "Feature 409 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 410 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 410 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 411 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.66, p-value = 0.4152\n",
      "Feature 411 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 412 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 412 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 413 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 413 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 414 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 414 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 416 (Missing 14 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.00, p-value = 0.3163\n",
      "Feature 416 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 417 (Missing 9 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.64, p-value = 0.4225\n",
      "Feature 417 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 418 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 418 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 419 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 419 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 420 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 420 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 421 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 421 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 422 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 422 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 424 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 424 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 425 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 425 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 426 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 426 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 427 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 427 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 428 (Missing 3 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.21, p-value = 0.6439\n",
      "Feature 428 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 429 (Missing 10 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.18, p-value = 0.6682\n",
      "Feature 429 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 431 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 431 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 432 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 432 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 433 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 433 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 434 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 434 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 435 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 435 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 436 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 436 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 437 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 437 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 438 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 438 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 439 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 439 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 440 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 440 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 441 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 6.08, p-value = 0.0137\n",
      "Feature 441 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 442 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 442 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 443 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 443 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 444 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 444 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 445 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 445 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 446 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 446 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 447 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 447 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 448 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 448 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 449 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 449 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 453 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 453 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 454 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 454 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 455 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 455 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 456 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 456 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 457 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 457 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 458 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 458 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 460 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 460 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 461 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 461 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 468 (Missing 4 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.18, p-value = 0.1396\n",
      "Feature 468 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 469 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 469 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 470 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 470 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 471 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 471 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 472 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 472 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 473 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 473 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 474 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 474 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 475 (Missing 7 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.50, p-value = 0.4796\n",
      "Feature 475 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 476 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 476 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 477 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 477 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 478 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 478 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 479 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 479 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 480 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 480 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 481 (Missing 6 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.43, p-value = 0.5129\n",
      "Feature 481 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 483 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 483 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 484 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 484 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 485 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 485 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 486 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 486 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 487 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 487 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 488 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 488 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 489 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 489 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 490 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 490 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 491 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 491 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 492 (Missing 12 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.86, p-value = 0.3538\n",
      "Feature 492 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 493 (Missing 1341 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.33, p-value = 0.2480\n",
      "Feature 493 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 497 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 497 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 498 (Missing 51 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.86, p-value = 0.1726\n",
      "Feature 498 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 500 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 500 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 501 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 501 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 511 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 511 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 512 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 512 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 517 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 517 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 518 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 518 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 519 (Missing 1018 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.87, p-value = 0.1709\n",
      "Feature 519 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 520 (Missing 715 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 8.67, p-value = 0.0032\n",
      "Feature 520 has predictive missingness (p < 0.05).\n",
      "\n",
      "Feature 526 (Missing 24 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 1.73, p-value = 0.1881\n",
      "Feature 526 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 540 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 540 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 541 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 541 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 542 (Missing 8 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.57, p-value = 0.4496\n",
      "Feature 542 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 543 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 543 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 544 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 544 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 545 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 545 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 546 (Missing 2 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.14, p-value = 0.7060\n",
      "Feature 546 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 547 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 547 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 548 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 548 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 549 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 549 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 550 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 550 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 551 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 551 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 552 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 552 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 553 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 553 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 554 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 554 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 555 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 555 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 556 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 556 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 557 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 557 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 558 (Missing 260 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.04, p-value = 0.8391\n",
      "Feature 558 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 559 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 559 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 560 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 560 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 561 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 561 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 562 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 562 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 563 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 563 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 564 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 564 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 565 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 565 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 566 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 566 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 567 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 567 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 568 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 568 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 569 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 569 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 570 (Missing 273 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 2.48, p-value = 0.1156\n",
      "Feature 570 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 579 (Missing 949 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.68, p-value = 0.4081\n",
      "Feature 579 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 580 (Missing 949 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.68, p-value = 0.4081\n",
      "Feature 580 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 581 (Missing 949 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.68, p-value = 0.4081\n",
      "Feature 581 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 582 (Missing 949 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.68, p-value = 0.4081\n",
      "Feature 582 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 583 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 583 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 584 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 584 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 585 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 585 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 586 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 586 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 587 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 587 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 588 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 588 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 589 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 589 has no predictive missingness (p >= 0.05).\n",
      "\n",
      "Feature 590 (Missing 1 values):\n",
      "Chi-square test for missingness vs. outcome: chi2 = 0.07, p-value = 0.7897\n",
      "Feature 590 has no predictive missingness (p >= 0.05).\n",
      "Features with predictive missingness:\n",
      " ['22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '73', '74', '113', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '248', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '346', '347', '386', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '520']\n",
      "Features without predictive missingness:\n",
      " ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '44', '45', '46', '47', '48', '49', '51', '52', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '71', '72', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '90', '91', '92', '93', '94', '95', '96', '97', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '119', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '143', '144', '145', '146', '147', '148', '149', '151', '152', '153', '154', '155', '156', '158', '159', '171', '172', '173', '174', '175', '176', '177', '178', '181', '182', '183', '184', '185', '186', '188', '189', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '207', '208', '209', '210', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '221', '225', '226', '228', '229', '239', '240', '245', '246', '247', '254', '268', '269', '270', '271', '272', '273', '274', '275', '276', '278', '279', '280', '281', '282', '283', '284', '286', '287', '288', '289', '290', '291', '293', '294', '306', '307', '308', '309', '310', '311', '312', '313', '317', '318', '319', '320', '321', '322', '324', '325', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '343', '344', '345', '348', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '359', '363', '364', '366', '367', '368', '369', '377', '378', '383', '384', '385', '392', '406', '407', '408', '409', '410', '411', '412', '413', '414', '416', '417', '418', '419', '420', '421', '422', '424', '425', '426', '427', '428', '429', '442', '443', '444', '445', '446', '447', '448', '449', '453', '454', '455', '456', '457', '458', '460', '461', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '479', '480', '481', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '493', '497', '498', '500', '501', '511', '512', '517', '518', '519', '526', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590']\n"
     ]
    }
   ],
   "source": [
    "# Reconcile Non-Constant Features Missing Values with the Outcome Variable.\n",
    "features_with_predictive_missingness = []\n",
    "features_without_predictive_missingness = []\n",
    "\n",
    "for feature in X_reduced1.columns: \n",
    "    if missing_counts.loc[feature] > 0:\n",
    "          \n",
    "     # Check Missingness Pattern vs. Outcome (Chi-Square Test).\n",
    "        missing_indicator = X_reduced1[feature].isna().astype(int)\n",
    "        contingency_table_missing = pd.crosstab(missing_indicator, Y)\n",
    "        chi2_missing, p_value_missing, _, _ = chi2_contingency(contingency_table_missing, correction=False)\n",
    "\n",
    "        print(f\"\\nFeature {feature} (Missing {missing_counts[feature]} values):\")\n",
    "        print(f\"Chi-square test for missingness vs. outcome: chi2 = {chi2_missing:.2f}, p-value = {p_value_missing:.4f}\")\n",
    "\n",
    "     # Decide If Missingness Is Predictive.\n",
    "\n",
    "        if (p_value_missing < 0.05):\n",
    "            print(f\"Feature {feature} has predictive missingness (p < 0.05).\")\n",
    "\n",
    "            features_with_predictive_missingness.append(feature)\n",
    "\n",
    "        else:\n",
    "            print(f\"Feature {feature} has no predictive missingness (p >= 0.05).\")\n",
    "\n",
    "            features_without_predictive_missingness.append(feature)\n",
    "\n",
    "print(f\"Features with predictive missingness:\\n\", features_with_predictive_missingness)\n",
    "print(f\"Features without predictive missingness:\\n\", features_without_predictive_missingness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0034568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Descriptive Statistics By Pass/Fail.\n",
    "\n",
    "# Combine X_reduced1 and Y for easier grouping.\n",
    "data = X_reduced1.copy()\n",
    "data['Y'] = Y\n",
    "\n",
    "# Split into pass and fail groups.\n",
    "pass_data = data[data['Y'] == 0].drop(columns='Y')\n",
    "fail_data = data[data['Y'] == 1].drop(columns='Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f891b4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>8.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>68.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>11.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>103.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>30.000</td>\n",
       "      <td>71.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>86.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>80.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>45.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "      <td>104.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3,007.526</td>\n",
       "      <td>2,495.060</td>\n",
       "      <td>2,200.442</td>\n",
       "      <td>1,355.781</td>\n",
       "      <td>1.303</td>\n",
       "      <td>101.491</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.471</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.963</td>\n",
       "      <td>199.884</td>\n",
       "      <td>8.283</td>\n",
       "      <td>412.900</td>\n",
       "      <td>9.929</td>\n",
       "      <td>0.971</td>\n",
       "      <td>189.955</td>\n",
       "      <td>12.506</td>\n",
       "      <td>1.407</td>\n",
       "      <td>-5,362.274</td>\n",
       "      <td>2,617.340</td>\n",
       "      <td>-3,739.078</td>\n",
       "      <td>-499.540</td>\n",
       "      <td>1.179</td>\n",
       "      <td>1.880</td>\n",
       "      <td>6.501</td>\n",
       "      <td>68.101</td>\n",
       "      <td>2.393</td>\n",
       "      <td>0.189</td>\n",
       "      <td>3.574</td>\n",
       "      <td>85.733</td>\n",
       "      <td>9.368</td>\n",
       "      <td>50.594</td>\n",
       "      <td>64.437</td>\n",
       "      <td>49.406</td>\n",
       "      <td>66.268</td>\n",
       "      <td>86.925</td>\n",
       "      <td>118.665</td>\n",
       "      <td>63.271</td>\n",
       "      <td>3.375</td>\n",
       "      <td>355.061</td>\n",
       "      <td>10.016</td>\n",
       "      <td>136.989</td>\n",
       "      <td>733.243</td>\n",
       "      <td>1.198</td>\n",
       "      <td>139.713</td>\n",
       "      <td>631.763</td>\n",
       "      <td>158.792</td>\n",
       "      <td>4.594</td>\n",
       "      <td>4.842</td>\n",
       "      <td>2,855.301</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.611</td>\n",
       "      <td>8.515</td>\n",
       "      <td>354.745</td>\n",
       "      <td>10.436</td>\n",
       "      <td>115.888</td>\n",
       "      <td>15.586</td>\n",
       "      <td>21.968</td>\n",
       "      <td>28.592</td>\n",
       "      <td>704.971</td>\n",
       "      <td>62.207</td>\n",
       "      <td>146.611</td>\n",
       "      <td>617.245</td>\n",
       "      <td>104.087</td>\n",
       "      <td>150.833</td>\n",
       "      <td>466.821</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.004</td>\n",
       "      <td>7.405</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.405</td>\n",
       "      <td>0.981</td>\n",
       "      <td>1,813.208</td>\n",
       "      <td>0.195</td>\n",
       "      <td>8,748.619</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.051</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.980</td>\n",
       "      <td>101.742</td>\n",
       "      <td>230.822</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.001</td>\n",
       "      <td>739.372</td>\n",
       "      <td>0.987</td>\n",
       "      <td>58.219</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.971</td>\n",
       "      <td>6.301</td>\n",
       "      <td>15.828</td>\n",
       "      <td>3.634</td>\n",
       "      <td>15.859</td>\n",
       "      <td>15.835</td>\n",
       "      <td>1.097</td>\n",
       "      <td>2.803</td>\n",
       "      <td>0.610</td>\n",
       "      <td>3.224</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.769</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1,005.701</td>\n",
       "      <td>39.050</td>\n",
       "      <td>112.058</td>\n",
       "      <td>140.474</td>\n",
       "      <td>128.559</td>\n",
       "      <td>59.794</td>\n",
       "      <td>412.068</td>\n",
       "      <td>0.318</td>\n",
       "      <td>6.437</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.018</td>\n",
       "      <td>8.078</td>\n",
       "      <td>6.667</td>\n",
       "      <td>11.444</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.013</td>\n",
       "      <td>7.729</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.039</td>\n",
       "      <td>1,237.800</td>\n",
       "      <td>1,172.311</td>\n",
       "      <td>750.398</td>\n",
       "      <td>3,753.942</td>\n",
       "      <td>4,653.233</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.381</td>\n",
       "      <td>3.010</td>\n",
       "      <td>1.289</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.392</td>\n",
       "      <td>0.706</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.391</td>\n",
       "      <td>18.037</td>\n",
       "      <td>0.510</td>\n",
       "      <td>10.293</td>\n",
       "      <td>28.086</td>\n",
       "      <td>0.146</td>\n",
       "      <td>7.123</td>\n",
       "      <td>18.120</td>\n",
       "      <td>47.050</td>\n",
       "      <td>0.342</td>\n",
       "      <td>11.828</td>\n",
       "      <td>22.303</td>\n",
       "      <td>0.637</td>\n",
       "      <td>14.731</td>\n",
       "      <td>19.570</td>\n",
       "      <td>8.389</td>\n",
       "      <td>10.905</td>\n",
       "      <td>33.045</td>\n",
       "      <td>96.335</td>\n",
       "      <td>11.367</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.692</td>\n",
       "      <td>70.873</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.074</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.015</td>\n",
       "      <td>122.626</td>\n",
       "      <td>0.063</td>\n",
       "      <td>1,073.229</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.323</td>\n",
       "      <td>3.807</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.005</td>\n",
       "      <td>111.077</td>\n",
       "      <td>0.003</td>\n",
       "      <td>3.044</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.399</td>\n",
       "      <td>0.069</td>\n",
       "      <td>19.692</td>\n",
       "      <td>3.856</td>\n",
       "      <td>28.372</td>\n",
       "      <td>47.854</td>\n",
       "      <td>43.372</td>\n",
       "      <td>20.534</td>\n",
       "      <td>134.597</td>\n",
       "      <td>0.106</td>\n",
       "      <td>2.168</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.657</td>\n",
       "      <td>2.075</td>\n",
       "      <td>3.485</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.563</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>398.871</td>\n",
       "      <td>549.384</td>\n",
       "      <td>351.508</td>\n",
       "      <td>1,761.278</td>\n",
       "      <td>2,295.065</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.422</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.159</td>\n",
       "      <td>5.636</td>\n",
       "      <td>0.162</td>\n",
       "      <td>3.026</td>\n",
       "      <td>8.349</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.186</td>\n",
       "      <td>5.447</td>\n",
       "      <td>14.316</td>\n",
       "      <td>0.092</td>\n",
       "      <td>3.327</td>\n",
       "      <td>6.836</td>\n",
       "      <td>0.195</td>\n",
       "      <td>4.174</td>\n",
       "      <td>10.855</td>\n",
       "      <td>2.668</td>\n",
       "      <td>3.182</td>\n",
       "      <td>10.010</td>\n",
       "      <td>48.558</td>\n",
       "      <td>3.415</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.819</td>\n",
       "      <td>22.450</td>\n",
       "      <td>7.783</td>\n",
       "      <td>5.601</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.006</td>\n",
       "      <td>39.619</td>\n",
       "      <td>0.019</td>\n",
       "      <td>348.516</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.406</td>\n",
       "      <td>1.138</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>36.429</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6.973</td>\n",
       "      <td>1.261</td>\n",
       "      <td>5.077</td>\n",
       "      <td>4.669</td>\n",
       "      <td>5.165</td>\n",
       "      <td>2.713</td>\n",
       "      <td>31.487</td>\n",
       "      <td>24.486</td>\n",
       "      <td>6.345</td>\n",
       "      <td>3.241</td>\n",
       "      <td>7.543</td>\n",
       "      <td>287.728</td>\n",
       "      <td>326.440</td>\n",
       "      <td>1.901</td>\n",
       "      <td>4.040</td>\n",
       "      <td>83.509</td>\n",
       "      <td>2.774</td>\n",
       "      <td>8.003</td>\n",
       "      <td>1.310</td>\n",
       "      <td>4.067</td>\n",
       "      <td>3.164</td>\n",
       "      <td>4.178</td>\n",
       "      <td>33.370</td>\n",
       "      <td>38.924</td>\n",
       "      <td>101.483</td>\n",
       "      <td>246.140</td>\n",
       "      <td>29.137</td>\n",
       "      <td>23.489</td>\n",
       "      <td>21.428</td>\n",
       "      <td>4.442</td>\n",
       "      <td>56.704</td>\n",
       "      <td>67.709</td>\n",
       "      <td>11.353</td>\n",
       "      <td>0.825</td>\n",
       "      <td>1.312</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.891</td>\n",
       "      <td>0.667</td>\n",
       "      <td>1.202</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.329</td>\n",
       "      <td>5.081</td>\n",
       "      <td>5.109</td>\n",
       "      <td>7.501</td>\n",
       "      <td>3.835</td>\n",
       "      <td>12.352</td>\n",
       "      <td>5.122</td>\n",
       "      <td>2.871</td>\n",
       "      <td>32.226</td>\n",
       "      <td>7.445</td>\n",
       "      <td>176.323</td>\n",
       "      <td>6.283</td>\n",
       "      <td>6.081</td>\n",
       "      <td>11.910</td>\n",
       "      <td>138.106</td>\n",
       "      <td>40.225</td>\n",
       "      <td>39.010</td>\n",
       "      <td>4.697</td>\n",
       "      <td>21.728</td>\n",
       "      <td>8.399</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.676</td>\n",
       "      <td>73.990</td>\n",
       "      <td>308.773</td>\n",
       "      <td>189.420</td>\n",
       "      <td>180.596</td>\n",
       "      <td>176.006</td>\n",
       "      <td>329.674</td>\n",
       "      <td>228.103</td>\n",
       "      <td>309.131</td>\n",
       "      <td>288.577</td>\n",
       "      <td>50.900</td>\n",
       "      <td>2.507</td>\n",
       "      <td>7.821</td>\n",
       "      <td>2.601</td>\n",
       "      <td>1.829</td>\n",
       "      <td>6.765</td>\n",
       "      <td>29.274</td>\n",
       "      <td>12.286</td>\n",
       "      <td>226.626</td>\n",
       "      <td>258.693</td>\n",
       "      <td>74.348</td>\n",
       "      <td>343.826</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.305</td>\n",
       "      <td>1.649</td>\n",
       "      <td>15.415</td>\n",
       "      <td>2.860</td>\n",
       "      <td>25.809</td>\n",
       "      <td>15.093</td>\n",
       "      <td>0.292</td>\n",
       "      <td>5.236</td>\n",
       "      <td>5.725</td>\n",
       "      <td>1.366</td>\n",
       "      <td>6.322</td>\n",
       "      <td>2.955</td>\n",
       "      <td>1.958</td>\n",
       "      <td>9.880</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.385</td>\n",
       "      <td>1.020</td>\n",
       "      <td>404.466</td>\n",
       "      <td>75.800</td>\n",
       "      <td>0.674</td>\n",
       "      <td>18.022</td>\n",
       "      <td>1.559</td>\n",
       "      <td>0.282</td>\n",
       "      <td>8.144</td>\n",
       "      <td>0.648</td>\n",
       "      <td>61.002</td>\n",
       "      <td>4.460</td>\n",
       "      <td>2.075</td>\n",
       "      <td>1.002</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.076</td>\n",
       "      <td>34.152</td>\n",
       "      <td>261.275</td>\n",
       "      <td>0.671</td>\n",
       "      <td>6.604</td>\n",
       "      <td>0.160</td>\n",
       "      <td>2.623</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.526</td>\n",
       "      <td>23.191</td>\n",
       "      <td>530.415</td>\n",
       "      <td>2.082</td>\n",
       "      <td>18.020</td>\n",
       "      <td>0.297</td>\n",
       "      <td>5.658</td>\n",
       "      <td>0.091</td>\n",
       "      <td>3.756</td>\n",
       "      <td>14.319</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>91.460</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.135</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.006</td>\n",
       "      <td>98.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>88.410</td>\n",
       "      <td>71.316</td>\n",
       "      <td>31.302</td>\n",
       "      <td>341.306</td>\n",
       "      <td>0.336</td>\n",
       "      <td>4.996</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2.602</td>\n",
       "      <td>2.747</td>\n",
       "      <td>7.538</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.577</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.015</td>\n",
       "      <td>950.378</td>\n",
       "      <td>458.717</td>\n",
       "      <td>1,448.097</td>\n",
       "      <td>2,378.672</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.343</td>\n",
       "      <td>1.691</td>\n",
       "      <td>3.280</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.445</td>\n",
       "      <td>2.983</td>\n",
       "      <td>2.438</td>\n",
       "      <td>1.244</td>\n",
       "      <td>1.799</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.686</td>\n",
       "      <td>1.594</td>\n",
       "      <td>26.573</td>\n",
       "      <td>1.121</td>\n",
       "      <td>5.719</td>\n",
       "      <td>0.157</td>\n",
       "      <td>8.421</td>\n",
       "      <td>11.584</td>\n",
       "      <td>0.179</td>\n",
       "      <td>4.724</td>\n",
       "      <td>7.697</td>\n",
       "      <td>64.841</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.042</td>\n",
       "      <td>30.378</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.077</td>\n",
       "      <td>10.802</td>\n",
       "      <td>5.642</td>\n",
       "      <td>0.344</td>\n",
       "      <td>10.265</td>\n",
       "      <td>7.676</td>\n",
       "      <td>7.539</td>\n",
       "      <td>9.100</td>\n",
       "      <td>12.039</td>\n",
       "      <td>623.921</td>\n",
       "      <td>6.982</td>\n",
       "      <td>9.826</td>\n",
       "      <td>37.628</td>\n",
       "      <td>11.530</td>\n",
       "      <td>4.872</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>53.902</td>\n",
       "      <td>0.122</td>\n",
       "      <td>314.424</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.316</td>\n",
       "      <td>1.275</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>46.830</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.171</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.714</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.840</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.059</td>\n",
       "      <td>5.811</td>\n",
       "      <td>2.627</td>\n",
       "      <td>59.635</td>\n",
       "      <td>53.653</td>\n",
       "      <td>47.478</td>\n",
       "      <td>13.272</td>\n",
       "      <td>274.965</td>\n",
       "      <td>0.308</td>\n",
       "      <td>2.053</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.071</td>\n",
       "      <td>3.201</td>\n",
       "      <td>4.400</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2.920</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.019</td>\n",
       "      <td>322.177</td>\n",
       "      <td>1,438.581</td>\n",
       "      <td>836.913</td>\n",
       "      <td>3,377.785</td>\n",
       "      <td>5,693.038</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.724</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.824</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.280</td>\n",
       "      <td>2.776</td>\n",
       "      <td>0.193</td>\n",
       "      <td>3.065</td>\n",
       "      <td>6.180</td>\n",
       "      <td>0.069</td>\n",
       "      <td>2.512</td>\n",
       "      <td>4.170</td>\n",
       "      <td>21.840</td>\n",
       "      <td>0.626</td>\n",
       "      <td>29.140</td>\n",
       "      <td>16.930</td>\n",
       "      <td>0.892</td>\n",
       "      <td>29.478</td>\n",
       "      <td>14.206</td>\n",
       "      <td>6.017</td>\n",
       "      <td>12.556</td>\n",
       "      <td>26.150</td>\n",
       "      <td>980.337</td>\n",
       "      <td>14.475</td>\n",
       "      <td>0.000</td>\n",
       "      <td>23.313</td>\n",
       "      <td>27.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.037</td>\n",
       "      <td>1.180</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.097</td>\n",
       "      <td>37.540</td>\n",
       "      <td>0.103</td>\n",
       "      <td>457.571</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.009</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.030</td>\n",
       "      <td>43.860</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.033</td>\n",
       "      <td>7.553</td>\n",
       "      <td>1.207</td>\n",
       "      <td>9.677</td>\n",
       "      <td>18.170</td>\n",
       "      <td>16.188</td>\n",
       "      <td>3.897</td>\n",
       "      <td>87.862</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.935</td>\n",
       "      <td>1.240</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007</td>\n",
       "      <td>121.158</td>\n",
       "      <td>719.578</td>\n",
       "      <td>423.299</td>\n",
       "      <td>1,602.811</td>\n",
       "      <td>2,846.455</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.874</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.947</td>\n",
       "      <td>1.998</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.709</td>\n",
       "      <td>1.246</td>\n",
       "      <td>6.688</td>\n",
       "      <td>0.085</td>\n",
       "      <td>7.408</td>\n",
       "      <td>4.696</td>\n",
       "      <td>0.291</td>\n",
       "      <td>7.460</td>\n",
       "      <td>9.547</td>\n",
       "      <td>3.692</td>\n",
       "      <td>3.911</td>\n",
       "      <td>6.535</td>\n",
       "      <td>494.563</td>\n",
       "      <td>4.935</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.024</td>\n",
       "      <td>8.599</td>\n",
       "      <td>10.931</td>\n",
       "      <td>10.982</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.040</td>\n",
       "      <td>11.913</td>\n",
       "      <td>0.032</td>\n",
       "      <td>151.465</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>14.906</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3.127</td>\n",
       "      <td>0.358</td>\n",
       "      <td>2.689</td>\n",
       "      <td>1.787</td>\n",
       "      <td>1.939</td>\n",
       "      <td>0.576</td>\n",
       "      <td>18.823</td>\n",
       "      <td>21.124</td>\n",
       "      <td>2.028</td>\n",
       "      <td>0.784</td>\n",
       "      <td>4.031</td>\n",
       "      <td>282.110</td>\n",
       "      <td>306.018</td>\n",
       "      <td>0.955</td>\n",
       "      <td>1.537</td>\n",
       "      <td>34.260</td>\n",
       "      <td>1.074</td>\n",
       "      <td>8.188</td>\n",
       "      <td>0.744</td>\n",
       "      <td>1.536</td>\n",
       "      <td>1.189</td>\n",
       "      <td>2.554</td>\n",
       "      <td>74.186</td>\n",
       "      <td>74.143</td>\n",
       "      <td>113.695</td>\n",
       "      <td>256.359</td>\n",
       "      <td>73.438</td>\n",
       "      <td>74.555</td>\n",
       "      <td>75.043</td>\n",
       "      <td>3.284</td>\n",
       "      <td>44.636</td>\n",
       "      <td>28.232</td>\n",
       "      <td>5.823</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.610</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.779</td>\n",
       "      <td>1.961</td>\n",
       "      <td>2.175</td>\n",
       "      <td>0.857</td>\n",
       "      <td>5.773</td>\n",
       "      <td>1.884</td>\n",
       "      <td>0.669</td>\n",
       "      <td>14.075</td>\n",
       "      <td>13.709</td>\n",
       "      <td>226.868</td>\n",
       "      <td>4.740</td>\n",
       "      <td>7.748</td>\n",
       "      <td>18.887</td>\n",
       "      <td>50.274</td>\n",
       "      <td>24.201</td>\n",
       "      <td>27.413</td>\n",
       "      <td>3.777</td>\n",
       "      <td>17.856</td>\n",
       "      <td>16.590</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.774</td>\n",
       "      <td>34.828</td>\n",
       "      <td>291.059</td>\n",
       "      <td>153.561</td>\n",
       "      <td>183.143</td>\n",
       "      <td>208.536</td>\n",
       "      <td>263.861</td>\n",
       "      <td>261.377</td>\n",
       "      <td>263.602</td>\n",
       "      <td>231.984</td>\n",
       "      <td>18.646</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.407</td>\n",
       "      <td>1.175</td>\n",
       "      <td>12.491</td>\n",
       "      <td>2.081</td>\n",
       "      <td>21.912</td>\n",
       "      <td>5.241</td>\n",
       "      <td>325.000</td>\n",
       "      <td>345.999</td>\n",
       "      <td>55.337</td>\n",
       "      <td>353.483</td>\n",
       "      <td>0.110</td>\n",
       "      <td>1.057</td>\n",
       "      <td>0.434</td>\n",
       "      <td>14.149</td>\n",
       "      <td>3.781</td>\n",
       "      <td>151.290</td>\n",
       "      <td>6.043</td>\n",
       "      <td>0.681</td>\n",
       "      <td>1.648</td>\n",
       "      <td>4.212</td>\n",
       "      <td>0.864</td>\n",
       "      <td>1.668</td>\n",
       "      <td>1.371</td>\n",
       "      <td>0.751</td>\n",
       "      <td>3.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.368</td>\n",
       "      <td>4.151</td>\n",
       "      <td>3.595</td>\n",
       "      <td>0.746</td>\n",
       "      <td>8.724</td>\n",
       "      <td>2.871</td>\n",
       "      <td>0.301</td>\n",
       "      <td>3.688</td>\n",
       "      <td>1.247</td>\n",
       "      <td>38.018</td>\n",
       "      <td>2.212</td>\n",
       "      <td>4.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.049</td>\n",
       "      <td>20.694</td>\n",
       "      <td>8.495</td>\n",
       "      <td>0.090</td>\n",
       "      <td>2.753</td>\n",
       "      <td>0.106</td>\n",
       "      <td>1.044</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1.048</td>\n",
       "      <td>12.813</td>\n",
       "      <td>21.405</td>\n",
       "      <td>0.361</td>\n",
       "      <td>55.581</td>\n",
       "      <td>0.124</td>\n",
       "      <td>15.621</td>\n",
       "      <td>0.036</td>\n",
       "      <td>12.632</td>\n",
       "      <td>6.446</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>61.717</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.409</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>79.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2,848.460</td>\n",
       "      <td>2,286.250</td>\n",
       "      <td>2,138.878</td>\n",
       "      <td>899.949</td>\n",
       "      <td>0.681</td>\n",
       "      <td>87.279</td>\n",
       "      <td>0.116</td>\n",
       "      <td>1.342</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.942</td>\n",
       "      <td>191.225</td>\n",
       "      <td>2.249</td>\n",
       "      <td>395.572</td>\n",
       "      <td>8.071</td>\n",
       "      <td>0.954</td>\n",
       "      <td>182.069</td>\n",
       "      <td>12.338</td>\n",
       "      <td>1.364</td>\n",
       "      <td>-7,150.250</td>\n",
       "      <td>609.500</td>\n",
       "      <td>-8,718.667</td>\n",
       "      <td>-10,366.333</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.439</td>\n",
       "      <td>0.740</td>\n",
       "      <td>60.267</td>\n",
       "      <td>1.267</td>\n",
       "      <td>0.108</td>\n",
       "      <td>2.203</td>\n",
       "      <td>83.581</td>\n",
       "      <td>8.278</td>\n",
       "      <td>49.835</td>\n",
       "      <td>63.689</td>\n",
       "      <td>42.258</td>\n",
       "      <td>65.701</td>\n",
       "      <td>84.733</td>\n",
       "      <td>115.962</td>\n",
       "      <td>2.332</td>\n",
       "      <td>0.927</td>\n",
       "      <td>344.943</td>\n",
       "      <td>9.711</td>\n",
       "      <td>117.397</td>\n",
       "      <td>706.305</td>\n",
       "      <td>0.828</td>\n",
       "      <td>128.602</td>\n",
       "      <td>616.757</td>\n",
       "      <td>40.261</td>\n",
       "      <td>4.486</td>\n",
       "      <td>4.748</td>\n",
       "      <td>2,802.000</td>\n",
       "      <td>0.914</td>\n",
       "      <td>0.936</td>\n",
       "      <td>4.472</td>\n",
       "      <td>-12.153</td>\n",
       "      <td>346.065</td>\n",
       "      <td>9.674</td>\n",
       "      <td>85.509</td>\n",
       "      <td>3.778</td>\n",
       "      <td>6.448</td>\n",
       "      <td>8.764</td>\n",
       "      <td>670.306</td>\n",
       "      <td>0.523</td>\n",
       "      <td>87.025</td>\n",
       "      <td>582.482</td>\n",
       "      <td>21.820</td>\n",
       "      <td>112.325</td>\n",
       "      <td>459.889</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.103</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.120</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>5.947</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.107</td>\n",
       "      <td>2.325</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1,632.312</td>\n",
       "      <td>0.146</td>\n",
       "      <td>7,929.630</td>\n",
       "      <td>-0.257</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.637</td>\n",
       "      <td>-0.949</td>\n",
       "      <td>-0.114</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.229</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.212</td>\n",
       "      <td>0.975</td>\n",
       "      <td>98.861</td>\n",
       "      <td>229.129</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.902</td>\n",
       "      <td>0.000</td>\n",
       "      <td>613.409</td>\n",
       "      <td>0.961</td>\n",
       "      <td>53.610</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.957</td>\n",
       "      <td>5.882</td>\n",
       "      <td>15.590</td>\n",
       "      <td>2.398</td>\n",
       "      <td>15.610</td>\n",
       "      <td>15.540</td>\n",
       "      <td>0.606</td>\n",
       "      <td>2.407</td>\n",
       "      <td>0.411</td>\n",
       "      <td>2.824</td>\n",
       "      <td>-2.789</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.995</td>\n",
       "      <td>2.191</td>\n",
       "      <td>986.099</td>\n",
       "      <td>33.366</td>\n",
       "      <td>61.000</td>\n",
       "      <td>55.400</td>\n",
       "      <td>23.700</td>\n",
       "      <td>37.200</td>\n",
       "      <td>113.065</td>\n",
       "      <td>0.059</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.005</td>\n",
       "      <td>1.748</td>\n",
       "      <td>2.018</td>\n",
       "      <td>4.218</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.571</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>591.601</td>\n",
       "      <td>73.000</td>\n",
       "      <td>76.000</td>\n",
       "      <td>141.000</td>\n",
       "      <td>32.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.105</td>\n",
       "      <td>12.100</td>\n",
       "      <td>0.171</td>\n",
       "      <td>4.430</td>\n",
       "      <td>16.048</td>\n",
       "      <td>0.054</td>\n",
       "      <td>2.560</td>\n",
       "      <td>7.570</td>\n",
       "      <td>11.289</td>\n",
       "      <td>0.080</td>\n",
       "      <td>2.080</td>\n",
       "      <td>10.900</td>\n",
       "      <td>0.161</td>\n",
       "      <td>3.500</td>\n",
       "      <td>3.210</td>\n",
       "      <td>2.080</td>\n",
       "      <td>3.343</td>\n",
       "      <td>16.146</td>\n",
       "      <td>0.051</td>\n",
       "      <td>3.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.730</td>\n",
       "      <td>9.412</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.770</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>52.544</td>\n",
       "      <td>0.015</td>\n",
       "      <td>267.700</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.495</td>\n",
       "      <td>2.257</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>31.770</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.147</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.166</td>\n",
       "      <td>0.020</td>\n",
       "      <td>7.120</td>\n",
       "      <td>1.736</td>\n",
       "      <td>15.956</td>\n",
       "      <td>17.192</td>\n",
       "      <td>8.434</td>\n",
       "      <td>13.199</td>\n",
       "      <td>42.790</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.905</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.596</td>\n",
       "      <td>0.600</td>\n",
       "      <td>1.371</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>163.344</td>\n",
       "      <td>34.072</td>\n",
       "      <td>37.027</td>\n",
       "      <td>71.066</td>\n",
       "      <td>16.872</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.048</td>\n",
       "      <td>4.090</td>\n",
       "      <td>0.052</td>\n",
       "      <td>1.317</td>\n",
       "      <td>4.234</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.846</td>\n",
       "      <td>2.335</td>\n",
       "      <td>3.542</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.658</td>\n",
       "      <td>3.823</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1.073</td>\n",
       "      <td>6.025</td>\n",
       "      <td>0.658</td>\n",
       "      <td>1.131</td>\n",
       "      <td>5.262</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.139</td>\n",
       "      <td>2.947</td>\n",
       "      <td>3.205</td>\n",
       "      <td>1.854</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.553</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.820</td>\n",
       "      <td>0.005</td>\n",
       "      <td>84.476</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8.319</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.238</td>\n",
       "      <td>0.626</td>\n",
       "      <td>2.823</td>\n",
       "      <td>1.823</td>\n",
       "      <td>0.953</td>\n",
       "      <td>1.710</td>\n",
       "      <td>8.779</td>\n",
       "      <td>5.680</td>\n",
       "      <td>2.552</td>\n",
       "      <td>2.027</td>\n",
       "      <td>3.051</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.890</td>\n",
       "      <td>29.677</td>\n",
       "      <td>1.022</td>\n",
       "      <td>1.467</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.847</td>\n",
       "      <td>1.360</td>\n",
       "      <td>1.049</td>\n",
       "      <td>1.329</td>\n",
       "      <td>3.336</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.579</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.705</td>\n",
       "      <td>12.329</td>\n",
       "      <td>25.125</td>\n",
       "      <td>1.575</td>\n",
       "      <td>0.418</td>\n",
       "      <td>0.379</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.088</td>\n",
       "      <td>3.330</td>\n",
       "      <td>1.680</td>\n",
       "      <td>3.465</td>\n",
       "      <td>2.147</td>\n",
       "      <td>4.498</td>\n",
       "      <td>1.794</td>\n",
       "      <td>1.187</td>\n",
       "      <td>9.729</td>\n",
       "      <td>1.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.084</td>\n",
       "      <td>1.448</td>\n",
       "      <td>3.160</td>\n",
       "      <td>22.060</td>\n",
       "      <td>11.312</td>\n",
       "      <td>9.349</td>\n",
       "      <td>2.355</td>\n",
       "      <td>3.528</td>\n",
       "      <td>2.075</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.127</td>\n",
       "      <td>16.762</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.460</td>\n",
       "      <td>0.758</td>\n",
       "      <td>5.836</td>\n",
       "      <td>1.090</td>\n",
       "      <td>0.036</td>\n",
       "      <td>2.951</td>\n",
       "      <td>9.101</td>\n",
       "      <td>2.965</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>18.006</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.974</td>\n",
       "      <td>3.180</td>\n",
       "      <td>0.382</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.213</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.929</td>\n",
       "      <td>2.350</td>\n",
       "      <td>0.239</td>\n",
       "      <td>2.616</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.708</td>\n",
       "      <td>4.576</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>5.836</td>\n",
       "      <td>0.511</td>\n",
       "      <td>394.004</td>\n",
       "      <td>71.254</td>\n",
       "      <td>0.080</td>\n",
       "      <td>8.520</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.037</td>\n",
       "      <td>3.365</td>\n",
       "      <td>0.099</td>\n",
       "      <td>8.842</td>\n",
       "      <td>2.104</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.020</td>\n",
       "      <td>7.237</td>\n",
       "      <td>242.286</td>\n",
       "      <td>0.551</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.412</td>\n",
       "      <td>3.342</td>\n",
       "      <td>317.196</td>\n",
       "      <td>1.074</td>\n",
       "      <td>5.030</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.565</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.948</td>\n",
       "      <td>5.180</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.747</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2,946.120</td>\n",
       "      <td>2,457.785</td>\n",
       "      <td>2,173.481</td>\n",
       "      <td>1,069.646</td>\n",
       "      <td>1.045</td>\n",
       "      <td>99.113</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.432</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.955</td>\n",
       "      <td>198.491</td>\n",
       "      <td>5.993</td>\n",
       "      <td>407.837</td>\n",
       "      <td>9.630</td>\n",
       "      <td>0.967</td>\n",
       "      <td>188.566</td>\n",
       "      <td>12.462</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-5,639.125</td>\n",
       "      <td>2,561.500</td>\n",
       "      <td>-4,401.125</td>\n",
       "      <td>-1,565.000</td>\n",
       "      <td>1.255</td>\n",
       "      <td>1.955</td>\n",
       "      <td>7.119</td>\n",
       "      <td>66.256</td>\n",
       "      <td>2.072</td>\n",
       "      <td>0.164</td>\n",
       "      <td>3.378</td>\n",
       "      <td>84.693</td>\n",
       "      <td>8.665</td>\n",
       "      <td>50.250</td>\n",
       "      <td>64.010</td>\n",
       "      <td>49.492</td>\n",
       "      <td>66.126</td>\n",
       "      <td>86.577</td>\n",
       "      <td>118.102</td>\n",
       "      <td>64.938</td>\n",
       "      <td>2.537</td>\n",
       "      <td>350.905</td>\n",
       "      <td>9.894</td>\n",
       "      <td>130.650</td>\n",
       "      <td>723.919</td>\n",
       "      <td>1.032</td>\n",
       "      <td>136.815</td>\n",
       "      <td>626.327</td>\n",
       "      <td>111.348</td>\n",
       "      <td>4.575</td>\n",
       "      <td>4.811</td>\n",
       "      <td>2,829.000</td>\n",
       "      <td>0.928</td>\n",
       "      <td>0.946</td>\n",
       "      <td>4.543</td>\n",
       "      <td>1.053</td>\n",
       "      <td>349.985</td>\n",
       "      <td>10.219</td>\n",
       "      <td>110.103</td>\n",
       "      <td>10.651</td>\n",
       "      <td>16.567</td>\n",
       "      <td>21.972</td>\n",
       "      <td>697.399</td>\n",
       "      <td>0.896</td>\n",
       "      <td>144.645</td>\n",
       "      <td>611.835</td>\n",
       "      <td>83.473</td>\n",
       "      <td>144.960</td>\n",
       "      <td>464.728</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>-0.026</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>7.096</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.109</td>\n",
       "      <td>2.383</td>\n",
       "      <td>0.973</td>\n",
       "      <td>1,782.208</td>\n",
       "      <td>0.162</td>\n",
       "      <td>8,564.130</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>-0.264</td>\n",
       "      <td>-0.032</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.054</td>\n",
       "      <td>-0.064</td>\n",
       "      <td>0.979</td>\n",
       "      <td>101.219</td>\n",
       "      <td>229.994</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>722.483</td>\n",
       "      <td>0.989</td>\n",
       "      <td>57.782</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.966</td>\n",
       "      <td>6.218</td>\n",
       "      <td>15.760</td>\n",
       "      <td>3.083</td>\n",
       "      <td>15.780</td>\n",
       "      <td>15.750</td>\n",
       "      <td>0.945</td>\n",
       "      <td>2.622</td>\n",
       "      <td>0.539</td>\n",
       "      <td>3.102</td>\n",
       "      <td>-0.426</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.997</td>\n",
       "      <td>2.277</td>\n",
       "      <td>1,001.871</td>\n",
       "      <td>37.025</td>\n",
       "      <td>88.000</td>\n",
       "      <td>91.450</td>\n",
       "      <td>96.050</td>\n",
       "      <td>52.400</td>\n",
       "      <td>264.955</td>\n",
       "      <td>0.146</td>\n",
       "      <td>5.058</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.010</td>\n",
       "      <td>5.782</td>\n",
       "      <td>4.535</td>\n",
       "      <td>7.770</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.006</td>\n",
       "      <td>5.516</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1,115.675</td>\n",
       "      <td>464.000</td>\n",
       "      <td>361.000</td>\n",
       "      <td>1,411.000</td>\n",
       "      <td>545.500</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.125</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.247</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.725</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.228</td>\n",
       "      <td>15.870</td>\n",
       "      <td>0.358</td>\n",
       "      <td>8.155</td>\n",
       "      <td>22.903</td>\n",
       "      <td>0.103</td>\n",
       "      <td>5.510</td>\n",
       "      <td>15.815</td>\n",
       "      <td>30.908</td>\n",
       "      <td>0.241</td>\n",
       "      <td>5.090</td>\n",
       "      <td>18.027</td>\n",
       "      <td>0.309</td>\n",
       "      <td>7.130</td>\n",
       "      <td>15.955</td>\n",
       "      <td>5.090</td>\n",
       "      <td>6.672</td>\n",
       "      <td>24.297</td>\n",
       "      <td>0.116</td>\n",
       "      <td>6.600</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.590</td>\n",
       "      <td>55.331</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.048</td>\n",
       "      <td>2.780</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.001</td>\n",
       "      <td>98.047</td>\n",
       "      <td>0.029</td>\n",
       "      <td>809.250</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.813</td>\n",
       "      <td>2.939</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>79.706</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.345</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.036</td>\n",
       "      <td>12.735</td>\n",
       "      <td>2.961</td>\n",
       "      <td>23.728</td>\n",
       "      <td>31.737</td>\n",
       "      <td>31.899</td>\n",
       "      <td>18.456</td>\n",
       "      <td>88.008</td>\n",
       "      <td>0.052</td>\n",
       "      <td>1.696</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.994</td>\n",
       "      <td>1.410</td>\n",
       "      <td>2.564</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.982</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.010</td>\n",
       "      <td>358.650</td>\n",
       "      <td>209.684</td>\n",
       "      <td>158.983</td>\n",
       "      <td>613.149</td>\n",
       "      <td>257.500</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.314</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.093</td>\n",
       "      <td>5.101</td>\n",
       "      <td>0.109</td>\n",
       "      <td>2.284</td>\n",
       "      <td>6.581</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.765</td>\n",
       "      <td>4.771</td>\n",
       "      <td>8.903</td>\n",
       "      <td>0.077</td>\n",
       "      <td>1.547</td>\n",
       "      <td>5.610</td>\n",
       "      <td>0.096</td>\n",
       "      <td>2.144</td>\n",
       "      <td>8.603</td>\n",
       "      <td>1.547</td>\n",
       "      <td>1.949</td>\n",
       "      <td>7.312</td>\n",
       "      <td>0.038</td>\n",
       "      <td>2.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.238</td>\n",
       "      <td>18.040</td>\n",
       "      <td>4.991</td>\n",
       "      <td>2.802</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.051</td>\n",
       "      <td>0.009</td>\n",
       "      <td>240.105</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.923</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>26.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.012</td>\n",
       "      <td>4.521</td>\n",
       "      <td>1.034</td>\n",
       "      <td>3.954</td>\n",
       "      <td>3.148</td>\n",
       "      <td>3.809</td>\n",
       "      <td>2.380</td>\n",
       "      <td>18.008</td>\n",
       "      <td>10.749</td>\n",
       "      <td>4.907</td>\n",
       "      <td>2.583</td>\n",
       "      <td>5.164</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.003</td>\n",
       "      <td>2.924</td>\n",
       "      <td>57.823</td>\n",
       "      <td>1.883</td>\n",
       "      <td>3.982</td>\n",
       "      <td>0.663</td>\n",
       "      <td>2.916</td>\n",
       "      <td>2.116</td>\n",
       "      <td>2.695</td>\n",
       "      <td>8.050</td>\n",
       "      <td>13.719</td>\n",
       "      <td>32.080</td>\n",
       "      <td>20.638</td>\n",
       "      <td>8.189</td>\n",
       "      <td>3.605</td>\n",
       "      <td>1.882</td>\n",
       "      <td>3.106</td>\n",
       "      <td>37.106</td>\n",
       "      <td>49.008</td>\n",
       "      <td>7.444</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.580</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.591</td>\n",
       "      <td>1.093</td>\n",
       "      <td>0.222</td>\n",
       "      <td>0.193</td>\n",
       "      <td>4.511</td>\n",
       "      <td>3.583</td>\n",
       "      <td>6.080</td>\n",
       "      <td>3.111</td>\n",
       "      <td>8.761</td>\n",
       "      <td>3.978</td>\n",
       "      <td>2.497</td>\n",
       "      <td>23.714</td>\n",
       "      <td>5.232</td>\n",
       "      <td>27.977</td>\n",
       "      <td>5.139</td>\n",
       "      <td>2.940</td>\n",
       "      <td>6.058</td>\n",
       "      <td>104.448</td>\n",
       "      <td>26.161</td>\n",
       "      <td>23.472</td>\n",
       "      <td>3.423</td>\n",
       "      <td>11.871</td>\n",
       "      <td>4.406</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.818</td>\n",
       "      <td>52.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>86.354</td>\n",
       "      <td>52.197</td>\n",
       "      <td>46.723</td>\n",
       "      <td>141.072</td>\n",
       "      <td>74.951</td>\n",
       "      <td>57.520</td>\n",
       "      <td>126.516</td>\n",
       "      <td>36.166</td>\n",
       "      <td>1.767</td>\n",
       "      <td>6.598</td>\n",
       "      <td>1.720</td>\n",
       "      <td>0.121</td>\n",
       "      <td>5.330</td>\n",
       "      <td>16.157</td>\n",
       "      <td>8.849</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>40.893</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.805</td>\n",
       "      <td>1.273</td>\n",
       "      <td>5.721</td>\n",
       "      <td>1.362</td>\n",
       "      <td>0.000</td>\n",
       "      <td>11.016</td>\n",
       "      <td>0.069</td>\n",
       "      <td>4.117</td>\n",
       "      <td>4.290</td>\n",
       "      <td>0.464</td>\n",
       "      <td>4.993</td>\n",
       "      <td>1.586</td>\n",
       "      <td>1.270</td>\n",
       "      <td>7.487</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.743</td>\n",
       "      <td>400.814</td>\n",
       "      <td>73.254</td>\n",
       "      <td>0.305</td>\n",
       "      <td>14.530</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.126</td>\n",
       "      <td>6.738</td>\n",
       "      <td>0.354</td>\n",
       "      <td>33.220</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.189</td>\n",
       "      <td>0.958</td>\n",
       "      <td>0.169</td>\n",
       "      <td>0.039</td>\n",
       "      <td>17.560</td>\n",
       "      <td>260.250</td>\n",
       "      <td>0.567</td>\n",
       "      <td>4.980</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.884</td>\n",
       "      <td>15.466</td>\n",
       "      <td>531.186</td>\n",
       "      <td>1.974</td>\n",
       "      <td>7.385</td>\n",
       "      <td>0.203</td>\n",
       "      <td>2.540</td>\n",
       "      <td>0.064</td>\n",
       "      <td>1.378</td>\n",
       "      <td>10.806</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.004</td>\n",
       "      <td>53.746</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.286</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.004</td>\n",
       "      <td>45.619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2,989.315</td>\n",
       "      <td>2,500.850</td>\n",
       "      <td>2,196.689</td>\n",
       "      <td>1,281.537</td>\n",
       "      <td>1.306</td>\n",
       "      <td>101.639</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.469</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.963</td>\n",
       "      <td>199.756</td>\n",
       "      <td>8.528</td>\n",
       "      <td>412.547</td>\n",
       "      <td>9.909</td>\n",
       "      <td>0.971</td>\n",
       "      <td>189.543</td>\n",
       "      <td>12.490</td>\n",
       "      <td>1.407</td>\n",
       "      <td>-5,447.250</td>\n",
       "      <td>2,635.750</td>\n",
       "      <td>-3,771.250</td>\n",
       "      <td>37.250</td>\n",
       "      <td>1.281</td>\n",
       "      <td>1.988</td>\n",
       "      <td>7.258</td>\n",
       "      <td>68.156</td>\n",
       "      <td>2.389</td>\n",
       "      <td>0.188</td>\n",
       "      <td>3.427</td>\n",
       "      <td>85.239</td>\n",
       "      <td>8.894</td>\n",
       "      <td>50.381</td>\n",
       "      <td>64.139</td>\n",
       "      <td>49.619</td>\n",
       "      <td>66.279</td>\n",
       "      <td>86.844</td>\n",
       "      <td>118.257</td>\n",
       "      <td>77.105</td>\n",
       "      <td>3.093</td>\n",
       "      <td>354.081</td>\n",
       "      <td>10.008</td>\n",
       "      <td>136.599</td>\n",
       "      <td>732.328</td>\n",
       "      <td>1.272</td>\n",
       "      <td>139.850</td>\n",
       "      <td>631.018</td>\n",
       "      <td>193.089</td>\n",
       "      <td>4.595</td>\n",
       "      <td>4.845</td>\n",
       "      <td>2,850.000</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.950</td>\n",
       "      <td>4.608</td>\n",
       "      <td>5.522</td>\n",
       "      <td>353.678</td>\n",
       "      <td>10.437</td>\n",
       "      <td>115.773</td>\n",
       "      <td>13.410</td>\n",
       "      <td>20.860</td>\n",
       "      <td>27.975</td>\n",
       "      <td>704.897</td>\n",
       "      <td>1.013</td>\n",
       "      <td>147.065</td>\n",
       "      <td>617.347</td>\n",
       "      <td>100.656</td>\n",
       "      <td>151.610</td>\n",
       "      <td>466.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.007</td>\n",
       "      <td>7.384</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.112</td>\n",
       "      <td>2.409</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1,809.817</td>\n",
       "      <td>0.185</td>\n",
       "      <td>8,745.810</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>0.980</td>\n",
       "      <td>101.595</td>\n",
       "      <td>230.493</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.948</td>\n",
       "      <td>0.000</td>\n",
       "      <td>745.590</td>\n",
       "      <td>0.991</td>\n",
       "      <td>58.463</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.970</td>\n",
       "      <td>6.305</td>\n",
       "      <td>15.810</td>\n",
       "      <td>3.526</td>\n",
       "      <td>15.845</td>\n",
       "      <td>15.825</td>\n",
       "      <td>1.078</td>\n",
       "      <td>2.764</td>\n",
       "      <td>0.606</td>\n",
       "      <td>3.237</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.771</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.324</td>\n",
       "      <td>1,004.916</td>\n",
       "      <td>38.698</td>\n",
       "      <td>103.000</td>\n",
       "      <td>131.950</td>\n",
       "      <td>136.500</td>\n",
       "      <td>56.750</td>\n",
       "      <td>329.588</td>\n",
       "      <td>0.238</td>\n",
       "      <td>6.255</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.019</td>\n",
       "      <td>8.081</td>\n",
       "      <td>5.861</td>\n",
       "      <td>11.430</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.013</td>\n",
       "      <td>7.707</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1,324.350</td>\n",
       "      <td>686.000</td>\n",
       "      <td>507.000</td>\n",
       "      <td>3,185.000</td>\n",
       "      <td>1,790.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.187</td>\n",
       "      <td>2.700</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.799</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.286</td>\n",
       "      <td>17.615</td>\n",
       "      <td>0.475</td>\n",
       "      <td>9.985</td>\n",
       "      <td>29.824</td>\n",
       "      <td>0.131</td>\n",
       "      <td>6.520</td>\n",
       "      <td>18.400</td>\n",
       "      <td>46.684</td>\n",
       "      <td>0.272</td>\n",
       "      <td>7.085</td>\n",
       "      <td>20.035</td>\n",
       "      <td>0.474</td>\n",
       "      <td>8.605</td>\n",
       "      <td>17.830</td>\n",
       "      <td>7.085</td>\n",
       "      <td>8.904</td>\n",
       "      <td>30.730</td>\n",
       "      <td>0.172</td>\n",
       "      <td>9.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.300</td>\n",
       "      <td>71.701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.063</td>\n",
       "      <td>3.675</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.002</td>\n",
       "      <td>120.472</td>\n",
       "      <td>0.038</td>\n",
       "      <td>988.700</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.057</td>\n",
       "      <td>3.821</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>106.057</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.917</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.069</td>\n",
       "      <td>18.080</td>\n",
       "      <td>3.697</td>\n",
       "      <td>27.616</td>\n",
       "      <td>47.266</td>\n",
       "      <td>45.355</td>\n",
       "      <td>19.620</td>\n",
       "      <td>110.756</td>\n",
       "      <td>0.078</td>\n",
       "      <td>2.087</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.594</td>\n",
       "      <td>1.906</td>\n",
       "      <td>3.345</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.522</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.012</td>\n",
       "      <td>407.871</td>\n",
       "      <td>313.216</td>\n",
       "      <td>225.984</td>\n",
       "      <td>1,396.915</td>\n",
       "      <td>824.032</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.221</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.118</td>\n",
       "      <td>5.531</td>\n",
       "      <td>0.157</td>\n",
       "      <td>2.829</td>\n",
       "      <td>9.013</td>\n",
       "      <td>0.039</td>\n",
       "      <td>2.044</td>\n",
       "      <td>5.476</td>\n",
       "      <td>14.590</td>\n",
       "      <td>0.087</td>\n",
       "      <td>2.037</td>\n",
       "      <td>6.135</td>\n",
       "      <td>0.135</td>\n",
       "      <td>2.577</td>\n",
       "      <td>9.495</td>\n",
       "      <td>2.037</td>\n",
       "      <td>2.625</td>\n",
       "      <td>9.794</td>\n",
       "      <td>0.053</td>\n",
       "      <td>2.721</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.210</td>\n",
       "      <td>23.117</td>\n",
       "      <td>5.899</td>\n",
       "      <td>3.451</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.241</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.001</td>\n",
       "      <td>38.737</td>\n",
       "      <td>0.012</td>\n",
       "      <td>332.336</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.311</td>\n",
       "      <td>1.174</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>33.447</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.023</td>\n",
       "      <td>6.087</td>\n",
       "      <td>1.240</td>\n",
       "      <td>4.746</td>\n",
       "      <td>4.389</td>\n",
       "      <td>5.335</td>\n",
       "      <td>2.592</td>\n",
       "      <td>27.017</td>\n",
       "      <td>19.143</td>\n",
       "      <td>6.210</td>\n",
       "      <td>3.071</td>\n",
       "      <td>6.737</td>\n",
       "      <td>258.743</td>\n",
       "      <td>332.569</td>\n",
       "      <td>1.950</td>\n",
       "      <td>4.039</td>\n",
       "      <td>72.062</td>\n",
       "      <td>2.717</td>\n",
       "      <td>5.363</td>\n",
       "      <td>1.301</td>\n",
       "      <td>4.024</td>\n",
       "      <td>3.022</td>\n",
       "      <td>3.423</td>\n",
       "      <td>13.473</td>\n",
       "      <td>18.381</td>\n",
       "      <td>60.505</td>\n",
       "      <td>168.968</td>\n",
       "      <td>11.360</td>\n",
       "      <td>4.758</td>\n",
       "      <td>2.824</td>\n",
       "      <td>3.931</td>\n",
       "      <td>50.467</td>\n",
       "      <td>65.438</td>\n",
       "      <td>11.781</td>\n",
       "      <td>0.809</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.894</td>\n",
       "      <td>0.661</td>\n",
       "      <td>1.203</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.242</td>\n",
       "      <td>4.992</td>\n",
       "      <td>4.696</td>\n",
       "      <td>7.196</td>\n",
       "      <td>4.120</td>\n",
       "      <td>10.710</td>\n",
       "      <td>4.735</td>\n",
       "      <td>2.928</td>\n",
       "      <td>27.764</td>\n",
       "      <td>5.912</td>\n",
       "      <td>82.805</td>\n",
       "      <td>5.646</td>\n",
       "      <td>4.505</td>\n",
       "      <td>7.592</td>\n",
       "      <td>134.872</td>\n",
       "      <td>33.055</td>\n",
       "      <td>34.014</td>\n",
       "      <td>4.369</td>\n",
       "      <td>17.583</td>\n",
       "      <td>6.120</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.300</td>\n",
       "      <td>65.028</td>\n",
       "      <td>269.087</td>\n",
       "      <td>144.195</td>\n",
       "      <td>132.157</td>\n",
       "      <td>99.449</td>\n",
       "      <td>267.753</td>\n",
       "      <td>105.453</td>\n",
       "      <td>312.613</td>\n",
       "      <td>230.635</td>\n",
       "      <td>49.553</td>\n",
       "      <td>2.355</td>\n",
       "      <td>7.935</td>\n",
       "      <td>2.418</td>\n",
       "      <td>0.238</td>\n",
       "      <td>6.619</td>\n",
       "      <td>22.835</td>\n",
       "      <td>11.407</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>58.232</td>\n",
       "      <td>278.720</td>\n",
       "      <td>0.231</td>\n",
       "      <td>1.036</td>\n",
       "      <td>1.662</td>\n",
       "      <td>9.052</td>\n",
       "      <td>2.228</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.359</td>\n",
       "      <td>0.093</td>\n",
       "      <td>4.969</td>\n",
       "      <td>5.076</td>\n",
       "      <td>1.516</td>\n",
       "      <td>6.295</td>\n",
       "      <td>3.004</td>\n",
       "      <td>1.789</td>\n",
       "      <td>9.398</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.887</td>\n",
       "      <td>404.546</td>\n",
       "      <td>74.199</td>\n",
       "      <td>0.503</td>\n",
       "      <td>16.360</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.218</td>\n",
       "      <td>7.521</td>\n",
       "      <td>0.498</td>\n",
       "      <td>58.805</td>\n",
       "      <td>4.076</td>\n",
       "      <td>1.653</td>\n",
       "      <td>0.977</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.062</td>\n",
       "      <td>30.958</td>\n",
       "      <td>264.272</td>\n",
       "      <td>0.659</td>\n",
       "      <td>5.665</td>\n",
       "      <td>0.134</td>\n",
       "      <td>2.252</td>\n",
       "      <td>0.054</td>\n",
       "      <td>2.234</td>\n",
       "      <td>19.213</td>\n",
       "      <td>532.246</td>\n",
       "      <td>2.204</td>\n",
       "      <td>8.700</td>\n",
       "      <td>0.282</td>\n",
       "      <td>3.097</td>\n",
       "      <td>0.089</td>\n",
       "      <td>1.633</td>\n",
       "      <td>13.504</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>75.508</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.759</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>76.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3,053.280</td>\n",
       "      <td>2,532.010</td>\n",
       "      <td>2,218.936</td>\n",
       "      <td>1,558.445</td>\n",
       "      <td>1.511</td>\n",
       "      <td>104.114</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.510</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.970</td>\n",
       "      <td>201.829</td>\n",
       "      <td>10.404</td>\n",
       "      <td>418.007</td>\n",
       "      <td>10.196</td>\n",
       "      <td>0.977</td>\n",
       "      <td>191.725</td>\n",
       "      <td>12.548</td>\n",
       "      <td>1.417</td>\n",
       "      <td>-5,267.750</td>\n",
       "      <td>2,756.000</td>\n",
       "      <td>-3,223.000</td>\n",
       "      <td>771.375</td>\n",
       "      <td>1.306</td>\n",
       "      <td>2.002</td>\n",
       "      <td>7.331</td>\n",
       "      <td>70.222</td>\n",
       "      <td>2.656</td>\n",
       "      <td>0.213</td>\n",
       "      <td>3.511</td>\n",
       "      <td>85.742</td>\n",
       "      <td>9.201</td>\n",
       "      <td>50.508</td>\n",
       "      <td>64.236</td>\n",
       "      <td>49.750</td>\n",
       "      <td>66.401</td>\n",
       "      <td>87.040</td>\n",
       "      <td>118.772</td>\n",
       "      <td>79.235</td>\n",
       "      <td>4.019</td>\n",
       "      <td>359.878</td>\n",
       "      <td>10.120</td>\n",
       "      <td>141.644</td>\n",
       "      <td>740.856</td>\n",
       "      <td>1.348</td>\n",
       "      <td>142.747</td>\n",
       "      <td>637.198</td>\n",
       "      <td>207.443</td>\n",
       "      <td>4.612</td>\n",
       "      <td>4.869</td>\n",
       "      <td>2,875.000</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.951</td>\n",
       "      <td>4.670</td>\n",
       "      <td>15.469</td>\n",
       "      <td>359.621</td>\n",
       "      <td>10.672</td>\n",
       "      <td>120.980</td>\n",
       "      <td>18.060</td>\n",
       "      <td>26.265</td>\n",
       "      <td>33.535</td>\n",
       "      <td>713.977</td>\n",
       "      <td>1.096</td>\n",
       "      <td>149.508</td>\n",
       "      <td>623.161</td>\n",
       "      <td>112.213</td>\n",
       "      <td>160.651</td>\n",
       "      <td>468.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.029</td>\n",
       "      <td>7.694</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.428</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1,846.956</td>\n",
       "      <td>0.199</td>\n",
       "      <td>8,931.225</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.981</td>\n",
       "      <td>102.127</td>\n",
       "      <td>231.099</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>763.425</td>\n",
       "      <td>0.991</td>\n",
       "      <td>59.030</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.978</td>\n",
       "      <td>6.380</td>\n",
       "      <td>15.880</td>\n",
       "      <td>4.184</td>\n",
       "      <td>15.910</td>\n",
       "      <td>15.902</td>\n",
       "      <td>1.239</td>\n",
       "      <td>2.891</td>\n",
       "      <td>0.678</td>\n",
       "      <td>3.342</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.368</td>\n",
       "      <td>1,011.034</td>\n",
       "      <td>40.405</td>\n",
       "      <td>120.000</td>\n",
       "      <td>183.150</td>\n",
       "      <td>164.000</td>\n",
       "      <td>64.800</td>\n",
       "      <td>487.251</td>\n",
       "      <td>0.441</td>\n",
       "      <td>7.388</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.024</td>\n",
       "      <td>9.779</td>\n",
       "      <td>7.890</td>\n",
       "      <td>13.860</td>\n",
       "      <td>0.944</td>\n",
       "      <td>0.017</td>\n",
       "      <td>9.325</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.043</td>\n",
       "      <td>1,396.775</td>\n",
       "      <td>1,045.000</td>\n",
       "      <td>653.500</td>\n",
       "      <td>4,634.500</td>\n",
       "      <td>7,145.000</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.266</td>\n",
       "      <td>3.250</td>\n",
       "      <td>1.450</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.418</td>\n",
       "      <td>20.225</td>\n",
       "      <td>0.618</td>\n",
       "      <td>12.883</td>\n",
       "      <td>32.293</td>\n",
       "      <td>0.172</td>\n",
       "      <td>8.255</td>\n",
       "      <td>21.285</td>\n",
       "      <td>57.656</td>\n",
       "      <td>0.302</td>\n",
       "      <td>9.485</td>\n",
       "      <td>21.918</td>\n",
       "      <td>0.781</td>\n",
       "      <td>11.400</td>\n",
       "      <td>20.998</td>\n",
       "      <td>9.450</td>\n",
       "      <td>12.295</td>\n",
       "      <td>35.027</td>\n",
       "      <td>0.264</td>\n",
       "      <td>12.815</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.720</td>\n",
       "      <td>88.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.093</td>\n",
       "      <td>4.455</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.006</td>\n",
       "      <td>144.514</td>\n",
       "      <td>0.055</td>\n",
       "      <td>1,249.750</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.380</td>\n",
       "      <td>4.365</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>132.345</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.092</td>\n",
       "      <td>25.577</td>\n",
       "      <td>4.424</td>\n",
       "      <td>30.985</td>\n",
       "      <td>62.085</td>\n",
       "      <td>56.554</td>\n",
       "      <td>21.460</td>\n",
       "      <td>152.869</td>\n",
       "      <td>0.141</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.008</td>\n",
       "      <td>3.165</td>\n",
       "      <td>2.440</td>\n",
       "      <td>4.050</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3.009</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.013</td>\n",
       "      <td>478.307</td>\n",
       "      <td>461.934</td>\n",
       "      <td>300.196</td>\n",
       "      <td>2,262.575</td>\n",
       "      <td>3,614.284</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.064</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.315</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.146</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.158</td>\n",
       "      <td>6.112</td>\n",
       "      <td>0.194</td>\n",
       "      <td>3.762</td>\n",
       "      <td>9.840</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.511</td>\n",
       "      <td>6.259</td>\n",
       "      <td>17.871</td>\n",
       "      <td>0.097</td>\n",
       "      <td>3.032</td>\n",
       "      <td>6.778</td>\n",
       "      <td>0.233</td>\n",
       "      <td>3.395</td>\n",
       "      <td>10.503</td>\n",
       "      <td>2.983</td>\n",
       "      <td>3.343</td>\n",
       "      <td>10.825</td>\n",
       "      <td>0.075</td>\n",
       "      <td>3.649</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.021</td>\n",
       "      <td>27.101</td>\n",
       "      <td>7.264</td>\n",
       "      <td>5.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.543</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>47.986</td>\n",
       "      <td>0.017</td>\n",
       "      <td>415.346</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.443</td>\n",
       "      <td>1.282</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>44.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.119</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.033</td>\n",
       "      <td>9.435</td>\n",
       "      <td>1.446</td>\n",
       "      <td>5.453</td>\n",
       "      <td>6.018</td>\n",
       "      <td>6.565</td>\n",
       "      <td>2.909</td>\n",
       "      <td>38.313</td>\n",
       "      <td>29.280</td>\n",
       "      <td>7.252</td>\n",
       "      <td>3.844</td>\n",
       "      <td>8.398</td>\n",
       "      <td>489.331</td>\n",
       "      <td>544.972</td>\n",
       "      <td>2.464</td>\n",
       "      <td>4.940</td>\n",
       "      <td>106.510</td>\n",
       "      <td>3.315</td>\n",
       "      <td>9.437</td>\n",
       "      <td>1.706</td>\n",
       "      <td>4.969</td>\n",
       "      <td>4.058</td>\n",
       "      <td>4.785</td>\n",
       "      <td>18.949</td>\n",
       "      <td>24.021</td>\n",
       "      <td>119.848</td>\n",
       "      <td>342.065</td>\n",
       "      <td>13.713</td>\n",
       "      <td>6.145</td>\n",
       "      <td>3.859</td>\n",
       "      <td>4.875</td>\n",
       "      <td>65.212</td>\n",
       "      <td>80.012</td>\n",
       "      <td>15.075</td>\n",
       "      <td>0.950</td>\n",
       "      <td>1.559</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.962</td>\n",
       "      <td>0.762</td>\n",
       "      <td>1.289</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.354</td>\n",
       "      <td>5.709</td>\n",
       "      <td>6.129</td>\n",
       "      <td>9.012</td>\n",
       "      <td>4.422</td>\n",
       "      <td>14.348</td>\n",
       "      <td>5.926</td>\n",
       "      <td>3.363</td>\n",
       "      <td>38.190</td>\n",
       "      <td>6.644</td>\n",
       "      <td>226.120</td>\n",
       "      <td>6.126</td>\n",
       "      <td>7.617</td>\n",
       "      <td>10.314</td>\n",
       "      <td>170.605</td>\n",
       "      <td>46.595</td>\n",
       "      <td>43.275</td>\n",
       "      <td>4.941</td>\n",
       "      <td>26.593</td>\n",
       "      <td>8.603</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.699</td>\n",
       "      <td>93.340</td>\n",
       "      <td>557.295</td>\n",
       "      <td>246.779</td>\n",
       "      <td>243.885</td>\n",
       "      <td>225.544</td>\n",
       "      <td>485.135</td>\n",
       "      <td>320.085</td>\n",
       "      <td>492.283</td>\n",
       "      <td>367.175</td>\n",
       "      <td>61.323</td>\n",
       "      <td>3.044</td>\n",
       "      <td>9.095</td>\n",
       "      <td>3.200</td>\n",
       "      <td>0.665</td>\n",
       "      <td>7.933</td>\n",
       "      <td>32.108</td>\n",
       "      <td>14.387</td>\n",
       "      <td>477.300</td>\n",
       "      <td>477.876</td>\n",
       "      <td>85.076</td>\n",
       "      <td>697.778</td>\n",
       "      <td>0.307</td>\n",
       "      <td>1.352</td>\n",
       "      <td>1.891</td>\n",
       "      <td>17.977</td>\n",
       "      <td>3.064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.812</td>\n",
       "      <td>0.127</td>\n",
       "      <td>6.298</td>\n",
       "      <td>6.263</td>\n",
       "      <td>2.094</td>\n",
       "      <td>7.295</td>\n",
       "      <td>3.926</td>\n",
       "      <td>2.545</td>\n",
       "      <td>12.064</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>1.258</td>\n",
       "      <td>407.376</td>\n",
       "      <td>79.993</td>\n",
       "      <td>0.787</td>\n",
       "      <td>20.047</td>\n",
       "      <td>1.440</td>\n",
       "      <td>0.335</td>\n",
       "      <td>8.780</td>\n",
       "      <td>0.604</td>\n",
       "      <td>77.382</td>\n",
       "      <td>4.952</td>\n",
       "      <td>1.816</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.100</td>\n",
       "      <td>47.433</td>\n",
       "      <td>265.149</td>\n",
       "      <td>0.727</td>\n",
       "      <td>8.360</td>\n",
       "      <td>0.197</td>\n",
       "      <td>3.318</td>\n",
       "      <td>0.077</td>\n",
       "      <td>3.236</td>\n",
       "      <td>27.917</td>\n",
       "      <td>534.536</td>\n",
       "      <td>2.307</td>\n",
       "      <td>10.350</td>\n",
       "      <td>0.351</td>\n",
       "      <td>3.586</td>\n",
       "      <td>0.109</td>\n",
       "      <td>1.945</td>\n",
       "      <td>16.170</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.007</td>\n",
       "      <td>109.427</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.393</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.007</td>\n",
       "      <td>120.742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3,266.550</td>\n",
       "      <td>2,714.300</td>\n",
       "      <td>2,315.267</td>\n",
       "      <td>2,360.133</td>\n",
       "      <td>2.421</td>\n",
       "      <td>119.354</td>\n",
       "      <td>0.128</td>\n",
       "      <td>1.611</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.982</td>\n",
       "      <td>207.030</td>\n",
       "      <td>14.188</td>\n",
       "      <td>434.794</td>\n",
       "      <td>12.200</td>\n",
       "      <td>0.982</td>\n",
       "      <td>197.843</td>\n",
       "      <td>12.657</td>\n",
       "      <td>1.437</td>\n",
       "      <td>-1,368.750</td>\n",
       "      <td>3,493.750</td>\n",
       "      <td>135.750</td>\n",
       "      <td>6,476.000</td>\n",
       "      <td>1.383</td>\n",
       "      <td>2.043</td>\n",
       "      <td>7.579</td>\n",
       "      <td>75.944</td>\n",
       "      <td>3.244</td>\n",
       "      <td>0.285</td>\n",
       "      <td>4.704</td>\n",
       "      <td>103.108</td>\n",
       "      <td>23.345</td>\n",
       "      <td>57.742</td>\n",
       "      <td>74.851</td>\n",
       "      <td>50.165</td>\n",
       "      <td>67.199</td>\n",
       "      <td>88.419</td>\n",
       "      <td>127.394</td>\n",
       "      <td>84.980</td>\n",
       "      <td>6.250</td>\n",
       "      <td>366.804</td>\n",
       "      <td>10.450</td>\n",
       "      <td>156.623</td>\n",
       "      <td>761.818</td>\n",
       "      <td>1.473</td>\n",
       "      <td>150.511</td>\n",
       "      <td>647.264</td>\n",
       "      <td>239.493</td>\n",
       "      <td>4.681</td>\n",
       "      <td>4.931</td>\n",
       "      <td>2,936.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.959</td>\n",
       "      <td>4.848</td>\n",
       "      <td>53.682</td>\n",
       "      <td>367.216</td>\n",
       "      <td>11.785</td>\n",
       "      <td>168.194</td>\n",
       "      <td>53.945</td>\n",
       "      <td>42.630</td>\n",
       "      <td>56.896</td>\n",
       "      <td>727.561</td>\n",
       "      <td>6,363.801</td>\n",
       "      <td>157.081</td>\n",
       "      <td>636.826</td>\n",
       "      <td>233.611</td>\n",
       "      <td>168.870</td>\n",
       "      <td>493.666</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.119</td>\n",
       "      <td>8.871</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.115</td>\n",
       "      <td>2.508</td>\n",
       "      <td>0.993</td>\n",
       "      <td>1,955.883</td>\n",
       "      <td>1.386</td>\n",
       "      <td>9,731.440</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.520</td>\n",
       "      <td>1.075</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.983</td>\n",
       "      <td>105.199</td>\n",
       "      <td>234.777</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.969</td>\n",
       "      <td>0.023</td>\n",
       "      <td>914.825</td>\n",
       "      <td>0.992</td>\n",
       "      <td>60.947</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.981</td>\n",
       "      <td>6.750</td>\n",
       "      <td>16.070</td>\n",
       "      <td>5.760</td>\n",
       "      <td>16.100</td>\n",
       "      <td>16.090</td>\n",
       "      <td>1.867</td>\n",
       "      <td>3.955</td>\n",
       "      <td>0.890</td>\n",
       "      <td>3.823</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.876</td>\n",
       "      <td>1.009</td>\n",
       "      <td>2.424</td>\n",
       "      <td>1,015.568</td>\n",
       "      <td>46.035</td>\n",
       "      <td>634.000</td>\n",
       "      <td>254.600</td>\n",
       "      <td>254.000</td>\n",
       "      <td>111.900</td>\n",
       "      <td>1,533.777</td>\n",
       "      <td>2.818</td>\n",
       "      <td>12.390</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.446</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.060</td>\n",
       "      <td>25.164</td>\n",
       "      <td>17.747</td>\n",
       "      <td>24.113</td>\n",
       "      <td>5.866</td>\n",
       "      <td>0.058</td>\n",
       "      <td>24.504</td>\n",
       "      <td>0.930</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.077</td>\n",
       "      <td>1,625.700</td>\n",
       "      <td>7,709.000</td>\n",
       "      <td>4,170.000</td>\n",
       "      <td>19,368.000</td>\n",
       "      <td>21,016.000</td>\n",
       "      <td>0.931</td>\n",
       "      <td>1.802</td>\n",
       "      <td>3.077</td>\n",
       "      <td>21.100</td>\n",
       "      <td>8.400</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.881</td>\n",
       "      <td>1.082</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.448</td>\n",
       "      <td>1.078</td>\n",
       "      <td>0.435</td>\n",
       "      <td>1.121</td>\n",
       "      <td>26.710</td>\n",
       "      <td>1.054</td>\n",
       "      <td>20.150</td>\n",
       "      <td>42.781</td>\n",
       "      <td>0.583</td>\n",
       "      <td>16.780</td>\n",
       "      <td>26.840</td>\n",
       "      <td>111.677</td>\n",
       "      <td>4.838</td>\n",
       "      <td>263.230</td>\n",
       "      <td>158.350</td>\n",
       "      <td>8.898</td>\n",
       "      <td>265.970</td>\n",
       "      <td>156.080</td>\n",
       "      <td>54.260</td>\n",
       "      <td>129.641</td>\n",
       "      <td>262.689</td>\n",
       "      <td>9,997.725</td>\n",
       "      <td>150.870</td>\n",
       "      <td>0.000</td>\n",
       "      <td>250.210</td>\n",
       "      <td>142.078</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.178</td>\n",
       "      <td>7.040</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.989</td>\n",
       "      <td>254.543</td>\n",
       "      <td>0.985</td>\n",
       "      <td>3,601.300</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>6.184</td>\n",
       "      <td>6.520</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.207</td>\n",
       "      <td>346.793</td>\n",
       "      <td>0.039</td>\n",
       "      <td>6.407</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.158</td>\n",
       "      <td>38.321</td>\n",
       "      <td>8.478</td>\n",
       "      <td>110.295</td>\n",
       "      <td>79.971</td>\n",
       "      <td>85.026</td>\n",
       "      <td>36.405</td>\n",
       "      <td>483.478</td>\n",
       "      <td>0.902</td>\n",
       "      <td>4.154</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.021</td>\n",
       "      <td>7.430</td>\n",
       "      <td>5.214</td>\n",
       "      <td>6.836</td>\n",
       "      <td>1.785</td>\n",
       "      <td>0.019</td>\n",
       "      <td>7.314</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.028</td>\n",
       "      <td>561.571</td>\n",
       "      <td>3,234.697</td>\n",
       "      <td>2,005.874</td>\n",
       "      <td>9,299.359</td>\n",
       "      <td>9,883.231</td>\n",
       "      <td>0.525</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.769</td>\n",
       "      <td>5.360</td>\n",
       "      <td>2.688</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.292</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.411</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.457</td>\n",
       "      <td>8.802</td>\n",
       "      <td>0.376</td>\n",
       "      <td>5.587</td>\n",
       "      <td>11.886</td>\n",
       "      <td>0.158</td>\n",
       "      <td>4.616</td>\n",
       "      <td>7.962</td>\n",
       "      <td>36.056</td>\n",
       "      <td>0.681</td>\n",
       "      <td>63.332</td>\n",
       "      <td>45.490</td>\n",
       "      <td>2.912</td>\n",
       "      <td>63.992</td>\n",
       "      <td>94.259</td>\n",
       "      <td>38.368</td>\n",
       "      <td>40.955</td>\n",
       "      <td>65.550</td>\n",
       "      <td>5,043.633</td>\n",
       "      <td>51.793</td>\n",
       "      <td>0.000</td>\n",
       "      <td>65.112</td>\n",
       "      <td>43.948</td>\n",
       "      <td>93.243</td>\n",
       "      <td>92.586</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.094</td>\n",
       "      <td>2.690</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.409</td>\n",
       "      <td>83.226</td>\n",
       "      <td>0.309</td>\n",
       "      <td>1,072.203</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.615</td>\n",
       "      <td>1.935</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.065</td>\n",
       "      <td>105.208</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.051</td>\n",
       "      <td>14.204</td>\n",
       "      <td>2.823</td>\n",
       "      <td>28.733</td>\n",
       "      <td>8.777</td>\n",
       "      <td>10.827</td>\n",
       "      <td>4.921</td>\n",
       "      <td>101.455</td>\n",
       "      <td>179.339</td>\n",
       "      <td>12.047</td>\n",
       "      <td>5.203</td>\n",
       "      <td>30.108</td>\n",
       "      <td>980.119</td>\n",
       "      <td>976.191</td>\n",
       "      <td>6.277</td>\n",
       "      <td>12.595</td>\n",
       "      <td>201.190</td>\n",
       "      <td>5.873</td>\n",
       "      <td>54.934</td>\n",
       "      <td>6.011</td>\n",
       "      <td>12.859</td>\n",
       "      <td>7.446</td>\n",
       "      <td>14.687</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>582.364</td>\n",
       "      <td>960.256</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>32.274</td>\n",
       "      <td>444.706</td>\n",
       "      <td>178.434</td>\n",
       "      <td>26.036</td>\n",
       "      <td>1.277</td>\n",
       "      <td>3.394</td>\n",
       "      <td>0.892</td>\n",
       "      <td>1.351</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.625</td>\n",
       "      <td>0.501</td>\n",
       "      <td>0.937</td>\n",
       "      <td>7.290</td>\n",
       "      <td>10.453</td>\n",
       "      <td>15.507</td>\n",
       "      <td>5.861</td>\n",
       "      <td>45.749</td>\n",
       "      <td>12.518</td>\n",
       "      <td>4.325</td>\n",
       "      <td>78.372</td>\n",
       "      <td>104.125</td>\n",
       "      <td>968.584</td>\n",
       "      <td>43.724</td>\n",
       "      <td>75.503</td>\n",
       "      <td>158.133</td>\n",
       "      <td>289.331</td>\n",
       "      <td>149.141</td>\n",
       "      <td>227.854</td>\n",
       "      <td>38.214</td>\n",
       "      <td>157.103</td>\n",
       "      <td>173.363</td>\n",
       "      <td>0.000</td>\n",
       "      <td>40.528</td>\n",
       "      <td>166.713</td>\n",
       "      <td>976.402</td>\n",
       "      <td>804.278</td>\n",
       "      <td>902.857</td>\n",
       "      <td>957.798</td>\n",
       "      <td>988.285</td>\n",
       "      <td>990.881</td>\n",
       "      <td>987.978</td>\n",
       "      <td>922.222</td>\n",
       "      <td>115.780</td>\n",
       "      <td>8.040</td>\n",
       "      <td>9.634</td>\n",
       "      <td>9.402</td>\n",
       "      <td>127.573</td>\n",
       "      <td>14.178</td>\n",
       "      <td>131.489</td>\n",
       "      <td>40.282</td>\n",
       "      <td>987.826</td>\n",
       "      <td>984.615</td>\n",
       "      <td>400.000</td>\n",
       "      <td>986.667</td>\n",
       "      <td>0.498</td>\n",
       "      <td>6.226</td>\n",
       "      <td>2.786</td>\n",
       "      <td>60.547</td>\n",
       "      <td>36.870</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>46.359</td>\n",
       "      <td>4.021</td>\n",
       "      <td>11.952</td>\n",
       "      <td>44.276</td>\n",
       "      <td>3.108</td>\n",
       "      <td>9.962</td>\n",
       "      <td>6.580</td>\n",
       "      <td>3.838</td>\n",
       "      <td>19.953</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>10.562</td>\n",
       "      <td>2.149</td>\n",
       "      <td>413.142</td>\n",
       "      <td>83.016</td>\n",
       "      <td>6.271</td>\n",
       "      <td>90.700</td>\n",
       "      <td>25.470</td>\n",
       "      <td>2.525</td>\n",
       "      <td>38.421</td>\n",
       "      <td>11.160</td>\n",
       "      <td>303.550</td>\n",
       "      <td>23.020</td>\n",
       "      <td>35.581</td>\n",
       "      <td>1.502</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.272</td>\n",
       "      <td>99.037</td>\n",
       "      <td>279.470</td>\n",
       "      <td>0.917</td>\n",
       "      <td>14.490</td>\n",
       "      <td>0.622</td>\n",
       "      <td>5.259</td>\n",
       "      <td>0.256</td>\n",
       "      <td>5.658</td>\n",
       "      <td>70.931</td>\n",
       "      <td>538.848</td>\n",
       "      <td>2.562</td>\n",
       "      <td>432.940</td>\n",
       "      <td>0.686</td>\n",
       "      <td>111.289</td>\n",
       "      <td>0.207</td>\n",
       "      <td>90.424</td>\n",
       "      <td>61.896</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.015</td>\n",
       "      <td>289.923</td>\n",
       "      <td>0.509</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.168</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.015</td>\n",
       "      <td>474.081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1          2          3          4        5        7        8  \\\n",
       "count    104.000    103.000    104.000    104.000  104.000  104.000  104.000   \n",
       "mean   3,007.526  2,495.060  2,200.442  1,355.781    1.303  101.491    0.122   \n",
       "std       88.410     71.316     31.302    341.306    0.336    4.996    0.002   \n",
       "min    2,848.460  2,286.250  2,138.878    899.949    0.681   87.279    0.116   \n",
       "25%    2,946.120  2,457.785  2,173.481  1,069.646    1.045   99.113    0.121   \n",
       "50%    2,989.315  2,500.850  2,196.689  1,281.537    1.306  101.639    0.122   \n",
       "75%    3,053.280  2,532.010  2,218.936  1,558.445    1.511  104.114    0.124   \n",
       "max    3,266.550  2,714.300  2,315.267  2,360.133    2.421  119.354    0.128   \n",
       "\n",
       "             9       10       11       12       13       15       16       17  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     1.471   -0.003    0.001    0.963  199.884    8.283  412.900    9.929   \n",
       "std      0.060    0.014    0.010    0.010    2.602    2.747    7.538    0.565   \n",
       "min      1.342   -0.034   -0.026    0.942  191.225    2.249  395.572    8.071   \n",
       "25%      1.432   -0.010   -0.005    0.955  198.491    5.993  407.837    9.630   \n",
       "50%      1.469   -0.002    0.002    0.963  199.756    8.528  412.547    9.909   \n",
       "75%      1.510    0.005    0.008    0.970  201.829   10.404  418.007   10.196   \n",
       "max      1.611    0.028    0.029    0.982  207.030   14.188  434.794   12.200   \n",
       "\n",
       "            18       19       20       21          22         23          24  \\\n",
       "count  104.000  104.000  103.000  104.000     103.000    103.000     103.000   \n",
       "mean     0.971  189.955   12.506    1.407  -5,362.274  2,617.340  -3,739.078   \n",
       "std      0.007    2.577    0.061    0.015     950.378    458.717   1,448.097   \n",
       "min      0.954  182.069   12.338    1.364  -7,150.250    609.500  -8,718.667   \n",
       "25%      0.967  188.566   12.462    1.397  -5,639.125  2,561.500  -4,401.125   \n",
       "50%      0.971  189.543   12.490    1.407  -5,447.250  2,635.750  -3,771.250   \n",
       "75%      0.977  191.725   12.548    1.417  -5,267.750  2,756.000  -3,223.000   \n",
       "max      0.982  197.843   12.657    1.437  -1,368.750  3,493.750     135.750   \n",
       "\n",
       "                25       26       27       28       29       30       31  \\\n",
       "count      103.000  103.000  103.000  103.000  103.000  103.000  103.000   \n",
       "mean      -499.540    1.179    1.880    6.501   68.101    2.393    0.189   \n",
       "std      2,378.672    0.256    0.343    1.691    3.280    0.387    0.032   \n",
       "min    -10,366.333    0.225    0.439    0.740   60.267    1.267    0.108   \n",
       "25%     -1,565.000    1.255    1.955    7.119   66.256    2.072    0.164   \n",
       "50%         37.250    1.281    1.988    7.258   68.156    2.389    0.188   \n",
       "75%        771.375    1.306    2.002    7.331   70.222    2.656    0.213   \n",
       "max      6,476.000    1.383    2.043    7.579   75.944    3.244    0.285   \n",
       "\n",
       "            32       33       34       35       36       37       38       39  \\\n",
       "count  103.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     3.574   85.733    9.368   50.594   64.437   49.406   66.268   86.925   \n",
       "std      0.445    2.983    2.438    1.244    1.799    1.244    0.283    0.686   \n",
       "min      2.203   83.581    8.278   49.835   63.689   42.258   65.701   84.733   \n",
       "25%      3.378   84.693    8.665   50.250   64.010   49.492   66.126   86.577   \n",
       "50%      3.427   85.239    8.894   50.381   64.139   49.619   66.279   86.844   \n",
       "75%      3.511   85.742    9.201   50.508   64.236   49.750   66.401   87.040   \n",
       "max      4.704  103.108   23.345   57.742   74.851   50.165   67.199   88.419   \n",
       "\n",
       "            40       41       42       44       45       46       47       48  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean   118.665   63.271    3.375  355.061   10.016  136.989  733.243    1.198   \n",
       "std      1.594   26.573    1.121    5.719    0.157    8.421   11.584    0.179   \n",
       "min    115.962    2.332    0.927  344.943    9.711  117.397  706.305    0.828   \n",
       "25%    118.102   64.938    2.537  350.905    9.894  130.650  723.919    1.032   \n",
       "50%    118.257   77.105    3.093  354.081   10.008  136.599  732.328    1.272   \n",
       "75%    118.772   79.235    4.019  359.878   10.120  141.644  740.856    1.348   \n",
       "max    127.394   84.980    6.250  366.804   10.450  156.623  761.818    1.473   \n",
       "\n",
       "            49       51       52       54       55         56       57  \\\n",
       "count  104.000  104.000  104.000  103.000  103.000    103.000  103.000   \n",
       "mean   139.713  631.763  158.792    4.594    4.842  2,855.301    0.930   \n",
       "std      4.724    7.697   64.841    0.034    0.042     30.378    0.005   \n",
       "min    128.602  616.757   40.261    4.486    4.748  2,802.000    0.914   \n",
       "25%    136.815  626.327  111.348    4.575    4.811  2,829.000    0.928   \n",
       "50%    139.850  631.018  193.089    4.595    4.845  2,850.000    0.932   \n",
       "75%    142.747  637.198  207.443    4.612    4.869  2,875.000    0.934   \n",
       "max    150.511  647.264  239.493    4.681    4.931  2,936.000    0.938   \n",
       "\n",
       "            58       59       60       61       62       63       64       65  \\\n",
       "count  103.000  103.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.949    4.611    8.515  354.745   10.436  115.888   15.586   21.968   \n",
       "std      0.004    0.077   10.802    5.642    0.344   10.265    7.676    7.539   \n",
       "min      0.936    4.472  -12.153  346.065    9.674   85.509    3.778    6.448   \n",
       "25%      0.946    4.543    1.053  349.985   10.219  110.103   10.651   16.567   \n",
       "50%      0.950    4.608    5.522  353.678   10.437  115.773   13.410   20.860   \n",
       "75%      0.951    4.670   15.469  359.621   10.672  120.980   18.060   26.265   \n",
       "max      0.959    4.848   53.682  367.216   11.785  168.194   53.945   42.630   \n",
       "\n",
       "            66       67         68       69       71       72       73  \\\n",
       "count  104.000  104.000    104.000  104.000  104.000  104.000   68.000   \n",
       "mean    28.592  704.971     62.207  146.611  617.245  104.087  150.833   \n",
       "std      9.100   12.039    623.921    6.982    9.826   37.628   11.530   \n",
       "min      8.764  670.306      0.523   87.025  582.482   21.820  112.325   \n",
       "25%     21.972  697.399      0.896  144.645  611.835   83.473  144.960   \n",
       "50%     27.975  704.897      1.013  147.065  617.347  100.656  151.610   \n",
       "75%     33.535  713.977      1.096  149.508  623.161  112.213  160.651   \n",
       "max     56.896  727.561  6,363.801  157.081  636.826  233.611  168.870   \n",
       "\n",
       "            74       75       76       77       78       79       80       81  \\\n",
       "count   68.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean   466.821    0.000   -0.005   -0.036   -0.006   -0.021    0.010   -0.024   \n",
       "std      4.872    0.000    0.032    0.035    0.034    0.049    0.025    0.058   \n",
       "min    459.889    0.000   -0.067   -0.186   -0.090   -0.103   -0.033   -0.120   \n",
       "25%    464.728    0.000   -0.020   -0.054   -0.030   -0.062   -0.009   -0.084   \n",
       "50%    466.131    0.000   -0.006   -0.035   -0.009   -0.024    0.008   -0.009   \n",
       "75%    468.040    0.000    0.005   -0.020    0.012    0.003    0.022    0.011   \n",
       "max    493.666    0.000    0.232    0.031    0.091    0.135    0.080    0.106   \n",
       "\n",
       "            82       83       84       85      86       87       88  \\\n",
       "count  104.000  104.000  104.000  104.000  11.000  104.000  104.000   \n",
       "mean    -0.020    0.004    7.405    0.133   0.111    2.405    0.981   \n",
       "std      0.019    0.035    0.542    0.005   0.003    0.033    0.022   \n",
       "min     -0.077   -0.074    5.947    0.124   0.107    2.325    0.775   \n",
       "25%     -0.026   -0.024    7.096    0.129   0.109    2.383    0.973   \n",
       "50%     -0.019    0.007    7.384    0.132   0.112    2.409    0.987   \n",
       "75%     -0.008    0.029    7.694    0.136   0.114    2.428    0.990   \n",
       "max      0.035    0.119    8.871    0.145   0.115    2.508    0.993   \n",
       "\n",
       "              89       90         91       92       93       94       95  \\\n",
       "count    104.000  103.000    103.000  104.000  104.000  104.000  104.000   \n",
       "mean   1,813.208    0.195  8,748.619    0.006    0.000   -0.000   -0.000   \n",
       "std       53.902    0.122    314.424    0.070    0.003    0.003    0.000   \n",
       "min    1,632.312    0.146  7,929.630   -0.257   -0.009   -0.011   -0.000   \n",
       "25%    1,782.208    0.162  8,564.130   -0.023   -0.001   -0.001   -0.000   \n",
       "50%    1,809.817    0.185  8,745.810    0.010    0.000    0.000    0.000   \n",
       "75%    1,846.956    0.199  8,931.225    0.052    0.002    0.001    0.000   \n",
       "max    1,955.883    1.386  9,731.440    0.177    0.012    0.011    0.001   \n",
       "\n",
       "            96       97       99      100      101      102      103      104  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.000    0.009   -0.051    0.012    0.000   -0.000   -0.009   -0.008   \n",
       "std      0.000    0.174    0.320    0.108    0.000    0.000    0.072    0.003   \n",
       "min     -0.000   -0.637   -0.949   -0.114   -0.001   -0.000   -0.229   -0.016   \n",
       "25%      0.000   -0.082   -0.264   -0.032   -0.000   -0.000   -0.049   -0.011   \n",
       "50%      0.000    0.002    0.000   -0.001    0.000    0.000   -0.015   -0.008   \n",
       "75%      0.000    0.127    0.152    0.030    0.000    0.000    0.013   -0.006   \n",
       "max      0.000    0.520    1.075    0.885    0.002    0.001    0.298    0.006   \n",
       "\n",
       "           105      106      107      108      109     110      111      112  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  30.000   30.000   30.000   \n",
       "mean    -0.000   -0.000    0.000   -0.007   -0.011   0.980  101.742  230.822   \n",
       "std      0.001    0.003    0.003    0.096    0.087   0.002    1.316    1.275   \n",
       "min     -0.012   -0.012   -0.011   -0.523   -0.212   0.975   98.861  229.129   \n",
       "25%     -0.000   -0.002   -0.001   -0.054   -0.064   0.979  101.219  229.994   \n",
       "50%      0.000    0.000    0.000    0.000   -0.015   0.980  101.595  230.493   \n",
       "75%      0.001    0.001    0.001    0.043    0.038   0.981  102.127  231.099   \n",
       "max      0.002    0.009    0.011    0.200    0.332   0.983  105.199  234.777   \n",
       "\n",
       "          113      114      115      116      117      118      119      120  \\\n",
       "count  71.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean    0.464    0.945    0.001  739.372    0.987   58.219    0.598    0.971   \n",
       "std     0.007    0.011    0.003   46.830    0.009    1.171    0.007    0.007   \n",
       "min     0.438    0.902    0.000  613.409    0.961   53.610    0.579    0.957   \n",
       "25%     0.461    0.939    0.000  722.483    0.989   57.782    0.594    0.966   \n",
       "50%     0.465    0.948    0.000  745.590    0.991   58.463    0.598    0.970   \n",
       "75%     0.469    0.952    0.000  763.425    0.991   59.030    0.602    0.978   \n",
       "max     0.477    0.969    0.023  914.825    0.992   60.947    0.613    0.981   \n",
       "\n",
       "           121      122      123      124      125      126      127      128  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     6.301   15.828    3.634   15.859   15.835    1.097    2.803    0.610   \n",
       "std      0.128    0.094    0.714    0.104    0.114    0.225    0.269    0.104   \n",
       "min      5.882   15.590    2.398   15.610   15.540    0.606    2.407    0.411   \n",
       "25%      6.218   15.760    3.083   15.780   15.750    0.945    2.622    0.539   \n",
       "50%      6.305   15.810    3.526   15.845   15.825    1.078    2.764    0.606   \n",
       "75%      6.380   15.880    4.184   15.910   15.902    1.239    2.891    0.678   \n",
       "max      6.750   16.070    5.760   16.100   16.090    1.867    3.955    0.890   \n",
       "\n",
       "           129      130      131      132      133        134      135  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000    104.000  104.000   \n",
       "mean     3.224   -0.083    0.769    0.998    2.319  1,005.701   39.050   \n",
       "std      0.184    0.840    0.062    0.002    0.059      5.811    2.627   \n",
       "min      2.824   -2.789    0.603    0.995    2.191    986.099   33.366   \n",
       "25%      3.102   -0.426    0.718    0.997    2.277  1,001.871   37.025   \n",
       "50%      3.237    0.000    0.771    0.998    2.324  1,004.916   38.698   \n",
       "75%      3.342    0.284    0.821    0.999    2.368  1,011.034   40.405   \n",
       "max      3.823    2.458    0.876    1.009    2.424  1,015.568   46.035   \n",
       "\n",
       "           136      137      138      139        140      141      143  \\\n",
       "count  104.000  104.000  103.000  104.000    104.000  104.000  104.000   \n",
       "mean   112.058  140.474  128.559   59.794    412.068    0.318    6.437   \n",
       "std     59.635   53.653   47.478   13.272    274.965    0.308    2.053   \n",
       "min     61.000   55.400   23.700   37.200    113.065    0.059    2.300   \n",
       "25%     88.000   91.450   96.050   52.400    264.955    0.146    5.058   \n",
       "50%    103.000  131.950  136.500   56.750    329.588    0.238    6.255   \n",
       "75%    120.000  183.150  164.000   64.800    487.251    0.441    7.388   \n",
       "max    634.000  254.600  254.000  111.900  1,533.777    2.818   12.390   \n",
       "\n",
       "           144      145      146      147      148      149      151      152  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.004    0.111    0.059    0.051    0.018    8.078    6.667   11.444   \n",
       "std      0.001    0.061    0.017    0.017    0.009    3.071    3.201    4.400   \n",
       "min      0.003    0.047    0.030    0.023    0.005    1.748    2.018    4.218   \n",
       "25%      0.003    0.077    0.046    0.040    0.010    5.782    4.535    7.770   \n",
       "50%      0.004    0.100    0.057    0.049    0.019    8.081    5.861   11.430   \n",
       "75%      0.005    0.123    0.070    0.058    0.024    9.779    7.890   13.860   \n",
       "max      0.006    0.446    0.110    0.119    0.060   25.164   17.747   24.113   \n",
       "\n",
       "           153      154      155      156      157    158        159  \\\n",
       "count  104.000  104.000  104.000  103.000  104.000  8.000      8.000   \n",
       "mean     0.793    0.013    7.729    0.396    0.059  0.039  1,237.800   \n",
       "std      0.847    0.007    2.920    0.149    0.036  0.019    322.177   \n",
       "min      0.154    0.004    1.571    0.170    0.015  0.016    591.601   \n",
       "25%      0.395    0.006    5.516    0.265    0.038  0.032  1,115.675   \n",
       "50%      0.522    0.013    7.707    0.380    0.048  0.038  1,324.350   \n",
       "75%      0.944    0.017    9.325    0.505    0.067  0.043  1,396.775   \n",
       "max      5.866    0.058   24.504    0.930    0.203  0.077  1,625.700   \n",
       "\n",
       "             160        161         162         163      164      165  \\\n",
       "count    103.000    103.000     103.000     103.000  103.000  103.000   \n",
       "mean   1,172.311    750.398   3,753.942   4,653.233    0.186    0.208   \n",
       "std    1,438.581    836.913   3,377.785   5,693.038    0.203    0.428   \n",
       "min       73.000     76.000     141.000      32.000    0.034    0.006   \n",
       "25%      464.000    361.000   1,411.000     545.500    0.099    0.072   \n",
       "50%      686.000    507.000   3,185.000   1,790.000    0.135    0.095   \n",
       "75%    1,045.000    653.500   4,634.500   7,145.000    0.171    0.123   \n",
       "max    7,709.000  4,170.000  19,368.000  21,016.000    0.931    1.802   \n",
       "\n",
       "           166      167      168      169      170      171      172      173  \\\n",
       "count  103.000  103.000  103.000  103.000  103.000  104.000  104.000  104.000   \n",
       "mean     0.381    3.010    1.289    0.126    0.392    0.706    0.121    0.330   \n",
       "std      0.724    2.150    0.824    0.052    0.193    0.171    0.056    0.063   \n",
       "min      0.031    1.200    0.400    0.047    0.065    0.357    0.033    0.152   \n",
       "25%      0.125    2.100    0.900    0.092    0.247    0.576    0.084    0.293   \n",
       "50%      0.187    2.700    1.200    0.117    0.394    0.697    0.115    0.327   \n",
       "75%      0.266    3.250    1.450    0.149    0.529    0.809    0.148    0.378   \n",
       "max      3.077   21.100    8.400    0.365    0.881    1.082    0.297    0.448   \n",
       "\n",
       "           174      175      176      177      178      181      182      183  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.574    0.330    0.797    0.241    0.391   18.037    0.510   10.293   \n",
       "std      0.088    0.063    0.105    0.074    0.280    2.776    0.193    3.065   \n",
       "min      0.371    0.152    0.587    0.076    0.105   12.100    0.171    4.430   \n",
       "25%      0.519    0.293    0.725    0.196    0.228   15.870    0.358    8.155   \n",
       "50%      0.573    0.327    0.799    0.236    0.286   17.615    0.475    9.985   \n",
       "75%      0.618    0.378    0.859    0.290    0.418   20.225    0.618   12.883   \n",
       "max      0.864    0.448    1.078    0.435    1.121   26.710    1.054   20.150   \n",
       "\n",
       "           184      185      186      188      189      196      197      198  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  103.000  104.000  104.000   \n",
       "mean    28.086    0.146    7.123   18.120   47.050    0.342   11.828   22.303   \n",
       "std      6.180    0.069    2.512    4.170   21.840    0.626   29.140   16.930   \n",
       "min     16.048    0.054    2.560    7.570   11.289    0.080    2.080   10.900   \n",
       "25%     22.903    0.103    5.510   15.815   30.908    0.241    5.090   18.027   \n",
       "50%     29.824    0.131    6.520   18.400   46.684    0.272    7.085   20.035   \n",
       "75%     32.293    0.172    8.255   21.285   57.656    0.302    9.485   21.918   \n",
       "max     42.781    0.583   16.780   26.840  111.677    4.838  263.230  158.350   \n",
       "\n",
       "           199      200      201      202      203      204        205  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000    104.000   \n",
       "mean     0.637   14.731   19.570    8.389   10.905   33.045     96.335   \n",
       "std      0.892   29.478   14.206    6.017   12.556   26.150    980.337   \n",
       "min      0.161    3.500    3.210    2.080    3.343   16.146      0.051   \n",
       "25%      0.309    7.130   15.955    5.090    6.672   24.297      0.116   \n",
       "50%      0.474    8.605   17.830    7.085    8.904   30.730      0.172   \n",
       "75%      0.781   11.400   20.998    9.450   12.295   35.027      0.264   \n",
       "max      8.898  265.970  156.080   54.260  129.641  262.689  9,997.725   \n",
       "\n",
       "           206      207      208      209      210      211      212      213  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean    11.367    0.000   22.692   70.873    0.000    0.105    0.061    0.052   \n",
       "std     14.475    0.000   23.313   27.344    0.000    0.070    0.035    0.027   \n",
       "min      3.100    0.000    6.730    9.412    0.000    0.034    0.012    0.009   \n",
       "25%      6.600    0.000   17.590   55.331    0.000    0.069    0.045    0.031   \n",
       "50%      9.160    0.000   20.300   71.701    0.000    0.086    0.054    0.043   \n",
       "75%     12.815    0.000   22.720   88.025    0.000    0.123    0.066    0.069   \n",
       "max    150.870    0.000  250.210  142.078    0.000    0.516    0.276    0.154   \n",
       "\n",
       "           214      215      216      217      218      219      220     221  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  11.000   \n",
       "mean     0.069    0.076    0.079    0.085    0.074    3.706    0.003   0.009   \n",
       "std      0.058    0.029    0.029    0.028    0.037    1.180    0.001   0.002   \n",
       "min      0.014    0.021    0.021    0.021    0.024    1.770    0.001   0.006   \n",
       "25%      0.043    0.057    0.059    0.068    0.048    2.780    0.002   0.007   \n",
       "50%      0.057    0.074    0.081    0.084    0.063    3.675    0.003   0.009   \n",
       "75%      0.081    0.090    0.096    0.102    0.093    4.455    0.004   0.010   \n",
       "max      0.578    0.181    0.173    0.210    0.178    7.040    0.011   0.011   \n",
       "\n",
       "           222      223      224      225        226      228      229  \\\n",
       "count  104.000  104.000  104.000  103.000    103.000  104.000  104.000   \n",
       "mean     0.063    0.015  122.626    0.063  1,073.229    0.021    0.018   \n",
       "std      0.029    0.097   37.540    0.103    457.571    0.012    0.009   \n",
       "min      0.026    0.000   52.544    0.015    267.700    0.008    0.007   \n",
       "25%      0.042    0.001   98.047    0.029    809.250    0.014    0.013   \n",
       "50%      0.058    0.002  120.472    0.038    988.700    0.017    0.016   \n",
       "75%      0.077    0.006  144.514    0.055  1,249.750    0.024    0.022   \n",
       "max      0.231    0.989  254.543    0.985  3,601.300    0.097    0.066   \n",
       "\n",
       "           239      240     245     246     247     248      249      250  \\\n",
       "count  104.000  104.000  30.000  30.000  30.000  71.000  104.000  104.000   \n",
       "mean     0.005    0.004   0.002   1.323   3.807   0.071    0.027    0.005   \n",
       "std      0.002    0.002   0.001   1.049   1.009   0.065    0.034    0.030   \n",
       "min      0.001    0.002   0.001   0.495   2.257   0.015    0.004    0.000   \n",
       "25%      0.004    0.004   0.001   0.813   2.939   0.027    0.013    0.000   \n",
       "50%      0.005    0.004   0.002   1.057   3.821   0.042    0.021    0.000   \n",
       "75%      0.006    0.005   0.003   1.380   4.365   0.082    0.029    0.000   \n",
       "max      0.016    0.016   0.005   6.184   6.520   0.282    0.332    0.207   \n",
       "\n",
       "           251      252      253      254      255      256      268      269  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean   111.077    0.003    3.044    0.034    0.013    0.399    0.069   19.692   \n",
       "std     43.860    0.007    0.938    0.025    0.008    0.106    0.033    7.553   \n",
       "min     31.770    0.000    1.147    0.014    0.002    0.166    0.020    7.120   \n",
       "25%     79.706    0.001    2.345    0.026    0.005    0.321    0.036   12.735   \n",
       "50%    106.057    0.001    2.917    0.030    0.015    0.393    0.069   18.080   \n",
       "75%    132.345    0.001    3.706    0.037    0.020    0.464    0.092   25.577   \n",
       "max    346.793    0.039    6.407    0.259    0.030    0.627    0.158   38.321   \n",
       "\n",
       "           270      271      272      273      274      275      276      278  \\\n",
       "count  104.000  104.000  104.000  103.000  104.000  104.000  104.000  104.000   \n",
       "mean     3.856   28.372   47.854   43.372   20.534  134.597    0.106    2.168   \n",
       "std      1.207    9.677   18.170   16.188    3.897   87.862    0.100    0.676   \n",
       "min      1.736   15.956   17.192    8.434   13.199   42.790    0.022    0.905   \n",
       "25%      2.961   23.728   31.737   31.899   18.456   88.008    0.052    1.696   \n",
       "50%      3.697   27.616   47.266   45.355   19.620  110.756    0.078    2.087   \n",
       "75%      4.424   30.985   62.085   56.554   21.460  152.869    0.141    2.514   \n",
       "max      8.478  110.295   79.971   85.026   36.405  483.478    0.902    4.154   \n",
       "\n",
       "           279      280      281      282      283      284      286      287  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.001    0.038    0.017    0.014    0.006    2.657    2.075    3.485   \n",
       "std      0.000    0.021    0.004    0.004    0.003    0.895    0.935    1.240   \n",
       "min      0.001    0.015    0.010    0.008    0.002    0.596    0.600    1.371   \n",
       "25%      0.001    0.026    0.013    0.011    0.003    1.994    1.410    2.564   \n",
       "50%      0.001    0.034    0.016    0.013    0.006    2.594    1.906    3.345   \n",
       "75%      0.001    0.042    0.020    0.015    0.008    3.165    2.440    4.050   \n",
       "max      0.002    0.148    0.025    0.033    0.021    7.430    5.214    6.836   \n",
       "\n",
       "           288      289      290      291      292    293      294        295  \\\n",
       "count  104.000  104.000  104.000  103.000  104.000  8.000    8.000    103.000   \n",
       "mean     0.235    0.004    2.563    0.109    0.020  0.012  398.871    549.384   \n",
       "std      0.255    0.002    0.861    0.042    0.012  0.007  121.158    719.578   \n",
       "min      0.059    0.001    0.509    0.047    0.005  0.006  163.344     34.072   \n",
       "25%      0.120    0.002    1.982    0.068    0.013  0.010  358.650    209.684   \n",
       "50%      0.156    0.004    2.522    0.101    0.016  0.012  407.871    313.216   \n",
       "75%      0.271    0.005    3.009    0.153    0.023  0.013  478.307    461.934   \n",
       "max      1.785    0.019    7.314    0.192    0.072  0.028  561.571  3,234.697   \n",
       "\n",
       "             296        297        298      299      300      301      302  \\\n",
       "count    103.000    103.000    103.000  103.000  103.000  103.000  103.000   \n",
       "mean     351.508  1,761.278  2,295.065    0.089    0.104    0.188    0.970   \n",
       "std      423.299  1,602.811  2,846.455    0.109    0.230    0.388    0.560   \n",
       "min       37.027     71.066     16.872    0.018    0.003    0.013    0.362   \n",
       "25%      158.983    613.149    257.500    0.044    0.032    0.058    0.728   \n",
       "50%      225.984  1,396.915    824.032    0.059    0.043    0.083    0.885   \n",
       "75%      300.196  2,262.575  3,614.284    0.078    0.056    0.122    1.064   \n",
       "max    2,005.874  9,299.359  9,883.231    0.525    1.031    1.769    5.360   \n",
       "\n",
       "           303      304      305      306      307      308      309      310  \\\n",
       "count  103.000  103.000  103.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.422    0.041    0.130    0.269    0.049    0.131    0.218    0.131   \n",
       "std      0.259    0.016    0.063    0.059    0.023    0.024    0.031    0.024   \n",
       "min      0.142    0.016    0.020    0.143    0.013    0.063    0.145    0.063   \n",
       "25%      0.314    0.030    0.081    0.225    0.034    0.120    0.202    0.120   \n",
       "50%      0.388    0.039    0.137    0.262    0.046    0.131    0.221    0.131   \n",
       "75%      0.468    0.051    0.170    0.315    0.056    0.146    0.232    0.146   \n",
       "max      2.688    0.107    0.292    0.385    0.136    0.177    0.324    0.177   \n",
       "\n",
       "           311      312      313      317      318      319      320      321  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.311    0.095    0.159    5.636    0.162    3.026    8.349    0.043   \n",
       "std      0.038    0.030    0.117    0.874    0.066    0.947    1.998    0.019   \n",
       "min      0.222    0.033    0.048    4.090    0.052    1.317    4.234    0.018   \n",
       "25%      0.286    0.076    0.093    5.101    0.109    2.284    6.581    0.031   \n",
       "50%      0.309    0.092    0.118    5.531    0.157    2.829    9.013    0.039   \n",
       "75%      0.335    0.114    0.158    6.112    0.194    3.762    9.840    0.050   \n",
       "max      0.411    0.178    0.457    8.802    0.376    5.587   11.886    0.158   \n",
       "\n",
       "           322      324      325      332      333      334      335      336  \\\n",
       "count  104.000  104.000  104.000  103.000  104.000  104.000  104.000  104.000   \n",
       "mean     2.186    5.447   14.316    0.092    3.327    6.836    0.195    4.174   \n",
       "std      0.709    1.246    6.688    0.085    7.408    4.696    0.291    7.460   \n",
       "min      0.846    2.335    3.542    0.022    0.658    3.823    0.043    1.073   \n",
       "25%      1.765    4.771    8.903    0.077    1.547    5.610    0.096    2.144   \n",
       "50%      2.044    5.476   14.590    0.087    2.037    6.135    0.135    2.577   \n",
       "75%      2.511    6.259   17.871    0.097    3.032    6.778    0.233    3.395   \n",
       "max      4.616    7.962   36.056    0.681   63.332   45.490    2.912   63.992   \n",
       "\n",
       "           337      338      339      340        341      342      343  \\\n",
       "count  104.000  104.000  104.000  104.000    104.000  104.000  104.000   \n",
       "mean    10.855    2.668    3.182   10.010     48.558    3.415    0.000   \n",
       "std      9.547    3.692    3.911    6.535    494.563    4.935    0.000   \n",
       "min      6.025    0.658    1.131    5.262      0.015    0.952    0.000   \n",
       "25%      8.603    1.547    1.949    7.312      0.038    2.015    0.000   \n",
       "50%      9.495    2.037    2.625    9.794      0.053    2.721    0.000   \n",
       "75%     10.503    2.983    3.343   10.825      0.075    3.649    0.000   \n",
       "max     94.259   38.368   40.955   65.550  5,043.633   51.793    0.000   \n",
       "\n",
       "           344      345     346     347      348      349      350      351  \\\n",
       "count  104.000  104.000  68.000  68.000  104.000  104.000  104.000  104.000   \n",
       "mean     6.819   22.450   7.783   5.601    0.000    0.030    0.027    0.023   \n",
       "std      6.024    8.599  10.931  10.982    0.000    0.024    0.015    0.012   \n",
       "min      2.139    2.947   3.205   1.854    0.000    0.011    0.005    0.004   \n",
       "25%      5.238   18.040   4.991   2.802    0.000    0.021    0.020    0.014   \n",
       "50%      6.210   23.117   5.899   3.451    0.000    0.025    0.024    0.019   \n",
       "75%      7.021   27.101   7.264   5.157    0.000    0.037    0.029    0.031   \n",
       "max     65.112   43.948  93.243  92.586    0.000    0.220    0.121    0.067   \n",
       "\n",
       "           352      353      354      355      356      357      358     359  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  11.000   \n",
       "mean     0.032    0.022    0.039    0.042    0.035    1.284    0.001   0.002   \n",
       "std      0.028    0.008    0.015    0.014    0.019    0.404    0.000   0.000   \n",
       "min      0.007    0.006    0.010    0.009    0.010    0.553    0.000   0.002   \n",
       "25%      0.021    0.016    0.027    0.031    0.021    0.975    0.001   0.002   \n",
       "50%      0.026    0.023    0.041    0.043    0.029    1.241    0.001   0.002   \n",
       "75%      0.036    0.026    0.050    0.050    0.046    1.543    0.001   0.003   \n",
       "max      0.284    0.048    0.083    0.098    0.094    2.690    0.003   0.003   \n",
       "\n",
       "           360      361      362      363        364      366      367  \\\n",
       "count  104.000  104.000  104.000  103.000    103.000  104.000  104.000   \n",
       "mean     0.021    0.006   39.619    0.019    348.516    0.006    0.005   \n",
       "std      0.010    0.040   11.913    0.032    151.465    0.004    0.002   \n",
       "min      0.009    0.000   16.820    0.005     84.476    0.003    0.002   \n",
       "25%      0.014    0.000   32.051    0.009    240.105    0.004    0.004   \n",
       "50%      0.019    0.001   38.737    0.012    332.336    0.005    0.004   \n",
       "75%      0.025    0.002   47.986    0.017    415.346    0.006    0.005   \n",
       "max      0.089    0.409   83.226    0.309  1,072.203    0.032    0.019   \n",
       "\n",
       "           368      369      377      378     383     384     385     386  \\\n",
       "count  104.000  104.000  104.000  104.000  30.000  30.000  30.000  71.000   \n",
       "mean     0.004    0.003    0.002    0.002   0.001   0.406   1.138   0.013   \n",
       "std      0.003    0.002    0.001    0.001   0.000   0.274   0.298   0.011   \n",
       "min      0.000    0.000    0.001    0.001   0.000   0.165   0.675   0.004   \n",
       "25%      0.003    0.002    0.001    0.001   0.000   0.286   0.923   0.006   \n",
       "50%      0.003    0.003    0.002    0.002   0.001   0.311   1.174   0.009   \n",
       "75%      0.004    0.004    0.002    0.002   0.001   0.443   1.282   0.017   \n",
       "max      0.032    0.017    0.005    0.005   0.002   1.615   1.935   0.057   \n",
       "\n",
       "           387      388      389      390      391      392      393      394  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.009    0.002   36.429    0.001    0.913    0.011    0.004    0.134   \n",
       "std      0.010    0.009   14.906    0.002    0.264    0.007    0.003    0.033   \n",
       "min      0.001    0.000    8.319    0.000    0.391    0.005    0.001    0.057   \n",
       "25%      0.004    0.000   26.016    0.000    0.700    0.008    0.001    0.111   \n",
       "50%      0.007    0.000   33.447    0.000    0.870    0.010    0.005    0.133   \n",
       "75%      0.010    0.000   44.027    0.000    1.119    0.011    0.007    0.156   \n",
       "max      0.101    0.065  105.208    0.012    1.749    0.079    0.010    0.202   \n",
       "\n",
       "           406      407      408      409      410      411      412      413  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  103.000  104.000  104.000   \n",
       "mean     0.024    6.973    1.261    5.077    4.669    5.165    2.713   31.487   \n",
       "std      0.012    3.127    0.358    2.689    1.787    1.939    0.576   18.823   \n",
       "min      0.006    2.238    0.626    2.823    1.823    0.953    1.710    8.779   \n",
       "25%      0.012    4.521    1.034    3.954    3.148    3.809    2.380   18.008   \n",
       "50%      0.023    6.087    1.240    4.746    4.389    5.335    2.592   27.017   \n",
       "75%      0.033    9.435    1.446    5.453    6.018    6.565    2.909   38.313   \n",
       "max      0.051   14.204    2.823   28.733    8.777   10.827    4.921  101.455   \n",
       "\n",
       "           414      416      417      418      419      420      421      422  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean    24.486    6.345    3.241    7.543  287.728  326.440    1.901    4.040   \n",
       "std     21.124    2.028    0.784    4.031  282.110  306.018    0.955    1.537   \n",
       "min      5.680    2.552    2.027    3.051    0.000    0.000    0.487    0.890   \n",
       "25%     10.749    4.907    2.583    5.164    0.000    0.000    1.003    2.924   \n",
       "50%     19.143    6.210    3.071    6.737  258.743  332.569    1.950    4.039   \n",
       "75%     29.280    7.252    3.844    8.398  489.331  544.972    2.464    4.940   \n",
       "max    179.339   12.047    5.203   30.108  980.119  976.191    6.277   12.595   \n",
       "\n",
       "           424      425      426      427      428      429      430      431  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  103.000  104.000  103.000   \n",
       "mean    83.509    2.774    8.003    1.310    4.067    3.164    4.178   33.370   \n",
       "std     34.260    1.074    8.188    0.744    1.536    1.189    2.554   74.186   \n",
       "min     29.677    1.022    1.467    0.363    0.847    1.360    1.049    1.329   \n",
       "25%     57.823    1.883    3.982    0.663    2.916    2.116    2.695    8.050   \n",
       "50%     72.062    2.717    5.363    1.301    4.024    3.022    3.423   13.473   \n",
       "75%    106.510    3.315    9.437    1.706    4.969    4.058    4.785   18.949   \n",
       "max    201.190    5.873   54.934    6.011   12.859    7.446   14.687  400.000   \n",
       "\n",
       "           432      433      434      435      436      437      438      439  \\\n",
       "count  103.000  103.000  103.000  103.000  103.000  103.000  103.000  103.000   \n",
       "mean    38.924  101.483  246.140   29.137   23.489   21.428    4.442   56.704   \n",
       "std     74.143  113.695  256.359   73.438   74.555   75.043    3.284   44.636   \n",
       "min      3.336    0.000    0.000    2.579    0.300    0.628    1.705   12.329   \n",
       "25%     13.719   32.080   20.638    8.189    3.605    1.882    3.106   37.106   \n",
       "50%     18.381   60.505  168.968   11.360    4.758    2.824    3.931   50.467   \n",
       "75%     24.021  119.848  342.065   13.713    6.145    3.859    4.875   65.212   \n",
       "max    400.000  582.364  960.256  400.000  400.000  400.000   32.274  444.706   \n",
       "\n",
       "           440      441      442      443      444      445      446      447  \\\n",
       "count  103.000  103.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean    67.709   11.353    0.825    1.312    0.654    0.891    0.667    1.202   \n",
       "std     28.232    5.823    0.204    0.610    0.128    0.141    0.123    0.160   \n",
       "min     25.125    1.575    0.418    0.379    0.277    0.581    0.309    0.884   \n",
       "25%     49.008    7.444    0.680    0.900    0.580    0.807    0.591    1.093   \n",
       "50%     65.438   11.781    0.809    1.237    0.645    0.894    0.661    1.203   \n",
       "75%     80.012   15.075    0.950    1.559    0.751    0.962    0.762    1.289   \n",
       "max    178.434   26.036    1.277    3.394    0.892    1.351    0.900    1.625   \n",
       "\n",
       "           448      449      453      454      455      456      457      458  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     0.277    0.329    5.081    5.109    7.501    3.835   12.352    5.122   \n",
       "std      0.086    0.235    0.779    1.961    2.175    0.857    5.773    1.884   \n",
       "min      0.086    0.088    3.330    1.680    3.465    2.147    4.498    1.794   \n",
       "25%      0.222    0.193    4.511    3.583    6.080    3.111    8.761    3.978   \n",
       "50%      0.272    0.242    4.992    4.696    7.196    4.120   10.710    4.735   \n",
       "75%      0.334    0.354    5.709    6.129    9.012    4.422   14.348    5.926   \n",
       "max      0.501    0.937    7.290   10.453   15.507    5.861   45.749   12.518   \n",
       "\n",
       "           460      461      468      469      470      471      472      473  \\\n",
       "count  104.000  104.000  103.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     2.871   32.226    7.445  176.323    6.283    6.081   11.910  138.106   \n",
       "std      0.669   14.075   13.709  226.868    4.740    7.748   18.887   50.274   \n",
       "min      1.187    9.729    1.716    0.000    3.084    1.448    3.160   22.060   \n",
       "25%      2.497   23.714    5.232   27.977    5.139    2.940    6.058  104.448   \n",
       "50%      2.928   27.764    5.912   82.805    5.646    4.505    7.592  134.872   \n",
       "75%      3.363   38.190    6.644  226.120    6.126    7.617   10.314  170.605   \n",
       "max      4.325   78.372  104.125  968.584   43.724   75.503  158.133  289.331   \n",
       "\n",
       "           474      475      476      477      478      479      480      481  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean    40.225   39.010    4.697   21.728    8.399    0.000    3.676   73.990   \n",
       "std     24.201   27.413    3.777   17.856   16.590    0.000    3.774   34.828   \n",
       "min     11.312    9.349    2.355    3.528    2.075    0.000    1.127   16.762   \n",
       "25%     26.161   23.472    3.423   11.871    4.406    0.000    2.818   52.020   \n",
       "50%     33.055   34.014    4.369   17.583    6.120    0.000    3.300   65.028   \n",
       "75%     46.595   43.275    4.941   26.593    8.603    0.000    3.699   93.340   \n",
       "max    149.141  227.854   38.214  157.103  173.363    0.000   40.528  166.713   \n",
       "\n",
       "           483      484      485      486      487      488      489      490  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean   308.773  189.420  180.596  176.006  329.674  228.103  309.131  288.577   \n",
       "std    291.059  153.561  183.143  208.536  263.861  261.377  263.602  231.984   \n",
       "min      0.000    0.000    0.000    0.000    0.000    0.000    0.000    0.000   \n",
       "25%      0.000   86.354   52.197   46.723  141.072   74.951   57.520  126.516   \n",
       "50%    269.087  144.195  132.157   99.449  267.753  105.453  312.613  230.635   \n",
       "75%    557.295  246.779  243.885  225.544  485.135  320.085  492.283  367.175   \n",
       "max    976.402  804.278  902.857  957.798  988.285  990.881  987.978  922.222   \n",
       "\n",
       "           491      492     493      494      495      496      497      498  \\\n",
       "count  104.000  104.000  11.000  104.000  104.000  104.000  103.000  103.000   \n",
       "mean    50.900    2.507   7.821    2.601    1.829    6.765   29.274   12.286   \n",
       "std     18.646    1.133   1.407    1.175   12.491    2.081   21.912    5.241   \n",
       "min     24.460    0.758   5.836    1.090    0.036    2.951    9.101    2.965   \n",
       "25%     36.166    1.767   6.598    1.720    0.121    5.330   16.157    8.849   \n",
       "50%     49.553    2.355   7.935    2.418    0.238    6.619   22.835   11.407   \n",
       "75%     61.323    3.044   9.095    3.200    0.665    7.933   32.108   14.387   \n",
       "max    115.780    8.040   9.634    9.402  127.573   14.178  131.489   40.282   \n",
       "\n",
       "           500      501      511      512     517     518     519     520  \\\n",
       "count  104.000  104.000  104.000  104.000  30.000  30.000  30.000  71.000   \n",
       "mean   226.626  258.693   74.348  343.826   0.231   1.305   1.649  15.415   \n",
       "std    325.000  345.999   55.337  353.483   0.110   1.057   0.434  14.149   \n",
       "min      0.000    0.000   18.006    0.000   0.072   0.484   0.974   3.180   \n",
       "25%      0.000    0.000   40.893    0.000   0.140   0.805   1.273   5.721   \n",
       "50%      0.000    0.000   58.232  278.720   0.231   1.036   1.662   9.052   \n",
       "75%    477.300  477.876   85.076  697.778   0.307   1.352   1.891  17.977   \n",
       "max    987.826  984.615  400.000  986.667   0.498   6.226   2.786  60.547   \n",
       "\n",
       "           521        522      523      524      525      526      527  \\\n",
       "count  104.000    104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     2.860     25.809   15.093    0.292    5.236    5.725    1.366   \n",
       "std      3.781    151.290    6.043    0.681    1.648    4.212    0.864   \n",
       "min      0.382      0.000    4.213    0.026    1.929    2.350    0.239   \n",
       "25%      1.362      0.000   11.016    0.069    4.117    4.290    0.464   \n",
       "50%      2.228      0.000   14.359    0.093    4.969    5.076    1.516   \n",
       "75%      3.064      0.000   17.812    0.127    6.298    6.263    2.094   \n",
       "max     36.870  1,000.000   46.359    4.021   11.952   44.276    3.108   \n",
       "\n",
       "           528      540      541      542      543      544      545      546  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean     6.322    2.955    1.958    9.880    0.111    0.008    0.003    7.385   \n",
       "std      1.668    1.371    0.751    3.001    0.002    0.001    0.000    0.844   \n",
       "min      2.616    0.852    0.708    4.576    0.107    0.006    0.002    5.836   \n",
       "25%      4.993    1.586    1.270    7.487    0.110    0.008    0.003    7.116   \n",
       "50%      6.295    3.004    1.789    9.398    0.110    0.008    0.003    7.116   \n",
       "75%      7.295    3.926    2.545   12.064    0.112    0.008    0.003    7.116   \n",
       "max      9.962    6.580    3.838   19.953    0.118    0.012    0.003   10.562   \n",
       "\n",
       "          547      548     549     550     551     552     553     554  \\\n",
       "count  86.000   86.000  86.000  86.000  86.000  86.000  86.000  86.000   \n",
       "mean    1.020  404.466  75.800   0.674  18.022   1.559   0.282   8.144   \n",
       "std     0.368    4.151   3.595   0.746   8.724   2.871   0.301   3.688   \n",
       "min     0.511  394.004  71.254   0.080   8.520   0.240   0.037   3.365   \n",
       "25%     0.743  400.814  73.254   0.305  14.530   0.890   0.126   6.738   \n",
       "50%     0.887  404.546  74.199   0.503  16.360   1.255   0.218   7.521   \n",
       "75%     1.258  407.376  79.993   0.787  20.047   1.440   0.335   8.780   \n",
       "max     2.149  413.142  83.016   6.271  90.700  25.470   2.525  38.421   \n",
       "\n",
       "          555      556     557     558      559      560      561      562  \\\n",
       "count  86.000   86.000  86.000  86.000  104.000  104.000  104.000  104.000   \n",
       "mean    0.648   61.002   4.460   2.075    1.002    0.344    0.076   34.152   \n",
       "std     1.247   38.018   2.212   4.000    0.107    0.210    0.049   20.694   \n",
       "min     0.099    8.842   2.104   0.328    0.907    0.071    0.020    7.237   \n",
       "25%     0.354   33.220   3.625   1.189    0.958    0.169    0.039   17.560   \n",
       "50%     0.498   58.805   4.076   1.653    0.977    0.310    0.062   30.958   \n",
       "75%     0.604   77.382   4.952   1.816    0.999    0.485    0.100   47.433   \n",
       "max    11.160  303.550  23.020  35.581    1.502    0.980    0.272   99.037   \n",
       "\n",
       "           563     564     565     566     567     568     569     570  \\\n",
       "count   80.000  80.000  80.000  80.000  80.000  80.000  80.000  80.000   \n",
       "mean   261.275   0.671   6.604   0.160   2.623   0.066   2.526  23.191   \n",
       "std      8.495   0.090   2.753   0.106   1.044   0.043   1.048  12.813   \n",
       "min    242.286   0.551   1.100   0.025   0.412   0.009   0.412   3.342   \n",
       "25%    260.250   0.567   4.980   0.088   2.090   0.038   1.884  15.466   \n",
       "50%    264.272   0.659   5.665   0.134   2.252   0.054   2.234  19.213   \n",
       "75%    265.149   0.727   8.360   0.197   3.318   0.077   3.236  27.917   \n",
       "max    279.470   0.917  14.490   0.622   5.259   0.256   5.658  70.931   \n",
       "\n",
       "           571      572      573      574      575      576      577      578  \\\n",
       "count  104.000  104.000  104.000  104.000  104.000  104.000  104.000  104.000   \n",
       "mean   530.415    2.082   18.020    0.297    5.658    0.091    3.756   14.319   \n",
       "std     21.405    0.361   55.581    0.124   15.621    0.036   12.632    6.446   \n",
       "min    317.196    1.074    5.030    0.067    1.565    0.023    0.948    5.180   \n",
       "25%    531.186    1.974    7.385    0.203    2.540    0.064    1.378   10.806   \n",
       "50%    532.246    2.204    8.700    0.282    3.097    0.089    1.633   13.504   \n",
       "75%    534.536    2.307   10.350    0.351    3.586    0.109    1.945   16.170   \n",
       "max    538.848    2.562  432.940    0.686  111.289    0.207   90.424   61.896   \n",
       "\n",
       "          579     580     581      582      583      584      585      586  \\\n",
       "count  45.000  45.000  45.000   45.000  104.000  104.000  104.000  104.000   \n",
       "mean    0.022   0.018   0.006   91.460    0.501    0.016    0.004    3.135   \n",
       "std     0.011   0.010   0.003   61.717    0.004    0.007    0.001    1.409   \n",
       "min    -0.001   0.004   0.002    0.000    0.491    0.009    0.002    1.747   \n",
       "25%     0.015   0.012   0.004   53.746    0.498    0.011    0.003    2.286   \n",
       "50%     0.020   0.017   0.005   75.508    0.501    0.014    0.004    2.759   \n",
       "75%     0.028   0.021   0.007  109.427    0.503    0.017    0.004    3.393   \n",
       "max     0.058   0.056   0.015  289.923    0.509    0.046    0.010    9.168   \n",
       "\n",
       "           587      588      589      590  \n",
       "count  104.000  104.000  104.000  104.000  \n",
       "mean     0.022    0.018    0.006   98.736  \n",
       "std      0.011    0.009    0.003   79.922  \n",
       "min     -0.003    0.004    0.002    0.000  \n",
       "25%      0.015    0.011    0.004   45.619  \n",
       "50%      0.021    0.017    0.005   76.059  \n",
       "75%      0.028    0.022    0.007  120.742  \n",
       "max      0.058    0.056    0.015  474.081  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Statistics For Fail.\n",
    "fail_data.describe().map('{:,.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "422f8a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,451.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>781.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,458.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>130.000</td>\n",
       "      <td>130.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,451.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>781.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,458.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>130.000</td>\n",
       "      <td>130.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>705.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,451.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>781.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,458.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,449.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,454.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,460.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,456.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,457.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,451.000</td>\n",
       "      <td>215.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,413.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>519.000</td>\n",
       "      <td>781.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,439.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,455.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,461.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,221.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,214.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>1,463.000</td>\n",
       "      <td>573.000</td>\n",
       "      <td>573.000</td>\n",
       "      <td>573.000</td>\n",
       "      <td>573.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "      <td>1,462.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3,014.947</td>\n",
       "      <td>2,495.906</td>\n",
       "      <td>2,200.555</td>\n",
       "      <td>1,399.290</td>\n",
       "      <td>4.405</td>\n",
       "      <td>101.086</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.462</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>199.962</td>\n",
       "      <td>9.057</td>\n",
       "      <td>413.099</td>\n",
       "      <td>9.906</td>\n",
       "      <td>0.971</td>\n",
       "      <td>190.054</td>\n",
       "      <td>12.479</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-5,636.438</td>\n",
       "      <td>2,705.158</td>\n",
       "      <td>-3,811.036</td>\n",
       "      <td>-284.442</td>\n",
       "      <td>1.206</td>\n",
       "      <td>1.943</td>\n",
       "      <td>6.648</td>\n",
       "      <td>69.598</td>\n",
       "      <td>2.364</td>\n",
       "      <td>0.184</td>\n",
       "      <td>3.680</td>\n",
       "      <td>85.309</td>\n",
       "      <td>8.931</td>\n",
       "      <td>50.582</td>\n",
       "      <td>64.564</td>\n",
       "      <td>49.418</td>\n",
       "      <td>66.218</td>\n",
       "      <td>86.830</td>\n",
       "      <td>118.681</td>\n",
       "      <td>68.240</td>\n",
       "      <td>3.351</td>\n",
       "      <td>355.573</td>\n",
       "      <td>10.032</td>\n",
       "      <td>136.726</td>\n",
       "      <td>733.703</td>\n",
       "      <td>1.177</td>\n",
       "      <td>139.991</td>\n",
       "      <td>632.289</td>\n",
       "      <td>157.323</td>\n",
       "      <td>4.593</td>\n",
       "      <td>4.838</td>\n",
       "      <td>2,856.234</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.592</td>\n",
       "      <td>2.563</td>\n",
       "      <td>355.189</td>\n",
       "      <td>10.422</td>\n",
       "      <td>116.546</td>\n",
       "      <td>13.876</td>\n",
       "      <td>20.440</td>\n",
       "      <td>27.027</td>\n",
       "      <td>706.790</td>\n",
       "      <td>13.468</td>\n",
       "      <td>147.497</td>\n",
       "      <td>619.234</td>\n",
       "      <td>104.346</td>\n",
       "      <td>150.316</td>\n",
       "      <td>468.136</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7.455</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.402</td>\n",
       "      <td>0.983</td>\n",
       "      <td>1,807.432</td>\n",
       "      <td>0.188</td>\n",
       "      <td>8,833.290</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.980</td>\n",
       "      <td>101.294</td>\n",
       "      <td>231.877</td>\n",
       "      <td>0.457</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>747.953</td>\n",
       "      <td>0.987</td>\n",
       "      <td>58.655</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.971</td>\n",
       "      <td>6.312</td>\n",
       "      <td>15.794</td>\n",
       "      <td>3.917</td>\n",
       "      <td>15.828</td>\n",
       "      <td>15.792</td>\n",
       "      <td>1.191</td>\n",
       "      <td>2.747</td>\n",
       "      <td>0.651</td>\n",
       "      <td>3.190</td>\n",
       "      <td>-0.588</td>\n",
       "      <td>0.743</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1,003.925</td>\n",
       "      <td>39.416</td>\n",
       "      <td>118.382</td>\n",
       "      <td>138.032</td>\n",
       "      <td>122.278</td>\n",
       "      <td>57.446</td>\n",
       "      <td>417.104</td>\n",
       "      <td>27.927</td>\n",
       "      <td>6.656</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.017</td>\n",
       "      <td>8.499</td>\n",
       "      <td>6.825</td>\n",
       "      <td>14.233</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.012</td>\n",
       "      <td>7.696</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.048</td>\n",
       "      <td>1,027.457</td>\n",
       "      <td>862.276</td>\n",
       "      <td>541.605</td>\n",
       "      <td>4,088.895</td>\n",
       "      <td>4,807.294</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.243</td>\n",
       "      <td>2.773</td>\n",
       "      <td>1.232</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.401</td>\n",
       "      <td>0.683</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.395</td>\n",
       "      <td>19.083</td>\n",
       "      <td>0.549</td>\n",
       "      <td>10.815</td>\n",
       "      <td>26.560</td>\n",
       "      <td>0.145</td>\n",
       "      <td>7.383</td>\n",
       "      <td>17.923</td>\n",
       "      <td>42.938</td>\n",
       "      <td>0.283</td>\n",
       "      <td>8.464</td>\n",
       "      <td>19.935</td>\n",
       "      <td>0.552</td>\n",
       "      <td>11.304</td>\n",
       "      <td>17.459</td>\n",
       "      <td>7.800</td>\n",
       "      <td>10.118</td>\n",
       "      <td>29.861</td>\n",
       "      <td>27.642</td>\n",
       "      <td>8.885</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20.211</td>\n",
       "      <td>73.435</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.071</td>\n",
       "      <td>3.776</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.008</td>\n",
       "      <td>122.862</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1,038.711</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.753</td>\n",
       "      <td>4.168</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>109.550</td>\n",
       "      <td>0.004</td>\n",
       "      <td>4.759</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.071</td>\n",
       "      <td>19.491</td>\n",
       "      <td>3.772</td>\n",
       "      <td>29.324</td>\n",
       "      <td>45.928</td>\n",
       "      <td>41.152</td>\n",
       "      <td>20.156</td>\n",
       "      <td>136.414</td>\n",
       "      <td>9.310</td>\n",
       "      <td>2.214</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.814</td>\n",
       "      <td>2.123</td>\n",
       "      <td>4.315</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.580</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>331.655</td>\n",
       "      <td>391.418</td>\n",
       "      <td>246.059</td>\n",
       "      <td>1,887.538</td>\n",
       "      <td>2,346.192</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.402</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.160</td>\n",
       "      <td>6.001</td>\n",
       "      <td>0.173</td>\n",
       "      <td>3.200</td>\n",
       "      <td>7.885</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.269</td>\n",
       "      <td>5.390</td>\n",
       "      <td>13.262</td>\n",
       "      <td>0.083</td>\n",
       "      <td>2.541</td>\n",
       "      <td>6.172</td>\n",
       "      <td>0.166</td>\n",
       "      <td>3.374</td>\n",
       "      <td>9.656</td>\n",
       "      <td>2.303</td>\n",
       "      <td>3.027</td>\n",
       "      <td>9.280</td>\n",
       "      <td>12.255</td>\n",
       "      <td>2.683</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.154</td>\n",
       "      <td>23.272</td>\n",
       "      <td>7.975</td>\n",
       "      <td>5.787</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.300</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>39.959</td>\n",
       "      <td>0.018</td>\n",
       "      <td>332.212</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.549</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.065</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.469</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6.713</td>\n",
       "      <td>1.230</td>\n",
       "      <td>5.360</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.913</td>\n",
       "      <td>2.609</td>\n",
       "      <td>30.870</td>\n",
       "      <td>25.694</td>\n",
       "      <td>6.651</td>\n",
       "      <td>3.416</td>\n",
       "      <td>8.237</td>\n",
       "      <td>322.575</td>\n",
       "      <td>307.824</td>\n",
       "      <td>1.816</td>\n",
       "      <td>4.184</td>\n",
       "      <td>77.244</td>\n",
       "      <td>3.354</td>\n",
       "      <td>6.710</td>\n",
       "      <td>1.228</td>\n",
       "      <td>4.058</td>\n",
       "      <td>4.296</td>\n",
       "      <td>4.171</td>\n",
       "      <td>17.368</td>\n",
       "      <td>21.191</td>\n",
       "      <td>99.219</td>\n",
       "      <td>202.658</td>\n",
       "      <td>13.719</td>\n",
       "      <td>8.376</td>\n",
       "      <td>6.533</td>\n",
       "      <td>3.987</td>\n",
       "      <td>54.560</td>\n",
       "      <td>70.851</td>\n",
       "      <td>11.539</td>\n",
       "      <td>0.800</td>\n",
       "      <td>1.348</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.646</td>\n",
       "      <td>1.173</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.332</td>\n",
       "      <td>5.366</td>\n",
       "      <td>5.486</td>\n",
       "      <td>7.911</td>\n",
       "      <td>3.623</td>\n",
       "      <td>12.324</td>\n",
       "      <td>5.274</td>\n",
       "      <td>2.836</td>\n",
       "      <td>28.982</td>\n",
       "      <td>6.168</td>\n",
       "      <td>227.591</td>\n",
       "      <td>5.618</td>\n",
       "      <td>5.317</td>\n",
       "      <td>9.477</td>\n",
       "      <td>137.873</td>\n",
       "      <td>39.370</td>\n",
       "      <td>37.539</td>\n",
       "      <td>4.232</td>\n",
       "      <td>20.018</td>\n",
       "      <td>6.105</td>\n",
       "      <td>0.137</td>\n",
       "      <td>3.255</td>\n",
       "      <td>75.649</td>\n",
       "      <td>319.116</td>\n",
       "      <td>207.803</td>\n",
       "      <td>217.796</td>\n",
       "      <td>202.926</td>\n",
       "      <td>300.543</td>\n",
       "      <td>240.276</td>\n",
       "      <td>355.759</td>\n",
       "      <td>270.984</td>\n",
       "      <td>51.386</td>\n",
       "      <td>2.438</td>\n",
       "      <td>8.189</td>\n",
       "      <td>2.525</td>\n",
       "      <td>0.894</td>\n",
       "      <td>6.811</td>\n",
       "      <td>29.909</td>\n",
       "      <td>11.787</td>\n",
       "      <td>265.799</td>\n",
       "      <td>239.721</td>\n",
       "      <td>54.441</td>\n",
       "      <td>271.150</td>\n",
       "      <td>0.705</td>\n",
       "      <td>1.764</td>\n",
       "      <td>1.815</td>\n",
       "      <td>11.393</td>\n",
       "      <td>2.684</td>\n",
       "      <td>10.601</td>\n",
       "      <td>14.703</td>\n",
       "      <td>0.465</td>\n",
       "      <td>5.720</td>\n",
       "      <td>5.549</td>\n",
       "      <td>1.449</td>\n",
       "      <td>6.401</td>\n",
       "      <td>3.040</td>\n",
       "      <td>1.942</td>\n",
       "      <td>9.592</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.628</td>\n",
       "      <td>1.041</td>\n",
       "      <td>403.482</td>\n",
       "      <td>75.671</td>\n",
       "      <td>0.662</td>\n",
       "      <td>16.942</td>\n",
       "      <td>1.208</td>\n",
       "      <td>0.276</td>\n",
       "      <td>7.673</td>\n",
       "      <td>0.493</td>\n",
       "      <td>57.517</td>\n",
       "      <td>4.200</td>\n",
       "      <td>1.591</td>\n",
       "      <td>0.994</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.072</td>\n",
       "      <td>32.152</td>\n",
       "      <td>262.826</td>\n",
       "      <td>0.680</td>\n",
       "      <td>6.434</td>\n",
       "      <td>0.145</td>\n",
       "      <td>2.610</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2.448</td>\n",
       "      <td>20.981</td>\n",
       "      <td>530.531</td>\n",
       "      <td>2.103</td>\n",
       "      <td>29.192</td>\n",
       "      <td>0.349</td>\n",
       "      <td>9.411</td>\n",
       "      <td>0.106</td>\n",
       "      <td>5.692</td>\n",
       "      <td>16.808</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>98.443</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.063</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>99.736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>72.462</td>\n",
       "      <td>81.033</td>\n",
       "      <td>29.392</td>\n",
       "      <td>447.985</td>\n",
       "      <td>58.339</td>\n",
       "      <td>6.317</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>3.300</td>\n",
       "      <td>2.794</td>\n",
       "      <td>17.711</td>\n",
       "      <td>2.484</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.796</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.017</td>\n",
       "      <td>593.799</td>\n",
       "      <td>279.780</td>\n",
       "      <td>1,375.644</td>\n",
       "      <td>2,936.247</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.173</td>\n",
       "      <td>1.207</td>\n",
       "      <td>3.453</td>\n",
       "      <td>0.410</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.939</td>\n",
       "      <td>1.227</td>\n",
       "      <td>1.179</td>\n",
       "      <td>2.621</td>\n",
       "      <td>1.179</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.424</td>\n",
       "      <td>1.822</td>\n",
       "      <td>23.847</td>\n",
       "      <td>2.426</td>\n",
       "      <td>6.270</td>\n",
       "      <td>0.176</td>\n",
       "      <td>7.810</td>\n",
       "      <td>12.214</td>\n",
       "      <td>0.190</td>\n",
       "      <td>4.511</td>\n",
       "      <td>8.709</td>\n",
       "      <td>60.659</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.061</td>\n",
       "      <td>25.402</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.086</td>\n",
       "      <td>9.313</td>\n",
       "      <td>6.055</td>\n",
       "      <td>0.269</td>\n",
       "      <td>8.503</td>\n",
       "      <td>7.068</td>\n",
       "      <td>4.729</td>\n",
       "      <td>6.951</td>\n",
       "      <td>11.588</td>\n",
       "      <td>271.321</td>\n",
       "      <td>3.970</td>\n",
       "      <td>9.508</td>\n",
       "      <td>31.197</td>\n",
       "      <td>18.924</td>\n",
       "      <td>18.396</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.012</td>\n",
       "      <td>53.509</td>\n",
       "      <td>0.043</td>\n",
       "      <td>401.114</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.906</td>\n",
       "      <td>2.130</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.001</td>\n",
       "      <td>49.062</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.704</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.252</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.269</td>\n",
       "      <td>1.237</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.053</td>\n",
       "      <td>6.572</td>\n",
       "      <td>3.014</td>\n",
       "      <td>57.391</td>\n",
       "      <td>53.943</td>\n",
       "      <td>52.564</td>\n",
       "      <td>12.266</td>\n",
       "      <td>262.539</td>\n",
       "      <td>524.762</td>\n",
       "      <td>3.636</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.028</td>\n",
       "      <td>19.379</td>\n",
       "      <td>3.246</td>\n",
       "      <td>32.059</td>\n",
       "      <td>24.181</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5.367</td>\n",
       "      <td>1.160</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.040</td>\n",
       "      <td>409.358</td>\n",
       "      <td>940.047</td>\n",
       "      <td>549.479</td>\n",
       "      <td>4,293.509</td>\n",
       "      <td>6,611.570</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.223</td>\n",
       "      <td>0.374</td>\n",
       "      <td>1.008</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.283</td>\n",
       "      <td>3.336</td>\n",
       "      <td>0.226</td>\n",
       "      <td>4.230</td>\n",
       "      <td>6.871</td>\n",
       "      <td>0.113</td>\n",
       "      <td>7.410</td>\n",
       "      <td>8.842</td>\n",
       "      <td>21.684</td>\n",
       "      <td>0.374</td>\n",
       "      <td>14.281</td>\n",
       "      <td>9.932</td>\n",
       "      <td>0.503</td>\n",
       "      <td>15.084</td>\n",
       "      <td>8.145</td>\n",
       "      <td>5.033</td>\n",
       "      <td>14.762</td>\n",
       "      <td>16.663</td>\n",
       "      <td>523.304</td>\n",
       "      <td>11.291</td>\n",
       "      <td>0.052</td>\n",
       "      <td>17.005</td>\n",
       "      <td>28.119</td>\n",
       "      <td>1.209</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.052</td>\n",
       "      <td>56.208</td>\n",
       "      <td>0.068</td>\n",
       "      <td>431.414</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.087</td>\n",
       "      <td>4.451</td>\n",
       "      <td>10.329</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.014</td>\n",
       "      <td>55.292</td>\n",
       "      <td>0.039</td>\n",
       "      <td>66.602</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.029</td>\n",
       "      <td>7.332</td>\n",
       "      <td>1.149</td>\n",
       "      <td>8.304</td>\n",
       "      <td>17.844</td>\n",
       "      <td>17.838</td>\n",
       "      <td>3.826</td>\n",
       "      <td>85.473</td>\n",
       "      <td>174.895</td>\n",
       "      <td>1.225</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>6.065</td>\n",
       "      <td>0.965</td>\n",
       "      <td>10.098</td>\n",
       "      <td>7.645</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.658</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.012</td>\n",
       "      <td>138.110</td>\n",
       "      <td>453.682</td>\n",
       "      <td>269.843</td>\n",
       "      <td>1,998.921</td>\n",
       "      <td>3,252.894</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.117</td>\n",
       "      <td>1.024</td>\n",
       "      <td>0.073</td>\n",
       "      <td>1.232</td>\n",
       "      <td>2.189</td>\n",
       "      <td>0.033</td>\n",
       "      <td>2.183</td>\n",
       "      <td>2.586</td>\n",
       "      <td>6.607</td>\n",
       "      <td>0.062</td>\n",
       "      <td>5.497</td>\n",
       "      <td>3.290</td>\n",
       "      <td>0.161</td>\n",
       "      <td>5.642</td>\n",
       "      <td>7.391</td>\n",
       "      <td>1.456</td>\n",
       "      <td>5.750</td>\n",
       "      <td>6.040</td>\n",
       "      <td>236.663</td>\n",
       "      <td>3.558</td>\n",
       "      <td>0.012</td>\n",
       "      <td>5.322</td>\n",
       "      <td>8.916</td>\n",
       "      <td>18.026</td>\n",
       "      <td>17.559</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.386</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>17.367</td>\n",
       "      <td>0.021</td>\n",
       "      <td>137.826</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>1.377</td>\n",
       "      <td>3.258</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.004</td>\n",
       "      <td>17.381</td>\n",
       "      <td>0.012</td>\n",
       "      <td>21.036</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.808</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.570</td>\n",
       "      <td>1.777</td>\n",
       "      <td>2.135</td>\n",
       "      <td>0.549</td>\n",
       "      <td>18.390</td>\n",
       "      <td>48.652</td>\n",
       "      <td>4.061</td>\n",
       "      <td>1.050</td>\n",
       "      <td>4.054</td>\n",
       "      <td>288.053</td>\n",
       "      <td>326.851</td>\n",
       "      <td>3.154</td>\n",
       "      <td>7.144</td>\n",
       "      <td>32.447</td>\n",
       "      <td>6.539</td>\n",
       "      <td>23.972</td>\n",
       "      <td>1.011</td>\n",
       "      <td>3.122</td>\n",
       "      <td>10.995</td>\n",
       "      <td>6.626</td>\n",
       "      <td>31.479</td>\n",
       "      <td>31.836</td>\n",
       "      <td>127.057</td>\n",
       "      <td>223.287</td>\n",
       "      <td>29.211</td>\n",
       "      <td>29.350</td>\n",
       "      <td>29.507</td>\n",
       "      <td>1.419</td>\n",
       "      <td>33.256</td>\n",
       "      <td>38.991</td>\n",
       "      <td>6.195</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.663</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.926</td>\n",
       "      <td>2.269</td>\n",
       "      <td>3.112</td>\n",
       "      <td>0.943</td>\n",
       "      <td>8.269</td>\n",
       "      <td>4.670</td>\n",
       "      <td>1.381</td>\n",
       "      <td>13.260</td>\n",
       "      <td>8.204</td>\n",
       "      <td>230.740</td>\n",
       "      <td>3.004</td>\n",
       "      <td>4.725</td>\n",
       "      <td>9.235</td>\n",
       "      <td>47.527</td>\n",
       "      <td>22.335</td>\n",
       "      <td>24.635</td>\n",
       "      <td>2.506</td>\n",
       "      <td>14.710</td>\n",
       "      <td>9.556</td>\n",
       "      <td>5.240</td>\n",
       "      <td>2.538</td>\n",
       "      <td>35.827</td>\n",
       "      <td>280.363</td>\n",
       "      <td>195.384</td>\n",
       "      <td>214.970</td>\n",
       "      <td>219.364</td>\n",
       "      <td>288.975</td>\n",
       "      <td>264.086</td>\n",
       "      <td>250.991</td>\n",
       "      <td>227.796</td>\n",
       "      <td>18.012</td>\n",
       "      <td>1.231</td>\n",
       "      <td>1.776</td>\n",
       "      <td>0.958</td>\n",
       "      <td>5.985</td>\n",
       "      <td>3.328</td>\n",
       "      <td>24.814</td>\n",
       "      <td>4.936</td>\n",
       "      <td>324.709</td>\n",
       "      <td>321.393</td>\n",
       "      <td>35.770</td>\n",
       "      <td>327.497</td>\n",
       "      <td>11.091</td>\n",
       "      <td>5.023</td>\n",
       "      <td>4.849</td>\n",
       "      <td>15.923</td>\n",
       "      <td>5.816</td>\n",
       "      <td>98.808</td>\n",
       "      <td>7.175</td>\n",
       "      <td>4.289</td>\n",
       "      <td>21.381</td>\n",
       "      <td>3.900</td>\n",
       "      <td>0.965</td>\n",
       "      <td>1.904</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.731</td>\n",
       "      <td>2.889</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.342</td>\n",
       "      <td>0.391</td>\n",
       "      <td>5.117</td>\n",
       "      <td>3.377</td>\n",
       "      <td>0.668</td>\n",
       "      <td>4.586</td>\n",
       "      <td>1.184</td>\n",
       "      <td>0.275</td>\n",
       "      <td>2.046</td>\n",
       "      <td>0.523</td>\n",
       "      <td>35.006</td>\n",
       "      <td>1.187</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.052</td>\n",
       "      <td>18.902</td>\n",
       "      <td>7.564</td>\n",
       "      <td>0.124</td>\n",
       "      <td>2.626</td>\n",
       "      <td>0.079</td>\n",
       "      <td>1.032</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.993</td>\n",
       "      <td>10.010</td>\n",
       "      <td>17.197</td>\n",
       "      <td>0.268</td>\n",
       "      <td>88.048</td>\n",
       "      <td>0.255</td>\n",
       "      <td>27.534</td>\n",
       "      <td>0.069</td>\n",
       "      <td>17.182</td>\n",
       "      <td>12.792</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>89.252</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.684</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>94.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2,743.240</td>\n",
       "      <td>2,158.750</td>\n",
       "      <td>2,060.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>82.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.655</td>\n",
       "      <td>182.094</td>\n",
       "      <td>2.293</td>\n",
       "      <td>333.449</td>\n",
       "      <td>4.470</td>\n",
       "      <td>0.579</td>\n",
       "      <td>169.177</td>\n",
       "      <td>9.877</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-7,148.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9,986.750</td>\n",
       "      <td>-14,804.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.400</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.034</td>\n",
       "      <td>2.070</td>\n",
       "      <td>83.183</td>\n",
       "      <td>7.603</td>\n",
       "      <td>49.835</td>\n",
       "      <td>63.677</td>\n",
       "      <td>40.229</td>\n",
       "      <td>64.919</td>\n",
       "      <td>85.141</td>\n",
       "      <td>111.713</td>\n",
       "      <td>1.434</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>342.755</td>\n",
       "      <td>9.464</td>\n",
       "      <td>108.846</td>\n",
       "      <td>699.814</td>\n",
       "      <td>0.497</td>\n",
       "      <td>125.798</td>\n",
       "      <td>607.393</td>\n",
       "      <td>40.787</td>\n",
       "      <td>3.706</td>\n",
       "      <td>3.932</td>\n",
       "      <td>2,801.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.932</td>\n",
       "      <td>4.220</td>\n",
       "      <td>-28.988</td>\n",
       "      <td>324.714</td>\n",
       "      <td>9.461</td>\n",
       "      <td>81.490</td>\n",
       "      <td>1.659</td>\n",
       "      <td>8.514</td>\n",
       "      <td>4.308</td>\n",
       "      <td>632.423</td>\n",
       "      <td>0.414</td>\n",
       "      <td>106.841</td>\n",
       "      <td>581.777</td>\n",
       "      <td>21.433</td>\n",
       "      <td>-59.478</td>\n",
       "      <td>456.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.148</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>5.826</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.105</td>\n",
       "      <td>2.243</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1,627.471</td>\n",
       "      <td>0.111</td>\n",
       "      <td>7,397.310</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>-5.272</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.402</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.785</td>\n",
       "      <td>88.194</td>\n",
       "      <td>213.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.000</td>\n",
       "      <td>544.025</td>\n",
       "      <td>0.890</td>\n",
       "      <td>52.807</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.841</td>\n",
       "      <td>5.126</td>\n",
       "      <td>15.460</td>\n",
       "      <td>1.671</td>\n",
       "      <td>15.170</td>\n",
       "      <td>15.430</td>\n",
       "      <td>0.312</td>\n",
       "      <td>2.340</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.779</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2.191</td>\n",
       "      <td>980.451</td>\n",
       "      <td>33.366</td>\n",
       "      <td>58.000</td>\n",
       "      <td>36.100</td>\n",
       "      <td>19.200</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1.337</td>\n",
       "      <td>2.020</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>234.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>9.400</td>\n",
       "      <td>0.093</td>\n",
       "      <td>3.170</td>\n",
       "      <td>5.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.940</td>\n",
       "      <td>6.220</td>\n",
       "      <td>6.613</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.750</td>\n",
       "      <td>9.220</td>\n",
       "      <td>0.090</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.560</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.728</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.010</td>\n",
       "      <td>5.359</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.264</td>\n",
       "      <td>0.009</td>\n",
       "      <td>168.800</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6.098</td>\n",
       "      <td>1.302</td>\n",
       "      <td>15.547</td>\n",
       "      <td>10.402</td>\n",
       "      <td>6.943</td>\n",
       "      <td>8.651</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>82.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.788</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.657</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.611</td>\n",
       "      <td>1.710</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.537</td>\n",
       "      <td>2.837</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.790</td>\n",
       "      <td>5.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.200</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.256</td>\n",
       "      <td>2.056</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.720</td>\n",
       "      <td>0.003</td>\n",
       "      <td>60.988</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.424</td>\n",
       "      <td>2.738</td>\n",
       "      <td>1.216</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.042</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.722</td>\n",
       "      <td>23.020</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.121</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.671</td>\n",
       "      <td>0.904</td>\n",
       "      <td>2.329</td>\n",
       "      <td>0.695</td>\n",
       "      <td>3.049</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.991</td>\n",
       "      <td>7.953</td>\n",
       "      <td>1.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.833</td>\n",
       "      <td>2.403</td>\n",
       "      <td>11.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.687</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646</td>\n",
       "      <td>8.841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.723</td>\n",
       "      <td>0.556</td>\n",
       "      <td>4.888</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.772</td>\n",
       "      <td>4.814</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.681</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.171</td>\n",
       "      <td>2.170</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.614</td>\n",
       "      <td>3.276</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.429</td>\n",
       "      <td>0.444</td>\n",
       "      <td>372.822</td>\n",
       "      <td>71.038</td>\n",
       "      <td>0.045</td>\n",
       "      <td>6.110</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.019</td>\n",
       "      <td>2.786</td>\n",
       "      <td>0.052</td>\n",
       "      <td>4.827</td>\n",
       "      <td>1.497</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.018</td>\n",
       "      <td>7.237</td>\n",
       "      <td>242.534</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.371</td>\n",
       "      <td>3.250</td>\n",
       "      <td>328.466</td>\n",
       "      <td>0.980</td>\n",
       "      <td>3.540</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.664</td>\n",
       "      <td>4.582</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.198</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2,968.780</td>\n",
       "      <td>2,452.150</td>\n",
       "      <td>2,181.189</td>\n",
       "      <td>1,081.876</td>\n",
       "      <td>1.016</td>\n",
       "      <td>97.778</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.410</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.958</td>\n",
       "      <td>198.124</td>\n",
       "      <td>7.175</td>\n",
       "      <td>405.977</td>\n",
       "      <td>9.565</td>\n",
       "      <td>0.969</td>\n",
       "      <td>188.284</td>\n",
       "      <td>12.460</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-5,971.250</td>\n",
       "      <td>2,578.250</td>\n",
       "      <td>-4,371.188</td>\n",
       "      <td>-1,473.583</td>\n",
       "      <td>1.093</td>\n",
       "      <td>1.906</td>\n",
       "      <td>5.258</td>\n",
       "      <td>67.425</td>\n",
       "      <td>2.089</td>\n",
       "      <td>0.162</td>\n",
       "      <td>3.362</td>\n",
       "      <td>84.478</td>\n",
       "      <td>8.577</td>\n",
       "      <td>50.254</td>\n",
       "      <td>64.025</td>\n",
       "      <td>49.420</td>\n",
       "      <td>66.038</td>\n",
       "      <td>86.578</td>\n",
       "      <td>118.012</td>\n",
       "      <td>75.340</td>\n",
       "      <td>2.694</td>\n",
       "      <td>350.795</td>\n",
       "      <td>9.929</td>\n",
       "      <td>130.739</td>\n",
       "      <td>724.463</td>\n",
       "      <td>0.983</td>\n",
       "      <td>136.940</td>\n",
       "      <td>625.857</td>\n",
       "      <td>115.636</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.816</td>\n",
       "      <td>2,837.000</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.947</td>\n",
       "      <td>4.532</td>\n",
       "      <td>-1.970</td>\n",
       "      <td>350.693</td>\n",
       "      <td>10.285</td>\n",
       "      <td>112.109</td>\n",
       "      <td>10.349</td>\n",
       "      <td>17.402</td>\n",
       "      <td>23.156</td>\n",
       "      <td>698.797</td>\n",
       "      <td>0.890</td>\n",
       "      <td>145.266</td>\n",
       "      <td>612.820</td>\n",
       "      <td>87.717</td>\n",
       "      <td>145.326</td>\n",
       "      <td>464.391</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>7.107</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.377</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1,777.200</td>\n",
       "      <td>0.170</td>\n",
       "      <td>8,564.910</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.090</td>\n",
       "      <td>-0.215</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.979</td>\n",
       "      <td>100.348</td>\n",
       "      <td>230.413</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>721.023</td>\n",
       "      <td>0.990</td>\n",
       "      <td>57.995</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.965</td>\n",
       "      <td>6.248</td>\n",
       "      <td>15.730</td>\n",
       "      <td>3.208</td>\n",
       "      <td>15.760</td>\n",
       "      <td>15.720</td>\n",
       "      <td>0.974</td>\n",
       "      <td>2.564</td>\n",
       "      <td>0.549</td>\n",
       "      <td>3.074</td>\n",
       "      <td>-0.946</td>\n",
       "      <td>0.685</td>\n",
       "      <td>0.996</td>\n",
       "      <td>2.277</td>\n",
       "      <td>999.939</td>\n",
       "      <td>37.369</td>\n",
       "      <td>92.000</td>\n",
       "      <td>89.800</td>\n",
       "      <td>80.500</td>\n",
       "      <td>50.700</td>\n",
       "      <td>241.589</td>\n",
       "      <td>0.130</td>\n",
       "      <td>5.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.402</td>\n",
       "      <td>4.453</td>\n",
       "      <td>8.103</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.007</td>\n",
       "      <td>5.961</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.027</td>\n",
       "      <td>717.075</td>\n",
       "      <td>408.500</td>\n",
       "      <td>291.000</td>\n",
       "      <td>1,313.500</td>\n",
       "      <td>440.250</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.132</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.222</td>\n",
       "      <td>16.900</td>\n",
       "      <td>0.380</td>\n",
       "      <td>7.723</td>\n",
       "      <td>21.063</td>\n",
       "      <td>0.102</td>\n",
       "      <td>5.370</td>\n",
       "      <td>14.480</td>\n",
       "      <td>24.356</td>\n",
       "      <td>0.215</td>\n",
       "      <td>5.037</td>\n",
       "      <td>17.070</td>\n",
       "      <td>0.296</td>\n",
       "      <td>6.730</td>\n",
       "      <td>14.070</td>\n",
       "      <td>5.018</td>\n",
       "      <td>6.057</td>\n",
       "      <td>24.730</td>\n",
       "      <td>0.114</td>\n",
       "      <td>6.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.260</td>\n",
       "      <td>56.224</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.950</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>95.040</td>\n",
       "      <td>0.030</td>\n",
       "      <td>715.200</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.915</td>\n",
       "      <td>2.704</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>75.501</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.200</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.044</td>\n",
       "      <td>13.828</td>\n",
       "      <td>2.957</td>\n",
       "      <td>25.045</td>\n",
       "      <td>29.755</td>\n",
       "      <td>26.919</td>\n",
       "      <td>18.246</td>\n",
       "      <td>80.992</td>\n",
       "      <td>0.044</td>\n",
       "      <td>1.698</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.223</td>\n",
       "      <td>1.442</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.098</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>228.382</td>\n",
       "      <td>184.138</td>\n",
       "      <td>127.943</td>\n",
       "      <td>600.425</td>\n",
       "      <td>202.474</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>5.326</td>\n",
       "      <td>0.119</td>\n",
       "      <td>2.322</td>\n",
       "      <td>6.224</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.667</td>\n",
       "      <td>4.256</td>\n",
       "      <td>7.481</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.547</td>\n",
       "      <td>5.442</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.029</td>\n",
       "      <td>8.270</td>\n",
       "      <td>1.542</td>\n",
       "      <td>1.898</td>\n",
       "      <td>7.630</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.896</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.974</td>\n",
       "      <td>17.834</td>\n",
       "      <td>4.429</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.028</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.178</td>\n",
       "      <td>0.009</td>\n",
       "      <td>227.448</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.187</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.672</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.014</td>\n",
       "      <td>4.575</td>\n",
       "      <td>0.966</td>\n",
       "      <td>4.149</td>\n",
       "      <td>2.997</td>\n",
       "      <td>3.236</td>\n",
       "      <td>2.315</td>\n",
       "      <td>18.470</td>\n",
       "      <td>11.456</td>\n",
       "      <td>4.927</td>\n",
       "      <td>2.677</td>\n",
       "      <td>5.786</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.032</td>\n",
       "      <td>3.204</td>\n",
       "      <td>55.800</td>\n",
       "      <td>1.966</td>\n",
       "      <td>3.745</td>\n",
       "      <td>0.748</td>\n",
       "      <td>3.122</td>\n",
       "      <td>1.931</td>\n",
       "      <td>2.563</td>\n",
       "      <td>6.925</td>\n",
       "      <td>10.923</td>\n",
       "      <td>30.904</td>\n",
       "      <td>9.883</td>\n",
       "      <td>7.509</td>\n",
       "      <td>3.485</td>\n",
       "      <td>1.954</td>\n",
       "      <td>3.068</td>\n",
       "      <td>36.247</td>\n",
       "      <td>48.087</td>\n",
       "      <td>5.312</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.539</td>\n",
       "      <td>0.804</td>\n",
       "      <td>0.555</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.187</td>\n",
       "      <td>4.786</td>\n",
       "      <td>3.774</td>\n",
       "      <td>5.801</td>\n",
       "      <td>2.876</td>\n",
       "      <td>8.819</td>\n",
       "      <td>3.824</td>\n",
       "      <td>2.286</td>\n",
       "      <td>19.936</td>\n",
       "      <td>4.609</td>\n",
       "      <td>40.727</td>\n",
       "      <td>4.830</td>\n",
       "      <td>2.812</td>\n",
       "      <td>5.791</td>\n",
       "      <td>105.595</td>\n",
       "      <td>24.850</td>\n",
       "      <td>23.149</td>\n",
       "      <td>3.498</td>\n",
       "      <td>11.576</td>\n",
       "      <td>4.073</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.612</td>\n",
       "      <td>53.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>80.848</td>\n",
       "      <td>77.342</td>\n",
       "      <td>50.865</td>\n",
       "      <td>0.000</td>\n",
       "      <td>53.298</td>\n",
       "      <td>152.912</td>\n",
       "      <td>111.800</td>\n",
       "      <td>38.589</td>\n",
       "      <td>1.747</td>\n",
       "      <td>6.947</td>\n",
       "      <td>1.662</td>\n",
       "      <td>0.139</td>\n",
       "      <td>5.269</td>\n",
       "      <td>16.351</td>\n",
       "      <td>8.113</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.900</td>\n",
       "      <td>1.168</td>\n",
       "      <td>4.093</td>\n",
       "      <td>1.557</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.127</td>\n",
       "      <td>0.074</td>\n",
       "      <td>3.755</td>\n",
       "      <td>4.087</td>\n",
       "      <td>0.486</td>\n",
       "      <td>4.855</td>\n",
       "      <td>1.894</td>\n",
       "      <td>1.396</td>\n",
       "      <td>7.504</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.801</td>\n",
       "      <td>400.684</td>\n",
       "      <td>73.254</td>\n",
       "      <td>0.224</td>\n",
       "      <td>14.530</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.095</td>\n",
       "      <td>6.738</td>\n",
       "      <td>0.344</td>\n",
       "      <td>27.018</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.036</td>\n",
       "      <td>15.723</td>\n",
       "      <td>259.958</td>\n",
       "      <td>0.567</td>\n",
       "      <td>4.980</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.884</td>\n",
       "      <td>15.466</td>\n",
       "      <td>530.675</td>\n",
       "      <td>1.984</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.242</td>\n",
       "      <td>2.571</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.409</td>\n",
       "      <td>11.572</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>46.102</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.312</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>44.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3,012.680</td>\n",
       "      <td>2,499.150</td>\n",
       "      <td>2,201.067</td>\n",
       "      <td>1,285.214</td>\n",
       "      <td>1.317</td>\n",
       "      <td>101.512</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.461</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.966</td>\n",
       "      <td>199.509</td>\n",
       "      <td>8.982</td>\n",
       "      <td>412.204</td>\n",
       "      <td>9.847</td>\n",
       "      <td>0.973</td>\n",
       "      <td>189.671</td>\n",
       "      <td>12.500</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-5,529.000</td>\n",
       "      <td>2,665.250</td>\n",
       "      <td>-3,821.000</td>\n",
       "      <td>-83.125</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.986</td>\n",
       "      <td>7.265</td>\n",
       "      <td>69.244</td>\n",
       "      <td>2.378</td>\n",
       "      <td>0.187</td>\n",
       "      <td>3.431</td>\n",
       "      <td>85.124</td>\n",
       "      <td>8.766</td>\n",
       "      <td>50.400</td>\n",
       "      <td>64.167</td>\n",
       "      <td>49.600</td>\n",
       "      <td>66.229</td>\n",
       "      <td>86.820</td>\n",
       "      <td>118.408</td>\n",
       "      <td>78.330</td>\n",
       "      <td>3.074</td>\n",
       "      <td>353.634</td>\n",
       "      <td>10.037</td>\n",
       "      <td>136.377</td>\n",
       "      <td>733.577</td>\n",
       "      <td>1.250</td>\n",
       "      <td>140.037</td>\n",
       "      <td>631.395</td>\n",
       "      <td>182.280</td>\n",
       "      <td>4.596</td>\n",
       "      <td>4.843</td>\n",
       "      <td>2,854.000</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.570</td>\n",
       "      <td>0.722</td>\n",
       "      <td>353.799</td>\n",
       "      <td>10.437</td>\n",
       "      <td>116.216</td>\n",
       "      <td>13.226</td>\n",
       "      <td>20.012</td>\n",
       "      <td>26.217</td>\n",
       "      <td>706.752</td>\n",
       "      <td>0.976</td>\n",
       "      <td>147.636</td>\n",
       "      <td>619.099</td>\n",
       "      <td>102.651</td>\n",
       "      <td>152.379</td>\n",
       "      <td>466.070</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>7.479</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.988</td>\n",
       "      <td>1,809.249</td>\n",
       "      <td>0.190</td>\n",
       "      <td>8,835.570</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.981</td>\n",
       "      <td>101.477</td>\n",
       "      <td>231.219</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>751.375</td>\n",
       "      <td>0.991</td>\n",
       "      <td>58.550</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.969</td>\n",
       "      <td>6.315</td>\n",
       "      <td>15.790</td>\n",
       "      <td>3.882</td>\n",
       "      <td>15.830</td>\n",
       "      <td>15.770</td>\n",
       "      <td>1.144</td>\n",
       "      <td>2.735</td>\n",
       "      <td>0.655</td>\n",
       "      <td>3.195</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.312</td>\n",
       "      <td>1,003.971</td>\n",
       "      <td>38.992</td>\n",
       "      <td>109.000</td>\n",
       "      <td>135.000</td>\n",
       "      <td>116.600</td>\n",
       "      <td>55.900</td>\n",
       "      <td>340.682</td>\n",
       "      <td>0.236</td>\n",
       "      <td>6.260</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.016</td>\n",
       "      <td>7.915</td>\n",
       "      <td>5.955</td>\n",
       "      <td>10.974</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.011</td>\n",
       "      <td>7.485</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.035</td>\n",
       "      <td>995.700</td>\n",
       "      <td>615.000</td>\n",
       "      <td>433.000</td>\n",
       "      <td>2,595.000</td>\n",
       "      <td>1,780.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.183</td>\n",
       "      <td>2.600</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.299</td>\n",
       "      <td>18.760</td>\n",
       "      <td>0.527</td>\n",
       "      <td>10.230</td>\n",
       "      <td>26.870</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.750</td>\n",
       "      <td>17.800</td>\n",
       "      <td>39.886</td>\n",
       "      <td>0.258</td>\n",
       "      <td>6.770</td>\n",
       "      <td>19.350</td>\n",
       "      <td>0.421</td>\n",
       "      <td>8.560</td>\n",
       "      <td>17.145</td>\n",
       "      <td>6.755</td>\n",
       "      <td>8.454</td>\n",
       "      <td>30.085</td>\n",
       "      <td>0.158</td>\n",
       "      <td>7.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.660</td>\n",
       "      <td>73.405</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.624</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.002</td>\n",
       "      <td>119.436</td>\n",
       "      <td>0.040</td>\n",
       "      <td>966.700</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.192</td>\n",
       "      <td>3.669</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>102.994</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.864</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.406</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.977</td>\n",
       "      <td>3.704</td>\n",
       "      <td>28.859</td>\n",
       "      <td>45.512</td>\n",
       "      <td>39.585</td>\n",
       "      <td>19.581</td>\n",
       "      <td>110.601</td>\n",
       "      <td>0.078</td>\n",
       "      <td>2.078</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.665</td>\n",
       "      <td>1.870</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.551</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>310.995</td>\n",
       "      <td>275.147</td>\n",
       "      <td>194.766</td>\n",
       "      <td>1,194.193</td>\n",
       "      <td>819.642</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.301</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.122</td>\n",
       "      <td>5.862</td>\n",
       "      <td>0.164</td>\n",
       "      <td>2.904</td>\n",
       "      <td>8.356</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.079</td>\n",
       "      <td>5.445</td>\n",
       "      <td>12.312</td>\n",
       "      <td>0.085</td>\n",
       "      <td>2.066</td>\n",
       "      <td>5.969</td>\n",
       "      <td>0.129</td>\n",
       "      <td>2.509</td>\n",
       "      <td>9.060</td>\n",
       "      <td>2.055</td>\n",
       "      <td>2.546</td>\n",
       "      <td>9.458</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.360</td>\n",
       "      <td>0.000</td>\n",
       "      <td>5.998</td>\n",
       "      <td>23.306</td>\n",
       "      <td>5.523</td>\n",
       "      <td>3.025</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>39.743</td>\n",
       "      <td>0.013</td>\n",
       "      <td>308.438</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1.106</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.879</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.920</td>\n",
       "      <td>1.240</td>\n",
       "      <td>4.950</td>\n",
       "      <td>4.495</td>\n",
       "      <td>4.697</td>\n",
       "      <td>2.546</td>\n",
       "      <td>26.103</td>\n",
       "      <td>20.265</td>\n",
       "      <td>6.172</td>\n",
       "      <td>3.238</td>\n",
       "      <td>7.455</td>\n",
       "      <td>306.534</td>\n",
       "      <td>266.396</td>\n",
       "      <td>1.628</td>\n",
       "      <td>3.943</td>\n",
       "      <td>69.555</td>\n",
       "      <td>2.660</td>\n",
       "      <td>4.757</td>\n",
       "      <td>1.126</td>\n",
       "      <td>3.935</td>\n",
       "      <td>2.493</td>\n",
       "      <td>3.454</td>\n",
       "      <td>10.985</td>\n",
       "      <td>16.205</td>\n",
       "      <td>57.648</td>\n",
       "      <td>150.074</td>\n",
       "      <td>10.150</td>\n",
       "      <td>4.527</td>\n",
       "      <td>2.762</td>\n",
       "      <td>3.771</td>\n",
       "      <td>49.002</td>\n",
       "      <td>65.442</td>\n",
       "      <td>12.118</td>\n",
       "      <td>0.806</td>\n",
       "      <td>1.266</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.650</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.251</td>\n",
       "      <td>5.289</td>\n",
       "      <td>5.253</td>\n",
       "      <td>7.435</td>\n",
       "      <td>3.694</td>\n",
       "      <td>11.448</td>\n",
       "      <td>4.796</td>\n",
       "      <td>2.824</td>\n",
       "      <td>26.080</td>\n",
       "      <td>5.633</td>\n",
       "      <td>157.166</td>\n",
       "      <td>5.455</td>\n",
       "      <td>4.019</td>\n",
       "      <td>7.375</td>\n",
       "      <td>138.329</td>\n",
       "      <td>34.314</td>\n",
       "      <td>32.651</td>\n",
       "      <td>4.275</td>\n",
       "      <td>15.928</td>\n",
       "      <td>5.191</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.178</td>\n",
       "      <td>70.694</td>\n",
       "      <td>295.305</td>\n",
       "      <td>148.697</td>\n",
       "      <td>139.656</td>\n",
       "      <td>113.520</td>\n",
       "      <td>248.981</td>\n",
       "      <td>112.932</td>\n",
       "      <td>351.293</td>\n",
       "      <td>218.936</td>\n",
       "      <td>48.490</td>\n",
       "      <td>2.247</td>\n",
       "      <td>8.021</td>\n",
       "      <td>2.537</td>\n",
       "      <td>0.231</td>\n",
       "      <td>6.608</td>\n",
       "      <td>21.916</td>\n",
       "      <td>10.885</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>46.377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.174</td>\n",
       "      <td>1.167</td>\n",
       "      <td>1.586</td>\n",
       "      <td>5.633</td>\n",
       "      <td>2.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.646</td>\n",
       "      <td>0.100</td>\n",
       "      <td>4.875</td>\n",
       "      <td>5.138</td>\n",
       "      <td>1.557</td>\n",
       "      <td>6.425</td>\n",
       "      <td>3.087</td>\n",
       "      <td>1.786</td>\n",
       "      <td>9.459</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.912</td>\n",
       "      <td>402.966</td>\n",
       "      <td>74.078</td>\n",
       "      <td>0.469</td>\n",
       "      <td>16.340</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.197</td>\n",
       "      <td>7.399</td>\n",
       "      <td>0.477</td>\n",
       "      <td>54.278</td>\n",
       "      <td>4.067</td>\n",
       "      <td>1.521</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.059</td>\n",
       "      <td>29.509</td>\n",
       "      <td>264.272</td>\n",
       "      <td>0.650</td>\n",
       "      <td>5.140</td>\n",
       "      <td>0.119</td>\n",
       "      <td>2.148</td>\n",
       "      <td>0.049</td>\n",
       "      <td>1.988</td>\n",
       "      <td>16.822</td>\n",
       "      <td>532.414</td>\n",
       "      <td>2.114</td>\n",
       "      <td>8.650</td>\n",
       "      <td>0.294</td>\n",
       "      <td>2.970</td>\n",
       "      <td>0.090</td>\n",
       "      <td>1.625</td>\n",
       "      <td>13.898</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>72.023</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>71.677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3,057.030</td>\n",
       "      <td>2,539.460</td>\n",
       "      <td>2,218.055</td>\n",
       "      <td>1,591.224</td>\n",
       "      <td>1.529</td>\n",
       "      <td>104.587</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.517</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.971</td>\n",
       "      <td>202.035</td>\n",
       "      <td>10.887</td>\n",
       "      <td>419.117</td>\n",
       "      <td>10.123</td>\n",
       "      <td>0.977</td>\n",
       "      <td>192.241</td>\n",
       "      <td>12.547</td>\n",
       "      <td>1.415</td>\n",
       "      <td>-5,361.562</td>\n",
       "      <td>2,851.750</td>\n",
       "      <td>-3,368.312</td>\n",
       "      <td>1,381.062</td>\n",
       "      <td>1.304</td>\n",
       "      <td>2.003</td>\n",
       "      <td>7.329</td>\n",
       "      <td>72.331</td>\n",
       "      <td>2.653</td>\n",
       "      <td>0.207</td>\n",
       "      <td>3.539</td>\n",
       "      <td>85.742</td>\n",
       "      <td>9.043</td>\n",
       "      <td>50.580</td>\n",
       "      <td>64.352</td>\n",
       "      <td>49.746</td>\n",
       "      <td>66.340</td>\n",
       "      <td>87.002</td>\n",
       "      <td>118.945</td>\n",
       "      <td>80.265</td>\n",
       "      <td>3.483</td>\n",
       "      <td>360.843</td>\n",
       "      <td>10.154</td>\n",
       "      <td>142.107</td>\n",
       "      <td>741.460</td>\n",
       "      <td>1.340</td>\n",
       "      <td>143.232</td>\n",
       "      <td>638.337</td>\n",
       "      <td>206.961</td>\n",
       "      <td>4.617</td>\n",
       "      <td>4.869</td>\n",
       "      <td>2,874.000</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.952</td>\n",
       "      <td>4.669</td>\n",
       "      <td>3.898</td>\n",
       "      <td>359.685</td>\n",
       "      <td>10.588</td>\n",
       "      <td>120.926</td>\n",
       "      <td>16.243</td>\n",
       "      <td>22.664</td>\n",
       "      <td>29.785</td>\n",
       "      <td>714.633</td>\n",
       "      <td>1.064</td>\n",
       "      <td>149.968</td>\n",
       "      <td>625.270</td>\n",
       "      <td>115.767</td>\n",
       "      <td>158.073</td>\n",
       "      <td>467.888</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.027</td>\n",
       "      <td>7.809</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.115</td>\n",
       "      <td>2.429</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1,841.789</td>\n",
       "      <td>0.201</td>\n",
       "      <td>9,074.490</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.982</td>\n",
       "      <td>102.074</td>\n",
       "      <td>233.191</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>777.209</td>\n",
       "      <td>0.991</td>\n",
       "      <td>59.145</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.978</td>\n",
       "      <td>6.376</td>\n",
       "      <td>15.860</td>\n",
       "      <td>4.406</td>\n",
       "      <td>15.900</td>\n",
       "      <td>15.870</td>\n",
       "      <td>1.343</td>\n",
       "      <td>2.872</td>\n",
       "      <td>0.723</td>\n",
       "      <td>3.310</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.814</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.357</td>\n",
       "      <td>1,008.614</td>\n",
       "      <td>40.849</td>\n",
       "      <td>128.000</td>\n",
       "      <td>180.900</td>\n",
       "      <td>161.600</td>\n",
       "      <td>62.900</td>\n",
       "      <td>502.206</td>\n",
       "      <td>0.438</td>\n",
       "      <td>7.510</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.021</td>\n",
       "      <td>9.565</td>\n",
       "      <td>8.299</td>\n",
       "      <td>14.394</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.015</td>\n",
       "      <td>9.048</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.050</td>\n",
       "      <td>1,248.000</td>\n",
       "      <td>959.750</td>\n",
       "      <td>622.000</td>\n",
       "      <td>5,063.000</td>\n",
       "      <td>6,273.250</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.254</td>\n",
       "      <td>3.200</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.423</td>\n",
       "      <td>21.078</td>\n",
       "      <td>0.691</td>\n",
       "      <td>13.395</td>\n",
       "      <td>31.575</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8.457</td>\n",
       "      <td>20.848</td>\n",
       "      <td>57.675</td>\n",
       "      <td>0.296</td>\n",
       "      <td>9.572</td>\n",
       "      <td>21.440</td>\n",
       "      <td>0.722</td>\n",
       "      <td>11.460</td>\n",
       "      <td>20.093</td>\n",
       "      <td>9.490</td>\n",
       "      <td>11.930</td>\n",
       "      <td>33.376</td>\n",
       "      <td>0.229</td>\n",
       "      <td>9.790</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.340</td>\n",
       "      <td>90.545</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.395</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.005</td>\n",
       "      <td>144.503</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1,266.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.785</td>\n",
       "      <td>4.480</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>131.758</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.804</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.092</td>\n",
       "      <td>24.653</td>\n",
       "      <td>4.377</td>\n",
       "      <td>31.810</td>\n",
       "      <td>59.240</td>\n",
       "      <td>54.043</td>\n",
       "      <td>22.135</td>\n",
       "      <td>163.085</td>\n",
       "      <td>0.145</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>3.145</td>\n",
       "      <td>2.619</td>\n",
       "      <td>4.321</td>\n",
       "      <td>0.192</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3.025</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>400.583</td>\n",
       "      <td>426.967</td>\n",
       "      <td>272.954</td>\n",
       "      <td>2,341.377</td>\n",
       "      <td>3,139.696</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.115</td>\n",
       "      <td>1.045</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.179</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.160</td>\n",
       "      <td>6.578</td>\n",
       "      <td>0.219</td>\n",
       "      <td>4.056</td>\n",
       "      <td>9.466</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.638</td>\n",
       "      <td>6.345</td>\n",
       "      <td>17.925</td>\n",
       "      <td>0.096</td>\n",
       "      <td>2.788</td>\n",
       "      <td>6.528</td>\n",
       "      <td>0.210</td>\n",
       "      <td>3.358</td>\n",
       "      <td>10.003</td>\n",
       "      <td>2.775</td>\n",
       "      <td>3.407</td>\n",
       "      <td>10.416</td>\n",
       "      <td>0.066</td>\n",
       "      <td>2.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.875</td>\n",
       "      <td>28.888</td>\n",
       "      <td>6.816</td>\n",
       "      <td>3.970</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>46.959</td>\n",
       "      <td>0.019</td>\n",
       "      <td>410.783</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.549</td>\n",
       "      <td>1.399</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.498</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.152</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.260</td>\n",
       "      <td>1.415</td>\n",
       "      <td>5.795</td>\n",
       "      <td>5.934</td>\n",
       "      <td>6.454</td>\n",
       "      <td>2.849</td>\n",
       "      <td>38.140</td>\n",
       "      <td>29.307</td>\n",
       "      <td>7.704</td>\n",
       "      <td>4.022</td>\n",
       "      <td>9.231</td>\n",
       "      <td>524.988</td>\n",
       "      <td>590.918</td>\n",
       "      <td>2.188</td>\n",
       "      <td>4.771</td>\n",
       "      <td>92.151</td>\n",
       "      <td>3.484</td>\n",
       "      <td>6.605</td>\n",
       "      <td>1.518</td>\n",
       "      <td>4.767</td>\n",
       "      <td>3.492</td>\n",
       "      <td>4.755</td>\n",
       "      <td>17.365</td>\n",
       "      <td>21.605</td>\n",
       "      <td>120.155</td>\n",
       "      <td>299.769</td>\n",
       "      <td>12.694</td>\n",
       "      <td>5.794</td>\n",
       "      <td>3.809</td>\n",
       "      <td>4.667</td>\n",
       "      <td>66.667</td>\n",
       "      <td>85.265</td>\n",
       "      <td>15.841</td>\n",
       "      <td>0.927</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.732</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.746</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.351</td>\n",
       "      <td>5.952</td>\n",
       "      <td>6.926</td>\n",
       "      <td>9.638</td>\n",
       "      <td>4.327</td>\n",
       "      <td>14.388</td>\n",
       "      <td>6.089</td>\n",
       "      <td>3.304</td>\n",
       "      <td>35.163</td>\n",
       "      <td>6.373</td>\n",
       "      <td>343.291</td>\n",
       "      <td>5.999</td>\n",
       "      <td>6.952</td>\n",
       "      <td>9.709</td>\n",
       "      <td>168.251</td>\n",
       "      <td>47.886</td>\n",
       "      <td>45.186</td>\n",
       "      <td>4.725</td>\n",
       "      <td>23.518</td>\n",
       "      <td>6.584</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.622</td>\n",
       "      <td>92.934</td>\n",
       "      <td>513.855</td>\n",
       "      <td>263.520</td>\n",
       "      <td>296.700</td>\n",
       "      <td>294.679</td>\n",
       "      <td>501.875</td>\n",
       "      <td>400.870</td>\n",
       "      <td>510.874</td>\n",
       "      <td>377.798</td>\n",
       "      <td>61.594</td>\n",
       "      <td>2.821</td>\n",
       "      <td>9.059</td>\n",
       "      <td>3.199</td>\n",
       "      <td>0.558</td>\n",
       "      <td>7.882</td>\n",
       "      <td>32.431</td>\n",
       "      <td>14.468</td>\n",
       "      <td>540.418</td>\n",
       "      <td>506.083</td>\n",
       "      <td>63.059</td>\n",
       "      <td>542.857</td>\n",
       "      <td>0.260</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.935</td>\n",
       "      <td>10.637</td>\n",
       "      <td>2.894</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.809</td>\n",
       "      <td>0.134</td>\n",
       "      <td>6.472</td>\n",
       "      <td>6.329</td>\n",
       "      <td>2.218</td>\n",
       "      <td>7.616</td>\n",
       "      <td>3.952</td>\n",
       "      <td>2.454</td>\n",
       "      <td>11.169</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>8.073</td>\n",
       "      <td>1.286</td>\n",
       "      <td>407.436</td>\n",
       "      <td>78.274</td>\n",
       "      <td>0.853</td>\n",
       "      <td>18.940</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.362</td>\n",
       "      <td>8.627</td>\n",
       "      <td>0.561</td>\n",
       "      <td>74.398</td>\n",
       "      <td>4.692</td>\n",
       "      <td>1.816</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.089</td>\n",
       "      <td>44.113</td>\n",
       "      <td>265.812</td>\n",
       "      <td>0.772</td>\n",
       "      <td>7.745</td>\n",
       "      <td>0.185</td>\n",
       "      <td>3.096</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.950</td>\n",
       "      <td>24.653</td>\n",
       "      <td>534.356</td>\n",
       "      <td>2.289</td>\n",
       "      <td>10.130</td>\n",
       "      <td>0.367</td>\n",
       "      <td>3.493</td>\n",
       "      <td>0.113</td>\n",
       "      <td>1.902</td>\n",
       "      <td>17.216</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>117.094</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.282</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>114.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3,356.350</td>\n",
       "      <td>2,846.440</td>\n",
       "      <td>2,315.267</td>\n",
       "      <td>3,715.042</td>\n",
       "      <td>1,114.537</td>\n",
       "      <td>129.252</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1.656</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.985</td>\n",
       "      <td>272.045</td>\n",
       "      <td>19.547</td>\n",
       "      <td>824.927</td>\n",
       "      <td>102.868</td>\n",
       "      <td>0.985</td>\n",
       "      <td>215.598</td>\n",
       "      <td>12.990</td>\n",
       "      <td>1.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3,656.250</td>\n",
       "      <td>2,363.000</td>\n",
       "      <td>14,106.000</td>\n",
       "      <td>1.375</td>\n",
       "      <td>2.053</td>\n",
       "      <td>7.659</td>\n",
       "      <td>77.900</td>\n",
       "      <td>3.511</td>\n",
       "      <td>0.284</td>\n",
       "      <td>4.804</td>\n",
       "      <td>105.604</td>\n",
       "      <td>23.345</td>\n",
       "      <td>59.771</td>\n",
       "      <td>94.264</td>\n",
       "      <td>50.165</td>\n",
       "      <td>67.959</td>\n",
       "      <td>88.419</td>\n",
       "      <td>133.390</td>\n",
       "      <td>86.120</td>\n",
       "      <td>37.880</td>\n",
       "      <td>377.297</td>\n",
       "      <td>11.053</td>\n",
       "      <td>176.314</td>\n",
       "      <td>789.752</td>\n",
       "      <td>1.511</td>\n",
       "      <td>163.251</td>\n",
       "      <td>667.742</td>\n",
       "      <td>258.543</td>\n",
       "      <td>4.764</td>\n",
       "      <td>5.011</td>\n",
       "      <td>2,936.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.960</td>\n",
       "      <td>4.848</td>\n",
       "      <td>168.145</td>\n",
       "      <td>373.866</td>\n",
       "      <td>11.706</td>\n",
       "      <td>287.151</td>\n",
       "      <td>188.092</td>\n",
       "      <td>48.988</td>\n",
       "      <td>118.084</td>\n",
       "      <td>770.608</td>\n",
       "      <td>7,272.828</td>\n",
       "      <td>167.831</td>\n",
       "      <td>722.602</td>\n",
       "      <td>238.477</td>\n",
       "      <td>175.413</td>\n",
       "      <td>692.426</td>\n",
       "      <td>4.196</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.144</td>\n",
       "      <td>8.990</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.118</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2,105.182</td>\n",
       "      <td>1.473</td>\n",
       "      <td>10,746.600</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.509</td>\n",
       "      <td>2.570</td>\n",
       "      <td>0.872</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.984</td>\n",
       "      <td>106.923</td>\n",
       "      <td>236.955</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.041</td>\n",
       "      <td>924.532</td>\n",
       "      <td>0.992</td>\n",
       "      <td>311.734</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.983</td>\n",
       "      <td>7.522</td>\n",
       "      <td>16.070</td>\n",
       "      <td>6.889</td>\n",
       "      <td>16.100</td>\n",
       "      <td>16.100</td>\n",
       "      <td>2.465</td>\n",
       "      <td>3.991</td>\n",
       "      <td>1.175</td>\n",
       "      <td>3.895</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.019</td>\n",
       "      <td>2.472</td>\n",
       "      <td>1,020.994</td>\n",
       "      <td>64.129</td>\n",
       "      <td>994.000</td>\n",
       "      <td>295.800</td>\n",
       "      <td>334.700</td>\n",
       "      <td>141.800</td>\n",
       "      <td>1,770.691</td>\n",
       "      <td>9,998.894</td>\n",
       "      <td>103.390</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.978</td>\n",
       "      <td>742.942</td>\n",
       "      <td>22.318</td>\n",
       "      <td>536.564</td>\n",
       "      <td>924.378</td>\n",
       "      <td>0.239</td>\n",
       "      <td>191.548</td>\n",
       "      <td>12.710</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.288</td>\n",
       "      <td>2,505.300</td>\n",
       "      <td>7,791.000</td>\n",
       "      <td>4,099.000</td>\n",
       "      <td>37,943.000</td>\n",
       "      <td>36,871.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.817</td>\n",
       "      <td>3.286</td>\n",
       "      <td>18.900</td>\n",
       "      <td>16.300</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.143</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.858</td>\n",
       "      <td>48.670</td>\n",
       "      <td>3.573</td>\n",
       "      <td>55.000</td>\n",
       "      <td>72.947</td>\n",
       "      <td>3.228</td>\n",
       "      <td>267.910</td>\n",
       "      <td>307.930</td>\n",
       "      <td>191.830</td>\n",
       "      <td>4.838</td>\n",
       "      <td>396.110</td>\n",
       "      <td>252.870</td>\n",
       "      <td>10.017</td>\n",
       "      <td>390.120</td>\n",
       "      <td>199.620</td>\n",
       "      <td>126.530</td>\n",
       "      <td>490.561</td>\n",
       "      <td>500.349</td>\n",
       "      <td>9,998.448</td>\n",
       "      <td>320.050</td>\n",
       "      <td>2.000</td>\n",
       "      <td>457.650</td>\n",
       "      <td>172.349</td>\n",
       "      <td>46.150</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.594</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.674</td>\n",
       "      <td>8.802</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1,768.880</td>\n",
       "      <td>1.436</td>\n",
       "      <td>2,887.200</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.984</td>\n",
       "      <td>99.902</td>\n",
       "      <td>237.184</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1,119.704</td>\n",
       "      <td>0.991</td>\n",
       "      <td>2,549.988</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.158</td>\n",
       "      <td>40.855</td>\n",
       "      <td>10.153</td>\n",
       "      <td>158.526</td>\n",
       "      <td>132.648</td>\n",
       "      <td>122.117</td>\n",
       "      <td>43.574</td>\n",
       "      <td>659.170</td>\n",
       "      <td>3,332.596</td>\n",
       "      <td>32.171</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.308</td>\n",
       "      <td>232.805</td>\n",
       "      <td>6.870</td>\n",
       "      <td>207.016</td>\n",
       "      <td>292.227</td>\n",
       "      <td>0.075</td>\n",
       "      <td>59.519</td>\n",
       "      <td>4.420</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.083</td>\n",
       "      <td>879.226</td>\n",
       "      <td>3,933.755</td>\n",
       "      <td>2,004.878</td>\n",
       "      <td>15,559.952</td>\n",
       "      <td>18,520.468</td>\n",
       "      <td>0.526</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.812</td>\n",
       "      <td>5.711</td>\n",
       "      <td>5.155</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.755</td>\n",
       "      <td>13.096</td>\n",
       "      <td>1.003</td>\n",
       "      <td>15.893</td>\n",
       "      <td>20.046</td>\n",
       "      <td>0.947</td>\n",
       "      <td>79.151</td>\n",
       "      <td>89.192</td>\n",
       "      <td>51.868</td>\n",
       "      <td>1.096</td>\n",
       "      <td>174.894</td>\n",
       "      <td>90.516</td>\n",
       "      <td>3.413</td>\n",
       "      <td>172.712</td>\n",
       "      <td>214.863</td>\n",
       "      <td>38.900</td>\n",
       "      <td>196.688</td>\n",
       "      <td>197.499</td>\n",
       "      <td>5,043.879</td>\n",
       "      <td>97.709</td>\n",
       "      <td>0.447</td>\n",
       "      <td>156.336</td>\n",
       "      <td>59.324</td>\n",
       "      <td>257.011</td>\n",
       "      <td>187.759</td>\n",
       "      <td>13.915</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.279</td>\n",
       "      <td>2.835</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.409</td>\n",
       "      <td>547.172</td>\n",
       "      <td>0.416</td>\n",
       "      <td>911.857</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.627</td>\n",
       "      <td>30.998</td>\n",
       "      <td>74.844</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.131</td>\n",
       "      <td>348.829</td>\n",
       "      <td>0.313</td>\n",
       "      <td>805.394</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.051</td>\n",
       "      <td>14.728</td>\n",
       "      <td>3.313</td>\n",
       "      <td>44.310</td>\n",
       "      <td>9.576</td>\n",
       "      <td>13.807</td>\n",
       "      <td>6.215</td>\n",
       "      <td>128.282</td>\n",
       "      <td>899.119</td>\n",
       "      <td>116.862</td>\n",
       "      <td>9.690</td>\n",
       "      <td>39.038</td>\n",
       "      <td>999.316</td>\n",
       "      <td>998.681</td>\n",
       "      <td>111.496</td>\n",
       "      <td>273.095</td>\n",
       "      <td>424.215</td>\n",
       "      <td>103.181</td>\n",
       "      <td>898.609</td>\n",
       "      <td>24.990</td>\n",
       "      <td>113.223</td>\n",
       "      <td>118.753</td>\n",
       "      <td>186.616</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>994.286</td>\n",
       "      <td>995.745</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>26.397</td>\n",
       "      <td>851.613</td>\n",
       "      <td>657.762</td>\n",
       "      <td>33.058</td>\n",
       "      <td>1.267</td>\n",
       "      <td>5.132</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.109</td>\n",
       "      <td>1.764</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.475</td>\n",
       "      <td>13.978</td>\n",
       "      <td>34.490</td>\n",
       "      <td>42.070</td>\n",
       "      <td>10.184</td>\n",
       "      <td>232.126</td>\n",
       "      <td>164.109</td>\n",
       "      <td>47.777</td>\n",
       "      <td>149.385</td>\n",
       "      <td>109.007</td>\n",
       "      <td>999.877</td>\n",
       "      <td>77.801</td>\n",
       "      <td>87.135</td>\n",
       "      <td>212.656</td>\n",
       "      <td>492.772</td>\n",
       "      <td>358.950</td>\n",
       "      <td>415.435</td>\n",
       "      <td>79.116</td>\n",
       "      <td>274.887</td>\n",
       "      <td>289.826</td>\n",
       "      <td>200.000</td>\n",
       "      <td>63.334</td>\n",
       "      <td>221.975</td>\n",
       "      <td>999.413</td>\n",
       "      <td>989.474</td>\n",
       "      <td>996.859</td>\n",
       "      <td>994.000</td>\n",
       "      <td>999.491</td>\n",
       "      <td>995.745</td>\n",
       "      <td>997.519</td>\n",
       "      <td>994.004</td>\n",
       "      <td>142.844</td>\n",
       "      <td>12.770</td>\n",
       "      <td>21.044</td>\n",
       "      <td>7.648</td>\n",
       "      <td>127.573</td>\n",
       "      <td>107.693</td>\n",
       "      <td>219.644</td>\n",
       "      <td>37.153</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>999.234</td>\n",
       "      <td>451.485</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>252.860</td>\n",
       "      <td>113.276</td>\n",
       "      <td>111.350</td>\n",
       "      <td>184.349</td>\n",
       "      <td>111.737</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>137.984</td>\n",
       "      <td>111.333</td>\n",
       "      <td>818.000</td>\n",
       "      <td>80.041</td>\n",
       "      <td>8.204</td>\n",
       "      <td>14.448</td>\n",
       "      <td>6.580</td>\n",
       "      <td>4.082</td>\n",
       "      <td>25.779</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21.044</td>\n",
       "      <td>3.979</td>\n",
       "      <td>421.702</td>\n",
       "      <td>83.720</td>\n",
       "      <td>7.066</td>\n",
       "      <td>131.680</td>\n",
       "      <td>39.330</td>\n",
       "      <td>2.718</td>\n",
       "      <td>56.930</td>\n",
       "      <td>17.478</td>\n",
       "      <td>249.189</td>\n",
       "      <td>35.320</td>\n",
       "      <td>54.292</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.446</td>\n",
       "      <td>101.115</td>\n",
       "      <td>311.404</td>\n",
       "      <td>1.299</td>\n",
       "      <td>32.580</td>\n",
       "      <td>0.689</td>\n",
       "      <td>14.014</td>\n",
       "      <td>0.293</td>\n",
       "      <td>12.746</td>\n",
       "      <td>84.802</td>\n",
       "      <td>589.508</td>\n",
       "      <td>2.740</td>\n",
       "      <td>454.560</td>\n",
       "      <td>2.197</td>\n",
       "      <td>170.020</td>\n",
       "      <td>0.550</td>\n",
       "      <td>89.262</td>\n",
       "      <td>96.960</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.104</td>\n",
       "      <td>99.303</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1          2          3          4          5          7  \\\n",
       "count  1,457.000  1,457.000  1,449.000  1,449.000  1,449.000  1,449.000   \n",
       "mean   3,014.947  2,495.906  2,200.555  1,399.290      4.405    101.086   \n",
       "std       72.462     81.033     29.392    447.985     58.339      6.317   \n",
       "min    2,743.240  2,158.750  2,060.660      0.000      0.681     82.131   \n",
       "25%    2,968.780  2,452.150  2,181.189  1,081.876      1.016     97.778   \n",
       "50%    3,012.680  2,499.150  2,201.067  1,285.214      1.317    101.512   \n",
       "75%    3,057.030  2,539.460  2,218.055  1,591.224      1.529    104.587   \n",
       "max    3,356.350  2,846.440  2,315.267  3,715.042  1,114.537    129.252   \n",
       "\n",
       "               8          9         10         11         12         13  \\\n",
       "count  1,454.000  1,461.000  1,461.000  1,461.000  1,461.000  1,461.000   \n",
       "mean       0.122      1.462     -0.001      0.000      0.964    199.962   \n",
       "std        0.009      0.075      0.015      0.009      0.013      3.300   \n",
       "min        0.000      1.191     -0.053     -0.035      0.655    182.094   \n",
       "25%        0.121      1.410     -0.011     -0.006      0.958    198.124   \n",
       "50%        0.122      1.461     -0.001      0.000      0.966    199.509   \n",
       "75%        0.124      1.517      0.009      0.006      0.971    202.035   \n",
       "max        0.129      1.656      0.075      0.053      0.985    272.045   \n",
       "\n",
       "              15         16         17         18         19         20  \\\n",
       "count  1,460.000  1,460.000  1,460.000  1,460.000  1,460.000  1,454.000   \n",
       "mean       9.057    413.099      9.906      0.971    190.054     12.479   \n",
       "std        2.794     17.711      2.484      0.012      2.796      0.225   \n",
       "min        2.293    333.449      4.470      0.579    169.177      9.877   \n",
       "25%        7.175    405.977      9.565      0.969    188.284     12.460   \n",
       "50%        8.982    412.204      9.847      0.973    189.671     12.500   \n",
       "75%       10.887    419.117     10.123      0.977    192.241     12.547   \n",
       "max       19.547    824.927    102.868      0.985    215.598     12.990   \n",
       "\n",
       "              21          22         23          24           25         26  \\\n",
       "count  1,463.000   1,462.000  1,462.000   1,462.000    1,462.000  1,462.000   \n",
       "mean       1.405  -5,636.438  2,705.158  -3,811.036     -284.442      1.206   \n",
       "std        0.017     593.799    279.780   1,375.644    2,936.247      0.171   \n",
       "min        1.180  -7,148.000      0.000  -9,986.750  -14,804.500      0.000   \n",
       "25%        1.397  -5,971.250  2,578.250  -4,371.188   -1,473.583      1.093   \n",
       "50%        1.406  -5,529.000  2,665.250  -3,821.000      -83.125      1.283   \n",
       "75%        1.415  -5,361.562  2,851.750  -3,368.312    1,381.062      1.304   \n",
       "max        1.453       0.000  3,656.250   2,363.000   14,106.000      1.375   \n",
       "\n",
       "              27         28         29         30         31         32  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       1.943      6.648     69.598      2.364      0.184      3.680   \n",
       "std        0.173      1.207      3.453      0.410      0.033      0.541   \n",
       "min        0.000      0.000     59.400      0.667      0.034      2.070   \n",
       "25%        1.906      5.258     67.425      2.089      0.162      3.362   \n",
       "50%        1.986      7.265     69.244      2.378      0.187      3.431   \n",
       "75%        2.003      7.329     72.331      2.653      0.207      3.539   \n",
       "max        2.053      7.659     77.900      3.511      0.284      4.804   \n",
       "\n",
       "              33         34         35         36         37         38  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean      85.309      8.931     50.582     64.564     49.418     66.218   \n",
       "std        1.939      1.227      1.179      2.621      1.179      0.305   \n",
       "min       83.183      7.603     49.835     63.677     40.229     64.919   \n",
       "25%       84.478      8.577     50.254     64.025     49.420     66.038   \n",
       "50%       85.124      8.766     50.400     64.167     49.600     66.229   \n",
       "75%       85.742      9.043     50.580     64.352     49.746     66.340   \n",
       "max      105.604     23.345     59.771     94.264     50.165     67.959   \n",
       "\n",
       "              39         40         41         42         44         45  \\\n",
       "count  1,462.000  1,462.000  1,439.000  1,439.000  1,462.000  1,462.000   \n",
       "mean      86.830    118.681     68.240      3.351    355.573     10.032   \n",
       "std        0.424      1.822     23.847      2.426      6.270      0.176   \n",
       "min       85.141    111.713      1.434     -0.076    342.755      9.464   \n",
       "25%       86.578    118.012     75.340      2.694    350.795      9.929   \n",
       "50%       86.820    118.408     78.330      3.074    353.634     10.037   \n",
       "75%       87.002    118.945     80.265      3.483    360.843     10.154   \n",
       "max       88.419    133.390     86.120     37.880    377.297     11.053   \n",
       "\n",
       "              46         47         48         49         51         52  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean     136.726    733.703      1.177    139.991    632.289    157.323   \n",
       "std        7.810     12.214      0.190      4.511      8.709     60.659   \n",
       "min      108.846    699.814      0.497    125.798    607.393     40.787   \n",
       "25%      130.739    724.463      0.983    136.940    625.857    115.636   \n",
       "50%      136.377    733.577      1.250    140.037    631.395    182.280   \n",
       "75%      142.107    741.460      1.340    143.232    638.337    206.961   \n",
       "max      176.314    789.752      1.511    163.251    667.742    258.543   \n",
       "\n",
       "              54         55         56         57         58         59  \\\n",
       "count  1,460.000  1,460.000  1,460.000  1,460.000  1,460.000  1,460.000   \n",
       "mean       4.593      4.838  2,856.234      0.929      0.949      4.592   \n",
       "std        0.056      0.061     25.402      0.007      0.004      0.086   \n",
       "min        3.706      3.932  2,801.000      0.875      0.932      4.220   \n",
       "25%        4.574      4.816  2,837.000      0.925      0.947      4.532   \n",
       "50%        4.596      4.843  2,854.000      0.931      0.949      4.570   \n",
       "75%        4.617      4.869  2,874.000      0.933      0.952      4.669   \n",
       "max        4.764      5.011  2,936.000      0.938      0.960      4.848   \n",
       "\n",
       "              60         61         62         63         64         65  \\\n",
       "count  1,456.000  1,457.000  1,457.000  1,457.000  1,456.000  1,456.000   \n",
       "mean       2.563    355.189     10.422    116.546     13.876     20.440   \n",
       "std        9.313      6.055      0.269      8.503      7.068      4.729   \n",
       "min      -28.988    324.714      9.461     81.490      1.659      8.514   \n",
       "25%       -1.970    350.693     10.285    112.109     10.349     17.402   \n",
       "50%        0.722    353.799     10.437    116.216     13.226     20.012   \n",
       "75%        3.898    359.685     10.588    120.926     16.243     22.664   \n",
       "max      168.145    373.866     11.706    287.151    188.092     48.988   \n",
       "\n",
       "              66         67         68         69         71         72  \\\n",
       "count  1,456.000  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000   \n",
       "mean      27.027    706.790     13.468    147.497    619.234    104.346   \n",
       "std        6.951     11.588    271.321      3.970      9.508     31.197   \n",
       "min        4.308    632.423      0.414    106.841    581.777     21.433   \n",
       "25%       23.156    698.797      0.890    145.266    612.820     87.717   \n",
       "50%       26.217    706.752      0.976    147.636    619.099    102.651   \n",
       "75%       29.785    714.633      1.064    149.968    625.270    115.767   \n",
       "max      118.084    770.608  7,272.828    167.831    722.602    238.477   \n",
       "\n",
       "            73       74         75         76         77         78  \\\n",
       "count  705.000  705.000  1,457.000  1,439.000  1,439.000  1,439.000   \n",
       "mean   150.316  468.136      0.003     -0.007     -0.029     -0.007   \n",
       "std     18.924   18.396      0.110      0.021      0.033      0.031   \n",
       "min    -59.478  456.045      0.000     -0.105     -0.148     -0.105   \n",
       "25%    145.326  464.391      0.000     -0.019     -0.052     -0.029   \n",
       "50%    152.379  466.070      0.000     -0.006     -0.028     -0.010   \n",
       "75%    158.073  467.888      0.000      0.007     -0.006      0.009   \n",
       "max    175.413  692.426      4.196      0.117      0.072      0.133   \n",
       "\n",
       "              79         80         81         82         83         84  \\\n",
       "count  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000  1,462.000   \n",
       "mean      -0.013      0.003     -0.018     -0.021      0.006      7.455   \n",
       "std        0.048      0.023      0.049      0.017      0.036      0.514   \n",
       "min       -0.348     -0.057     -0.144     -0.098     -0.213      5.826   \n",
       "25%       -0.046     -0.011     -0.040     -0.027     -0.017      7.107   \n",
       "50%       -0.012      0.000     -0.009     -0.020      0.008      7.479   \n",
       "75%        0.013      0.012      0.009     -0.012      0.027      7.809   \n",
       "max        0.249      0.101      0.119      0.058      0.144      8.990   \n",
       "\n",
       "              85       86         87         88         89         90  \\\n",
       "count  1,451.000  215.000  1,463.000  1,463.000  1,463.000  1,413.000   \n",
       "mean       0.133    0.113      2.402      0.983  1,807.432      0.188   \n",
       "std        0.005    0.003      0.038      0.012     53.509      0.043   \n",
       "min        0.117    0.105      2.243      0.775  1,627.471      0.111   \n",
       "25%        0.130    0.111      2.377      0.976  1,777.200      0.170   \n",
       "50%        0.133    0.114      2.404      0.988  1,809.249      0.190   \n",
       "75%        0.136    0.115      2.429      0.990  1,841.789      0.201   \n",
       "max        0.150    0.118      2.555      0.994  2,105.182      1.473   \n",
       "\n",
       "               91         92         93         94         95         96  \\\n",
       "count   1,413.000  1,457.000  1,461.000  1,461.000  1,457.000  1,457.000   \n",
       "mean    8,833.290      0.002      0.001     -0.001     -0.000      0.000   \n",
       "std       401.114      0.089      0.003      0.003      0.000      0.000   \n",
       "min     7,397.310     -0.357     -0.013     -0.017     -0.002     -0.001   \n",
       "25%     8,564.910     -0.044     -0.001     -0.002     -0.000      0.000   \n",
       "50%     8,835.570      0.000      0.000     -0.000      0.000      0.000   \n",
       "75%     9,074.490      0.050      0.002      0.001      0.000      0.000   \n",
       "max    10,746.600      0.363      0.028      0.013      0.001      0.001   \n",
       "\n",
       "              97         99        100        101        102        103  \\\n",
       "count  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000   \n",
       "mean       0.018     -0.016      0.001     -0.000     -0.000      0.002   \n",
       "std        0.223      0.434      0.058      0.000      0.000      0.062   \n",
       "min       -1.480     -5.272     -0.528     -0.003     -0.002     -0.535   \n",
       "25%       -0.090     -0.215     -0.030     -0.000     -0.000     -0.035   \n",
       "50%        0.004      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.121      0.190      0.030      0.000      0.000      0.034   \n",
       "max        2.509      2.570      0.872      0.002      0.002      0.263   \n",
       "\n",
       "             104        105        106        107        108        109  \\\n",
       "count  1,461.000  1,461.000  1,457.000  1,457.000  1,457.000  1,457.000   \n",
       "mean      -0.010     -0.000     -0.000      0.001     -0.001     -0.011   \n",
       "std        0.003      0.001      0.003      0.003      0.087      0.087   \n",
       "min       -0.033     -0.012     -0.028     -0.013     -0.402     -0.345   \n",
       "25%       -0.012     -0.000     -0.002     -0.001     -0.048     -0.065   \n",
       "50%       -0.010      0.000     -0.000      0.000      0.000     -0.011   \n",
       "75%       -0.008      0.000      0.001      0.002      0.049      0.038   \n",
       "max        0.020      0.007      0.013      0.017      0.486      0.394   \n",
       "\n",
       "           110      111      112      113        114        115        116  \\\n",
       "count  519.000  519.000  519.000  781.000  1,463.000  1,463.000  1,463.000   \n",
       "mean     0.980  101.294  231.877    0.457      0.945      0.000    747.953   \n",
       "std      0.009    1.906    2.130    0.051      0.012      0.001     49.062   \n",
       "min      0.785   88.194  213.008    0.000      0.853      0.000    544.025   \n",
       "25%      0.979  100.348  230.413    0.459      0.939      0.000    721.023   \n",
       "50%      0.981  101.477  231.219    0.463      0.946      0.000    751.375   \n",
       "75%      0.982  102.074  233.191    0.466      0.952      0.000    777.209   \n",
       "max      0.984  106.923  236.955    0.488      0.976      0.041    924.532   \n",
       "\n",
       "             117        118        119        120        121        122  \\\n",
       "count  1,463.000  1,463.000  1,439.000  1,463.000  1,463.000  1,454.000   \n",
       "mean       0.987     58.655      0.598      0.971      6.312     15.794   \n",
       "std        0.010      6.704      0.008      0.009      0.124      0.100   \n",
       "min        0.890     52.807      0.527      0.841      5.126     15.460   \n",
       "25%        0.990     57.995      0.594      0.965      6.248     15.730   \n",
       "50%        0.991     58.550      0.599      0.969      6.315     15.790   \n",
       "75%        0.991     59.145      0.603      0.978      6.376     15.860   \n",
       "max        0.992    311.734      0.625      0.983      7.522     16.070   \n",
       "\n",
       "             123        124        125        126        127        128  \\\n",
       "count  1,454.000  1,454.000  1,454.000  1,454.000  1,454.000  1,454.000   \n",
       "mean       3.917     15.828     15.792      1.191      2.747      0.651   \n",
       "std        0.913      0.108      0.114      0.283      0.252      0.137   \n",
       "min        1.671     15.170     15.430      0.312      2.340      0.316   \n",
       "25%        3.208     15.760     15.720      0.974      2.564      0.549   \n",
       "50%        3.882     15.830     15.770      1.144      2.735      0.655   \n",
       "75%        4.406     15.900     15.870      1.343      2.872      0.723   \n",
       "max        6.889     16.100     16.100      2.465      3.991      1.175   \n",
       "\n",
       "             129        130        131        132        133        134  \\\n",
       "count  1,454.000  1,454.000  1,454.000  1,454.000  1,455.000  1,455.000   \n",
       "mean       3.190     -0.588      0.743      0.998      2.319  1,003.925   \n",
       "std        0.269      1.237      0.084      0.002      0.053      6.572   \n",
       "min        0.000     -3.779      0.420      0.994      2.191    980.451   \n",
       "25%        3.074     -0.946      0.685      0.996      2.277    999.939   \n",
       "50%        3.195     -0.142      0.759      0.998      2.312  1,003.971   \n",
       "75%        3.310      0.047      0.814      0.999      2.357  1,008.614   \n",
       "max        3.895      2.458      0.888      1.019      2.472  1,020.994   \n",
       "\n",
       "             135        136        137        138        139        140  \\\n",
       "count  1,455.000  1,458.000  1,457.000  1,457.000  1,449.000  1,449.000   \n",
       "mean      39.416    118.382    138.032    122.278     57.446    417.104   \n",
       "std        3.014     57.391     53.943     52.564     12.266    262.539   \n",
       "min       33.366     58.000     36.100     19.200     19.800      0.000   \n",
       "25%       37.369     92.000     89.800     80.500     50.700    241.589   \n",
       "50%       38.992    109.000    135.000    116.600     55.900    340.682   \n",
       "75%       40.849    128.000    180.900    161.600     62.900    502.206   \n",
       "max       64.129    994.000    295.800    334.700    141.800  1,770.691   \n",
       "\n",
       "             141        143        144        145        146        147  \\\n",
       "count  1,449.000  1,449.000  1,454.000  1,461.000  1,461.000  1,461.000   \n",
       "mean      27.927      6.656      0.004      0.121      0.064      0.055   \n",
       "std      524.762      3.636      0.001      0.061      0.027      0.022   \n",
       "min        0.032      1.740      0.000      0.032      0.021      0.023   \n",
       "25%        0.130      5.110      0.003      0.084      0.048      0.043   \n",
       "50%        0.236      6.260      0.004      0.108      0.059      0.050   \n",
       "75%        0.438      7.510      0.005      0.134      0.072      0.062   \n",
       "max    9,998.894    103.390      0.012      0.625      0.251      0.248   \n",
       "\n",
       "             148        149        151        152        153        154  \\\n",
       "count  1,461.000  1,461.000  1,460.000  1,460.000  1,460.000  1,460.000   \n",
       "mean       0.017      8.499      6.825     14.233      1.225      0.012   \n",
       "std        0.028     19.379      3.246     32.059     24.181      0.009   \n",
       "min        0.004      1.421      1.337      2.020      0.161      0.004   \n",
       "25%        0.010      6.402      4.453      8.103      0.372      0.007   \n",
       "50%        0.016      7.915      5.955     10.974      0.466      0.011   \n",
       "75%        0.021      9.565      8.299     14.394      0.660      0.015   \n",
       "max        0.978    742.942     22.318    536.564    924.378      0.239   \n",
       "\n",
       "             155        156        157      158        159        160  \\\n",
       "count  1,460.000  1,454.000  1,463.000  130.000    130.000  1,462.000   \n",
       "mean       7.696      0.515      0.058    0.048  1,027.457    862.276   \n",
       "std        5.367      1.160      0.081    0.040    409.358    940.047   \n",
       "min        1.244      0.140      0.011    0.012    234.100      0.000   \n",
       "25%        5.961      0.240      0.036    0.027    717.075    408.500   \n",
       "50%        7.485      0.310      0.049    0.035    995.700    615.000   \n",
       "75%        9.048      0.438      0.067    0.050  1,248.000    959.750   \n",
       "max      191.548     12.710      2.202    0.288  2,505.300  7,791.000   \n",
       "\n",
       "             161         162         163        164        165        166  \\\n",
       "count  1,462.000   1,462.000   1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean     541.605   4,088.895   4,807.294      0.137      0.122      0.243   \n",
       "std      549.479   4,293.509   6,611.570      0.114      0.223      0.374   \n",
       "min        0.000       0.000       0.000      0.000      0.000      0.000   \n",
       "25%      291.000   1,313.500     440.250      0.091      0.068      0.132   \n",
       "50%      433.000   2,595.000   1,780.000      0.119      0.089      0.183   \n",
       "75%      622.000   5,063.000   6,273.250      0.153      0.116      0.254   \n",
       "max    4,099.000  37,943.000  36,871.000      0.957      1.817      3.286   \n",
       "\n",
       "             167        168        169        170        171        172  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       2.773      1.232      0.124      0.401      0.683      0.120   \n",
       "std        1.008      0.617      0.047      0.198      0.156      0.061   \n",
       "min        0.800      0.300      0.033      0.046      0.298      0.009   \n",
       "25%        2.100      0.900      0.090      0.229      0.576      0.080   \n",
       "50%        2.600      1.200      0.119      0.413      0.686      0.113   \n",
       "75%        3.200      1.500      0.151      0.536      0.795      0.140   \n",
       "max       18.900     16.300      0.725      1.143      1.153      0.494   \n",
       "\n",
       "             173        174        175        176        177        178  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.319      0.576      0.319      0.777      0.245      0.395   \n",
       "std        0.072      0.096      0.072      0.117      0.075      0.283   \n",
       "min        0.129      0.254      0.129      0.462      0.073      0.047   \n",
       "25%        0.273      0.517      0.273      0.692      0.196      0.222   \n",
       "50%        0.324      0.579      0.324      0.766      0.243      0.299   \n",
       "75%        0.369      0.635      0.369      0.839      0.294      0.423   \n",
       "max        0.548      0.864      0.548      1.172      0.441      1.858   \n",
       "\n",
       "             181        182        183        184        185        186  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean      19.083      0.549     10.815     26.560      0.145      7.383   \n",
       "std        3.336      0.226      4.230      6.871      0.113      7.410   \n",
       "min        9.400      0.093      3.170      5.014      0.030      1.940   \n",
       "25%       16.900      0.380      7.723     21.063      0.102      5.370   \n",
       "50%       18.760      0.527     10.230     26.870      0.133      6.750   \n",
       "75%       21.078      0.691     13.395     31.575      0.169      8.457   \n",
       "max       48.670      3.573     55.000     72.947      3.228    267.910   \n",
       "\n",
       "             188        189        196        197        198        199  \\\n",
       "count  1,462.000  1,462.000  1,460.000  1,456.000  1,457.000  1,457.000   \n",
       "mean      17.923     42.938      0.283      8.464     19.935      0.552   \n",
       "std        8.842     21.684      0.374     14.281      9.932      0.503   \n",
       "min        6.220      6.613      0.080      1.750      9.220      0.090   \n",
       "25%       14.480     24.356      0.215      5.037     17.070      0.296   \n",
       "50%       17.800     39.886      0.258      6.770     19.350      0.421   \n",
       "75%       20.848     57.675      0.296      9.572     21.440      0.722   \n",
       "max      307.930    191.830      4.838    396.110    252.870     10.017   \n",
       "\n",
       "             200        201        202        203        204        205  \\\n",
       "count  1,457.000  1,456.000  1,456.000  1,456.000  1,457.000  1,457.000   \n",
       "mean      11.304     17.459      7.800     10.118     29.861     27.642   \n",
       "std       15.084      8.145      5.033     14.762     16.663    523.304   \n",
       "min        2.770      3.560      0.000      0.000      7.728      0.043   \n",
       "25%        6.730     14.070      5.018      6.057     24.730      0.114   \n",
       "50%        8.560     17.145      6.755      8.454     30.085      0.158   \n",
       "75%       11.460     20.093      9.490     11.930     33.376      0.229   \n",
       "max      390.120    199.620    126.530    490.561    500.349  9,998.448   \n",
       "\n",
       "             206        207        208        209        210        211  \\\n",
       "count  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000  1,439.000   \n",
       "mean       8.885      0.001     20.211     73.435      0.032      0.088   \n",
       "std       11.291      0.052     17.005     28.119      1.209      0.039   \n",
       "min        2.300      0.000      4.010      5.359      0.000      0.032   \n",
       "25%        6.000      0.000     16.260     56.224      0.000      0.065   \n",
       "50%        7.660      0.000     19.660     73.405      0.000      0.079   \n",
       "75%        9.790      0.000     22.340     90.545      0.000      0.098   \n",
       "max      320.050      2.000    457.650    172.349     46.150      0.459   \n",
       "\n",
       "             212        213        214        215        216        217  \\\n",
       "count  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000   \n",
       "mean       0.056      0.051      0.060      0.084      0.081      0.083   \n",
       "std        0.024      0.032      0.053      0.058      0.031      0.026   \n",
       "min        0.002      0.007      0.004      0.019      0.006      0.010   \n",
       "25%        0.044      0.033      0.036      0.057      0.063      0.070   \n",
       "50%        0.053      0.042      0.056      0.076      0.083      0.085   \n",
       "75%        0.064      0.062      0.073      0.094      0.098      0.097   \n",
       "max        0.323      0.594      1.284      0.761      0.343      0.283   \n",
       "\n",
       "             218        219        220      221        222        223  \\\n",
       "count  1,439.000  1,462.000  1,451.000  215.000  1,463.000  1,463.000   \n",
       "mean       0.071      3.776      0.003    0.009      0.061      0.008   \n",
       "std        0.047      1.170      0.002    0.002      0.023      0.052   \n",
       "min        0.008      1.034      0.001    0.006      0.020      0.000   \n",
       "25%        0.046      2.950      0.002    0.008      0.040      0.001   \n",
       "50%        0.062      3.624      0.003    0.009      0.061      0.002   \n",
       "75%        0.086      4.395      0.004    0.010      0.076      0.005   \n",
       "max        0.674      8.802      0.016    0.024      0.188      0.991   \n",
       "\n",
       "             224        225        226        228        229        239  \\\n",
       "count  1,463.000  1,413.000  1,413.000  1,461.000  1,461.000  1,461.000   \n",
       "mean     122.862      0.059  1,038.711      0.019      0.018      0.005   \n",
       "std       56.208      0.068    431.414      0.011      0.011      0.002   \n",
       "min       32.264      0.009    168.800      0.006      0.007      0.001   \n",
       "25%       95.040      0.030    715.200      0.013      0.013      0.004   \n",
       "50%      119.436      0.040    966.700      0.016      0.015      0.005   \n",
       "75%      144.503      0.062  1,266.000      0.021      0.020      0.006   \n",
       "max    1,768.880      1.436  2,887.200      0.154      0.213      0.024   \n",
       "\n",
       "             240      245      246      247      248        249        250  \\\n",
       "count  1,461.000  519.000  519.000  519.000  781.000  1,463.000  1,463.000   \n",
       "mean       0.005    0.006    1.753    4.168    0.052      0.025      0.001   \n",
       "std        0.001    0.087    4.451   10.329    0.067      0.050      0.014   \n",
       "min        0.001    0.000    0.291    1.102    0.000      0.003      0.000   \n",
       "25%        0.004    0.001    0.915    2.704    0.019      0.015      0.000   \n",
       "50%        0.004    0.002    1.192    3.669    0.026      0.021      0.000   \n",
       "75%        0.005    0.003    1.785    4.480    0.050      0.027      0.000   \n",
       "max        0.024    1.984   99.902  237.184    0.491      0.973      0.414   \n",
       "\n",
       "             251        252        253        254        255        256  \\\n",
       "count  1,463.000  1,463.000  1,463.000  1,439.000  1,463.000  1,463.000   \n",
       "mean     109.550      0.004      4.759      0.033      0.014      0.404   \n",
       "std       55.292      0.039     66.602      0.022      0.009      0.121   \n",
       "min       21.011      0.000      0.767      0.009      0.002      0.127   \n",
       "25%       75.501      0.001      2.200      0.024      0.005      0.307   \n",
       "50%      102.994      0.001      2.864      0.031      0.015      0.406   \n",
       "75%      131.758      0.001      3.804      0.038      0.021      0.483   \n",
       "max    1,119.704      0.991  2,549.988      0.452      0.079      0.925   \n",
       "\n",
       "             268        269        270        271        272        273  \\\n",
       "count  1,455.000  1,455.000  1,455.000  1,458.000  1,457.000  1,457.000   \n",
       "mean       0.071     19.491      3.772     29.324     45.928     41.152   \n",
       "std        0.029      7.332      1.149      8.304     17.844     17.838   \n",
       "min        0.020      6.098      1.302     15.547     10.402      6.943   \n",
       "25%        0.044     13.828      2.957     25.045     29.755     26.919   \n",
       "50%        0.071     17.977      3.704     28.859     45.512     39.585   \n",
       "75%        0.092     24.653      4.377     31.810     59.240     54.043   \n",
       "max        0.158     40.855     10.153    158.526    132.648    122.117   \n",
       "\n",
       "             274        275        276        278        279        280  \\\n",
       "count  1,449.000  1,449.000  1,449.000  1,449.000  1,454.000  1,461.000   \n",
       "mean      20.156    136.414      9.310      2.214      0.001      0.041   \n",
       "std        3.826     85.473    174.895      1.225      0.000      0.020   \n",
       "min        8.651      0.000      0.011      0.561      0.000      0.011   \n",
       "25%       18.246     80.992      0.044      1.698      0.001      0.029   \n",
       "50%       19.581    110.601      0.078      2.078      0.001      0.037   \n",
       "75%       22.135    163.085      0.145      2.514      0.001      0.046   \n",
       "max       43.574    659.170  3,332.596     32.171      0.003      0.188   \n",
       "\n",
       "             281        282        283        284        286        287  \\\n",
       "count  1,461.000  1,461.000  1,461.000  1,461.000  1,460.000  1,460.000   \n",
       "mean       0.018      0.015      0.006      2.814      2.123      4.315   \n",
       "std        0.007      0.006      0.009      6.065      0.965     10.098   \n",
       "min        0.007      0.007      0.002      0.505      0.461      0.728   \n",
       "25%        0.014      0.012      0.003      2.223      1.442      2.458   \n",
       "50%        0.017      0.014      0.005      2.665      1.870      3.360   \n",
       "75%        0.021      0.017      0.007      3.145      2.619      4.321   \n",
       "max        0.075      0.060      0.308    232.805      6.870    207.016   \n",
       "\n",
       "             288        289        290        291        292      293  \\\n",
       "count  1,460.000  1,460.000  1,460.000  1,454.000  1,463.000  130.000   \n",
       "mean       0.377      0.004      2.580      0.124      0.020    0.015   \n",
       "std        7.645      0.003      1.658      0.280      0.026    0.012   \n",
       "min        0.051      0.001      0.396      0.042      0.004    0.004   \n",
       "25%        0.115      0.002      2.098      0.065      0.013    0.009   \n",
       "50%        0.138      0.004      2.551      0.082      0.017    0.011   \n",
       "75%        0.192      0.005      3.025      0.117      0.024    0.015   \n",
       "max      292.227      0.075     59.519      4.420      0.692    0.083   \n",
       "\n",
       "           294        295        296         297         298        299  \\\n",
       "count  130.000  1,462.000  1,462.000   1,462.000   1,462.000  1,462.000   \n",
       "mean   331.655    391.418    246.059   1,887.538   2,346.192      0.062   \n",
       "std    138.110    453.682    269.843   1,998.921   3,252.894      0.060   \n",
       "min     82.323      0.000      0.000       0.000       0.000      0.000   \n",
       "25%    228.382    184.138    127.943     600.425     202.474      0.040   \n",
       "50%    310.995    275.147    194.766   1,194.193     819.642      0.052   \n",
       "75%    400.583    426.967    272.954   2,341.377   3,139.696      0.068   \n",
       "max    879.226  3,933.755  2,004.878  15,559.952  18,520.468      0.526   \n",
       "\n",
       "             300        301        302        303        304        305  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.057      0.113      0.906      0.402      0.040      0.132   \n",
       "std        0.120      0.201      0.310      0.192      0.014      0.065   \n",
       "min        0.000      0.000      0.310      0.112      0.011      0.014   \n",
       "25%        0.030      0.059      0.717      0.296      0.030      0.071   \n",
       "50%        0.040      0.083      0.859      0.380      0.039      0.138   \n",
       "75%        0.052      0.115      1.045      0.477      0.049      0.179   \n",
       "max        1.031      1.812      5.711      5.155      0.226      0.334   \n",
       "\n",
       "             306        307        308        309        310        311  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.265      0.049      0.129      0.218      0.129      0.304   \n",
       "std        0.057      0.026      0.028      0.034      0.028      0.044   \n",
       "min        0.117      0.003      0.055      0.091      0.055      0.181   \n",
       "25%        0.225      0.033      0.113      0.198      0.113      0.277   \n",
       "50%        0.264      0.045      0.130      0.219      0.130      0.301   \n",
       "75%        0.307      0.055      0.150      0.238      0.150      0.332   \n",
       "max        0.475      0.225      0.211      0.324      0.211      0.444   \n",
       "\n",
       "             312        313        317        318        319        320  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.097      0.160      6.001      0.173      3.200      7.885   \n",
       "std        0.029      0.117      1.024      0.073      1.232      2.189   \n",
       "min        0.033      0.022      2.788      0.028      0.985      1.657   \n",
       "25%        0.078      0.091      5.326      0.119      2.322      6.224   \n",
       "50%        0.098      0.122      5.862      0.164      2.904      8.356   \n",
       "75%        0.116      0.160      6.578      0.219      4.056      9.466   \n",
       "max        0.178      0.755     13.096      1.003     15.893     20.046   \n",
       "\n",
       "             321        322        324        325        332        333  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,460.000  1,456.000   \n",
       "mean       0.043      2.269      5.390     13.262      0.083      2.541   \n",
       "std        0.033      2.183      2.586      6.607      0.062      5.497   \n",
       "min        0.008      0.611      1.710      2.235      0.022      0.537   \n",
       "25%        0.031      1.667      4.256      7.481      0.069      1.547   \n",
       "50%        0.040      2.079      5.445     12.312      0.085      2.066   \n",
       "75%        0.050      2.638      6.345     17.925      0.096      2.788   \n",
       "max        0.947     79.151     89.192     51.868      1.096    174.894   \n",
       "\n",
       "             334        335        336        337        338        339  \\\n",
       "count  1,457.000  1,457.000  1,457.000  1,456.000  1,456.000  1,456.000   \n",
       "mean       6.172      0.166      3.374      9.656      2.303      3.027   \n",
       "std        3.290      0.161      5.642      7.391      1.456      5.750   \n",
       "min        2.837      0.028      0.790      5.215      0.000      0.000   \n",
       "25%        5.442      0.088      2.029      8.270      1.542      1.898   \n",
       "50%        5.969      0.129      2.509      9.060      2.055      2.546   \n",
       "75%        6.528      0.210      3.358     10.003      2.775      3.407   \n",
       "max       90.516      3.413    172.712    214.863     38.900    196.688   \n",
       "\n",
       "             340        341        342        343        344        345  \\\n",
       "count  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000  1,457.000   \n",
       "mean       9.280     12.255      2.683      0.000      6.154     23.272   \n",
       "std        6.040    236.663      3.558      0.012      5.322      8.916   \n",
       "min        2.200      0.013      0.574      0.000      1.256      2.056   \n",
       "25%        7.630      0.035      1.896      0.000      4.974     17.834   \n",
       "50%        9.458      0.046      2.360      0.000      5.998     23.306   \n",
       "75%       10.416      0.066      2.946      0.000      6.875     28.888   \n",
       "max      197.499  5,043.879     97.709      0.447    156.336     59.324   \n",
       "\n",
       "           346      347        348        349        350        351  \\\n",
       "count  705.000  705.000  1,457.000  1,439.000  1,439.000  1,439.000   \n",
       "mean     7.975    5.787      0.010      0.024      0.025      0.023   \n",
       "std     18.026   17.559      0.365      0.010      0.010      0.014   \n",
       "min      1.769    1.018      0.000      0.010      0.001      0.003   \n",
       "25%      4.429    2.514      0.000      0.018      0.020      0.015   \n",
       "50%      5.523    3.025      0.000      0.022      0.024      0.019   \n",
       "75%      6.816    3.970      0.000      0.027      0.029      0.028   \n",
       "max    257.011  187.759     13.915      0.125      0.134      0.291   \n",
       "\n",
       "             352        353        354        355        356        357  \\\n",
       "count  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000  1,462.000   \n",
       "mean       0.027      0.023      0.040      0.042      0.034      1.300   \n",
       "std        0.024      0.013      0.016      0.013      0.023      0.386   \n",
       "min        0.002      0.006      0.003      0.004      0.004      0.380   \n",
       "25%        0.016      0.016      0.030      0.035      0.021      1.028   \n",
       "50%        0.025      0.022      0.042      0.044      0.029      1.256   \n",
       "75%        0.034      0.027      0.050      0.050      0.042      1.533   \n",
       "max        0.619      0.143      0.153      0.134      0.279      2.835   \n",
       "\n",
       "             358      359        360        361        362        363  \\\n",
       "count  1,451.000  215.000  1,463.000  1,463.000  1,463.000  1,413.000   \n",
       "mean       0.001    0.002      0.020      0.003     39.959      0.018   \n",
       "std        0.001    0.000      0.007      0.018     17.367      0.021   \n",
       "min        0.000    0.002      0.008      0.000     10.720      0.003   \n",
       "25%        0.001    0.002      0.014      0.000     32.178      0.009   \n",
       "50%        0.001    0.002      0.020      0.001     39.743      0.013   \n",
       "75%        0.001    0.003      0.025      0.002     46.959      0.019   \n",
       "max        0.005    0.005      0.057      0.409    547.172      0.416   \n",
       "\n",
       "             364        366        367        368        369        377  \\\n",
       "count  1,413.000  1,461.000  1,461.000  1,457.000  1,457.000  1,461.000   \n",
       "mean     332.212      0.005      0.005      0.004      0.003      0.002   \n",
       "std      137.826      0.003      0.002      0.003      0.002      0.001   \n",
       "min       60.988      0.002      0.002      0.000      0.000      0.000   \n",
       "25%      227.448      0.004      0.004      0.003      0.002      0.001   \n",
       "50%      308.438      0.005      0.004      0.003      0.003      0.002   \n",
       "75%      410.783      0.006      0.005      0.004      0.004      0.002   \n",
       "max      911.857      0.037      0.039      0.036      0.033      0.008   \n",
       "\n",
       "             378      383      384      385      386        387        388  \\\n",
       "count  1,461.000  519.000  519.000  519.000  781.000  1,463.000  1,463.000   \n",
       "mean       0.002    0.002    0.549    1.294    0.011      0.008      0.000   \n",
       "std        0.000    0.028    1.377    3.258    0.015      0.016      0.004   \n",
       "min        0.000    0.000    0.087    0.338    0.000      0.001      0.000   \n",
       "25%        0.001    0.000    0.297    0.842    0.005      0.005      0.000   \n",
       "50%        0.002    0.001    0.380    1.106    0.007      0.007      0.000   \n",
       "75%        0.002    0.001    0.549    1.399    0.011      0.009      0.000   \n",
       "max        0.008    0.627   30.998   74.844    0.207      0.307      0.131   \n",
       "\n",
       "             389        390        391        392        393        394  \\\n",
       "count  1,463.000  1,463.000  1,463.000  1,439.000  1,463.000  1,463.000   \n",
       "mean      35.065      0.001      1.469      0.011      0.005      0.134   \n",
       "std       17.381      0.012     21.036      0.007      0.003      0.039   \n",
       "min        6.310      0.000      0.305      0.003      0.001      0.034   \n",
       "25%       24.187      0.000      0.672      0.008      0.002      0.104   \n",
       "50%       32.453      0.000      0.879      0.010      0.005      0.134   \n",
       "75%       42.498      0.000      1.152      0.013      0.007      0.161   \n",
       "max      348.829      0.313    805.394      0.138      0.023      0.299   \n",
       "\n",
       "             406        407        408        409        410        411  \\\n",
       "count  1,455.000  1,455.000  1,455.000  1,458.000  1,457.000  1,457.000   \n",
       "mean       0.024      6.713      1.230      5.360      4.574      4.913   \n",
       "std        0.011      2.808      0.365      2.570      1.777      2.135   \n",
       "min        0.006      2.054      0.424      2.738      1.216      0.734   \n",
       "25%        0.014      4.575      0.966      4.149      2.997      3.236   \n",
       "50%        0.024      5.920      1.240      4.950      4.495      4.697   \n",
       "75%        0.032      8.260      1.415      5.795      5.934      6.454   \n",
       "max        0.051     14.728      3.313     44.310      9.576     13.807   \n",
       "\n",
       "             412        413        414        416        417        418  \\\n",
       "count  1,449.000  1,449.000  1,449.000  1,449.000  1,454.000  1,461.000   \n",
       "mean       2.609     30.870     25.694      6.651      3.416      8.237   \n",
       "std        0.549     18.390     48.652      4.061      1.050      4.054   \n",
       "min        0.961      0.000      4.042      1.534      0.000      2.153   \n",
       "25%        2.315     18.470     11.456      4.927      2.677      5.786   \n",
       "50%        2.546     26.103     20.265      6.172      3.238      7.455   \n",
       "75%        2.849     38.140     29.307      7.704      4.022      9.231   \n",
       "max        6.215    128.282    899.119    116.862      9.690     39.038   \n",
       "\n",
       "             419        420        421        422        424        425  \\\n",
       "count  1,461.000  1,461.000  1,461.000  1,461.000  1,460.000  1,460.000   \n",
       "mean     322.575    307.824      1.816      4.184     77.244      3.354   \n",
       "std      288.053    326.851      3.154      7.144     32.447      6.539   \n",
       "min        0.000      0.000      0.441      0.722     23.020      0.487   \n",
       "25%        0.000      0.000      1.032      3.204     55.800      1.966   \n",
       "50%      306.534    266.396      1.628      3.943     69.555      2.660   \n",
       "75%      524.988    590.918      2.188      4.771     92.151      3.484   \n",
       "max      999.316    998.681    111.496    273.095    424.215    103.181   \n",
       "\n",
       "             426        427        428        429        430        431  \\\n",
       "count  1,460.000  1,460.000  1,460.000  1,454.000  1,463.000  1,462.000   \n",
       "mean       6.710      1.228      4.058      4.296      4.171     17.368   \n",
       "std       23.972      1.011      3.122     10.995      6.626     31.479   \n",
       "min        1.536      0.381      0.664      1.120      0.784      0.000   \n",
       "25%        3.745      0.748      3.122      1.931      2.563      6.925   \n",
       "50%        4.757      1.126      3.935      2.493      3.454     10.985   \n",
       "75%        6.605      1.518      4.767      3.492      4.755     17.365   \n",
       "max      898.609     24.990    113.223    118.753    186.616    400.000   \n",
       "\n",
       "             432        433        434        435        436        437  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean      21.191     99.219    202.658     13.719      8.376      6.533   \n",
       "std       31.836    127.057    223.287     29.211     29.350     29.507   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%       10.923     30.904      9.883      7.509      3.485      1.954   \n",
       "50%       16.205     57.648    150.074     10.150      4.527      2.762   \n",
       "75%       21.605    120.155    299.769     12.694      5.794      3.809   \n",
       "max      400.000    994.286    995.745    400.000    400.000    400.000   \n",
       "\n",
       "             438        439        440        441        442        443  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       3.987     54.560     70.851     11.539      0.800      1.348   \n",
       "std        1.419     33.256     38.991      6.195      0.183      0.663   \n",
       "min        1.157      0.000     14.121      1.097      0.351      0.097   \n",
       "25%        3.068     36.247     48.087      5.312      0.680      0.908   \n",
       "50%        3.771     49.002     65.442     12.118      0.806      1.266   \n",
       "75%        4.667     66.667     85.265     15.841      0.927      1.578   \n",
       "max       26.397    851.613    657.762     33.058      1.267      5.132   \n",
       "\n",
       "             444        445        446        447        448        449  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.633      0.895      0.646      1.173      0.282      0.332   \n",
       "std        0.145      0.157      0.142      0.177      0.087      0.236   \n",
       "min        0.217      0.334      0.309      0.697      0.085      0.040   \n",
       "25%        0.539      0.804      0.555      1.046      0.226      0.187   \n",
       "50%        0.642      0.903      0.650      1.157      0.280      0.251   \n",
       "75%        0.732      0.989      0.746      1.272      0.339      0.351   \n",
       "max        1.085      1.351      1.109      1.764      0.508      1.475   \n",
       "\n",
       "             453        454        455        456        457        458  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       5.366      5.486      7.911      3.623     12.324      5.274   \n",
       "std        0.926      2.269      3.112      0.943      8.269      4.670   \n",
       "min        2.671      0.904      2.329      0.695      3.049      1.443   \n",
       "25%        4.786      3.774      5.801      2.876      8.819      3.824   \n",
       "50%        5.289      5.253      7.435      3.694     11.448      4.796   \n",
       "75%        5.952      6.926      9.638      4.327     14.388      6.089   \n",
       "max       13.978     34.490     42.070     10.184    232.126    164.109   \n",
       "\n",
       "             460        461        468        469        470        471  \\\n",
       "count  1,462.000  1,462.000  1,460.000  1,456.000  1,457.000  1,457.000   \n",
       "mean       2.836     28.982      6.168    227.591      5.618      5.317   \n",
       "std        1.381     13.260      8.204    230.740      3.004      4.725   \n",
       "min        0.991      7.953      1.716      0.000      2.601      0.833   \n",
       "25%        2.286     19.936      4.609     40.727      4.830      2.812   \n",
       "50%        2.824     26.080      5.633    157.166      5.455      4.019   \n",
       "75%        3.304     35.163      6.373    343.291      5.999      6.952   \n",
       "max       47.777    149.385    109.007    999.877     77.801     87.135   \n",
       "\n",
       "             472        473        474        475        476        477  \\\n",
       "count  1,457.000  1,456.000  1,456.000  1,456.000  1,457.000  1,457.000   \n",
       "mean       9.477    137.873     39.370     37.539      4.232     20.018   \n",
       "std        9.235     47.527     22.335     24.635      2.506     14.710   \n",
       "min        2.403     11.500      0.000      0.000      1.101      0.000   \n",
       "25%        5.791    105.595     24.850     23.149      3.498     11.576   \n",
       "50%        7.375    138.329     34.314     32.651      4.275     15.928   \n",
       "75%        9.709    168.251     47.886     45.186      4.725     23.518   \n",
       "max      212.656    492.772    358.950    415.435     79.116    274.887   \n",
       "\n",
       "             478        479        480        481        483        484  \\\n",
       "count  1,457.000  1,457.000  1,457.000  1,457.000  1,439.000  1,439.000   \n",
       "mean       6.105      0.137      3.255     75.649    319.116    207.803   \n",
       "std        9.556      5.240      2.538     35.827    280.363    195.384   \n",
       "min        1.687      0.000      0.646      8.841      0.000      0.000   \n",
       "25%        4.073      0.000      2.612     53.012      0.000     80.848   \n",
       "50%        5.191      0.000      3.178     70.694    295.305    148.697   \n",
       "75%        6.584      0.000      3.622     92.934    513.855    263.520   \n",
       "max      289.826    200.000     63.334    221.975    999.413    989.474   \n",
       "\n",
       "             485        486        487        488        489        490  \\\n",
       "count  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000  1,439.000   \n",
       "mean     217.796    202.926    300.543    240.276    355.759    270.984   \n",
       "std      214.970    219.364    288.975    264.086    250.991    227.796   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%       77.342     50.865      0.000     53.298    152.912    111.800   \n",
       "50%      139.656    113.520    248.981    112.932    351.293    218.936   \n",
       "75%      296.700    294.679    501.875    400.870    510.874    377.798   \n",
       "max      996.859    994.000    999.491    995.745    997.519    994.004   \n",
       "\n",
       "             491        492      493        494        495        496  \\\n",
       "count  1,462.000  1,451.000  215.000  1,463.000  1,463.000  1,463.000   \n",
       "mean      51.386      2.438    8.189      2.525      0.894      6.811   \n",
       "std       18.012      1.231    1.776      0.958      5.985      3.328   \n",
       "min       13.723      0.556    4.888      0.833      0.034      1.772   \n",
       "25%       38.589      1.747    6.947      1.662      0.139      5.269   \n",
       "50%       48.490      2.247    8.021      2.537      0.231      6.608   \n",
       "75%       61.594      2.821    9.059      3.199      0.558      7.882   \n",
       "max      142.844     12.770   21.044      7.648    127.573    107.693   \n",
       "\n",
       "             497        498        500        501        511        512  \\\n",
       "count  1,413.000  1,413.000  1,461.000  1,461.000  1,461.000  1,461.000   \n",
       "mean      29.909     11.787    265.799    239.721     54.441    271.150   \n",
       "std       24.814      4.936    324.709    321.393     35.770    327.497   \n",
       "min        4.814      1.950      0.000      0.000      0.000      0.000   \n",
       "25%       16.351      8.113      0.000      0.000     35.153      0.000   \n",
       "50%       21.916     10.885      0.000      0.000     46.377      0.000   \n",
       "75%       32.431     14.468    540.418    506.083     63.059    542.857   \n",
       "max      219.644     37.153  1,000.000    999.234    451.485  1,000.000   \n",
       "\n",
       "           517      518      519      520        521        522        523  \\\n",
       "count  519.000  519.000  519.000  781.000  1,463.000  1,463.000  1,463.000   \n",
       "mean     0.705    1.764    1.815   11.393      2.684     10.601     14.703   \n",
       "std     11.091    5.023    4.849   15.923      5.816     98.808      7.175   \n",
       "min      0.029    0.288    0.467    0.000      0.312      0.000      2.681   \n",
       "25%      0.121    0.900    1.168    4.093      1.557      0.000     10.127   \n",
       "50%      0.174    1.167    1.586    5.633      2.221      0.000     13.646   \n",
       "75%      0.260    1.769    1.935   10.637      2.894      0.000     17.809   \n",
       "max    252.860  113.276  111.350  184.349    111.737  1,000.000    137.984   \n",
       "\n",
       "             524        525        526        527        528        540  \\\n",
       "count  1,463.000  1,463.000  1,439.000  1,463.000  1,463.000  1,455.000   \n",
       "mean       0.465      5.720      5.549      1.449      6.401      3.040   \n",
       "std        4.289     21.381      3.900      0.965      1.904      1.244   \n",
       "min        0.026      1.310      1.540      0.171      2.170      0.852   \n",
       "25%        0.074      3.755      4.087      0.486      4.855      1.894   \n",
       "50%        0.100      4.875      5.138      1.557      6.425      3.087   \n",
       "75%        0.134      6.472      6.329      2.218      7.616      3.952   \n",
       "max      111.333    818.000     80.041      8.204     14.448      6.580   \n",
       "\n",
       "             541        542        543        544        545        546  \\\n",
       "count  1,455.000  1,455.000  1,461.000  1,461.000  1,461.000  1,461.000   \n",
       "mean       1.942      9.592      0.111      0.008      0.003      7.628   \n",
       "std        0.731      2.889      0.003      0.002      0.000      1.342   \n",
       "min        0.614      3.276      0.105      0.005      0.002      4.429   \n",
       "25%        1.396      7.504      0.110      0.008      0.002      7.116   \n",
       "50%        1.786      9.459      0.110      0.008      0.003      7.116   \n",
       "75%        2.454     11.169      0.114      0.009      0.003      8.073   \n",
       "max        4.082     25.779      0.118      0.024      0.005     21.044   \n",
       "\n",
       "             547        548        549        550        551        552  \\\n",
       "count  1,221.000  1,221.000  1,221.000  1,221.000  1,221.000  1,221.000   \n",
       "mean       1.041    403.482     75.671      0.662     16.942      1.208   \n",
       "std        0.391      5.117      3.377      0.668      4.586      1.184   \n",
       "min        0.444    372.822     71.038      0.045      6.110      0.120   \n",
       "25%        0.801    400.684     73.254      0.224     14.530      0.870   \n",
       "50%        0.912    402.966     74.078      0.469     16.340      1.150   \n",
       "75%        1.286    407.436     78.274      0.853     18.940      1.370   \n",
       "max        3.979    421.702     83.720      7.066    131.680     39.330   \n",
       "\n",
       "             553        554        555        556        557        558  \\\n",
       "count  1,221.000  1,221.000  1,221.000  1,221.000  1,221.000  1,221.000   \n",
       "mean       0.276      7.673      0.493     57.517      4.200      1.591   \n",
       "std        0.275      2.046      0.523     35.006      1.187      1.617   \n",
       "min        0.019      2.786      0.052      4.827      1.497      0.165   \n",
       "25%        0.095      6.738      0.344     27.018      3.625      1.183   \n",
       "50%        0.197      7.399      0.477     54.278      4.067      1.521   \n",
       "75%        0.362      8.627      0.561     74.398      4.692      1.816   \n",
       "max        2.718     56.930     17.478    249.189     35.320     54.292   \n",
       "\n",
       "             559        560        561        562        563        564  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,214.000  1,214.000   \n",
       "mean       0.994      0.324      0.072     32.152    262.826      0.680   \n",
       "std        0.082      0.201      0.052     18.902      7.564      0.124   \n",
       "min        0.892      0.070      0.018      7.237    242.534      0.305   \n",
       "25%        0.955      0.149      0.036     15.723    259.958      0.567   \n",
       "50%        0.972      0.291      0.059     29.509    264.272      0.650   \n",
       "75%        1.001      0.440      0.089     44.113    265.812      0.772   \n",
       "max        1.512      1.074      0.446    101.115    311.404      1.299   \n",
       "\n",
       "             565        566        567        568        569        570  \\\n",
       "count  1,214.000  1,214.000  1,214.000  1,214.000  1,214.000  1,214.000   \n",
       "mean       6.434      0.145      2.610      0.060      2.448     20.981   \n",
       "std        2.626      0.079      1.032      0.032      0.993     10.010   \n",
       "min        0.970      0.022      0.456      0.009      0.371      3.250   \n",
       "25%        4.980      0.088      2.090      0.038      1.884     15.466   \n",
       "50%        5.140      0.119      2.148      0.049      1.988     16.822   \n",
       "75%        7.745      0.185      3.096      0.075      2.950     24.653   \n",
       "max       32.580      0.689     14.014      0.293     12.746     84.802   \n",
       "\n",
       "             571        572        573        574        575        576  \\\n",
       "count  1,463.000  1,463.000  1,463.000  1,463.000  1,463.000  1,463.000   \n",
       "mean     530.531      2.103     29.192      0.349      9.411      0.106   \n",
       "std       17.197      0.268     88.048      0.255     27.534      0.069   \n",
       "min      328.466      0.980      3.540      0.067      1.040      0.023   \n",
       "25%      530.675      1.984      7.500      0.242      2.571      0.075   \n",
       "50%      532.414      2.114      8.650      0.294      2.970      0.090   \n",
       "75%      534.356      2.289     10.130      0.367      3.493      0.113   \n",
       "max      589.508      2.740    454.560      2.197    170.020      0.550   \n",
       "\n",
       "             577        578      579      580      581      582        583  \\\n",
       "count  1,463.000  1,463.000  573.000  573.000  573.000  573.000  1,462.000   \n",
       "mean       5.692     16.808    0.022    0.017    0.005   98.443      0.500   \n",
       "std       17.182     12.792    0.012    0.010    0.003   89.252      0.003   \n",
       "min        0.664      4.582   -0.017    0.003    0.001    0.000      0.478   \n",
       "25%        1.409     11.572    0.014    0.011    0.003   46.102      0.498   \n",
       "50%        1.625     13.898    0.021    0.015    0.005   72.023      0.500   \n",
       "75%        1.902     17.216    0.028    0.020    0.006  117.094      0.502   \n",
       "max       89.262     96.960    0.103    0.080    0.029  737.305      0.510   \n",
       "\n",
       "             584        585        586        587        588        589  \\\n",
       "count  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000  1,462.000   \n",
       "mean       0.015      0.004      3.063      0.021      0.016      0.005   \n",
       "std        0.018      0.004      3.684      0.012      0.009      0.003   \n",
       "min        0.006      0.002      1.198     -0.017      0.003      0.001   \n",
       "25%        0.012      0.003      2.312      0.013      0.011      0.003   \n",
       "50%        0.014      0.004      2.758      0.021      0.015      0.005   \n",
       "75%        0.016      0.004      3.282      0.028      0.020      0.006   \n",
       "max        0.477      0.104     99.303      0.103      0.080      0.029   \n",
       "\n",
       "             590  \n",
       "count  1,462.000  \n",
       "mean      99.736  \n",
       "std       94.831  \n",
       "min        0.000  \n",
       "25%       44.316  \n",
       "50%       71.677  \n",
       "75%      114.750  \n",
       "max      737.305  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Statistics For Pass.\n",
    "pass_data.describe().map('{:,.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf9f6df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "      <th>101</th>\n",
       "      <th>102</th>\n",
       "      <th>103</th>\n",
       "      <th>104</th>\n",
       "      <th>105</th>\n",
       "      <th>106</th>\n",
       "      <th>107</th>\n",
       "      <th>108</th>\n",
       "      <th>109</th>\n",
       "      <th>110</th>\n",
       "      <th>111</th>\n",
       "      <th>112</th>\n",
       "      <th>113</th>\n",
       "      <th>114</th>\n",
       "      <th>115</th>\n",
       "      <th>116</th>\n",
       "      <th>117</th>\n",
       "      <th>118</th>\n",
       "      <th>119</th>\n",
       "      <th>120</th>\n",
       "      <th>121</th>\n",
       "      <th>122</th>\n",
       "      <th>123</th>\n",
       "      <th>124</th>\n",
       "      <th>125</th>\n",
       "      <th>126</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "      <th>137</th>\n",
       "      <th>138</th>\n",
       "      <th>139</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "      <th>151</th>\n",
       "      <th>152</th>\n",
       "      <th>153</th>\n",
       "      <th>154</th>\n",
       "      <th>155</th>\n",
       "      <th>156</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>167</th>\n",
       "      <th>168</th>\n",
       "      <th>169</th>\n",
       "      <th>170</th>\n",
       "      <th>171</th>\n",
       "      <th>172</th>\n",
       "      <th>173</th>\n",
       "      <th>174</th>\n",
       "      <th>175</th>\n",
       "      <th>176</th>\n",
       "      <th>177</th>\n",
       "      <th>178</th>\n",
       "      <th>181</th>\n",
       "      <th>182</th>\n",
       "      <th>183</th>\n",
       "      <th>184</th>\n",
       "      <th>185</th>\n",
       "      <th>186</th>\n",
       "      <th>188</th>\n",
       "      <th>189</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "      <th>200</th>\n",
       "      <th>201</th>\n",
       "      <th>202</th>\n",
       "      <th>203</th>\n",
       "      <th>204</th>\n",
       "      <th>205</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "      <th>226</th>\n",
       "      <th>228</th>\n",
       "      <th>229</th>\n",
       "      <th>239</th>\n",
       "      <th>240</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "      <th>300</th>\n",
       "      <th>301</th>\n",
       "      <th>302</th>\n",
       "      <th>303</th>\n",
       "      <th>304</th>\n",
       "      <th>305</th>\n",
       "      <th>306</th>\n",
       "      <th>307</th>\n",
       "      <th>308</th>\n",
       "      <th>309</th>\n",
       "      <th>310</th>\n",
       "      <th>311</th>\n",
       "      <th>312</th>\n",
       "      <th>313</th>\n",
       "      <th>317</th>\n",
       "      <th>318</th>\n",
       "      <th>319</th>\n",
       "      <th>320</th>\n",
       "      <th>321</th>\n",
       "      <th>322</th>\n",
       "      <th>324</th>\n",
       "      <th>325</th>\n",
       "      <th>332</th>\n",
       "      <th>333</th>\n",
       "      <th>334</th>\n",
       "      <th>335</th>\n",
       "      <th>336</th>\n",
       "      <th>337</th>\n",
       "      <th>338</th>\n",
       "      <th>339</th>\n",
       "      <th>340</th>\n",
       "      <th>341</th>\n",
       "      <th>342</th>\n",
       "      <th>343</th>\n",
       "      <th>344</th>\n",
       "      <th>345</th>\n",
       "      <th>346</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "      <th>357</th>\n",
       "      <th>358</th>\n",
       "      <th>359</th>\n",
       "      <th>360</th>\n",
       "      <th>361</th>\n",
       "      <th>362</th>\n",
       "      <th>363</th>\n",
       "      <th>364</th>\n",
       "      <th>366</th>\n",
       "      <th>367</th>\n",
       "      <th>368</th>\n",
       "      <th>369</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>383</th>\n",
       "      <th>384</th>\n",
       "      <th>385</th>\n",
       "      <th>386</th>\n",
       "      <th>387</th>\n",
       "      <th>388</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>391</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>406</th>\n",
       "      <th>407</th>\n",
       "      <th>408</th>\n",
       "      <th>409</th>\n",
       "      <th>410</th>\n",
       "      <th>411</th>\n",
       "      <th>412</th>\n",
       "      <th>413</th>\n",
       "      <th>414</th>\n",
       "      <th>416</th>\n",
       "      <th>417</th>\n",
       "      <th>418</th>\n",
       "      <th>419</th>\n",
       "      <th>420</th>\n",
       "      <th>421</th>\n",
       "      <th>422</th>\n",
       "      <th>424</th>\n",
       "      <th>425</th>\n",
       "      <th>426</th>\n",
       "      <th>427</th>\n",
       "      <th>428</th>\n",
       "      <th>429</th>\n",
       "      <th>430</th>\n",
       "      <th>431</th>\n",
       "      <th>432</th>\n",
       "      <th>433</th>\n",
       "      <th>434</th>\n",
       "      <th>435</th>\n",
       "      <th>436</th>\n",
       "      <th>437</th>\n",
       "      <th>438</th>\n",
       "      <th>439</th>\n",
       "      <th>440</th>\n",
       "      <th>441</th>\n",
       "      <th>442</th>\n",
       "      <th>443</th>\n",
       "      <th>444</th>\n",
       "      <th>445</th>\n",
       "      <th>446</th>\n",
       "      <th>447</th>\n",
       "      <th>448</th>\n",
       "      <th>449</th>\n",
       "      <th>453</th>\n",
       "      <th>454</th>\n",
       "      <th>455</th>\n",
       "      <th>456</th>\n",
       "      <th>457</th>\n",
       "      <th>458</th>\n",
       "      <th>460</th>\n",
       "      <th>461</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "      <th>479</th>\n",
       "      <th>480</th>\n",
       "      <th>481</th>\n",
       "      <th>483</th>\n",
       "      <th>484</th>\n",
       "      <th>485</th>\n",
       "      <th>486</th>\n",
       "      <th>487</th>\n",
       "      <th>488</th>\n",
       "      <th>489</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>500</th>\n",
       "      <th>501</th>\n",
       "      <th>511</th>\n",
       "      <th>512</th>\n",
       "      <th>517</th>\n",
       "      <th>518</th>\n",
       "      <th>519</th>\n",
       "      <th>520</th>\n",
       "      <th>521</th>\n",
       "      <th>522</th>\n",
       "      <th>523</th>\n",
       "      <th>524</th>\n",
       "      <th>525</th>\n",
       "      <th>526</th>\n",
       "      <th>527</th>\n",
       "      <th>528</th>\n",
       "      <th>540</th>\n",
       "      <th>541</th>\n",
       "      <th>542</th>\n",
       "      <th>543</th>\n",
       "      <th>544</th>\n",
       "      <th>545</th>\n",
       "      <th>546</th>\n",
       "      <th>547</th>\n",
       "      <th>548</th>\n",
       "      <th>549</th>\n",
       "      <th>550</th>\n",
       "      <th>551</th>\n",
       "      <th>552</th>\n",
       "      <th>553</th>\n",
       "      <th>554</th>\n",
       "      <th>555</th>\n",
       "      <th>556</th>\n",
       "      <th>557</th>\n",
       "      <th>558</th>\n",
       "      <th>559</th>\n",
       "      <th>560</th>\n",
       "      <th>561</th>\n",
       "      <th>562</th>\n",
       "      <th>563</th>\n",
       "      <th>564</th>\n",
       "      <th>565</th>\n",
       "      <th>566</th>\n",
       "      <th>567</th>\n",
       "      <th>568</th>\n",
       "      <th>569</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>575</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>578</th>\n",
       "      <th>579</th>\n",
       "      <th>580</th>\n",
       "      <th>581</th>\n",
       "      <th>582</th>\n",
       "      <th>583</th>\n",
       "      <th>584</th>\n",
       "      <th>585</th>\n",
       "      <th>586</th>\n",
       "      <th>587</th>\n",
       "      <th>588</th>\n",
       "      <th>589</th>\n",
       "      <th>590</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>138.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>773.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,562.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,553.000</td>\n",
       "      <td>1,558.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,564.000</td>\n",
       "      <td>1,557.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,563.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,560.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,561.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,555.000</td>\n",
       "      <td>226.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,516.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>549.000</td>\n",
       "      <td>852.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,543.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,559.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,565.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,307.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,294.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>1,567.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>618.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "      <td>1,566.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3,014.453</td>\n",
       "      <td>2,495.850</td>\n",
       "      <td>2,200.547</td>\n",
       "      <td>1,396.377</td>\n",
       "      <td>4.197</td>\n",
       "      <td>101.113</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.463</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.964</td>\n",
       "      <td>199.957</td>\n",
       "      <td>9.005</td>\n",
       "      <td>413.086</td>\n",
       "      <td>9.908</td>\n",
       "      <td>0.971</td>\n",
       "      <td>190.047</td>\n",
       "      <td>12.481</td>\n",
       "      <td>1.405</td>\n",
       "      <td>-5,618.394</td>\n",
       "      <td>2,699.378</td>\n",
       "      <td>-3,806.300</td>\n",
       "      <td>-298.598</td>\n",
       "      <td>1.204</td>\n",
       "      <td>1.938</td>\n",
       "      <td>6.639</td>\n",
       "      <td>69.500</td>\n",
       "      <td>2.366</td>\n",
       "      <td>0.184</td>\n",
       "      <td>3.673</td>\n",
       "      <td>85.337</td>\n",
       "      <td>8.960</td>\n",
       "      <td>50.583</td>\n",
       "      <td>64.556</td>\n",
       "      <td>49.417</td>\n",
       "      <td>66.221</td>\n",
       "      <td>86.837</td>\n",
       "      <td>118.680</td>\n",
       "      <td>67.905</td>\n",
       "      <td>3.353</td>\n",
       "      <td>355.539</td>\n",
       "      <td>10.031</td>\n",
       "      <td>136.743</td>\n",
       "      <td>733.673</td>\n",
       "      <td>1.178</td>\n",
       "      <td>139.972</td>\n",
       "      <td>632.254</td>\n",
       "      <td>157.421</td>\n",
       "      <td>4.593</td>\n",
       "      <td>4.839</td>\n",
       "      <td>2,856.172</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.593</td>\n",
       "      <td>2.960</td>\n",
       "      <td>355.159</td>\n",
       "      <td>10.423</td>\n",
       "      <td>116.502</td>\n",
       "      <td>13.990</td>\n",
       "      <td>20.542</td>\n",
       "      <td>27.132</td>\n",
       "      <td>706.669</td>\n",
       "      <td>16.715</td>\n",
       "      <td>147.438</td>\n",
       "      <td>619.102</td>\n",
       "      <td>104.329</td>\n",
       "      <td>150.362</td>\n",
       "      <td>468.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.006</td>\n",
       "      <td>7.452</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.113</td>\n",
       "      <td>2.402</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1,807.815</td>\n",
       "      <td>0.189</td>\n",
       "      <td>8,827.537</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.980</td>\n",
       "      <td>101.318</td>\n",
       "      <td>231.819</td>\n",
       "      <td>0.458</td>\n",
       "      <td>0.945</td>\n",
       "      <td>0.000</td>\n",
       "      <td>747.384</td>\n",
       "      <td>0.987</td>\n",
       "      <td>58.626</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.971</td>\n",
       "      <td>6.311</td>\n",
       "      <td>15.796</td>\n",
       "      <td>3.898</td>\n",
       "      <td>15.830</td>\n",
       "      <td>15.795</td>\n",
       "      <td>1.185</td>\n",
       "      <td>2.751</td>\n",
       "      <td>0.648</td>\n",
       "      <td>3.192</td>\n",
       "      <td>-0.554</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.319</td>\n",
       "      <td>1,004.043</td>\n",
       "      <td>39.392</td>\n",
       "      <td>117.961</td>\n",
       "      <td>138.195</td>\n",
       "      <td>122.693</td>\n",
       "      <td>57.603</td>\n",
       "      <td>416.767</td>\n",
       "      <td>26.078</td>\n",
       "      <td>6.642</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.017</td>\n",
       "      <td>8.471</td>\n",
       "      <td>6.814</td>\n",
       "      <td>14.047</td>\n",
       "      <td>1.197</td>\n",
       "      <td>0.012</td>\n",
       "      <td>7.698</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1,039.651</td>\n",
       "      <td>882.681</td>\n",
       "      <td>555.346</td>\n",
       "      <td>4,066.850</td>\n",
       "      <td>4,797.155</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.128</td>\n",
       "      <td>0.252</td>\n",
       "      <td>2.789</td>\n",
       "      <td>1.236</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.684</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.395</td>\n",
       "      <td>19.013</td>\n",
       "      <td>0.547</td>\n",
       "      <td>10.781</td>\n",
       "      <td>26.661</td>\n",
       "      <td>0.145</td>\n",
       "      <td>7.366</td>\n",
       "      <td>17.936</td>\n",
       "      <td>43.211</td>\n",
       "      <td>0.287</td>\n",
       "      <td>8.688</td>\n",
       "      <td>20.093</td>\n",
       "      <td>0.557</td>\n",
       "      <td>11.532</td>\n",
       "      <td>17.600</td>\n",
       "      <td>7.839</td>\n",
       "      <td>10.170</td>\n",
       "      <td>30.073</td>\n",
       "      <td>32.218</td>\n",
       "      <td>9.050</td>\n",
       "      <td>0.001</td>\n",
       "      <td>20.376</td>\n",
       "      <td>73.264</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.072</td>\n",
       "      <td>3.771</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.009</td>\n",
       "      <td>122.847</td>\n",
       "      <td>0.059</td>\n",
       "      <td>1,041.057</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.006</td>\n",
       "      <td>1.730</td>\n",
       "      <td>4.149</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>109.651</td>\n",
       "      <td>0.004</td>\n",
       "      <td>4.645</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.071</td>\n",
       "      <td>19.505</td>\n",
       "      <td>3.778</td>\n",
       "      <td>29.260</td>\n",
       "      <td>46.057</td>\n",
       "      <td>41.298</td>\n",
       "      <td>20.181</td>\n",
       "      <td>136.292</td>\n",
       "      <td>8.693</td>\n",
       "      <td>2.211</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.804</td>\n",
       "      <td>2.120</td>\n",
       "      <td>4.260</td>\n",
       "      <td>0.368</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.579</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.014</td>\n",
       "      <td>335.551</td>\n",
       "      <td>401.815</td>\n",
       "      <td>252.999</td>\n",
       "      <td>1,879.228</td>\n",
       "      <td>2,342.827</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.160</td>\n",
       "      <td>5.977</td>\n",
       "      <td>0.173</td>\n",
       "      <td>3.189</td>\n",
       "      <td>7.916</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.264</td>\n",
       "      <td>5.393</td>\n",
       "      <td>13.332</td>\n",
       "      <td>0.083</td>\n",
       "      <td>2.593</td>\n",
       "      <td>6.216</td>\n",
       "      <td>0.168</td>\n",
       "      <td>3.427</td>\n",
       "      <td>9.736</td>\n",
       "      <td>2.327</td>\n",
       "      <td>3.038</td>\n",
       "      <td>9.329</td>\n",
       "      <td>14.674</td>\n",
       "      <td>2.732</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.199</td>\n",
       "      <td>23.217</td>\n",
       "      <td>7.958</td>\n",
       "      <td>5.770</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>39.936</td>\n",
       "      <td>0.018</td>\n",
       "      <td>333.320</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.285</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.155</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.432</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.024</td>\n",
       "      <td>6.731</td>\n",
       "      <td>1.232</td>\n",
       "      <td>5.341</td>\n",
       "      <td>4.580</td>\n",
       "      <td>4.929</td>\n",
       "      <td>2.616</td>\n",
       "      <td>30.911</td>\n",
       "      <td>25.613</td>\n",
       "      <td>6.631</td>\n",
       "      <td>3.404</td>\n",
       "      <td>8.191</td>\n",
       "      <td>320.259</td>\n",
       "      <td>309.061</td>\n",
       "      <td>1.821</td>\n",
       "      <td>4.175</td>\n",
       "      <td>77.660</td>\n",
       "      <td>3.315</td>\n",
       "      <td>6.796</td>\n",
       "      <td>1.234</td>\n",
       "      <td>4.059</td>\n",
       "      <td>4.221</td>\n",
       "      <td>4.172</td>\n",
       "      <td>18.422</td>\n",
       "      <td>22.358</td>\n",
       "      <td>99.368</td>\n",
       "      <td>205.519</td>\n",
       "      <td>14.734</td>\n",
       "      <td>9.371</td>\n",
       "      <td>7.513</td>\n",
       "      <td>4.017</td>\n",
       "      <td>54.701</td>\n",
       "      <td>70.644</td>\n",
       "      <td>11.527</td>\n",
       "      <td>0.802</td>\n",
       "      <td>1.345</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.895</td>\n",
       "      <td>0.647</td>\n",
       "      <td>1.175</td>\n",
       "      <td>0.282</td>\n",
       "      <td>0.332</td>\n",
       "      <td>5.347</td>\n",
       "      <td>5.461</td>\n",
       "      <td>7.884</td>\n",
       "      <td>3.637</td>\n",
       "      <td>12.326</td>\n",
       "      <td>5.264</td>\n",
       "      <td>2.838</td>\n",
       "      <td>29.197</td>\n",
       "      <td>6.252</td>\n",
       "      <td>224.173</td>\n",
       "      <td>5.662</td>\n",
       "      <td>5.368</td>\n",
       "      <td>9.639</td>\n",
       "      <td>137.888</td>\n",
       "      <td>39.427</td>\n",
       "      <td>37.637</td>\n",
       "      <td>4.263</td>\n",
       "      <td>20.132</td>\n",
       "      <td>6.258</td>\n",
       "      <td>0.128</td>\n",
       "      <td>3.283</td>\n",
       "      <td>75.538</td>\n",
       "      <td>318.418</td>\n",
       "      <td>206.564</td>\n",
       "      <td>215.289</td>\n",
       "      <td>201.112</td>\n",
       "      <td>302.506</td>\n",
       "      <td>239.455</td>\n",
       "      <td>352.616</td>\n",
       "      <td>272.170</td>\n",
       "      <td>51.354</td>\n",
       "      <td>2.443</td>\n",
       "      <td>8.171</td>\n",
       "      <td>2.530</td>\n",
       "      <td>0.956</td>\n",
       "      <td>6.808</td>\n",
       "      <td>29.866</td>\n",
       "      <td>11.821</td>\n",
       "      <td>263.196</td>\n",
       "      <td>240.981</td>\n",
       "      <td>55.764</td>\n",
       "      <td>275.979</td>\n",
       "      <td>0.679</td>\n",
       "      <td>1.739</td>\n",
       "      <td>1.806</td>\n",
       "      <td>11.728</td>\n",
       "      <td>2.696</td>\n",
       "      <td>11.610</td>\n",
       "      <td>14.729</td>\n",
       "      <td>0.454</td>\n",
       "      <td>5.688</td>\n",
       "      <td>5.560</td>\n",
       "      <td>1.443</td>\n",
       "      <td>6.396</td>\n",
       "      <td>3.034</td>\n",
       "      <td>1.943</td>\n",
       "      <td>9.612</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.611</td>\n",
       "      <td>1.040</td>\n",
       "      <td>403.546</td>\n",
       "      <td>75.680</td>\n",
       "      <td>0.663</td>\n",
       "      <td>17.013</td>\n",
       "      <td>1.231</td>\n",
       "      <td>0.277</td>\n",
       "      <td>7.704</td>\n",
       "      <td>0.504</td>\n",
       "      <td>57.747</td>\n",
       "      <td>4.217</td>\n",
       "      <td>1.623</td>\n",
       "      <td>0.995</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.072</td>\n",
       "      <td>32.285</td>\n",
       "      <td>262.730</td>\n",
       "      <td>0.680</td>\n",
       "      <td>6.445</td>\n",
       "      <td>0.146</td>\n",
       "      <td>2.611</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2.452</td>\n",
       "      <td>21.118</td>\n",
       "      <td>530.524</td>\n",
       "      <td>2.102</td>\n",
       "      <td>28.450</td>\n",
       "      <td>0.346</td>\n",
       "      <td>9.162</td>\n",
       "      <td>0.105</td>\n",
       "      <td>5.564</td>\n",
       "      <td>16.642</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.005</td>\n",
       "      <td>97.934</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.068</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.005</td>\n",
       "      <td>99.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>73.622</td>\n",
       "      <td>80.408</td>\n",
       "      <td>29.513</td>\n",
       "      <td>441.692</td>\n",
       "      <td>56.356</td>\n",
       "      <td>6.237</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>3.257</td>\n",
       "      <td>2.797</td>\n",
       "      <td>17.221</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.012</td>\n",
       "      <td>2.781</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.017</td>\n",
       "      <td>626.822</td>\n",
       "      <td>295.499</td>\n",
       "      <td>1,380.162</td>\n",
       "      <td>2,902.690</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.189</td>\n",
       "      <td>1.244</td>\n",
       "      <td>3.461</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.535</td>\n",
       "      <td>2.027</td>\n",
       "      <td>1.344</td>\n",
       "      <td>1.183</td>\n",
       "      <td>2.575</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.447</td>\n",
       "      <td>1.807</td>\n",
       "      <td>24.063</td>\n",
       "      <td>2.360</td>\n",
       "      <td>6.235</td>\n",
       "      <td>0.175</td>\n",
       "      <td>7.849</td>\n",
       "      <td>12.170</td>\n",
       "      <td>0.190</td>\n",
       "      <td>4.524</td>\n",
       "      <td>8.644</td>\n",
       "      <td>60.925</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.060</td>\n",
       "      <td>25.749</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.085</td>\n",
       "      <td>9.532</td>\n",
       "      <td>6.028</td>\n",
       "      <td>0.275</td>\n",
       "      <td>8.629</td>\n",
       "      <td>7.120</td>\n",
       "      <td>4.977</td>\n",
       "      <td>7.122</td>\n",
       "      <td>11.623</td>\n",
       "      <td>307.502</td>\n",
       "      <td>4.240</td>\n",
       "      <td>9.539</td>\n",
       "      <td>31.652</td>\n",
       "      <td>18.388</td>\n",
       "      <td>17.630</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.013</td>\n",
       "      <td>53.537</td>\n",
       "      <td>0.052</td>\n",
       "      <td>396.314</td>\n",
       "      <td>0.088</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.009</td>\n",
       "      <td>1.880</td>\n",
       "      <td>2.105</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.002</td>\n",
       "      <td>48.949</td>\n",
       "      <td>0.009</td>\n",
       "      <td>6.485</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.281</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.264</td>\n",
       "      <td>1.220</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.053</td>\n",
       "      <td>6.538</td>\n",
       "      <td>2.990</td>\n",
       "      <td>57.545</td>\n",
       "      <td>53.910</td>\n",
       "      <td>52.253</td>\n",
       "      <td>12.345</td>\n",
       "      <td>263.301</td>\n",
       "      <td>506.922</td>\n",
       "      <td>3.552</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.027</td>\n",
       "      <td>18.741</td>\n",
       "      <td>3.242</td>\n",
       "      <td>31.003</td>\n",
       "      <td>23.364</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5.239</td>\n",
       "      <td>1.122</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.040</td>\n",
       "      <td>406.849</td>\n",
       "      <td>983.043</td>\n",
       "      <td>574.809</td>\n",
       "      <td>4,239.245</td>\n",
       "      <td>6,553.569</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.407</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.633</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.157</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.283</td>\n",
       "      <td>3.312</td>\n",
       "      <td>0.224</td>\n",
       "      <td>4.164</td>\n",
       "      <td>6.836</td>\n",
       "      <td>0.110</td>\n",
       "      <td>7.189</td>\n",
       "      <td>8.610</td>\n",
       "      <td>21.712</td>\n",
       "      <td>0.395</td>\n",
       "      <td>15.721</td>\n",
       "      <td>10.552</td>\n",
       "      <td>0.538</td>\n",
       "      <td>16.446</td>\n",
       "      <td>8.691</td>\n",
       "      <td>5.104</td>\n",
       "      <td>14.623</td>\n",
       "      <td>17.462</td>\n",
       "      <td>565.101</td>\n",
       "      <td>11.541</td>\n",
       "      <td>0.051</td>\n",
       "      <td>17.498</td>\n",
       "      <td>28.067</td>\n",
       "      <td>1.168</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.046</td>\n",
       "      <td>1.170</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.056</td>\n",
       "      <td>55.156</td>\n",
       "      <td>0.071</td>\n",
       "      <td>433.170</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.085</td>\n",
       "      <td>4.336</td>\n",
       "      <td>10.045</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.016</td>\n",
       "      <td>54.597</td>\n",
       "      <td>0.037</td>\n",
       "      <td>64.355</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.030</td>\n",
       "      <td>7.344</td>\n",
       "      <td>1.152</td>\n",
       "      <td>8.402</td>\n",
       "      <td>17.866</td>\n",
       "      <td>17.738</td>\n",
       "      <td>3.830</td>\n",
       "      <td>85.608</td>\n",
       "      <td>168.949</td>\n",
       "      <td>1.196</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>5.864</td>\n",
       "      <td>0.963</td>\n",
       "      <td>9.764</td>\n",
       "      <td>7.386</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.617</td>\n",
       "      <td>0.271</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.011</td>\n",
       "      <td>137.692</td>\n",
       "      <td>477.050</td>\n",
       "      <td>283.531</td>\n",
       "      <td>1,975.111</td>\n",
       "      <td>3,226.924</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.117</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.072</td>\n",
       "      <td>1.216</td>\n",
       "      <td>2.179</td>\n",
       "      <td>0.032</td>\n",
       "      <td>2.117</td>\n",
       "      <td>2.519</td>\n",
       "      <td>6.616</td>\n",
       "      <td>0.063</td>\n",
       "      <td>5.645</td>\n",
       "      <td>3.403</td>\n",
       "      <td>0.173</td>\n",
       "      <td>5.782</td>\n",
       "      <td>7.556</td>\n",
       "      <td>1.699</td>\n",
       "      <td>5.645</td>\n",
       "      <td>6.075</td>\n",
       "      <td>261.738</td>\n",
       "      <td>3.668</td>\n",
       "      <td>0.011</td>\n",
       "      <td>5.372</td>\n",
       "      <td>8.895</td>\n",
       "      <td>17.513</td>\n",
       "      <td>17.077</td>\n",
       "      <td>0.352</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.020</td>\n",
       "      <td>17.056</td>\n",
       "      <td>0.022</td>\n",
       "      <td>138.802</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>1.341</td>\n",
       "      <td>3.168</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>17.227</td>\n",
       "      <td>0.012</td>\n",
       "      <td>20.326</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.011</td>\n",
       "      <td>2.830</td>\n",
       "      <td>0.365</td>\n",
       "      <td>2.578</td>\n",
       "      <td>1.777</td>\n",
       "      <td>2.123</td>\n",
       "      <td>0.551</td>\n",
       "      <td>18.414</td>\n",
       "      <td>47.308</td>\n",
       "      <td>3.958</td>\n",
       "      <td>1.035</td>\n",
       "      <td>4.055</td>\n",
       "      <td>287.704</td>\n",
       "      <td>325.448</td>\n",
       "      <td>3.058</td>\n",
       "      <td>6.914</td>\n",
       "      <td>32.597</td>\n",
       "      <td>6.325</td>\n",
       "      <td>23.258</td>\n",
       "      <td>0.996</td>\n",
       "      <td>3.042</td>\n",
       "      <td>10.633</td>\n",
       "      <td>6.435</td>\n",
       "      <td>36.060</td>\n",
       "      <td>36.395</td>\n",
       "      <td>126.189</td>\n",
       "      <td>225.779</td>\n",
       "      <td>34.109</td>\n",
       "      <td>34.370</td>\n",
       "      <td>34.558</td>\n",
       "      <td>1.611</td>\n",
       "      <td>34.108</td>\n",
       "      <td>38.376</td>\n",
       "      <td>6.169</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.919</td>\n",
       "      <td>2.251</td>\n",
       "      <td>3.060</td>\n",
       "      <td>0.938</td>\n",
       "      <td>8.126</td>\n",
       "      <td>4.538</td>\n",
       "      <td>1.346</td>\n",
       "      <td>13.335</td>\n",
       "      <td>8.674</td>\n",
       "      <td>230.767</td>\n",
       "      <td>3.152</td>\n",
       "      <td>4.983</td>\n",
       "      <td>10.174</td>\n",
       "      <td>47.698</td>\n",
       "      <td>22.457</td>\n",
       "      <td>24.823</td>\n",
       "      <td>2.611</td>\n",
       "      <td>14.940</td>\n",
       "      <td>10.185</td>\n",
       "      <td>5.062</td>\n",
       "      <td>2.639</td>\n",
       "      <td>35.752</td>\n",
       "      <td>281.011</td>\n",
       "      <td>192.864</td>\n",
       "      <td>213.127</td>\n",
       "      <td>218.690</td>\n",
       "      <td>287.364</td>\n",
       "      <td>263.838</td>\n",
       "      <td>252.044</td>\n",
       "      <td>228.047</td>\n",
       "      <td>18.049</td>\n",
       "      <td>1.224</td>\n",
       "      <td>1.759</td>\n",
       "      <td>0.974</td>\n",
       "      <td>6.615</td>\n",
       "      <td>3.260</td>\n",
       "      <td>24.622</td>\n",
       "      <td>4.957</td>\n",
       "      <td>324.771</td>\n",
       "      <td>323.003</td>\n",
       "      <td>37.692</td>\n",
       "      <td>329.665</td>\n",
       "      <td>10.784</td>\n",
       "      <td>4.891</td>\n",
       "      <td>4.716</td>\n",
       "      <td>15.814</td>\n",
       "      <td>5.702</td>\n",
       "      <td>103.123</td>\n",
       "      <td>7.104</td>\n",
       "      <td>4.148</td>\n",
       "      <td>20.663</td>\n",
       "      <td>3.920</td>\n",
       "      <td>0.958</td>\n",
       "      <td>1.889</td>\n",
       "      <td>1.253</td>\n",
       "      <td>0.732</td>\n",
       "      <td>2.896</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.316</td>\n",
       "      <td>0.389</td>\n",
       "      <td>5.064</td>\n",
       "      <td>3.391</td>\n",
       "      <td>0.673</td>\n",
       "      <td>4.967</td>\n",
       "      <td>1.361</td>\n",
       "      <td>0.276</td>\n",
       "      <td>2.193</td>\n",
       "      <td>0.599</td>\n",
       "      <td>35.208</td>\n",
       "      <td>1.280</td>\n",
       "      <td>1.870</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.201</td>\n",
       "      <td>0.052</td>\n",
       "      <td>19.026</td>\n",
       "      <td>7.631</td>\n",
       "      <td>0.122</td>\n",
       "      <td>2.634</td>\n",
       "      <td>0.081</td>\n",
       "      <td>1.033</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.997</td>\n",
       "      <td>10.213</td>\n",
       "      <td>17.500</td>\n",
       "      <td>0.275</td>\n",
       "      <td>86.305</td>\n",
       "      <td>0.248</td>\n",
       "      <td>26.920</td>\n",
       "      <td>0.068</td>\n",
       "      <td>16.921</td>\n",
       "      <td>12.485</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.003</td>\n",
       "      <td>87.521</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.578</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>93.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2,743.240</td>\n",
       "      <td>2,158.750</td>\n",
       "      <td>2,060.660</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.681</td>\n",
       "      <td>82.131</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.191</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.655</td>\n",
       "      <td>182.094</td>\n",
       "      <td>2.249</td>\n",
       "      <td>333.449</td>\n",
       "      <td>4.470</td>\n",
       "      <td>0.579</td>\n",
       "      <td>169.177</td>\n",
       "      <td>9.877</td>\n",
       "      <td>1.180</td>\n",
       "      <td>-7,150.250</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-9,986.750</td>\n",
       "      <td>-14,804.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>59.400</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.034</td>\n",
       "      <td>2.070</td>\n",
       "      <td>83.183</td>\n",
       "      <td>7.603</td>\n",
       "      <td>49.835</td>\n",
       "      <td>63.677</td>\n",
       "      <td>40.229</td>\n",
       "      <td>64.919</td>\n",
       "      <td>84.733</td>\n",
       "      <td>111.713</td>\n",
       "      <td>1.434</td>\n",
       "      <td>-0.076</td>\n",
       "      <td>342.755</td>\n",
       "      <td>9.464</td>\n",
       "      <td>108.846</td>\n",
       "      <td>699.814</td>\n",
       "      <td>0.497</td>\n",
       "      <td>125.798</td>\n",
       "      <td>607.393</td>\n",
       "      <td>40.261</td>\n",
       "      <td>3.706</td>\n",
       "      <td>3.932</td>\n",
       "      <td>2,801.000</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.932</td>\n",
       "      <td>4.220</td>\n",
       "      <td>-28.988</td>\n",
       "      <td>324.714</td>\n",
       "      <td>9.461</td>\n",
       "      <td>81.490</td>\n",
       "      <td>1.659</td>\n",
       "      <td>6.448</td>\n",
       "      <td>4.308</td>\n",
       "      <td>632.423</td>\n",
       "      <td>0.414</td>\n",
       "      <td>87.025</td>\n",
       "      <td>581.777</td>\n",
       "      <td>21.433</td>\n",
       "      <td>-59.478</td>\n",
       "      <td>456.045</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>-0.105</td>\n",
       "      <td>-0.348</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>-0.144</td>\n",
       "      <td>-0.098</td>\n",
       "      <td>-0.213</td>\n",
       "      <td>5.826</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.105</td>\n",
       "      <td>2.243</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1,627.471</td>\n",
       "      <td>0.111</td>\n",
       "      <td>7,397.310</td>\n",
       "      <td>-0.357</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-1.480</td>\n",
       "      <td>-5.272</td>\n",
       "      <td>-0.528</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.535</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.523</td>\n",
       "      <td>-0.345</td>\n",
       "      <td>0.785</td>\n",
       "      <td>88.194</td>\n",
       "      <td>213.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.000</td>\n",
       "      <td>544.025</td>\n",
       "      <td>0.890</td>\n",
       "      <td>52.807</td>\n",
       "      <td>0.527</td>\n",
       "      <td>0.841</td>\n",
       "      <td>5.126</td>\n",
       "      <td>15.460</td>\n",
       "      <td>1.671</td>\n",
       "      <td>15.170</td>\n",
       "      <td>15.430</td>\n",
       "      <td>0.312</td>\n",
       "      <td>2.340</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-3.779</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2.191</td>\n",
       "      <td>980.451</td>\n",
       "      <td>33.366</td>\n",
       "      <td>58.000</td>\n",
       "      <td>36.100</td>\n",
       "      <td>19.200</td>\n",
       "      <td>19.800</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>1.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.421</td>\n",
       "      <td>1.337</td>\n",
       "      <td>2.020</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.244</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.012</td>\n",
       "      <td>234.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.047</td>\n",
       "      <td>9.400</td>\n",
       "      <td>0.093</td>\n",
       "      <td>3.170</td>\n",
       "      <td>5.014</td>\n",
       "      <td>0.030</td>\n",
       "      <td>1.940</td>\n",
       "      <td>6.220</td>\n",
       "      <td>6.613</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.750</td>\n",
       "      <td>9.220</td>\n",
       "      <td>0.090</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.210</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7.728</td>\n",
       "      <td>0.043</td>\n",
       "      <td>2.300</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.010</td>\n",
       "      <td>5.359</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.008</td>\n",
       "      <td>1.034</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.264</td>\n",
       "      <td>0.009</td>\n",
       "      <td>168.800</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.102</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.000</td>\n",
       "      <td>21.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.767</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.020</td>\n",
       "      <td>6.098</td>\n",
       "      <td>1.302</td>\n",
       "      <td>15.547</td>\n",
       "      <td>10.402</td>\n",
       "      <td>6.943</td>\n",
       "      <td>8.651</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.561</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.505</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>82.323</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.117</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.022</td>\n",
       "      <td>2.788</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.985</td>\n",
       "      <td>1.657</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.611</td>\n",
       "      <td>1.710</td>\n",
       "      <td>2.235</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.537</td>\n",
       "      <td>2.837</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.790</td>\n",
       "      <td>5.215</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.200</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.574</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.256</td>\n",
       "      <td>2.056</td>\n",
       "      <td>1.769</td>\n",
       "      <td>1.018</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.720</td>\n",
       "      <td>0.003</td>\n",
       "      <td>60.988</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.310</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.006</td>\n",
       "      <td>2.054</td>\n",
       "      <td>0.424</td>\n",
       "      <td>2.738</td>\n",
       "      <td>1.216</td>\n",
       "      <td>0.734</td>\n",
       "      <td>0.961</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.042</td>\n",
       "      <td>1.534</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.153</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.441</td>\n",
       "      <td>0.722</td>\n",
       "      <td>23.020</td>\n",
       "      <td>0.487</td>\n",
       "      <td>1.467</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.664</td>\n",
       "      <td>1.120</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.157</td>\n",
       "      <td>0.000</td>\n",
       "      <td>14.121</td>\n",
       "      <td>1.097</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.671</td>\n",
       "      <td>0.904</td>\n",
       "      <td>2.329</td>\n",
       "      <td>0.695</td>\n",
       "      <td>3.049</td>\n",
       "      <td>1.443</td>\n",
       "      <td>0.991</td>\n",
       "      <td>7.953</td>\n",
       "      <td>1.716</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.601</td>\n",
       "      <td>0.833</td>\n",
       "      <td>2.403</td>\n",
       "      <td>11.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.101</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.687</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.646</td>\n",
       "      <td>8.841</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.723</td>\n",
       "      <td>0.556</td>\n",
       "      <td>4.888</td>\n",
       "      <td>0.833</td>\n",
       "      <td>0.034</td>\n",
       "      <td>1.772</td>\n",
       "      <td>4.814</td>\n",
       "      <td>1.950</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.288</td>\n",
       "      <td>0.467</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.312</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.681</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.310</td>\n",
       "      <td>1.540</td>\n",
       "      <td>0.171</td>\n",
       "      <td>2.170</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.614</td>\n",
       "      <td>3.276</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4.429</td>\n",
       "      <td>0.444</td>\n",
       "      <td>372.822</td>\n",
       "      <td>71.038</td>\n",
       "      <td>0.045</td>\n",
       "      <td>6.110</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.019</td>\n",
       "      <td>2.786</td>\n",
       "      <td>0.052</td>\n",
       "      <td>4.827</td>\n",
       "      <td>1.497</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.018</td>\n",
       "      <td>7.237</td>\n",
       "      <td>242.286</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.970</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.371</td>\n",
       "      <td>3.250</td>\n",
       "      <td>317.196</td>\n",
       "      <td>0.980</td>\n",
       "      <td>3.540</td>\n",
       "      <td>0.067</td>\n",
       "      <td>1.040</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.664</td>\n",
       "      <td>4.582</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.478</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.198</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2,966.260</td>\n",
       "      <td>2,452.248</td>\n",
       "      <td>2,181.044</td>\n",
       "      <td>1,081.876</td>\n",
       "      <td>1.018</td>\n",
       "      <td>97.920</td>\n",
       "      <td>0.121</td>\n",
       "      <td>1.411</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.958</td>\n",
       "      <td>198.131</td>\n",
       "      <td>7.095</td>\n",
       "      <td>406.127</td>\n",
       "      <td>9.568</td>\n",
       "      <td>0.968</td>\n",
       "      <td>188.300</td>\n",
       "      <td>12.460</td>\n",
       "      <td>1.397</td>\n",
       "      <td>-5,933.250</td>\n",
       "      <td>2,578.000</td>\n",
       "      <td>-4,371.750</td>\n",
       "      <td>-1,476.000</td>\n",
       "      <td>1.095</td>\n",
       "      <td>1.907</td>\n",
       "      <td>5.264</td>\n",
       "      <td>67.378</td>\n",
       "      <td>2.089</td>\n",
       "      <td>0.162</td>\n",
       "      <td>3.363</td>\n",
       "      <td>84.490</td>\n",
       "      <td>8.580</td>\n",
       "      <td>50.252</td>\n",
       "      <td>64.025</td>\n",
       "      <td>49.421</td>\n",
       "      <td>66.041</td>\n",
       "      <td>86.578</td>\n",
       "      <td>118.016</td>\n",
       "      <td>74.800</td>\n",
       "      <td>2.690</td>\n",
       "      <td>350.802</td>\n",
       "      <td>9.925</td>\n",
       "      <td>130.729</td>\n",
       "      <td>724.442</td>\n",
       "      <td>0.985</td>\n",
       "      <td>136.927</td>\n",
       "      <td>625.928</td>\n",
       "      <td>115.509</td>\n",
       "      <td>4.574</td>\n",
       "      <td>4.816</td>\n",
       "      <td>2,836.000</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.947</td>\n",
       "      <td>4.532</td>\n",
       "      <td>-1.872</td>\n",
       "      <td>350.596</td>\n",
       "      <td>10.283</td>\n",
       "      <td>112.023</td>\n",
       "      <td>10.364</td>\n",
       "      <td>17.365</td>\n",
       "      <td>23.056</td>\n",
       "      <td>698.770</td>\n",
       "      <td>0.891</td>\n",
       "      <td>145.237</td>\n",
       "      <td>612.774</td>\n",
       "      <td>87.484</td>\n",
       "      <td>145.305</td>\n",
       "      <td>464.458</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.052</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-0.027</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>7.104</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.111</td>\n",
       "      <td>2.377</td>\n",
       "      <td>0.976</td>\n",
       "      <td>1,777.470</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8,564.690</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.089</td>\n",
       "      <td>-0.219</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.979</td>\n",
       "      <td>100.389</td>\n",
       "      <td>230.374</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>721.023</td>\n",
       "      <td>0.990</td>\n",
       "      <td>57.978</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.965</td>\n",
       "      <td>6.246</td>\n",
       "      <td>15.730</td>\n",
       "      <td>3.202</td>\n",
       "      <td>15.762</td>\n",
       "      <td>15.723</td>\n",
       "      <td>0.974</td>\n",
       "      <td>2.572</td>\n",
       "      <td>0.549</td>\n",
       "      <td>3.074</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>0.689</td>\n",
       "      <td>0.996</td>\n",
       "      <td>2.277</td>\n",
       "      <td>999.996</td>\n",
       "      <td>37.347</td>\n",
       "      <td>92.000</td>\n",
       "      <td>90.000</td>\n",
       "      <td>81.300</td>\n",
       "      <td>50.900</td>\n",
       "      <td>243.786</td>\n",
       "      <td>0.132</td>\n",
       "      <td>5.110</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.010</td>\n",
       "      <td>6.360</td>\n",
       "      <td>4.459</td>\n",
       "      <td>8.090</td>\n",
       "      <td>0.374</td>\n",
       "      <td>0.007</td>\n",
       "      <td>5.927</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.027</td>\n",
       "      <td>721.675</td>\n",
       "      <td>411.000</td>\n",
       "      <td>295.000</td>\n",
       "      <td>1,321.000</td>\n",
       "      <td>451.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.132</td>\n",
       "      <td>2.100</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.576</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.517</td>\n",
       "      <td>0.277</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.222</td>\n",
       "      <td>16.850</td>\n",
       "      <td>0.378</td>\n",
       "      <td>7.732</td>\n",
       "      <td>21.171</td>\n",
       "      <td>0.102</td>\n",
       "      <td>5.390</td>\n",
       "      <td>14.505</td>\n",
       "      <td>24.711</td>\n",
       "      <td>0.218</td>\n",
       "      <td>5.040</td>\n",
       "      <td>17.130</td>\n",
       "      <td>0.296</td>\n",
       "      <td>6.740</td>\n",
       "      <td>14.155</td>\n",
       "      <td>5.020</td>\n",
       "      <td>6.094</td>\n",
       "      <td>24.653</td>\n",
       "      <td>0.114</td>\n",
       "      <td>6.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>16.350</td>\n",
       "      <td>56.158</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.946</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.001</td>\n",
       "      <td>95.147</td>\n",
       "      <td>0.030</td>\n",
       "      <td>718.725</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.911</td>\n",
       "      <td>2.726</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>76.132</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.206</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.044</td>\n",
       "      <td>13.828</td>\n",
       "      <td>2.957</td>\n",
       "      <td>24.982</td>\n",
       "      <td>30.014</td>\n",
       "      <td>27.093</td>\n",
       "      <td>18.247</td>\n",
       "      <td>81.216</td>\n",
       "      <td>0.045</td>\n",
       "      <td>1.698</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.210</td>\n",
       "      <td>1.438</td>\n",
       "      <td>2.467</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.002</td>\n",
       "      <td>2.092</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>229.809</td>\n",
       "      <td>185.090</td>\n",
       "      <td>130.220</td>\n",
       "      <td>603.033</td>\n",
       "      <td>210.937</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.717</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.073</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.091</td>\n",
       "      <td>5.302</td>\n",
       "      <td>0.117</td>\n",
       "      <td>2.320</td>\n",
       "      <td>6.245</td>\n",
       "      <td>0.031</td>\n",
       "      <td>1.670</td>\n",
       "      <td>4.273</td>\n",
       "      <td>7.579</td>\n",
       "      <td>0.069</td>\n",
       "      <td>1.547</td>\n",
       "      <td>5.454</td>\n",
       "      <td>0.089</td>\n",
       "      <td>2.036</td>\n",
       "      <td>8.289</td>\n",
       "      <td>1.543</td>\n",
       "      <td>1.901</td>\n",
       "      <td>7.589</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1.912</td>\n",
       "      <td>0.000</td>\n",
       "      <td>4.999</td>\n",
       "      <td>17.861</td>\n",
       "      <td>4.441</td>\n",
       "      <td>2.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.021</td>\n",
       "      <td>1.025</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.169</td>\n",
       "      <td>0.009</td>\n",
       "      <td>228.683</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.842</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>24.387</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.675</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.014</td>\n",
       "      <td>4.548</td>\n",
       "      <td>0.967</td>\n",
       "      <td>4.128</td>\n",
       "      <td>3.013</td>\n",
       "      <td>3.265</td>\n",
       "      <td>2.321</td>\n",
       "      <td>18.408</td>\n",
       "      <td>11.376</td>\n",
       "      <td>4.927</td>\n",
       "      <td>2.660</td>\n",
       "      <td>5.766</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.030</td>\n",
       "      <td>3.184</td>\n",
       "      <td>55.977</td>\n",
       "      <td>1.965</td>\n",
       "      <td>3.766</td>\n",
       "      <td>0.743</td>\n",
       "      <td>3.113</td>\n",
       "      <td>1.935</td>\n",
       "      <td>2.571</td>\n",
       "      <td>7.000</td>\n",
       "      <td>11.059</td>\n",
       "      <td>31.032</td>\n",
       "      <td>10.027</td>\n",
       "      <td>7.551</td>\n",
       "      <td>3.494</td>\n",
       "      <td>1.951</td>\n",
       "      <td>3.071</td>\n",
       "      <td>36.290</td>\n",
       "      <td>48.174</td>\n",
       "      <td>5.414</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.908</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.805</td>\n",
       "      <td>0.556</td>\n",
       "      <td>1.047</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.188</td>\n",
       "      <td>4.764</td>\n",
       "      <td>3.748</td>\n",
       "      <td>5.807</td>\n",
       "      <td>2.900</td>\n",
       "      <td>8.817</td>\n",
       "      <td>3.828</td>\n",
       "      <td>2.291</td>\n",
       "      <td>20.222</td>\n",
       "      <td>4.697</td>\n",
       "      <td>38.473</td>\n",
       "      <td>4.847</td>\n",
       "      <td>2.823</td>\n",
       "      <td>5.807</td>\n",
       "      <td>105.525</td>\n",
       "      <td>24.901</td>\n",
       "      <td>23.157</td>\n",
       "      <td>3.494</td>\n",
       "      <td>11.577</td>\n",
       "      <td>4.105</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.628</td>\n",
       "      <td>52.895</td>\n",
       "      <td>0.000</td>\n",
       "      <td>81.316</td>\n",
       "      <td>76.455</td>\n",
       "      <td>50.384</td>\n",
       "      <td>0.000</td>\n",
       "      <td>55.555</td>\n",
       "      <td>139.914</td>\n",
       "      <td>112.859</td>\n",
       "      <td>38.391</td>\n",
       "      <td>1.747</td>\n",
       "      <td>6.925</td>\n",
       "      <td>1.664</td>\n",
       "      <td>0.139</td>\n",
       "      <td>5.275</td>\n",
       "      <td>16.342</td>\n",
       "      <td>8.150</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>35.322</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.890</td>\n",
       "      <td>1.171</td>\n",
       "      <td>4.160</td>\n",
       "      <td>1.552</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.183</td>\n",
       "      <td>0.073</td>\n",
       "      <td>3.770</td>\n",
       "      <td>4.101</td>\n",
       "      <td>0.484</td>\n",
       "      <td>4.895</td>\n",
       "      <td>1.890</td>\n",
       "      <td>1.385</td>\n",
       "      <td>7.496</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.002</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.797</td>\n",
       "      <td>400.694</td>\n",
       "      <td>73.254</td>\n",
       "      <td>0.226</td>\n",
       "      <td>14.530</td>\n",
       "      <td>0.870</td>\n",
       "      <td>0.095</td>\n",
       "      <td>6.738</td>\n",
       "      <td>0.344</td>\n",
       "      <td>27.018</td>\n",
       "      <td>3.625</td>\n",
       "      <td>1.183</td>\n",
       "      <td>0.955</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.036</td>\n",
       "      <td>15.762</td>\n",
       "      <td>259.973</td>\n",
       "      <td>0.567</td>\n",
       "      <td>4.980</td>\n",
       "      <td>0.088</td>\n",
       "      <td>2.090</td>\n",
       "      <td>0.038</td>\n",
       "      <td>1.884</td>\n",
       "      <td>15.466</td>\n",
       "      <td>530.703</td>\n",
       "      <td>1.983</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.242</td>\n",
       "      <td>2.568</td>\n",
       "      <td>0.075</td>\n",
       "      <td>1.408</td>\n",
       "      <td>11.502</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>46.185</td>\n",
       "      <td>0.498</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.003</td>\n",
       "      <td>2.307</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.003</td>\n",
       "      <td>44.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3,011.490</td>\n",
       "      <td>2,499.405</td>\n",
       "      <td>2,201.067</td>\n",
       "      <td>1,285.214</td>\n",
       "      <td>1.317</td>\n",
       "      <td>101.512</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.462</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.966</td>\n",
       "      <td>199.536</td>\n",
       "      <td>8.967</td>\n",
       "      <td>412.219</td>\n",
       "      <td>9.852</td>\n",
       "      <td>0.973</td>\n",
       "      <td>189.664</td>\n",
       "      <td>12.500</td>\n",
       "      <td>1.406</td>\n",
       "      <td>-5,523.250</td>\n",
       "      <td>2,664.000</td>\n",
       "      <td>-3,820.750</td>\n",
       "      <td>-78.750</td>\n",
       "      <td>1.283</td>\n",
       "      <td>1.986</td>\n",
       "      <td>7.265</td>\n",
       "      <td>69.156</td>\n",
       "      <td>2.378</td>\n",
       "      <td>0.187</td>\n",
       "      <td>3.431</td>\n",
       "      <td>85.135</td>\n",
       "      <td>8.770</td>\n",
       "      <td>50.396</td>\n",
       "      <td>64.166</td>\n",
       "      <td>49.604</td>\n",
       "      <td>66.232</td>\n",
       "      <td>86.821</td>\n",
       "      <td>118.399</td>\n",
       "      <td>78.290</td>\n",
       "      <td>3.074</td>\n",
       "      <td>353.721</td>\n",
       "      <td>10.035</td>\n",
       "      <td>136.400</td>\n",
       "      <td>733.450</td>\n",
       "      <td>1.251</td>\n",
       "      <td>140.008</td>\n",
       "      <td>631.371</td>\n",
       "      <td>183.318</td>\n",
       "      <td>4.596</td>\n",
       "      <td>4.843</td>\n",
       "      <td>2,854.000</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.949</td>\n",
       "      <td>4.573</td>\n",
       "      <td>0.947</td>\n",
       "      <td>353.799</td>\n",
       "      <td>10.437</td>\n",
       "      <td>116.212</td>\n",
       "      <td>13.246</td>\n",
       "      <td>20.021</td>\n",
       "      <td>26.261</td>\n",
       "      <td>706.454</td>\n",
       "      <td>0.978</td>\n",
       "      <td>147.597</td>\n",
       "      <td>619.033</td>\n",
       "      <td>102.604</td>\n",
       "      <td>152.297</td>\n",
       "      <td>466.082</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>0.008</td>\n",
       "      <td>7.467</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.987</td>\n",
       "      <td>1,809.249</td>\n",
       "      <td>0.190</td>\n",
       "      <td>8,825.435</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.981</td>\n",
       "      <td>101.482</td>\n",
       "      <td>231.201</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.946</td>\n",
       "      <td>0.000</td>\n",
       "      <td>750.861</td>\n",
       "      <td>0.991</td>\n",
       "      <td>58.549</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.969</td>\n",
       "      <td>6.314</td>\n",
       "      <td>15.790</td>\n",
       "      <td>3.877</td>\n",
       "      <td>15.830</td>\n",
       "      <td>15.780</td>\n",
       "      <td>1.144</td>\n",
       "      <td>2.735</td>\n",
       "      <td>0.654</td>\n",
       "      <td>3.195</td>\n",
       "      <td>-0.142</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.998</td>\n",
       "      <td>2.312</td>\n",
       "      <td>1,004.050</td>\n",
       "      <td>38.903</td>\n",
       "      <td>109.000</td>\n",
       "      <td>134.600</td>\n",
       "      <td>117.700</td>\n",
       "      <td>55.900</td>\n",
       "      <td>339.561</td>\n",
       "      <td>0.236</td>\n",
       "      <td>6.260</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.016</td>\n",
       "      <td>7.917</td>\n",
       "      <td>5.951</td>\n",
       "      <td>10.994</td>\n",
       "      <td>0.469</td>\n",
       "      <td>0.011</td>\n",
       "      <td>7.513</td>\n",
       "      <td>0.320</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.035</td>\n",
       "      <td>1,020.300</td>\n",
       "      <td>623.000</td>\n",
       "      <td>438.000</td>\n",
       "      <td>2,614.000</td>\n",
       "      <td>1,784.000</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.184</td>\n",
       "      <td>2.600</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.686</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.578</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.299</td>\n",
       "      <td>18.690</td>\n",
       "      <td>0.524</td>\n",
       "      <td>10.170</td>\n",
       "      <td>27.200</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.735</td>\n",
       "      <td>17.865</td>\n",
       "      <td>40.210</td>\n",
       "      <td>0.259</td>\n",
       "      <td>6.780</td>\n",
       "      <td>19.370</td>\n",
       "      <td>0.424</td>\n",
       "      <td>8.570</td>\n",
       "      <td>17.235</td>\n",
       "      <td>6.760</td>\n",
       "      <td>8.462</td>\n",
       "      <td>30.097</td>\n",
       "      <td>0.158</td>\n",
       "      <td>7.740</td>\n",
       "      <td>0.000</td>\n",
       "      <td>19.720</td>\n",
       "      <td>73.248</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.062</td>\n",
       "      <td>3.631</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.002</td>\n",
       "      <td>119.436</td>\n",
       "      <td>0.040</td>\n",
       "      <td>967.300</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.185</td>\n",
       "      <td>3.673</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>103.094</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.865</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.071</td>\n",
       "      <td>17.977</td>\n",
       "      <td>3.704</td>\n",
       "      <td>28.773</td>\n",
       "      <td>45.676</td>\n",
       "      <td>40.019</td>\n",
       "      <td>19.581</td>\n",
       "      <td>110.601</td>\n",
       "      <td>0.078</td>\n",
       "      <td>2.083</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.005</td>\n",
       "      <td>2.658</td>\n",
       "      <td>1.875</td>\n",
       "      <td>3.360</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.549</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.011</td>\n",
       "      <td>317.867</td>\n",
       "      <td>278.672</td>\n",
       "      <td>195.826</td>\n",
       "      <td>1,202.412</td>\n",
       "      <td>820.099</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.121</td>\n",
       "      <td>5.832</td>\n",
       "      <td>0.163</td>\n",
       "      <td>2.899</td>\n",
       "      <td>8.389</td>\n",
       "      <td>0.040</td>\n",
       "      <td>2.078</td>\n",
       "      <td>5.459</td>\n",
       "      <td>12.505</td>\n",
       "      <td>0.085</td>\n",
       "      <td>2.063</td>\n",
       "      <td>5.980</td>\n",
       "      <td>0.129</td>\n",
       "      <td>2.514</td>\n",
       "      <td>9.074</td>\n",
       "      <td>2.054</td>\n",
       "      <td>2.561</td>\n",
       "      <td>9.474</td>\n",
       "      <td>0.046</td>\n",
       "      <td>2.377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.006</td>\n",
       "      <td>23.215</td>\n",
       "      <td>5.567</td>\n",
       "      <td>3.046</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.029</td>\n",
       "      <td>1.255</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.001</td>\n",
       "      <td>39.696</td>\n",
       "      <td>0.013</td>\n",
       "      <td>309.832</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.373</td>\n",
       "      <td>1.106</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>32.531</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.024</td>\n",
       "      <td>5.920</td>\n",
       "      <td>1.240</td>\n",
       "      <td>4.922</td>\n",
       "      <td>4.490</td>\n",
       "      <td>4.733</td>\n",
       "      <td>2.548</td>\n",
       "      <td>26.157</td>\n",
       "      <td>20.255</td>\n",
       "      <td>6.177</td>\n",
       "      <td>3.234</td>\n",
       "      <td>7.396</td>\n",
       "      <td>302.178</td>\n",
       "      <td>272.449</td>\n",
       "      <td>1.645</td>\n",
       "      <td>3.943</td>\n",
       "      <td>69.905</td>\n",
       "      <td>2.667</td>\n",
       "      <td>4.764</td>\n",
       "      <td>1.135</td>\n",
       "      <td>3.941</td>\n",
       "      <td>2.534</td>\n",
       "      <td>3.454</td>\n",
       "      <td>11.106</td>\n",
       "      <td>16.381</td>\n",
       "      <td>57.969</td>\n",
       "      <td>151.116</td>\n",
       "      <td>10.198</td>\n",
       "      <td>4.551</td>\n",
       "      <td>2.764</td>\n",
       "      <td>3.781</td>\n",
       "      <td>49.091</td>\n",
       "      <td>65.438</td>\n",
       "      <td>12.086</td>\n",
       "      <td>0.808</td>\n",
       "      <td>1.265</td>\n",
       "      <td>0.643</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.651</td>\n",
       "      <td>1.164</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.251</td>\n",
       "      <td>5.271</td>\n",
       "      <td>5.227</td>\n",
       "      <td>7.425</td>\n",
       "      <td>3.724</td>\n",
       "      <td>11.351</td>\n",
       "      <td>4.793</td>\n",
       "      <td>2.830</td>\n",
       "      <td>26.168</td>\n",
       "      <td>5.645</td>\n",
       "      <td>150.340</td>\n",
       "      <td>5.472</td>\n",
       "      <td>4.061</td>\n",
       "      <td>7.396</td>\n",
       "      <td>138.255</td>\n",
       "      <td>34.247</td>\n",
       "      <td>32.820</td>\n",
       "      <td>4.276</td>\n",
       "      <td>15.974</td>\n",
       "      <td>5.242</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.184</td>\n",
       "      <td>70.434</td>\n",
       "      <td>293.519</td>\n",
       "      <td>148.317</td>\n",
       "      <td>138.775</td>\n",
       "      <td>112.953</td>\n",
       "      <td>249.927</td>\n",
       "      <td>112.275</td>\n",
       "      <td>348.529</td>\n",
       "      <td>219.487</td>\n",
       "      <td>48.557</td>\n",
       "      <td>2.251</td>\n",
       "      <td>8.009</td>\n",
       "      <td>2.529</td>\n",
       "      <td>0.233</td>\n",
       "      <td>6.608</td>\n",
       "      <td>22.039</td>\n",
       "      <td>10.907</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>46.986</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.154</td>\n",
       "      <td>1.589</td>\n",
       "      <td>5.833</td>\n",
       "      <td>2.221</td>\n",
       "      <td>0.000</td>\n",
       "      <td>13.743</td>\n",
       "      <td>0.100</td>\n",
       "      <td>4.877</td>\n",
       "      <td>5.134</td>\n",
       "      <td>1.550</td>\n",
       "      <td>6.411</td>\n",
       "      <td>3.055</td>\n",
       "      <td>1.786</td>\n",
       "      <td>9.459</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.003</td>\n",
       "      <td>7.116</td>\n",
       "      <td>0.911</td>\n",
       "      <td>403.122</td>\n",
       "      <td>74.084</td>\n",
       "      <td>0.471</td>\n",
       "      <td>16.340</td>\n",
       "      <td>1.150</td>\n",
       "      <td>0.198</td>\n",
       "      <td>7.428</td>\n",
       "      <td>0.479</td>\n",
       "      <td>54.442</td>\n",
       "      <td>4.067</td>\n",
       "      <td>1.530</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.059</td>\n",
       "      <td>29.731</td>\n",
       "      <td>264.272</td>\n",
       "      <td>0.651</td>\n",
       "      <td>5.160</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2.150</td>\n",
       "      <td>0.049</td>\n",
       "      <td>2.000</td>\n",
       "      <td>16.988</td>\n",
       "      <td>532.398</td>\n",
       "      <td>2.119</td>\n",
       "      <td>8.650</td>\n",
       "      <td>0.293</td>\n",
       "      <td>2.976</td>\n",
       "      <td>0.089</td>\n",
       "      <td>1.625</td>\n",
       "      <td>13.818</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>72.289</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.004</td>\n",
       "      <td>2.758</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.005</td>\n",
       "      <td>71.900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3,056.650</td>\n",
       "      <td>2,538.823</td>\n",
       "      <td>2,218.055</td>\n",
       "      <td>1,591.224</td>\n",
       "      <td>1.526</td>\n",
       "      <td>104.587</td>\n",
       "      <td>0.124</td>\n",
       "      <td>1.517</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.971</td>\n",
       "      <td>202.007</td>\n",
       "      <td>10.862</td>\n",
       "      <td>419.089</td>\n",
       "      <td>10.128</td>\n",
       "      <td>0.977</td>\n",
       "      <td>192.189</td>\n",
       "      <td>12.547</td>\n",
       "      <td>1.415</td>\n",
       "      <td>-5,356.250</td>\n",
       "      <td>2,841.750</td>\n",
       "      <td>-3,352.750</td>\n",
       "      <td>1,377.250</td>\n",
       "      <td>1.304</td>\n",
       "      <td>2.003</td>\n",
       "      <td>7.330</td>\n",
       "      <td>72.267</td>\n",
       "      <td>2.656</td>\n",
       "      <td>0.207</td>\n",
       "      <td>3.531</td>\n",
       "      <td>85.742</td>\n",
       "      <td>9.061</td>\n",
       "      <td>50.579</td>\n",
       "      <td>64.345</td>\n",
       "      <td>49.748</td>\n",
       "      <td>66.343</td>\n",
       "      <td>87.002</td>\n",
       "      <td>118.940</td>\n",
       "      <td>80.200</td>\n",
       "      <td>3.521</td>\n",
       "      <td>360.772</td>\n",
       "      <td>10.152</td>\n",
       "      <td>142.098</td>\n",
       "      <td>741.455</td>\n",
       "      <td>1.340</td>\n",
       "      <td>143.196</td>\n",
       "      <td>638.136</td>\n",
       "      <td>206.977</td>\n",
       "      <td>4.617</td>\n",
       "      <td>4.869</td>\n",
       "      <td>2,874.000</td>\n",
       "      <td>0.933</td>\n",
       "      <td>0.952</td>\n",
       "      <td>4.669</td>\n",
       "      <td>4.385</td>\n",
       "      <td>359.674</td>\n",
       "      <td>10.592</td>\n",
       "      <td>120.927</td>\n",
       "      <td>16.376</td>\n",
       "      <td>22.814</td>\n",
       "      <td>29.915</td>\n",
       "      <td>714.597</td>\n",
       "      <td>1.065</td>\n",
       "      <td>149.959</td>\n",
       "      <td>625.170</td>\n",
       "      <td>115.499</td>\n",
       "      <td>158.438</td>\n",
       "      <td>467.890</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.027</td>\n",
       "      <td>7.808</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.115</td>\n",
       "      <td>2.429</td>\n",
       "      <td>0.990</td>\n",
       "      <td>1,841.873</td>\n",
       "      <td>0.200</td>\n",
       "      <td>9,065.432</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.982</td>\n",
       "      <td>102.078</td>\n",
       "      <td>233.036</td>\n",
       "      <td>0.466</td>\n",
       "      <td>0.952</td>\n",
       "      <td>0.000</td>\n",
       "      <td>776.782</td>\n",
       "      <td>0.991</td>\n",
       "      <td>59.134</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.978</td>\n",
       "      <td>6.376</td>\n",
       "      <td>15.860</td>\n",
       "      <td>4.392</td>\n",
       "      <td>15.900</td>\n",
       "      <td>15.870</td>\n",
       "      <td>1.338</td>\n",
       "      <td>2.873</td>\n",
       "      <td>0.714</td>\n",
       "      <td>3.311</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.999</td>\n",
       "      <td>2.358</td>\n",
       "      <td>1,008.671</td>\n",
       "      <td>40.805</td>\n",
       "      <td>127.000</td>\n",
       "      <td>181.000</td>\n",
       "      <td>161.600</td>\n",
       "      <td>62.900</td>\n",
       "      <td>502.206</td>\n",
       "      <td>0.439</td>\n",
       "      <td>7.500</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.021</td>\n",
       "      <td>9.585</td>\n",
       "      <td>8.275</td>\n",
       "      <td>14.347</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.015</td>\n",
       "      <td>9.055</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.049</td>\n",
       "      <td>1,277.750</td>\n",
       "      <td>966.000</td>\n",
       "      <td>625.000</td>\n",
       "      <td>5,034.000</td>\n",
       "      <td>6,384.000</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.255</td>\n",
       "      <td>3.200</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.151</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.423</td>\n",
       "      <td>20.973</td>\n",
       "      <td>0.689</td>\n",
       "      <td>13.338</td>\n",
       "      <td>31.687</td>\n",
       "      <td>0.169</td>\n",
       "      <td>8.450</td>\n",
       "      <td>20.860</td>\n",
       "      <td>57.675</td>\n",
       "      <td>0.296</td>\n",
       "      <td>9.555</td>\n",
       "      <td>21.460</td>\n",
       "      <td>0.726</td>\n",
       "      <td>11.460</td>\n",
       "      <td>20.163</td>\n",
       "      <td>9.490</td>\n",
       "      <td>11.953</td>\n",
       "      <td>33.506</td>\n",
       "      <td>0.231</td>\n",
       "      <td>9.940</td>\n",
       "      <td>0.000</td>\n",
       "      <td>22.370</td>\n",
       "      <td>90.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.099</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.086</td>\n",
       "      <td>4.405</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.005</td>\n",
       "      <td>144.503</td>\n",
       "      <td>0.061</td>\n",
       "      <td>1,261.300</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.003</td>\n",
       "      <td>1.762</td>\n",
       "      <td>4.480</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>131.758</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.795</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.481</td>\n",
       "      <td>0.092</td>\n",
       "      <td>24.653</td>\n",
       "      <td>4.379</td>\n",
       "      <td>31.702</td>\n",
       "      <td>59.595</td>\n",
       "      <td>54.277</td>\n",
       "      <td>22.097</td>\n",
       "      <td>162.038</td>\n",
       "      <td>0.145</td>\n",
       "      <td>2.514</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.007</td>\n",
       "      <td>3.146</td>\n",
       "      <td>2.607</td>\n",
       "      <td>4.311</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.005</td>\n",
       "      <td>3.025</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.015</td>\n",
       "      <td>403.989</td>\n",
       "      <td>428.555</td>\n",
       "      <td>273.953</td>\n",
       "      <td>2,341.289</td>\n",
       "      <td>3,190.616</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.116</td>\n",
       "      <td>1.046</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.332</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.160</td>\n",
       "      <td>6.548</td>\n",
       "      <td>0.218</td>\n",
       "      <td>4.021</td>\n",
       "      <td>9.481</td>\n",
       "      <td>0.050</td>\n",
       "      <td>2.633</td>\n",
       "      <td>6.345</td>\n",
       "      <td>17.925</td>\n",
       "      <td>0.096</td>\n",
       "      <td>2.791</td>\n",
       "      <td>6.550</td>\n",
       "      <td>0.210</td>\n",
       "      <td>3.360</td>\n",
       "      <td>10.042</td>\n",
       "      <td>2.785</td>\n",
       "      <td>3.405</td>\n",
       "      <td>10.440</td>\n",
       "      <td>0.067</td>\n",
       "      <td>2.985</td>\n",
       "      <td>0.000</td>\n",
       "      <td>6.885</td>\n",
       "      <td>28.873</td>\n",
       "      <td>6.825</td>\n",
       "      <td>4.086</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.042</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>47.079</td>\n",
       "      <td>0.019</td>\n",
       "      <td>412.330</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.541</td>\n",
       "      <td>1.387</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>42.652</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.148</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.032</td>\n",
       "      <td>8.585</td>\n",
       "      <td>1.417</td>\n",
       "      <td>5.787</td>\n",
       "      <td>5.937</td>\n",
       "      <td>6.458</td>\n",
       "      <td>2.853</td>\n",
       "      <td>38.140</td>\n",
       "      <td>29.307</td>\n",
       "      <td>7.571</td>\n",
       "      <td>4.011</td>\n",
       "      <td>9.169</td>\n",
       "      <td>524.002</td>\n",
       "      <td>582.935</td>\n",
       "      <td>2.215</td>\n",
       "      <td>4.784</td>\n",
       "      <td>92.911</td>\n",
       "      <td>3.471</td>\n",
       "      <td>6.883</td>\n",
       "      <td>1.540</td>\n",
       "      <td>4.769</td>\n",
       "      <td>3.609</td>\n",
       "      <td>4.756</td>\n",
       "      <td>17.423</td>\n",
       "      <td>21.765</td>\n",
       "      <td>120.173</td>\n",
       "      <td>305.026</td>\n",
       "      <td>12.754</td>\n",
       "      <td>5.823</td>\n",
       "      <td>3.822</td>\n",
       "      <td>4.679</td>\n",
       "      <td>66.667</td>\n",
       "      <td>84.973</td>\n",
       "      <td>15.796</td>\n",
       "      <td>0.928</td>\n",
       "      <td>1.578</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.748</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.351</td>\n",
       "      <td>5.913</td>\n",
       "      <td>6.902</td>\n",
       "      <td>9.577</td>\n",
       "      <td>4.342</td>\n",
       "      <td>14.388</td>\n",
       "      <td>6.089</td>\n",
       "      <td>3.309</td>\n",
       "      <td>35.279</td>\n",
       "      <td>6.387</td>\n",
       "      <td>335.922</td>\n",
       "      <td>6.006</td>\n",
       "      <td>7.007</td>\n",
       "      <td>9.720</td>\n",
       "      <td>168.410</td>\n",
       "      <td>47.728</td>\n",
       "      <td>45.169</td>\n",
       "      <td>4.742</td>\n",
       "      <td>23.737</td>\n",
       "      <td>6.704</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.625</td>\n",
       "      <td>93.120</td>\n",
       "      <td>514.586</td>\n",
       "      <td>262.865</td>\n",
       "      <td>294.667</td>\n",
       "      <td>288.893</td>\n",
       "      <td>501.607</td>\n",
       "      <td>397.506</td>\n",
       "      <td>510.647</td>\n",
       "      <td>377.144</td>\n",
       "      <td>61.495</td>\n",
       "      <td>2.840</td>\n",
       "      <td>9.079</td>\n",
       "      <td>3.199</td>\n",
       "      <td>0.563</td>\n",
       "      <td>7.897</td>\n",
       "      <td>32.438</td>\n",
       "      <td>14.469</td>\n",
       "      <td>536.205</td>\n",
       "      <td>505.401</td>\n",
       "      <td>64.249</td>\n",
       "      <td>555.294</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.760</td>\n",
       "      <td>1.933</td>\n",
       "      <td>10.972</td>\n",
       "      <td>2.904</td>\n",
       "      <td>0.000</td>\n",
       "      <td>17.809</td>\n",
       "      <td>0.133</td>\n",
       "      <td>6.451</td>\n",
       "      <td>6.329</td>\n",
       "      <td>2.212</td>\n",
       "      <td>7.594</td>\n",
       "      <td>3.947</td>\n",
       "      <td>2.458</td>\n",
       "      <td>11.238</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.003</td>\n",
       "      <td>8.021</td>\n",
       "      <td>1.286</td>\n",
       "      <td>407.431</td>\n",
       "      <td>78.397</td>\n",
       "      <td>0.850</td>\n",
       "      <td>19.035</td>\n",
       "      <td>1.370</td>\n",
       "      <td>0.358</td>\n",
       "      <td>8.637</td>\n",
       "      <td>0.562</td>\n",
       "      <td>74.629</td>\n",
       "      <td>4.703</td>\n",
       "      <td>1.816</td>\n",
       "      <td>1.001</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.089</td>\n",
       "      <td>44.113</td>\n",
       "      <td>265.707</td>\n",
       "      <td>0.769</td>\n",
       "      <td>7.800</td>\n",
       "      <td>0.186</td>\n",
       "      <td>3.099</td>\n",
       "      <td>0.075</td>\n",
       "      <td>2.971</td>\n",
       "      <td>24.772</td>\n",
       "      <td>534.356</td>\n",
       "      <td>2.291</td>\n",
       "      <td>10.130</td>\n",
       "      <td>0.367</td>\n",
       "      <td>3.493</td>\n",
       "      <td>0.112</td>\n",
       "      <td>1.902</td>\n",
       "      <td>17.081</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>116.539</td>\n",
       "      <td>0.502</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.004</td>\n",
       "      <td>3.295</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.006</td>\n",
       "      <td>114.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3,356.350</td>\n",
       "      <td>2,846.440</td>\n",
       "      <td>2,315.267</td>\n",
       "      <td>3,715.042</td>\n",
       "      <td>1,114.537</td>\n",
       "      <td>129.252</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1.656</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.985</td>\n",
       "      <td>272.045</td>\n",
       "      <td>19.547</td>\n",
       "      <td>824.927</td>\n",
       "      <td>102.868</td>\n",
       "      <td>0.985</td>\n",
       "      <td>215.598</td>\n",
       "      <td>12.990</td>\n",
       "      <td>1.453</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3,656.250</td>\n",
       "      <td>2,363.000</td>\n",
       "      <td>14,106.000</td>\n",
       "      <td>1.383</td>\n",
       "      <td>2.053</td>\n",
       "      <td>7.659</td>\n",
       "      <td>77.900</td>\n",
       "      <td>3.511</td>\n",
       "      <td>0.285</td>\n",
       "      <td>4.804</td>\n",
       "      <td>105.604</td>\n",
       "      <td>23.345</td>\n",
       "      <td>59.771</td>\n",
       "      <td>94.264</td>\n",
       "      <td>50.165</td>\n",
       "      <td>67.959</td>\n",
       "      <td>88.419</td>\n",
       "      <td>133.390</td>\n",
       "      <td>86.120</td>\n",
       "      <td>37.880</td>\n",
       "      <td>377.297</td>\n",
       "      <td>11.053</td>\n",
       "      <td>176.314</td>\n",
       "      <td>789.752</td>\n",
       "      <td>1.511</td>\n",
       "      <td>163.251</td>\n",
       "      <td>667.742</td>\n",
       "      <td>258.543</td>\n",
       "      <td>4.764</td>\n",
       "      <td>5.011</td>\n",
       "      <td>2,936.000</td>\n",
       "      <td>0.938</td>\n",
       "      <td>0.960</td>\n",
       "      <td>4.848</td>\n",
       "      <td>168.145</td>\n",
       "      <td>373.866</td>\n",
       "      <td>11.785</td>\n",
       "      <td>287.151</td>\n",
       "      <td>188.092</td>\n",
       "      <td>48.988</td>\n",
       "      <td>118.084</td>\n",
       "      <td>770.608</td>\n",
       "      <td>7,272.828</td>\n",
       "      <td>167.831</td>\n",
       "      <td>722.602</td>\n",
       "      <td>238.477</td>\n",
       "      <td>175.413</td>\n",
       "      <td>692.426</td>\n",
       "      <td>4.196</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.249</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.144</td>\n",
       "      <td>8.990</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.118</td>\n",
       "      <td>2.555</td>\n",
       "      <td>0.994</td>\n",
       "      <td>2,105.182</td>\n",
       "      <td>1.473</td>\n",
       "      <td>10,746.600</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>2.509</td>\n",
       "      <td>2.570</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.394</td>\n",
       "      <td>0.984</td>\n",
       "      <td>106.923</td>\n",
       "      <td>236.955</td>\n",
       "      <td>0.488</td>\n",
       "      <td>0.976</td>\n",
       "      <td>0.041</td>\n",
       "      <td>924.532</td>\n",
       "      <td>0.992</td>\n",
       "      <td>311.734</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.983</td>\n",
       "      <td>7.522</td>\n",
       "      <td>16.070</td>\n",
       "      <td>6.889</td>\n",
       "      <td>16.100</td>\n",
       "      <td>16.100</td>\n",
       "      <td>2.465</td>\n",
       "      <td>3.991</td>\n",
       "      <td>1.175</td>\n",
       "      <td>3.895</td>\n",
       "      <td>2.458</td>\n",
       "      <td>0.888</td>\n",
       "      <td>1.019</td>\n",
       "      <td>2.472</td>\n",
       "      <td>1,020.994</td>\n",
       "      <td>64.129</td>\n",
       "      <td>994.000</td>\n",
       "      <td>295.800</td>\n",
       "      <td>334.700</td>\n",
       "      <td>141.800</td>\n",
       "      <td>1,770.691</td>\n",
       "      <td>9,998.894</td>\n",
       "      <td>103.390</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.248</td>\n",
       "      <td>0.978</td>\n",
       "      <td>742.942</td>\n",
       "      <td>22.318</td>\n",
       "      <td>536.564</td>\n",
       "      <td>924.378</td>\n",
       "      <td>0.239</td>\n",
       "      <td>191.548</td>\n",
       "      <td>12.710</td>\n",
       "      <td>2.202</td>\n",
       "      <td>0.288</td>\n",
       "      <td>2,505.300</td>\n",
       "      <td>7,791.000</td>\n",
       "      <td>4,170.000</td>\n",
       "      <td>37,943.000</td>\n",
       "      <td>36,871.000</td>\n",
       "      <td>0.957</td>\n",
       "      <td>1.817</td>\n",
       "      <td>3.286</td>\n",
       "      <td>21.100</td>\n",
       "      <td>16.300</td>\n",
       "      <td>0.725</td>\n",
       "      <td>1.143</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.494</td>\n",
       "      <td>0.548</td>\n",
       "      <td>0.864</td>\n",
       "      <td>0.548</td>\n",
       "      <td>1.172</td>\n",
       "      <td>0.441</td>\n",
       "      <td>1.858</td>\n",
       "      <td>48.670</td>\n",
       "      <td>3.573</td>\n",
       "      <td>55.000</td>\n",
       "      <td>72.947</td>\n",
       "      <td>3.228</td>\n",
       "      <td>267.910</td>\n",
       "      <td>307.930</td>\n",
       "      <td>191.830</td>\n",
       "      <td>4.838</td>\n",
       "      <td>396.110</td>\n",
       "      <td>252.870</td>\n",
       "      <td>10.017</td>\n",
       "      <td>390.120</td>\n",
       "      <td>199.620</td>\n",
       "      <td>126.530</td>\n",
       "      <td>490.561</td>\n",
       "      <td>500.349</td>\n",
       "      <td>9,998.448</td>\n",
       "      <td>320.050</td>\n",
       "      <td>2.000</td>\n",
       "      <td>457.650</td>\n",
       "      <td>172.349</td>\n",
       "      <td>46.150</td>\n",
       "      <td>0.516</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.594</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.761</td>\n",
       "      <td>0.343</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.674</td>\n",
       "      <td>8.802</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.991</td>\n",
       "      <td>1,768.880</td>\n",
       "      <td>1.436</td>\n",
       "      <td>3,601.300</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>1.984</td>\n",
       "      <td>99.902</td>\n",
       "      <td>237.184</td>\n",
       "      <td>0.491</td>\n",
       "      <td>0.973</td>\n",
       "      <td>0.414</td>\n",
       "      <td>1,119.704</td>\n",
       "      <td>0.991</td>\n",
       "      <td>2,549.988</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.925</td>\n",
       "      <td>0.158</td>\n",
       "      <td>40.855</td>\n",
       "      <td>10.153</td>\n",
       "      <td>158.526</td>\n",
       "      <td>132.648</td>\n",
       "      <td>122.117</td>\n",
       "      <td>43.574</td>\n",
       "      <td>659.170</td>\n",
       "      <td>3,332.596</td>\n",
       "      <td>32.171</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.308</td>\n",
       "      <td>232.805</td>\n",
       "      <td>6.870</td>\n",
       "      <td>207.016</td>\n",
       "      <td>292.227</td>\n",
       "      <td>0.075</td>\n",
       "      <td>59.519</td>\n",
       "      <td>4.420</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.083</td>\n",
       "      <td>879.226</td>\n",
       "      <td>3,933.755</td>\n",
       "      <td>2,005.874</td>\n",
       "      <td>15,559.952</td>\n",
       "      <td>18,520.468</td>\n",
       "      <td>0.526</td>\n",
       "      <td>1.031</td>\n",
       "      <td>1.812</td>\n",
       "      <td>5.711</td>\n",
       "      <td>5.155</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.334</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.755</td>\n",
       "      <td>13.096</td>\n",
       "      <td>1.003</td>\n",
       "      <td>15.893</td>\n",
       "      <td>20.046</td>\n",
       "      <td>0.947</td>\n",
       "      <td>79.151</td>\n",
       "      <td>89.192</td>\n",
       "      <td>51.868</td>\n",
       "      <td>1.096</td>\n",
       "      <td>174.894</td>\n",
       "      <td>90.516</td>\n",
       "      <td>3.413</td>\n",
       "      <td>172.712</td>\n",
       "      <td>214.863</td>\n",
       "      <td>38.900</td>\n",
       "      <td>196.688</td>\n",
       "      <td>197.499</td>\n",
       "      <td>5,043.879</td>\n",
       "      <td>97.709</td>\n",
       "      <td>0.447</td>\n",
       "      <td>156.336</td>\n",
       "      <td>59.324</td>\n",
       "      <td>257.011</td>\n",
       "      <td>187.759</td>\n",
       "      <td>13.915</td>\n",
       "      <td>0.220</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.279</td>\n",
       "      <td>2.835</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.409</td>\n",
       "      <td>547.172</td>\n",
       "      <td>0.416</td>\n",
       "      <td>1,072.203</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.627</td>\n",
       "      <td>30.998</td>\n",
       "      <td>74.844</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.131</td>\n",
       "      <td>348.829</td>\n",
       "      <td>0.313</td>\n",
       "      <td>805.394</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.299</td>\n",
       "      <td>0.051</td>\n",
       "      <td>14.728</td>\n",
       "      <td>3.313</td>\n",
       "      <td>44.310</td>\n",
       "      <td>9.576</td>\n",
       "      <td>13.807</td>\n",
       "      <td>6.215</td>\n",
       "      <td>128.282</td>\n",
       "      <td>899.119</td>\n",
       "      <td>116.862</td>\n",
       "      <td>9.690</td>\n",
       "      <td>39.038</td>\n",
       "      <td>999.316</td>\n",
       "      <td>998.681</td>\n",
       "      <td>111.496</td>\n",
       "      <td>273.095</td>\n",
       "      <td>424.215</td>\n",
       "      <td>103.181</td>\n",
       "      <td>898.609</td>\n",
       "      <td>24.990</td>\n",
       "      <td>113.223</td>\n",
       "      <td>118.753</td>\n",
       "      <td>186.616</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>994.286</td>\n",
       "      <td>995.745</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>400.000</td>\n",
       "      <td>32.274</td>\n",
       "      <td>851.613</td>\n",
       "      <td>657.762</td>\n",
       "      <td>33.058</td>\n",
       "      <td>1.277</td>\n",
       "      <td>5.132</td>\n",
       "      <td>1.085</td>\n",
       "      <td>1.351</td>\n",
       "      <td>1.109</td>\n",
       "      <td>1.764</td>\n",
       "      <td>0.508</td>\n",
       "      <td>1.475</td>\n",
       "      <td>13.978</td>\n",
       "      <td>34.490</td>\n",
       "      <td>42.070</td>\n",
       "      <td>10.184</td>\n",
       "      <td>232.126</td>\n",
       "      <td>164.109</td>\n",
       "      <td>47.777</td>\n",
       "      <td>149.385</td>\n",
       "      <td>109.007</td>\n",
       "      <td>999.877</td>\n",
       "      <td>77.801</td>\n",
       "      <td>87.135</td>\n",
       "      <td>212.656</td>\n",
       "      <td>492.772</td>\n",
       "      <td>358.950</td>\n",
       "      <td>415.435</td>\n",
       "      <td>79.116</td>\n",
       "      <td>274.887</td>\n",
       "      <td>289.826</td>\n",
       "      <td>200.000</td>\n",
       "      <td>63.334</td>\n",
       "      <td>221.975</td>\n",
       "      <td>999.413</td>\n",
       "      <td>989.474</td>\n",
       "      <td>996.859</td>\n",
       "      <td>994.000</td>\n",
       "      <td>999.491</td>\n",
       "      <td>995.745</td>\n",
       "      <td>997.519</td>\n",
       "      <td>994.004</td>\n",
       "      <td>142.844</td>\n",
       "      <td>12.770</td>\n",
       "      <td>21.044</td>\n",
       "      <td>9.402</td>\n",
       "      <td>127.573</td>\n",
       "      <td>107.693</td>\n",
       "      <td>219.644</td>\n",
       "      <td>40.282</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>999.234</td>\n",
       "      <td>451.485</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>252.860</td>\n",
       "      <td>113.276</td>\n",
       "      <td>111.350</td>\n",
       "      <td>184.349</td>\n",
       "      <td>111.737</td>\n",
       "      <td>1,000.000</td>\n",
       "      <td>137.984</td>\n",
       "      <td>111.333</td>\n",
       "      <td>818.000</td>\n",
       "      <td>80.041</td>\n",
       "      <td>8.204</td>\n",
       "      <td>14.448</td>\n",
       "      <td>6.580</td>\n",
       "      <td>4.082</td>\n",
       "      <td>25.779</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.005</td>\n",
       "      <td>21.044</td>\n",
       "      <td>3.979</td>\n",
       "      <td>421.702</td>\n",
       "      <td>83.720</td>\n",
       "      <td>7.066</td>\n",
       "      <td>131.680</td>\n",
       "      <td>39.330</td>\n",
       "      <td>2.718</td>\n",
       "      <td>56.930</td>\n",
       "      <td>17.478</td>\n",
       "      <td>303.550</td>\n",
       "      <td>35.320</td>\n",
       "      <td>54.292</td>\n",
       "      <td>1.512</td>\n",
       "      <td>1.074</td>\n",
       "      <td>0.446</td>\n",
       "      <td>101.115</td>\n",
       "      <td>311.404</td>\n",
       "      <td>1.299</td>\n",
       "      <td>32.580</td>\n",
       "      <td>0.689</td>\n",
       "      <td>14.014</td>\n",
       "      <td>0.293</td>\n",
       "      <td>12.746</td>\n",
       "      <td>84.802</td>\n",
       "      <td>589.508</td>\n",
       "      <td>2.740</td>\n",
       "      <td>454.560</td>\n",
       "      <td>2.197</td>\n",
       "      <td>170.020</td>\n",
       "      <td>0.550</td>\n",
       "      <td>90.424</td>\n",
       "      <td>96.960</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.104</td>\n",
       "      <td>99.303</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.029</td>\n",
       "      <td>737.305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               1          2          3          4          5          7  \\\n",
       "count  1,561.000  1,560.000  1,553.000  1,553.000  1,553.000  1,553.000   \n",
       "mean   3,014.453  2,495.850  2,200.547  1,396.377      4.197    101.113   \n",
       "std       73.622     80.408     29.513    441.692     56.356      6.237   \n",
       "min    2,743.240  2,158.750  2,060.660      0.000      0.681     82.131   \n",
       "25%    2,966.260  2,452.248  2,181.044  1,081.876      1.018     97.920   \n",
       "50%    3,011.490  2,499.405  2,201.067  1,285.214      1.317    101.512   \n",
       "75%    3,056.650  2,538.823  2,218.055  1,591.224      1.526    104.587   \n",
       "max    3,356.350  2,846.440  2,315.267  3,715.042  1,114.537    129.252   \n",
       "\n",
       "               8          9         10         11         12         13  \\\n",
       "count  1,558.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       0.122      1.463     -0.001      0.000      0.964    199.957   \n",
       "std        0.009      0.074      0.015      0.009      0.012      3.257   \n",
       "min        0.000      1.191     -0.053     -0.035      0.655    182.094   \n",
       "25%        0.121      1.411     -0.011     -0.006      0.958    198.131   \n",
       "50%        0.122      1.462     -0.001      0.000      0.966    199.536   \n",
       "75%        0.124      1.517      0.008      0.006      0.971    202.007   \n",
       "max        0.129      1.656      0.075      0.053      0.985    272.045   \n",
       "\n",
       "              15         16         17         18         19         20  \\\n",
       "count  1,564.000  1,564.000  1,564.000  1,564.000  1,564.000  1,557.000   \n",
       "mean       9.005    413.086      9.908      0.971    190.047     12.481   \n",
       "std        2.797     17.221      2.404      0.012      2.781      0.218   \n",
       "min        2.249    333.449      4.470      0.579    169.177      9.877   \n",
       "25%        7.095    406.127      9.568      0.968    188.300     12.460   \n",
       "50%        8.967    412.219      9.852      0.973    189.664     12.500   \n",
       "75%       10.862    419.089     10.128      0.977    192.189     12.547   \n",
       "max       19.547    824.927    102.868      0.985    215.598     12.990   \n",
       "\n",
       "              21          22         23          24           25         26  \\\n",
       "count  1,567.000   1,565.000  1,565.000   1,565.000    1,565.000  1,565.000   \n",
       "mean       1.405  -5,618.394  2,699.378  -3,806.300     -298.598      1.204   \n",
       "std        0.017     626.822    295.499   1,380.162    2,902.690      0.178   \n",
       "min        1.180  -7,150.250      0.000  -9,986.750  -14,804.500      0.000   \n",
       "25%        1.397  -5,933.250  2,578.000  -4,371.750   -1,476.000      1.095   \n",
       "50%        1.406  -5,523.250  2,664.000  -3,820.750      -78.750      1.283   \n",
       "75%        1.415  -5,356.250  2,841.750  -3,352.750    1,377.250      1.304   \n",
       "max        1.453       0.000  3,656.250   2,363.000   14,106.000      1.383   \n",
       "\n",
       "              27         28         29         30         31         32  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       1.938      6.639     69.500      2.366      0.184      3.673   \n",
       "std        0.189      1.244      3.461      0.409      0.033      0.535   \n",
       "min        0.000      0.000     59.400      0.667      0.034      2.070   \n",
       "25%        1.907      5.264     67.378      2.089      0.162      3.363   \n",
       "50%        1.986      7.265     69.156      2.378      0.187      3.431   \n",
       "75%        2.003      7.330     72.267      2.656      0.207      3.531   \n",
       "max        2.053      7.659     77.900      3.511      0.285      4.804   \n",
       "\n",
       "              33         34         35         36         37         38  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean      85.337      8.960     50.583     64.556     49.417     66.221   \n",
       "std        2.027      1.344      1.183      2.575      1.183      0.304   \n",
       "min       83.183      7.603     49.835     63.677     40.229     64.919   \n",
       "25%       84.490      8.580     50.252     64.025     49.421     66.041   \n",
       "50%       85.135      8.770     50.396     64.166     49.604     66.232   \n",
       "75%       85.742      9.061     50.579     64.345     49.748     66.343   \n",
       "max      105.604     23.345     59.771     94.264     50.165     67.959   \n",
       "\n",
       "              39         40         41         42         44         45  \\\n",
       "count  1,566.000  1,566.000  1,543.000  1,543.000  1,566.000  1,566.000   \n",
       "mean      86.837    118.680     67.905      3.353    355.539     10.031   \n",
       "std        0.447      1.807     24.063      2.360      6.235      0.175   \n",
       "min       84.733    111.713      1.434     -0.076    342.755      9.464   \n",
       "25%       86.578    118.016     74.800      2.690    350.802      9.925   \n",
       "50%       86.821    118.399     78.290      3.074    353.721     10.035   \n",
       "75%       87.002    118.940     80.200      3.521    360.772     10.152   \n",
       "max       88.419    133.390     86.120     37.880    377.297     11.053   \n",
       "\n",
       "              46         47         48         49         51         52  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean     136.743    733.673      1.178    139.972    632.254    157.421   \n",
       "std        7.849     12.170      0.190      4.524      8.644     60.925   \n",
       "min      108.846    699.814      0.497    125.798    607.393     40.261   \n",
       "25%      130.729    724.442      0.985    136.927    625.928    115.509   \n",
       "50%      136.400    733.450      1.251    140.008    631.371    183.318   \n",
       "75%      142.098    741.455      1.340    143.196    638.136    206.977   \n",
       "max      176.314    789.752      1.511    163.251    667.742    258.543   \n",
       "\n",
       "              54         55         56         57         58         59  \\\n",
       "count  1,563.000  1,563.000  1,563.000  1,563.000  1,563.000  1,563.000   \n",
       "mean       4.593      4.839  2,856.172      0.929      0.949      4.593   \n",
       "std        0.055      0.060     25.749      0.007      0.004      0.085   \n",
       "min        3.706      3.932  2,801.000      0.875      0.932      4.220   \n",
       "25%        4.574      4.816  2,836.000      0.925      0.947      4.532   \n",
       "50%        4.596      4.843  2,854.000      0.931      0.949      4.573   \n",
       "75%        4.617      4.869  2,874.000      0.933      0.952      4.669   \n",
       "max        4.764      5.011  2,936.000      0.938      0.960      4.848   \n",
       "\n",
       "              60         61         62         63         64         65  \\\n",
       "count  1,560.000  1,561.000  1,561.000  1,561.000  1,560.000  1,560.000   \n",
       "mean       2.960    355.159     10.423    116.502     13.990     20.542   \n",
       "std        9.532      6.028      0.275      8.629      7.120      4.977   \n",
       "min      -28.988    324.714      9.461     81.490      1.659      6.448   \n",
       "25%       -1.872    350.596     10.283    112.023     10.364     17.365   \n",
       "50%        0.947    353.799     10.437    116.212     13.246     20.021   \n",
       "75%        4.385    359.674     10.592    120.927     16.376     22.814   \n",
       "max      168.145    373.866     11.785    287.151    188.092     48.988   \n",
       "\n",
       "              66         67         68         69         71         72  \\\n",
       "count  1,560.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean      27.132    706.669     16.715    147.438    619.102    104.329   \n",
       "std        7.122     11.623    307.502      4.240      9.539     31.652   \n",
       "min        4.308    632.423      0.414     87.025    581.777     21.433   \n",
       "25%       23.056    698.770      0.891    145.237    612.774     87.484   \n",
       "50%       26.261    706.454      0.978    147.597    619.033    102.604   \n",
       "75%       29.915    714.597      1.065    149.959    625.170    115.499   \n",
       "max      118.084    770.608  7,272.828    167.831    722.602    238.477   \n",
       "\n",
       "            73       74         75         76         77         78  \\\n",
       "count  773.000  773.000  1,561.000  1,543.000  1,543.000  1,543.000   \n",
       "mean   150.362  468.020      0.003     -0.007     -0.029     -0.007   \n",
       "std     18.388   17.630      0.106      0.022      0.033      0.031   \n",
       "min    -59.478  456.045      0.000     -0.105     -0.186     -0.105   \n",
       "25%    145.305  464.458      0.000     -0.020     -0.052     -0.029   \n",
       "50%    152.297  466.082      0.000     -0.006     -0.029     -0.010   \n",
       "75%    158.438  467.890      0.000      0.007     -0.006      0.009   \n",
       "max    175.413  692.426      4.196      0.232      0.072      0.133   \n",
       "\n",
       "              79         80         81         82         83         84  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,566.000   \n",
       "mean      -0.014      0.003     -0.019     -0.021      0.006      7.452   \n",
       "std        0.048      0.023      0.049      0.017      0.036      0.516   \n",
       "min       -0.348     -0.057     -0.144     -0.098     -0.213      5.826   \n",
       "25%       -0.048     -0.011     -0.044     -0.027     -0.018      7.104   \n",
       "50%       -0.013      0.001     -0.009     -0.020      0.008      7.467   \n",
       "75%        0.012      0.013      0.009     -0.012      0.027      7.808   \n",
       "max        0.249      0.101      0.119      0.058      0.144      8.990   \n",
       "\n",
       "              85       86         87         88         89         90  \\\n",
       "count  1,555.000  226.000  1,567.000  1,567.000  1,567.000  1,516.000   \n",
       "mean       0.133    0.113      2.402      0.982  1,807.815      0.189   \n",
       "std        0.005    0.003      0.037      0.013     53.537      0.052   \n",
       "min        0.117    0.105      2.243      0.775  1,627.471      0.111   \n",
       "25%        0.130    0.111      2.377      0.976  1,777.470      0.169   \n",
       "50%        0.133    0.114      2.404      0.987  1,809.249      0.190   \n",
       "75%        0.136    0.115      2.429      0.990  1,841.873      0.200   \n",
       "max        0.150    0.118      2.555      0.994  2,105.182      1.473   \n",
       "\n",
       "               91         92         93         94         95         96  \\\n",
       "count   1,516.000  1,561.000  1,565.000  1,565.000  1,561.000  1,561.000   \n",
       "mean    8,827.537      0.002      0.001     -0.001     -0.000      0.000   \n",
       "std       396.314      0.088      0.003      0.003      0.000      0.000   \n",
       "min     7,397.310     -0.357     -0.013     -0.017     -0.002     -0.001   \n",
       "25%     8,564.690     -0.043     -0.001     -0.002     -0.000      0.000   \n",
       "50%     8,825.435      0.000      0.000     -0.000      0.000      0.000   \n",
       "75%     9,065.432      0.051      0.002      0.001      0.000      0.000   \n",
       "max    10,746.600      0.363      0.028      0.013      0.001      0.001   \n",
       "\n",
       "              97         99        100        101        102        103  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       0.017     -0.018      0.002     -0.000     -0.000      0.001   \n",
       "std        0.220      0.427      0.063      0.000      0.000      0.063   \n",
       "min       -1.480     -5.272     -0.528     -0.003     -0.002     -0.535   \n",
       "25%       -0.089     -0.219     -0.030     -0.000     -0.000     -0.036   \n",
       "50%        0.004      0.000      0.000      0.000      0.000      0.000   \n",
       "75%        0.122      0.189      0.030      0.000      0.000      0.034   \n",
       "max        2.509      2.570      0.885      0.002      0.002      0.298   \n",
       "\n",
       "             104        105        106        107        108        109  \\\n",
       "count  1,565.000  1,565.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean      -0.010     -0.000     -0.000      0.001     -0.002     -0.011   \n",
       "std        0.003      0.001      0.003      0.003      0.087      0.087   \n",
       "min       -0.033     -0.012     -0.028     -0.013     -0.523     -0.345   \n",
       "25%       -0.012     -0.000     -0.002     -0.001     -0.049     -0.065   \n",
       "50%       -0.010      0.000     -0.000      0.000      0.000     -0.011   \n",
       "75%       -0.008      0.000      0.001      0.002      0.049      0.038   \n",
       "max        0.020      0.007      0.013      0.017      0.486      0.394   \n",
       "\n",
       "           110      111      112      113        114        115        116  \\\n",
       "count  549.000  549.000  549.000  852.000  1,567.000  1,567.000  1,567.000   \n",
       "mean     0.980  101.318  231.819    0.458      0.945      0.000    747.384   \n",
       "std      0.009    1.880    2.105    0.049      0.012      0.002     48.949   \n",
       "min      0.785   88.194  213.008    0.000      0.853      0.000    544.025   \n",
       "25%      0.979  100.389  230.374    0.459      0.939      0.000    721.023   \n",
       "50%      0.981  101.482  231.201    0.463      0.946      0.000    750.861   \n",
       "75%      0.982  102.078  233.036    0.466      0.952      0.000    776.782   \n",
       "max      0.984  106.923  236.955    0.488      0.976      0.041    924.532   \n",
       "\n",
       "             117        118        119        120        121        122  \\\n",
       "count  1,567.000  1,567.000  1,543.000  1,567.000  1,567.000  1,558.000   \n",
       "mean       0.987     58.626      0.598      0.971      6.311     15.796   \n",
       "std        0.009      6.485      0.008      0.009      0.124      0.100   \n",
       "min        0.890     52.807      0.527      0.841      5.126     15.460   \n",
       "25%        0.990     57.978      0.594      0.965      6.246     15.730   \n",
       "50%        0.991     58.549      0.599      0.969      6.314     15.790   \n",
       "75%        0.991     59.134      0.603      0.978      6.376     15.860   \n",
       "max        0.992    311.734      0.625      0.983      7.522     16.070   \n",
       "\n",
       "             123        124        125        126        127        128  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000  1,558.000   \n",
       "mean       3.898     15.830     15.795      1.185      2.751      0.648   \n",
       "std        0.904      0.108      0.114      0.281      0.253      0.135   \n",
       "min        1.671     15.170     15.430      0.312      2.340      0.316   \n",
       "25%        3.202     15.762     15.723      0.974      2.572      0.549   \n",
       "50%        3.877     15.830     15.780      1.144      2.735      0.654   \n",
       "75%        4.392     15.900     15.870      1.338      2.873      0.714   \n",
       "max        6.889     16.100     16.100      2.465      3.991      1.175   \n",
       "\n",
       "             129        130        131        132        133        134  \\\n",
       "count  1,558.000  1,558.000  1,558.000  1,558.000  1,559.000  1,559.000   \n",
       "mean       3.192     -0.554      0.745      0.998      2.319  1,004.043   \n",
       "std        0.264      1.220      0.083      0.002      0.053      6.538   \n",
       "min        0.000     -3.779      0.420      0.994      2.191    980.451   \n",
       "25%        3.074     -0.899      0.689      0.996      2.277    999.996   \n",
       "50%        3.195     -0.142      0.759      0.998      2.312  1,004.050   \n",
       "75%        3.311      0.047      0.815      0.999      2.358  1,008.671   \n",
       "max        3.895      2.458      0.888      1.019      2.472  1,020.994   \n",
       "\n",
       "             135        136        137        138        139        140  \\\n",
       "count  1,559.000  1,562.000  1,561.000  1,560.000  1,553.000  1,553.000   \n",
       "mean      39.392    117.961    138.195    122.693     57.603    416.767   \n",
       "std        2.990     57.545     53.910     52.253     12.345    263.301   \n",
       "min       33.366     58.000     36.100     19.200     19.800      0.000   \n",
       "25%       37.347     92.000     90.000     81.300     50.900    243.786   \n",
       "50%       38.903    109.000    134.600    117.700     55.900    339.561   \n",
       "75%       40.805    127.000    181.000    161.600     62.900    502.206   \n",
       "max       64.129    994.000    295.800    334.700    141.800  1,770.691   \n",
       "\n",
       "             141        143        144        145        146        147  \\\n",
       "count  1,553.000  1,553.000  1,558.000  1,565.000  1,565.000  1,565.000   \n",
       "mean      26.078      6.642      0.004      0.120      0.064      0.055   \n",
       "std      506.922      3.552      0.001      0.061      0.027      0.022   \n",
       "min        0.032      1.740      0.000      0.032      0.021      0.023   \n",
       "25%        0.132      5.110      0.003      0.084      0.048      0.042   \n",
       "50%        0.236      6.260      0.004      0.107      0.059      0.050   \n",
       "75%        0.439      7.500      0.005      0.133      0.072      0.061   \n",
       "max    9,998.894    103.390      0.012      0.625      0.251      0.248   \n",
       "\n",
       "             148        149        151        152        153        154  \\\n",
       "count  1,565.000  1,565.000  1,564.000  1,564.000  1,564.000  1,564.000   \n",
       "mean       0.017      8.471      6.814     14.047      1.197      0.012   \n",
       "std        0.027     18.741      3.242     31.003     23.364      0.009   \n",
       "min        0.004      1.421      1.337      2.020      0.154      0.004   \n",
       "25%        0.010      6.360      4.459      8.090      0.374      0.007   \n",
       "50%        0.016      7.917      5.951     10.994      0.469      0.011   \n",
       "75%        0.021      9.585      8.275     14.347      0.680      0.015   \n",
       "max        0.978    742.942     22.318    536.564    924.378      0.239   \n",
       "\n",
       "             155        156        157      158        159        160  \\\n",
       "count  1,564.000  1,557.000  1,567.000  138.000    138.000  1,565.000   \n",
       "mean       7.698      0.507      0.058    0.047  1,039.651    882.681   \n",
       "std        5.239      1.122      0.079    0.040    406.849    983.043   \n",
       "min        1.244      0.140      0.011    0.012    234.100      0.000   \n",
       "25%        5.927      0.240      0.036    0.027    721.675    411.000   \n",
       "50%        7.513      0.320      0.049    0.035  1,020.300    623.000   \n",
       "75%        9.055      0.450      0.067    0.049  1,277.750    966.000   \n",
       "max      191.548     12.710      2.202    0.288  2,505.300  7,791.000   \n",
       "\n",
       "             161         162         163        164        165        166  \\\n",
       "count  1,565.000   1,565.000   1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean     555.346   4,066.850   4,797.155      0.140      0.128      0.252   \n",
       "std      574.809   4,239.245   6,553.569      0.122      0.243      0.407   \n",
       "min        0.000       0.000       0.000      0.000      0.000      0.000   \n",
       "25%      295.000   1,321.000     451.000      0.091      0.068      0.132   \n",
       "50%      438.000   2,614.000   1,784.000      0.120      0.089      0.184   \n",
       "75%      625.000   5,034.000   6,384.000      0.154      0.116      0.255   \n",
       "max    4,170.000  37,943.000  36,871.000      0.957      1.817      3.286   \n",
       "\n",
       "             167        168        169        170        171        172  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,566.000  1,566.000   \n",
       "mean       2.789      1.236      0.124      0.400      0.684      0.120   \n",
       "std        1.120      0.633      0.048      0.198      0.157      0.061   \n",
       "min        0.800      0.300      0.033      0.046      0.298      0.009   \n",
       "25%        2.100      0.900      0.090      0.230      0.576      0.080   \n",
       "50%        2.600      1.200      0.119      0.412      0.686      0.113   \n",
       "75%        3.200      1.500      0.151      0.536      0.797      0.140   \n",
       "max       21.100     16.300      0.725      1.143      1.153      0.494   \n",
       "\n",
       "             173        174        175        176        177        178  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.320      0.576      0.320      0.778      0.245      0.395   \n",
       "std        0.071      0.096      0.071      0.116      0.075      0.283   \n",
       "min        0.129      0.254      0.129      0.462      0.073      0.047   \n",
       "25%        0.277      0.517      0.277      0.692      0.196      0.222   \n",
       "50%        0.324      0.578      0.324      0.768      0.243      0.299   \n",
       "75%        0.370      0.634      0.370      0.844      0.294      0.423   \n",
       "max        0.548      0.864      0.548      1.172      0.441      1.858   \n",
       "\n",
       "             181        182        183        184        185        186  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean      19.013      0.547     10.781     26.661      0.145      7.366   \n",
       "std        3.312      0.224      4.164      6.836      0.110      7.189   \n",
       "min        9.400      0.093      3.170      5.014      0.030      1.940   \n",
       "25%       16.850      0.378      7.732     21.171      0.102      5.390   \n",
       "50%       18.690      0.524     10.170     27.200      0.133      6.735   \n",
       "75%       20.973      0.689     13.338     31.687      0.169      8.450   \n",
       "max       48.670      3.573     55.000     72.947      3.228    267.910   \n",
       "\n",
       "             188        189        196        197        198        199  \\\n",
       "count  1,566.000  1,566.000  1,563.000  1,560.000  1,561.000  1,561.000   \n",
       "mean      17.936     43.211      0.287      8.688     20.093      0.557   \n",
       "std        8.610     21.712      0.395     15.721     10.552      0.538   \n",
       "min        6.220      6.613      0.080      1.750      9.220      0.090   \n",
       "25%       14.505     24.711      0.218      5.040     17.130      0.296   \n",
       "50%       17.865     40.210      0.259      6.780     19.370      0.424   \n",
       "75%       20.860     57.675      0.296      9.555     21.460      0.726   \n",
       "max      307.930    191.830      4.838    396.110    252.870     10.017   \n",
       "\n",
       "             200        201        202        203        204        205  \\\n",
       "count  1,561.000  1,560.000  1,560.000  1,560.000  1,561.000  1,561.000   \n",
       "mean      11.532     17.600      7.839     10.170     30.073     32.218   \n",
       "std       16.446      8.691      5.104     14.623     17.462    565.101   \n",
       "min        2.770      3.210      0.000      0.000      7.728      0.043   \n",
       "25%        6.740     14.155      5.020      6.094     24.653      0.114   \n",
       "50%        8.570     17.235      6.760      8.462     30.097      0.158   \n",
       "75%       11.460     20.163      9.490     11.953     33.506      0.231   \n",
       "max      390.120    199.620    126.530    490.561    500.349  9,998.448   \n",
       "\n",
       "             206        207        208        209        210        211  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,543.000   \n",
       "mean       9.050      0.001     20.376     73.264      0.030      0.089   \n",
       "std       11.541      0.051     17.498     28.067      1.168      0.042   \n",
       "min        2.300      0.000      4.010      5.359      0.000      0.032   \n",
       "25%        6.040      0.000     16.350     56.158      0.000      0.066   \n",
       "50%        7.740      0.000     19.720     73.248      0.000      0.080   \n",
       "75%        9.940      0.000     22.370     90.515      0.000      0.099   \n",
       "max      320.050      2.000    457.650    172.349     46.150      0.516   \n",
       "\n",
       "             212        213        214        215        216        217  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean       0.057      0.051      0.060      0.083      0.081      0.083   \n",
       "std        0.025      0.032      0.053      0.056      0.030      0.026   \n",
       "min        0.002      0.007      0.004      0.019      0.006      0.010   \n",
       "25%        0.044      0.033      0.036      0.057      0.063      0.070   \n",
       "50%        0.053      0.042      0.056      0.075      0.083      0.085   \n",
       "75%        0.064      0.062      0.074      0.094      0.098      0.098   \n",
       "max        0.323      0.594      1.284      0.761      0.343      0.283   \n",
       "\n",
       "             218        219        220      221        222        223  \\\n",
       "count  1,543.000  1,566.000  1,555.000  226.000  1,567.000  1,567.000   \n",
       "mean       0.072      3.771      0.003    0.009      0.061      0.009   \n",
       "std        0.046      1.170      0.002    0.002      0.023      0.056   \n",
       "min        0.008      1.034      0.001    0.006      0.020      0.000   \n",
       "25%        0.046      2.946      0.002    0.008      0.040      0.001   \n",
       "50%        0.062      3.631      0.003    0.009      0.061      0.002   \n",
       "75%        0.086      4.405      0.004    0.010      0.076      0.005   \n",
       "max        0.674      8.802      0.016    0.024      0.231      0.991   \n",
       "\n",
       "             224        225        226        228        229        239  \\\n",
       "count  1,567.000  1,516.000  1,516.000  1,565.000  1,565.000  1,565.000   \n",
       "mean     122.847      0.059  1,041.057      0.019      0.018      0.005   \n",
       "std       55.156      0.071    433.170      0.011      0.011      0.002   \n",
       "min       32.264      0.009    168.800      0.006      0.007      0.001   \n",
       "25%       95.147      0.030    718.725      0.013      0.013      0.004   \n",
       "50%      119.436      0.040    967.300      0.017      0.015      0.005   \n",
       "75%      144.503      0.061  1,261.300      0.021      0.020      0.006   \n",
       "max    1,768.880      1.436  3,601.300      0.154      0.213      0.024   \n",
       "\n",
       "             240      245      246      247      248        249        250  \\\n",
       "count  1,565.000  549.000  549.000  549.000  852.000  1,567.000  1,567.000   \n",
       "mean       0.005    0.006    1.730    4.149    0.053      0.025      0.001   \n",
       "std        0.001    0.085    4.336   10.045    0.067      0.049      0.016   \n",
       "min        0.001    0.000    0.291    1.102    0.000      0.003      0.000   \n",
       "25%        0.004    0.001    0.911    2.726    0.019      0.015      0.000   \n",
       "50%        0.004    0.002    1.185    3.673    0.027      0.021      0.000   \n",
       "75%        0.005    0.003    1.762    4.480    0.051      0.027      0.000   \n",
       "max        0.024    1.984   99.902  237.184    0.491      0.973      0.414   \n",
       "\n",
       "             251        252        253        254        255        256  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,543.000  1,567.000  1,567.000   \n",
       "mean     109.651      0.004      4.645      0.033      0.014      0.404   \n",
       "std       54.597      0.037     64.355      0.022      0.009      0.120   \n",
       "min       21.011      0.000      0.767      0.009      0.002      0.127   \n",
       "25%       76.132      0.001      2.206      0.025      0.005      0.308   \n",
       "50%      103.094      0.001      2.865      0.031      0.015      0.405   \n",
       "75%      131.758      0.001      3.795      0.038      0.021      0.481   \n",
       "max    1,119.704      0.991  2,549.988      0.452      0.079      0.925   \n",
       "\n",
       "             268        269        270        271        272        273  \\\n",
       "count  1,559.000  1,559.000  1,559.000  1,562.000  1,561.000  1,560.000   \n",
       "mean       0.071     19.505      3.778     29.260     46.057     41.298   \n",
       "std        0.030      7.344      1.152      8.402     17.866     17.738   \n",
       "min        0.020      6.098      1.302     15.547     10.402      6.943   \n",
       "25%        0.044     13.828      2.957     24.982     30.014     27.093   \n",
       "50%        0.071     17.977      3.704     28.773     45.676     40.019   \n",
       "75%        0.092     24.653      4.379     31.702     59.595     54.277   \n",
       "max        0.158     40.855     10.153    158.526    132.648    122.117   \n",
       "\n",
       "             274        275        276        278        279        280  \\\n",
       "count  1,553.000  1,553.000  1,553.000  1,553.000  1,558.000  1,565.000   \n",
       "mean      20.181    136.292      8.693      2.211      0.001      0.041   \n",
       "std        3.830     85.608    168.949      1.196      0.000      0.020   \n",
       "min        8.651      0.000      0.011      0.561      0.000      0.011   \n",
       "25%       18.247     81.216      0.045      1.698      0.001      0.028   \n",
       "50%       19.581    110.601      0.078      2.083      0.001      0.037   \n",
       "75%       22.097    162.038      0.145      2.514      0.001      0.046   \n",
       "max       43.574    659.170  3,332.596     32.171      0.003      0.188   \n",
       "\n",
       "             281        282        283        284        286        287  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,564.000  1,564.000   \n",
       "mean       0.018      0.015      0.006      2.804      2.120      4.260   \n",
       "std        0.006      0.006      0.009      5.864      0.963      9.764   \n",
       "min        0.007      0.007      0.002      0.505      0.461      0.728   \n",
       "25%        0.014      0.012      0.003      2.210      1.438      2.467   \n",
       "50%        0.017      0.014      0.005      2.658      1.875      3.360   \n",
       "75%        0.021      0.017      0.007      3.146      2.607      4.311   \n",
       "max        0.075      0.060      0.308    232.805      6.870    207.016   \n",
       "\n",
       "             288        289        290        291        292      293  \\\n",
       "count  1,564.000  1,564.000  1,564.000  1,557.000  1,567.000  138.000   \n",
       "mean       0.368      0.004      2.579      0.123      0.020    0.014   \n",
       "std        7.386      0.003      1.617      0.271      0.026    0.011   \n",
       "min        0.051      0.001      0.396      0.042      0.004    0.004   \n",
       "25%        0.115      0.002      2.092      0.065      0.013    0.009   \n",
       "50%        0.139      0.004      2.549      0.083      0.017    0.011   \n",
       "75%        0.198      0.005      3.025      0.118      0.024    0.015   \n",
       "max      292.227      0.075     59.519      4.420      0.692    0.083   \n",
       "\n",
       "           294        295        296         297         298        299  \\\n",
       "count  138.000  1,565.000  1,565.000   1,565.000   1,565.000  1,565.000   \n",
       "mean   335.551    401.815    252.999   1,879.228   2,342.827      0.064   \n",
       "std    137.692    477.050    283.531   1,975.111   3,226.924      0.064   \n",
       "min     82.323      0.000      0.000       0.000       0.000      0.000   \n",
       "25%    229.809    185.090    130.220     603.033     210.937      0.041   \n",
       "50%    317.867    278.672    195.826   1,202.412     820.099      0.053   \n",
       "75%    403.989    428.555    273.953   2,341.289   3,190.616      0.069   \n",
       "max    879.226  3,933.755  2,005.874  15,559.952  18,520.468      0.526   \n",
       "\n",
       "             300        301        302        303        304        305  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       0.060      0.118      0.910      0.403      0.040      0.132   \n",
       "std        0.131      0.219      0.332      0.198      0.015      0.065   \n",
       "min        0.000      0.000      0.310      0.112      0.011      0.014   \n",
       "25%        0.030      0.059      0.717      0.296      0.030      0.073   \n",
       "50%        0.040      0.083      0.860      0.381      0.039      0.137   \n",
       "75%        0.052      0.116      1.046      0.477      0.049      0.178   \n",
       "max        1.031      1.812      5.711      5.155      0.226      0.334   \n",
       "\n",
       "             306        307        308        309        310        311  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.265      0.049      0.129      0.218      0.129      0.305   \n",
       "std        0.057      0.025      0.027      0.034      0.027      0.043   \n",
       "min        0.117      0.003      0.055      0.091      0.055      0.181   \n",
       "25%        0.225      0.033      0.114      0.198      0.114      0.279   \n",
       "50%        0.264      0.045      0.130      0.219      0.130      0.303   \n",
       "75%        0.307      0.055      0.148      0.238      0.148      0.332   \n",
       "max        0.475      0.225      0.211      0.324      0.211      0.444   \n",
       "\n",
       "             312        313        317        318        319        320  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.097      0.160      5.977      0.173      3.189      7.916   \n",
       "std        0.029      0.117      1.019      0.072      1.216      2.179   \n",
       "min        0.033      0.022      2.788      0.028      0.985      1.657   \n",
       "25%        0.078      0.091      5.302      0.117      2.320      6.245   \n",
       "50%        0.098      0.121      5.832      0.163      2.899      8.389   \n",
       "75%        0.116      0.160      6.548      0.218      4.021      9.481   \n",
       "max        0.178      0.755     13.096      1.003     15.893     20.046   \n",
       "\n",
       "             321        322        324        325        332        333  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,563.000  1,560.000   \n",
       "mean       0.043      2.264      5.393     13.332      0.083      2.593   \n",
       "std        0.032      2.117      2.519      6.616      0.063      5.645   \n",
       "min        0.008      0.611      1.710      2.235      0.022      0.537   \n",
       "25%        0.031      1.670      4.273      7.579      0.069      1.547   \n",
       "50%        0.040      2.078      5.459     12.505      0.085      2.063   \n",
       "75%        0.050      2.633      6.345     17.925      0.096      2.791   \n",
       "max        0.947     79.151     89.192     51.868      1.096    174.894   \n",
       "\n",
       "             334        335        336        337        338        339  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,560.000  1,560.000  1,560.000   \n",
       "mean       6.216      0.168      3.427      9.736      2.327      3.038   \n",
       "std        3.403      0.173      5.782      7.556      1.699      5.645   \n",
       "min        2.837      0.028      0.790      5.215      0.000      0.000   \n",
       "25%        5.454      0.089      2.036      8.289      1.543      1.901   \n",
       "50%        5.980      0.129      2.514      9.074      2.054      2.561   \n",
       "75%        6.550      0.210      3.360     10.042      2.785      3.405   \n",
       "max       90.516      3.413    172.712    214.863     38.900    196.688   \n",
       "\n",
       "             340        341        342        343        344        345  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000  1,561.000   \n",
       "mean       9.329     14.674      2.732      0.000      6.199     23.217   \n",
       "std        6.075    261.738      3.668      0.011      5.372      8.895   \n",
       "min        2.200      0.013      0.574      0.000      1.256      2.056   \n",
       "25%        7.589      0.035      1.912      0.000      4.999     17.861   \n",
       "50%        9.474      0.046      2.377      0.000      6.006     23.215   \n",
       "75%       10.440      0.067      2.985      0.000      6.885     28.873   \n",
       "max      197.499  5,043.879     97.709      0.447    156.336     59.324   \n",
       "\n",
       "           346      347        348        349        350        351  \\\n",
       "count  773.000  773.000  1,561.000  1,543.000  1,543.000  1,543.000   \n",
       "mean     7.958    5.770      0.009      0.025      0.025      0.023   \n",
       "std     17.513   17.077      0.352      0.012      0.011      0.014   \n",
       "min      1.769    1.018      0.000      0.010      0.001      0.003   \n",
       "25%      4.441    2.533      0.000      0.018      0.020      0.015   \n",
       "50%      5.567    3.046      0.000      0.023      0.024      0.019   \n",
       "75%      6.825    4.086      0.000      0.027      0.029      0.029   \n",
       "max    257.011  187.759     13.915      0.220      0.134      0.291   \n",
       "\n",
       "             352        353        354        355        356        357  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,566.000   \n",
       "mean       0.028      0.023      0.040      0.042      0.035      1.299   \n",
       "std        0.025      0.013      0.015      0.013      0.022      0.387   \n",
       "min        0.002      0.006      0.003      0.004      0.004      0.380   \n",
       "25%        0.017      0.016      0.030      0.035      0.021      1.025   \n",
       "50%        0.025      0.022      0.042      0.044      0.029      1.255   \n",
       "75%        0.034      0.027      0.050      0.050      0.042      1.533   \n",
       "max        0.619      0.143      0.153      0.134      0.279      2.835   \n",
       "\n",
       "             358      359        360        361        362        363  \\\n",
       "count  1,555.000  226.000  1,567.000  1,567.000  1,567.000  1,516.000   \n",
       "mean       0.001    0.002      0.020      0.003     39.936      0.018   \n",
       "std        0.001    0.000      0.007      0.020     17.056      0.022   \n",
       "min        0.000    0.002      0.008      0.000     10.720      0.003   \n",
       "25%        0.001    0.002      0.014      0.000     32.169      0.009   \n",
       "50%        0.001    0.002      0.020      0.001     39.696      0.013   \n",
       "75%        0.001    0.003      0.025      0.002     47.079      0.019   \n",
       "max        0.005    0.005      0.089      0.409    547.172      0.416   \n",
       "\n",
       "             364        366        367        368        369        377  \\\n",
       "count  1,516.000  1,565.000  1,565.000  1,561.000  1,561.000  1,565.000   \n",
       "mean     333.320      0.005      0.005      0.004      0.003      0.002   \n",
       "std      138.802      0.003      0.002      0.003      0.002      0.001   \n",
       "min       60.988      0.002      0.002      0.000      0.000      0.000   \n",
       "25%      228.683      0.004      0.004      0.003      0.002      0.001   \n",
       "50%      309.832      0.005      0.004      0.003      0.003      0.002   \n",
       "75%      412.330      0.006      0.005      0.004      0.004      0.002   \n",
       "max    1,072.203      0.037      0.039      0.036      0.033      0.008   \n",
       "\n",
       "             378      383      384      385      386        387        388  \\\n",
       "count  1,565.000  549.000  549.000  549.000  852.000  1,567.000  1,567.000   \n",
       "mean       0.002    0.002    0.541    1.285    0.011      0.008      0.000   \n",
       "std        0.000    0.027    1.341    3.168    0.014      0.015      0.005   \n",
       "min        0.000    0.000    0.087    0.338    0.000      0.001      0.000   \n",
       "25%        0.001    0.000    0.295    0.842    0.005      0.005      0.000   \n",
       "50%        0.002    0.001    0.373    1.106    0.007      0.007      0.000   \n",
       "75%        0.002    0.001    0.541    1.387    0.011      0.009      0.000   \n",
       "max        0.008    0.627   30.998   74.844    0.207      0.307      0.131   \n",
       "\n",
       "             389        390        391        392        393        394  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,543.000  1,567.000  1,567.000   \n",
       "mean      35.155      0.001      1.432      0.011      0.005      0.134   \n",
       "std       17.227      0.012     20.326      0.007      0.003      0.038   \n",
       "min        6.310      0.000      0.305      0.003      0.001      0.034   \n",
       "25%       24.387      0.000      0.675      0.008      0.002      0.104   \n",
       "50%       32.531      0.000      0.877      0.010      0.005      0.134   \n",
       "75%       42.652      0.000      1.148      0.012      0.007      0.160   \n",
       "max      348.829      0.313    805.394      0.138      0.023      0.299   \n",
       "\n",
       "             406        407        408        409        410        411  \\\n",
       "count  1,559.000  1,559.000  1,559.000  1,562.000  1,561.000  1,560.000   \n",
       "mean       0.024      6.731      1.232      5.341      4.580      4.929   \n",
       "std        0.011      2.830      0.365      2.578      1.777      2.123   \n",
       "min        0.006      2.054      0.424      2.738      1.216      0.734   \n",
       "25%        0.014      4.548      0.967      4.128      3.013      3.265   \n",
       "50%        0.024      5.920      1.240      4.922      4.490      4.733   \n",
       "75%        0.032      8.585      1.417      5.787      5.937      6.458   \n",
       "max        0.051     14.728      3.313     44.310      9.576     13.807   \n",
       "\n",
       "             412        413        414        416        417        418  \\\n",
       "count  1,553.000  1,553.000  1,553.000  1,553.000  1,558.000  1,565.000   \n",
       "mean       2.616     30.911     25.613      6.631      3.404      8.191   \n",
       "std        0.551     18.414     47.308      3.958      1.035      4.055   \n",
       "min        0.961      0.000      4.042      1.534      0.000      2.153   \n",
       "25%        2.321     18.408     11.376      4.927      2.660      5.766   \n",
       "50%        2.548     26.157     20.255      6.177      3.234      7.396   \n",
       "75%        2.853     38.140     29.307      7.571      4.011      9.169   \n",
       "max        6.215    128.282    899.119    116.862      9.690     39.038   \n",
       "\n",
       "             419        420        421        422        424        425  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,564.000  1,564.000   \n",
       "mean     320.259    309.061      1.821      4.175     77.660      3.315   \n",
       "std      287.704    325.448      3.058      6.914     32.597      6.325   \n",
       "min        0.000      0.000      0.441      0.722     23.020      0.487   \n",
       "25%        0.000      0.000      1.030      3.184     55.977      1.965   \n",
       "50%      302.178    272.449      1.645      3.943     69.905      2.667   \n",
       "75%      524.002    582.935      2.215      4.784     92.911      3.471   \n",
       "max      999.316    998.681    111.496    273.095    424.215    103.181   \n",
       "\n",
       "             426        427        428        429        430        431  \\\n",
       "count  1,564.000  1,564.000  1,564.000  1,557.000  1,567.000  1,565.000   \n",
       "mean       6.796      1.234      4.059      4.221      4.172     18.422   \n",
       "std       23.258      0.996      3.042     10.633      6.435     36.060   \n",
       "min        1.467      0.363      0.664      1.120      0.784      0.000   \n",
       "25%        3.766      0.743      3.113      1.935      2.571      7.000   \n",
       "50%        4.764      1.135      3.941      2.534      3.454     11.106   \n",
       "75%        6.883      1.540      4.769      3.609      4.756     17.423   \n",
       "max      898.609     24.990    113.223    118.753    186.616    400.000   \n",
       "\n",
       "             432        433        434        435        436        437  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean      22.358     99.368    205.519     14.734      9.371      7.513   \n",
       "std       36.395    126.189    225.779     34.109     34.370     34.558   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%       11.059     31.032     10.027      7.551      3.494      1.951   \n",
       "50%       16.381     57.969    151.116     10.198      4.551      2.764   \n",
       "75%       21.765    120.173    305.026     12.754      5.823      3.822   \n",
       "max      400.000    994.286    995.745    400.000    400.000    400.000   \n",
       "\n",
       "             438        439        440        441        442        443  \\\n",
       "count  1,565.000  1,565.000  1,565.000  1,565.000  1,566.000  1,566.000   \n",
       "mean       4.017     54.701     70.644     11.527      0.802      1.345   \n",
       "std        1.611     34.108     38.376      6.169      0.184      0.659   \n",
       "min        1.157      0.000     14.121      1.097      0.351      0.097   \n",
       "25%        3.071     36.290     48.174      5.414      0.680      0.908   \n",
       "50%        3.781     49.091     65.438     12.086      0.808      1.265   \n",
       "75%        4.679     66.667     84.973     15.796      0.928      1.578   \n",
       "max       32.274    851.613    657.762     33.058      1.277      5.132   \n",
       "\n",
       "             444        445        446        447        448        449  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.634      0.895      0.647      1.175      0.282      0.332   \n",
       "std        0.144      0.156      0.141      0.176      0.086      0.236   \n",
       "min        0.217      0.334      0.309      0.697      0.085      0.040   \n",
       "25%        0.550      0.805      0.556      1.047      0.226      0.188   \n",
       "50%        0.643      0.903      0.651      1.164      0.280      0.251   \n",
       "75%        0.733      0.989      0.748      1.272      0.339      0.351   \n",
       "max        1.085      1.351      1.109      1.764      0.508      1.475   \n",
       "\n",
       "             453        454        455        456        457        458  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       5.347      5.461      7.884      3.637     12.326      5.264   \n",
       "std        0.919      2.251      3.060      0.938      8.126      4.538   \n",
       "min        2.671      0.904      2.329      0.695      3.049      1.443   \n",
       "25%        4.764      3.748      5.807      2.900      8.817      3.828   \n",
       "50%        5.271      5.227      7.425      3.724     11.351      4.793   \n",
       "75%        5.913      6.902      9.577      4.342     14.388      6.089   \n",
       "max       13.978     34.490     42.070     10.184    232.126    164.109   \n",
       "\n",
       "             460        461        468        469        470        471  \\\n",
       "count  1,566.000  1,566.000  1,563.000  1,560.000  1,561.000  1,561.000   \n",
       "mean       2.838     29.197      6.252    224.173      5.662      5.368   \n",
       "std        1.346     13.335      8.674    230.767      3.152      4.983   \n",
       "min        0.991      7.953      1.716      0.000      2.601      0.833   \n",
       "25%        2.291     20.222      4.697     38.473      4.847      2.823   \n",
       "50%        2.830     26.168      5.645    150.340      5.472      4.061   \n",
       "75%        3.309     35.279      6.387    335.922      6.006      7.007   \n",
       "max       47.777    149.385    109.007    999.877     77.801     87.135   \n",
       "\n",
       "             472        473        474        475        476        477  \\\n",
       "count  1,561.000  1,560.000  1,560.000  1,560.000  1,561.000  1,561.000   \n",
       "mean       9.639    137.888     39.427     37.637      4.263     20.132   \n",
       "std       10.174     47.698     22.457     24.823      2.611     14.940   \n",
       "min        2.403     11.500      0.000      0.000      1.101      0.000   \n",
       "25%        5.807    105.525     24.901     23.157      3.494     11.577   \n",
       "50%        7.396    138.255     34.247     32.820      4.276     15.974   \n",
       "75%        9.720    168.410     47.728     45.169      4.742     23.737   \n",
       "max      212.656    492.772    358.950    415.435     79.116    274.887   \n",
       "\n",
       "             478        479        480        481        483        484  \\\n",
       "count  1,561.000  1,561.000  1,561.000  1,561.000  1,543.000  1,543.000   \n",
       "mean       6.258      0.128      3.283     75.538    318.418    206.564   \n",
       "std       10.185      5.062      2.639     35.752    281.011    192.864   \n",
       "min        1.687      0.000      0.646      8.841      0.000      0.000   \n",
       "25%        4.105      0.000      2.628     52.895      0.000     81.316   \n",
       "50%        5.242      0.000      3.184     70.434    293.519    148.317   \n",
       "75%        6.704      0.000      3.625     93.120    514.586    262.865   \n",
       "max      289.826    200.000     63.334    221.975    999.413    989.474   \n",
       "\n",
       "             485        486        487        488        489        490  \\\n",
       "count  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000  1,543.000   \n",
       "mean     215.289    201.112    302.506    239.455    352.616    272.170   \n",
       "std      213.127    218.690    287.364    263.838    252.044    228.047   \n",
       "min        0.000      0.000      0.000      0.000      0.000      0.000   \n",
       "25%       76.455     50.384      0.000     55.555    139.914    112.859   \n",
       "50%      138.775    112.953    249.927    112.275    348.529    219.487   \n",
       "75%      294.667    288.893    501.607    397.506    510.647    377.144   \n",
       "max      996.859    994.000    999.491    995.745    997.519    994.004   \n",
       "\n",
       "             491        492      493        494        495        496  \\\n",
       "count  1,566.000  1,555.000  226.000  1,567.000  1,567.000  1,567.000   \n",
       "mean      51.354      2.443    8.171      2.530      0.956      6.808   \n",
       "std       18.049      1.224    1.759      0.974      6.615      3.260   \n",
       "min       13.723      0.556    4.888      0.833      0.034      1.772   \n",
       "25%       38.391      1.747    6.925      1.664      0.139      5.275   \n",
       "50%       48.557      2.251    8.009      2.529      0.233      6.608   \n",
       "75%       61.495      2.840    9.079      3.199      0.563      7.897   \n",
       "max      142.844     12.770   21.044      9.402    127.573    107.693   \n",
       "\n",
       "             497        498        500        501        511        512  \\\n",
       "count  1,516.000  1,516.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean      29.866     11.821    263.196    240.981     55.764    275.979   \n",
       "std       24.622      4.957    324.771    323.003     37.692    329.665   \n",
       "min        4.814      1.950      0.000      0.000      0.000      0.000   \n",
       "25%       16.342      8.150      0.000      0.000     35.322      0.000   \n",
       "50%       22.039     10.907      0.000      0.000     46.986      0.000   \n",
       "75%       32.438     14.469    536.205    505.401     64.249    555.294   \n",
       "max      219.644     40.282  1,000.000    999.234    451.485  1,000.000   \n",
       "\n",
       "           517      518      519      520        521        522        523  \\\n",
       "count  549.000  549.000  549.000  852.000  1,567.000  1,567.000  1,567.000   \n",
       "mean     0.679    1.739    1.806   11.728      2.696     11.610     14.729   \n",
       "std     10.784    4.891    4.716   15.814      5.702    103.123      7.104   \n",
       "min      0.029    0.288    0.467    0.000      0.312      0.000      2.681   \n",
       "25%      0.121    0.890    1.171    4.160      1.552      0.000     10.183   \n",
       "50%      0.175    1.154    1.589    5.833      2.221      0.000     13.743   \n",
       "75%      0.265    1.760    1.933   10.972      2.904      0.000     17.809   \n",
       "max    252.860  113.276  111.350  184.349    111.737  1,000.000    137.984   \n",
       "\n",
       "             524        525        526        527        528        540  \\\n",
       "count  1,567.000  1,567.000  1,543.000  1,567.000  1,567.000  1,559.000   \n",
       "mean       0.454      5.688      5.560      1.443      6.396      3.034   \n",
       "std        4.148     20.663      3.920      0.958      1.889      1.253   \n",
       "min        0.026      1.310      1.540      0.171      2.170      0.852   \n",
       "25%        0.073      3.770      4.101      0.484      4.895      1.890   \n",
       "50%        0.100      4.877      5.134      1.550      6.411      3.055   \n",
       "75%        0.133      6.451      6.329      2.212      7.594      3.947   \n",
       "max      111.333    818.000     80.041      8.204     14.448      6.580   \n",
       "\n",
       "             541        542        543        544        545        546  \\\n",
       "count  1,559.000  1,559.000  1,565.000  1,565.000  1,565.000  1,565.000   \n",
       "mean       1.943      9.612      0.111      0.008      0.003      7.611   \n",
       "std        0.732      2.896      0.003      0.002      0.000      1.316   \n",
       "min        0.614      3.276      0.105      0.005      0.002      4.429   \n",
       "25%        1.385      7.496      0.110      0.008      0.002      7.116   \n",
       "50%        1.786      9.459      0.110      0.008      0.003      7.116   \n",
       "75%        2.458     11.238      0.113      0.009      0.003      8.021   \n",
       "max        4.082     25.779      0.118      0.024      0.005     21.044   \n",
       "\n",
       "             547        548        549        550        551        552  \\\n",
       "count  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000   \n",
       "mean       1.040    403.546     75.680      0.663     17.013      1.231   \n",
       "std        0.389      5.064      3.391      0.673      4.967      1.361   \n",
       "min        0.444    372.822     71.038      0.045      6.110      0.120   \n",
       "25%        0.797    400.694     73.254      0.226     14.530      0.870   \n",
       "50%        0.911    403.122     74.084      0.471     16.340      1.150   \n",
       "75%        1.286    407.431     78.397      0.850     19.035      1.370   \n",
       "max        3.979    421.702     83.720      7.066    131.680     39.330   \n",
       "\n",
       "             553        554        555        556        557        558  \\\n",
       "count  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000  1,307.000   \n",
       "mean       0.277      7.704      0.504     57.747      4.217      1.623   \n",
       "std        0.276      2.193      0.599     35.208      1.280      1.870   \n",
       "min        0.019      2.786      0.052      4.827      1.497      0.165   \n",
       "25%        0.095      6.738      0.344     27.018      3.625      1.183   \n",
       "50%        0.198      7.428      0.479     54.442      4.067      1.530   \n",
       "75%        0.358      8.637      0.562     74.629      4.703      1.816   \n",
       "max        2.718     56.930     17.478    303.550     35.320     54.292   \n",
       "\n",
       "             559        560        561        562        563        564  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,294.000  1,294.000   \n",
       "mean       0.995      0.326      0.072     32.285    262.730      0.680   \n",
       "std        0.084      0.201      0.052     19.026      7.631      0.122   \n",
       "min        0.892      0.070      0.018      7.237    242.286      0.305   \n",
       "25%        0.955      0.150      0.036     15.762    259.973      0.567   \n",
       "50%        0.973      0.291      0.059     29.731    264.272      0.651   \n",
       "75%        1.001      0.444      0.089     44.113    265.707      0.769   \n",
       "max        1.512      1.074      0.446    101.115    311.404      1.299   \n",
       "\n",
       "             565        566        567        568        569        570  \\\n",
       "count  1,294.000  1,294.000  1,294.000  1,294.000  1,294.000  1,294.000   \n",
       "mean       6.445      0.146      2.611      0.060      2.452     21.118   \n",
       "std        2.634      0.081      1.033      0.033      0.997     10.213   \n",
       "min        0.970      0.022      0.412      0.009      0.371      3.250   \n",
       "25%        4.980      0.088      2.090      0.038      1.884     15.466   \n",
       "50%        5.160      0.120      2.150      0.049      2.000     16.988   \n",
       "75%        7.800      0.186      3.099      0.075      2.971     24.772   \n",
       "max       32.580      0.689     14.014      0.293     12.746     84.802   \n",
       "\n",
       "             571        572        573        574        575        576  \\\n",
       "count  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000  1,567.000   \n",
       "mean     530.524      2.102     28.450      0.346      9.162      0.105   \n",
       "std       17.500      0.275     86.305      0.248     26.920      0.068   \n",
       "min      317.196      0.980      3.540      0.067      1.040      0.023   \n",
       "25%      530.703      1.983      7.500      0.242      2.568      0.075   \n",
       "50%      532.398      2.119      8.650      0.293      2.976      0.089   \n",
       "75%      534.356      2.291     10.130      0.367      3.493      0.112   \n",
       "max      589.508      2.740    454.560      2.197    170.020      0.550   \n",
       "\n",
       "             577        578      579      580      581      582        583  \\\n",
       "count  1,567.000  1,567.000  618.000  618.000  618.000  618.000  1,566.000   \n",
       "mean       5.564     16.642    0.022    0.017    0.005   97.934      0.500   \n",
       "std       16.921     12.485    0.012    0.010    0.003   87.521      0.003   \n",
       "min        0.664      4.582   -0.017    0.003    0.001    0.000      0.478   \n",
       "25%        1.408     11.502    0.014    0.011    0.003   46.185      0.498   \n",
       "50%        1.625     13.818    0.020    0.015    0.005   72.289      0.500   \n",
       "75%        1.902     17.081    0.028    0.020    0.006  116.539      0.502   \n",
       "max       90.424     96.960    0.103    0.080    0.029  737.305      0.510   \n",
       "\n",
       "             584        585        586        587        588        589  \\\n",
       "count  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000  1,566.000   \n",
       "mean       0.015      0.004      3.068      0.021      0.016      0.005   \n",
       "std        0.017      0.004      3.578      0.012      0.009      0.003   \n",
       "min        0.006      0.002      1.198     -0.017      0.003      0.001   \n",
       "25%        0.012      0.003      2.307      0.013      0.011      0.003   \n",
       "50%        0.014      0.004      2.758      0.021      0.015      0.005   \n",
       "75%        0.017      0.004      3.295      0.028      0.020      0.006   \n",
       "max        0.477      0.104     99.303      0.103      0.080      0.029   \n",
       "\n",
       "             590  \n",
       "count  1,566.000  \n",
       "mean      99.670  \n",
       "std       93.892  \n",
       "min        0.000  \n",
       "25%       44.369  \n",
       "50%       71.900  \n",
       "75%      114.750  \n",
       "max      737.305  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descriptive Statistics For Both.\n",
    "X_reduced1.describe().map('{:,.3f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25664d24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2500x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Group features by missing values.\n",
    "\n",
    "missing_counts = X_reduced1.isna().sum()\n",
    "\n",
    "missing_groups = {}\n",
    "\n",
    "for feature in X_reduced1.columns:\n",
    "    if missing_counts[feature] > 0 and 'missing_' not in str(feature) and str(feature) != 'Y': \n",
    "        na_count = missing_counts[feature]                                \n",
    "        if na_count not in missing_groups:\n",
    "            missing_groups[na_count] = []\n",
    "            missing_groups.setdefault(na_count, []).append(feature)\n",
    "\n",
    "# Sort layers by NA count.\n",
    "\n",
    "layer_na_counts = sorted(missing_groups.keys())\n",
    "layer_labels = [f\"Layer_{na}NA\" for na in layer_na_counts]\n",
    "\n",
    "# Create plot data.\n",
    "\n",
    "plot_data = []\n",
    "\n",
    "for j in range(len(X_reduced1)):\n",
    "    color = 'green' if Y[j] == 0 else 'red'\n",
    "    non_missing_layers = []\n",
    "    for i, na_count in enumerate(layer_na_counts):\n",
    "        if X_reduced1[missing_groups[na_count]].iloc[j].notna().any():\n",
    "            non_missing_layers.append(i)\n",
    "\n",
    "    if non_missing_layers:\n",
    "        ymin = min(non_missing_layers)\n",
    "        ymax = max(non_missing_layers)\n",
    "        plot_data.append((j, ymin, ymax, color))\n",
    "\n",
    "# Group layers for color-coding.\n",
    "\n",
    "# Plot vertical lines.\n",
    "plt.figure(figsize=(25,10))\n",
    "for x, ymin, ymax, color in plot_data:\n",
    "    plt.vlines(x, ymin - 0.5, ymax + 0.5, colors=color, linewidth=1.5, alpha=1.0, zorder=1)\n",
    "\n",
    "# Customize axes.\n",
    "plt.yticks(range(len(layer_labels)), layer_labels, fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(0, len(X_reduced1), 50), fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Sample Index\", fontsize=18, fontweight='bold')\n",
    "plt.ylabel(\"Missingness Layers\", fontsize=18, fontweight='bold')\n",
    "plt.title(\"Missingness Layers Across Samples\", fontsize=24, fontweight='bold')\n",
    "plt.xlim(0, len(X_reduced1) + 100)\n",
    "plt.ylim(-0.5, len(layer_labels) - 0.5)\n",
    "\n",
    "# Add horizontal grid lines.\n",
    "grid_positions = np.arange(0.5, len(layer_labels) - 0.5, 1)\n",
    "plt.hlines(grid_positions, 0, len(X_reduced1) - 1, colors='black', linestyles='--', alpha=0.5, zorder=2)\n",
    "\n",
    "# Add legend for pass/fail.\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [Line2D([0], [0], color='green', lw=2, label='Pass'),\n",
    "                   Line2D([0], [0], color='red', lw=2, label='Fail'),\n",
    "                   Line2D([0], [0], color='white', lw=2, label='NA', linestyle='--')]\n",
    "plt.legend(handles=legend_elements, loc='upper right', fontsize=15)\n",
    "\n",
    "# Display visualization.\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('secom_missingness_layers_lines.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20866722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Dictionary to store stats for all features\n",
    "feature_stats = {}\n",
    "\n",
    "for na in all_layers:\n",
    "    features = missing_groups.get(na, [])\n",
    "    for feature in features:\n",
    "        non_missing = X_raw[feature].dropna()\n",
    "        indices = non_missing.index\n",
    "        y_subset = Y[indices]\n",
    "       \n",
    "        # Zero/near-zero stats\n",
    "        near_zero = non_missing[non_missing <= 0.01]\n",
    "        zero_count = len(near_zero)\n",
    "        zero_proportion = zero_count / len(non_missing) if len(non_missing) > 0 else 0\n",
    "       \n",
    "        # Non-zero proportion\n",
    "        non_zero_proportion = 1 - zero_proportion\n",
    "       \n",
    "        # Distribution stats\n",
    "        skewness = skew(non_missing) if len(non_missing) > 0 else 0\n",
    "        non_zero = non_missing[non_missing > 0.01]\n",
    "        non_zero_mean = non_zero.mean() if len(non_zero) > 0 else 0\n",
    "        non_zero_median = non_zero.median() if len(non_zero) > 0 else 0\n",
    "       \n",
    "        # Association with y\n",
    "        zero_indices = near_zero.index\n",
    "        non_zero_indices = non_zero.index\n",
    "        failure_rate_zeros = y_subset[zero_indices].mean() if len(zero_indices) > 0 else 0\n",
    "        failure_rate_non_zeros = y_subset[non_zero_indices].mean() if len(non_zero_indices) > 0 else 0\n",
    "       \n",
    "        feature_stats[feature] = {\n",
    "            'layer': na,\n",
    "            'zero_count': zero_count,\n",
    "            'zero_proportion': zero_proportion,\n",
    "            'non_zero_proportion': non_zero_proportion,\n",
    "            'skewness': skewness,\n",
    "            'non_zero_mean': non_zero_mean,\n",
    "            'non_zero_median': non_zero_median,\n",
    "            'failure_rate_zeros': failure_rate_zeros,\n",
    "            'failure_rate_non_zeros': failure_rate_non_zeros\n",
    "        }\n",
    "\n",
    "# Display stats for all features\n",
    "print(\"Per-Feature Zero/Near-Zero Analysis:\")\n",
    "for feature in sorted(feature_stats.keys()):\n",
    "    stats = feature_stats[feature]\n",
    "    print(f\"Feature {feature} (Layer_{stats['layer']}NA):\")\n",
    "    print(f\"  Zero/Near-Zero Count: {stats['zero_count']}\")\n",
    "    print(f\"  Zero Proportion: {stats['zero_proportion']:.3f}\")\n",
    "    print(f\"  Non-Zero Proportion: {stats['non_zero_proportion']:.3f}\")\n",
    "    print(f\"  Skewness: {stats['skewness']:.3f}\")\n",
    "    print(f\"  Non-Zero Mean: {stats['non_zero_mean']:.3f}\")\n",
    "    print(f\"  Non-Zero Median: {stats['non_zero_median']:.3f}\")\n",
    "    print(f\"  Failure Rate (Zeros): {stats['failure_rate_zeros']:.3f}\")\n",
    "    print(f\"  Failure Rate (Non-Zeros): {stats['failure_rate_non_zeros']:.3f}\")\n",
    "    if stats['zero_proportion'] > 0.2 and stats['failure_rate_zeros'] < stats['failure_rate_non_zeros'] - 0.05:\n",
    "        print(\"  Warning: Potential sensor performance issue\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1049716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in X_reduced1: 474\n",
      "Total samples in X_reduced1: 1567\n",
      "Creating missingness indicators...\n",
      "\n",
      "Missingness Indicators Created: 8\n",
      "missing_73\n",
      "missing_74\n",
      "missing_113\n",
      "missing_248\n",
      "missing_346\n",
      "missing_347\n",
      "missing_386\n",
      "missing_520\n",
      "\n",
      "Composite Indicator Created:\n",
      "missing_late_utilization\n",
      "\n",
      "Calculating proportion of NA features per sample...\n",
      "Added 'proportion_na_features' column to X_indicators. Mean proportion: 0.045\n",
      "Min proportion: 0.007, Max proportion: 0.258\n",
      "Features to remove (Layers 1341NA and 1429NA): ['86', '158', '159', '221', '293', '294', '359', '493']\n",
      "\n",
      "Number of features in X_indicators after removal: 476\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Start with X_reduced1\n",
    "X_indicators = X_reduced1.copy()\n",
    "\n",
    "# Print basic info about X_reduced1\n",
    "print(f\"Number of features in X_reduced1: {len(X_reduced1.columns)}\", flush=True)\n",
    "print(f\"Total samples in X_reduced1: {len(X_reduced1)}\", flush=True)\n",
    "\n",
    "# Section 2: Convert column names to strings\n",
    "X_indicators.columns = X_indicators.columns.astype(str)\n",
    "\n",
    "# Section 3: Add missingness indicators\n",
    "# Predefined features with predictive missingness\n",
    "predictive_features = ['73', '74', '113', '248', '346', '347', '386', '520']\n",
    "\n",
    "# List to store missingness indicators\n",
    "missing_indicators = []\n",
    "\n",
    "print(\"Creating missingness indicators...\", flush=True)\n",
    "for feature in predictive_features:\n",
    "    if feature not in X_indicators.columns:\n",
    "        print(f\"Warning: Feature {feature} not found in X_indicators.columns\", flush=True)\n",
    "        continue\n",
    "   \n",
    "    X_indicators[f'missing_{feature}'] = X_indicators[feature].isna().astype(int)\n",
    "    missing_indicators.append(f'missing_{feature}')\n",
    "\n",
    "# Sort the indicator list numerically by feature number\n",
    "missing_indicators.sort(key=lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "# Print indicators\n",
    "print(f\"\\nMissingness Indicators Created: {len(missing_indicators)}\", flush=True)\n",
    "for ind in missing_indicators:\n",
    "    print(ind, flush=True)\n",
    "\n",
    "# Section 4: Add composite indicators\n",
    "late_features = ['245', '246', '247', '383', '384', '385', '517', '518', '519', '582']\n",
    "\n",
    "# Verify that late_features exist in X_indicators\n",
    "missing_late_features = [feat for feat in late_features if feat not in X_indicators.columns]\n",
    "if missing_late_features:\n",
    "    print(f\"Warning: The following late_features are not in X_indicators.columns: {missing_late_features}\", flush=True)\n",
    "\n",
    "# Create composite indicator\n",
    "X_indicators['missing_late_utilization'] = X_indicators[late_features].isna().any(axis=1).astype(int)\n",
    "\n",
    "# Print confirmation of composite indicator\n",
    "print(\"\\nComposite Indicator Created:\", flush=True)\n",
    "print(\"missing_late_utilization\", flush=True)\n",
    "\n",
    "# Calculate proportion of NA features per sample (relative to original 590 features)\n",
    "print(\"\\nCalculating proportion of NA features per sample...\")\n",
    "na_count_per_sample = X.isna().sum(axis=1)  # X has 590 features (sensor measurements only)\n",
    "denominator = 590  # Number of sensor measurement features\n",
    "proportion_na_per_sample = na_count_per_sample / denominator\n",
    "X_indicators['proportion_na_features'] = proportion_na_per_sample\n",
    "print(f\"Added 'proportion_na_features' column to X_indicators. Mean proportion: {proportion_na_per_sample.mean():.3f}\")\n",
    "print(f\"Min proportion: {proportion_na_per_sample.min():.3f}, Max proportion: {proportion_na_per_sample.max():.3f}\")\n",
    "\n",
    "# Section 5: Remove features associated with Layers 1341NA and 1429NA\n",
    "\n",
    "# Identify features to remove (exact match for layer names)\n",
    "layers_to_remove = ['1341NA', '1429NA']\n",
    "features_to_remove = ['86', '158', '159', '221', '293', '294', '359', '493']\n",
    "for feature in X_indicators.columns:\n",
    "    # Skip indicator columns\n",
    "    if feature.startswith('missing_'):\n",
    "        continue\n",
    "    # Check if the feature name exactly matches or contains the layer name\n",
    "    if feature in layers_to_remove or any(layer in feature for layer in layers_to_remove):\n",
    "        features_to_remove.append(feature)\n",
    "\n",
    "# Remove the identified features\n",
    "if features_to_remove:\n",
    "    print(f\"Features to remove (Layers 1341NA and 1429NA): {features_to_remove}\", flush=True)\n",
    "    X_indicators.drop(columns=features_to_remove, inplace=True)\n",
    "else:\n",
    "    print(\"No features found for Layers 1341NA and 1429NA based on name matching.\", flush=True)\n",
    "\n",
    "# Print updated info about X_indicators\n",
    "print(f\"\\nNumber of features in X_indicators after removal: {len(X_indicators.columns)}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "314e8a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 75: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 115: Zero count = 1545, Minimum value = 0.000000\n",
      "Feature 207: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 210: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 250: Zero count = 1545, Minimum value = 0.000000\n",
      "Feature 343: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 348: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 388: Zero count = 1545, Minimum value = 0.000000\n",
      "Feature 479: Zero count = 1560, Minimum value = 0.000000\n",
      "Feature 522: Zero count = 1546, Minimum value = 0.000000\n",
      "Features to remove (zeros >= 1341 and min == 0): ['75', '115', '207', '210', '250', '343', '348', '388', '479', '522']\n",
      "\n",
      "Number of features in X_indicators after zero-based removal: 466\n",
      "\n",
      "==================================================\n",
      "Section 7: Converting zeros to NaN based on criteria...\n",
      "Feature 4: Zeros = 1, Min = 0.000000, Max = 3715.041700, Non-zero Skew = 1.74, Non-zero Mean = 1397.276355, Non-zero Std = 440.408119, Z-score of Zero = 3.17\n",
      "Feature 8: Zeros = 8, Min = 0.000000, Max = 0.128600, Non-zero Skew = 0.00, Non-zero Mean = 0.122450, Non-zero Std = 0.001918, Z-score of Zero = 63.84\n",
      "Feature 10: Zeros = 5, Min = -0.053400, Max = 0.074900, Non-zero Skew = 0.33, Non-zero Mean = -0.000844, Non-zero Std = 0.015140, Z-score of Zero = 0.06\n",
      "Feature 11: Zeros = 7, Min = -0.034900, Max = 0.053000, Non-zero Skew = 0.06, Non-zero Mean = 0.000146, Non-zero Std = 0.009323, Z-score of Zero = 0.02\n",
      "Feature 22: Zeros = 1, Min = -7150.250000, Max = 0.000000, Non-zero Skew = 2.09, Non-zero Mean = -5621.985934, Non-zero Std = 610.695104, Z-score of Zero = 9.21\n",
      "Feature 23: Zeros = 1, Min = 0.000000, Max = 3656.250000, Non-zero Skew = -1.85, Non-zero Mean = 2701.104380, Non-zero Std = 287.594001, Z-score of Zero = 9.39\n",
      "Feature 24: Zeros = 1, Min = -9986.750000, Max = 2363.000000, Non-zero Skew = 0.35, Non-zero Mean = -3808.733429, Non-zero Std = 1377.240371, Z-score of Zero = 2.77\n",
      "Feature 25: Zeros = 1, Min = -14804.500000, Max = 14106.000000, Non-zero Skew = -0.05, Non-zero Mean = -298.789055, Non-zero Std = 2903.608703, Z-score of Zero = 0.10\n",
      "Feature 26: Zeros = 1, Min = 0.000000, Max = 1.382800, Non-zero Skew = -2.27, Non-zero Mean = 1.204614, Non-zero Std = 0.175027, Z-score of Zero = 6.88\n",
      "Feature 27: Zeros = 1, Min = 0.000000, Max = 2.052800, Non-zero Skew = -5.93, Non-zero Mean = 1.939716, Non-zero Std = 0.183100, Z-score of Zero = 10.59\n",
      "Feature 28: Zeros = 1, Min = 0.000000, Max = 7.658800, Non-zero Skew = -2.07, Non-zero Mean = 6.642873, Non-zero Std = 1.233261, Z-score of Zero = 5.39\n",
      "Feature 76: Zeros = 3, Min = -0.104900, Max = 0.231500, Non-zero Skew = 0.39, Non-zero Mean = -0.006916, Non-zero Std = 0.022312, Z-score of Zero = 0.31\n",
      "Feature 78: Zeros = 5, Min = -0.104600, Max = 0.133100, Non-zero Skew = 0.59, Non-zero Mean = -0.007064, Non-zero Std = 0.031417, Z-score of Zero = 0.22\n",
      "Feature 80: Zeros = 2, Min = -0.056800, Max = 0.101300, Non-zero Skew = 1.00, Non-zero Mean = 0.003463, Non-zero Std = 0.023094, Z-score of Zero = 0.15\n",
      "Feature 81: Zeros = 2, Min = -0.143700, Max = 0.118600, Non-zero Skew = -0.18, Non-zero Mean = -0.018555, Non-zero Std = 0.049253, Z-score of Zero = 0.38\n",
      "Feature 82: Zeros = 2, Min = -0.098200, Max = 0.058400, Non-zero Skew = -0.68, Non-zero Mean = -0.021181, Non-zero Std = 0.017015, Z-score of Zero = 1.24\n",
      "Feature 83: Zeros = 3, Min = -0.212900, Max = 0.143700, Non-zero Skew = 0.23, Non-zero Mean = 0.006067, Non-zero Std = 0.036108, Z-score of Zero = 0.17\n",
      "Feature 113: Zeros = 9, Min = 0.000000, Max = 0.488500, Non-zero Skew = -16.84, Non-zero Mean = 0.462422, Non-zero Std = 0.012616, Z-score of Zero = 36.65\n",
      "Feature 129: Zeros = 5, Min = 0.000000, Max = 3.895000, Non-zero Skew = 0.64, Non-zero Mean = 3.202460, Non-zero Std = 0.192558, Z-score of Zero = 16.63\n",
      "Feature 140: Zeros = 1, Min = 0.000000, Max = 1770.690900, Non-zero Skew = 2.08, Non-zero Mean = 417.035499, Non-zero Std = 263.172663, Z-score of Zero = 1.58\n",
      "Feature 144: Zeros = 8, Min = 0.000000, Max = 0.012100, Non-zero Skew = 1.31, Non-zero Mean = 0.004191, Non-zero Std = 0.001250, Z-score of Zero = 3.35\n",
      "Feature 160: Zeros = 1, Min = 0.000000, Max = 7791.000000, Non-zero Skew = 4.20, Non-zero Mean = 883.244885, Non-zero Std = 983.103791, Z-score of Zero = 0.90\n",
      "Feature 161: Zeros = 1, Min = 0.000000, Max = 4170.000000, Non-zero Skew = 3.99, Non-zero Mean = 555.701407, Non-zero Std = 574.820720, Z-score of Zero = 0.97\n",
      "Feature 162: Zeros = 1, Min = 0.000000, Max = 37943.000000, Non-zero Skew = 2.23, Non-zero Mean = 4069.450767, Non-zero Std = 4239.352313, Z-score of Zero = 0.96\n",
      "Feature 163: Zeros = 5, Min = 0.000000, Max = 36871.000000, Non-zero Skew = 1.82, Non-zero Mean = 4812.530128, Non-zero Std = 6558.427747, Z-score of Zero = 0.73\n",
      "Feature 164: Zeros = 1, Min = 0.000000, Max = 0.957000, Non-zero Skew = 5.26, Non-zero Mean = 0.140294, Non-zero Std = 0.121976, Z-score of Zero = 1.15\n",
      "Feature 165: Zeros = 1, Min = 0.000000, Max = 1.817000, Non-zero Skew = 6.51, Non-zero Mean = 0.128024, Non-zero Std = 0.242590, Z-score of Zero = 0.53\n",
      "Feature 166: Zeros = 1, Min = 0.000000, Max = 3.286000, Non-zero Skew = 6.38, Non-zero Mean = 0.252187, Non-zero Std = 0.407410, Z-score of Zero = 0.62\n",
      "Feature 202: Zeros = 1, Min = 0.000000, Max = 126.530000, Non-zero Skew = 9.73, Non-zero Mean = 7.844387, Non-zero Std = 5.102266, Z-score of Zero = 1.54\n",
      "Feature 203: Zeros = 1, Min = 0.000000, Max = 490.561000, Non-zero Skew = 24.66, Non-zero Mean = 10.176987, Non-zero Std = 14.625325, Z-score of Zero = 0.70\n",
      "Feature 248: Zeros = 10, Min = 0.000000, Max = 0.491400, Non-zero Skew = 3.07, Non-zero Mean = 0.054007, Non-zero Std = 0.067021, Z-score of Zero = 0.81\n",
      "Feature 275: Zeros = 1, Min = 0.000000, Max = 659.169600, Non-zero Skew = 2.17, Non-zero Mean = 136.380243, Non-zero Std = 85.565376, Z-score of Zero = 1.59\n",
      "Feature 279: Zeros = 8, Min = 0.000000, Max = 0.003400, Non-zero Skew = 0.70, Non-zero Mean = 0.001123, Non-zero Std = 0.000331, Z-score of Zero = 3.39\n",
      "Feature 295: Zeros = 1, Min = 0.000000, Max = 3933.755000, Non-zero Skew = 4.42, Non-zero Mean = 402.071665, Non-zero Std = 477.094344, Z-score of Zero = 0.84\n",
      "Feature 296: Zeros = 1, Min = 0.000000, Max = 2005.874400, Non-zero Skew = 4.22, Non-zero Mean = 253.160882, Non-zero Std = 283.549137, Z-score of Zero = 0.89\n",
      "Feature 297: Zeros = 1, Min = 0.000000, Max = 15559.952500, Non-zero Skew = 2.19, Non-zero Mean = 1880.429922, Non-zero Std = 1975.170855, Z-score of Zero = 0.95\n",
      "Feature 298: Zeros = 5, Min = 0.000000, Max = 18520.468300, Non-zero Skew = 1.78, Non-zero Mean = 2350.336039, Non-zero Std = 3229.361675, Z-score of Zero = 0.73\n",
      "Feature 299: Zeros = 1, Min = 0.000000, Max = 0.526400, Non-zero Skew = 5.75, Non-zero Mean = 0.063845, Non-zero Std = 0.064225, Z-score of Zero = 0.99\n",
      "Feature 300: Zeros = 1, Min = 0.000000, Max = 1.031200, Non-zero Skew = 6.66, Non-zero Mean = 0.060305, Non-zero Std = 0.130858, Z-score of Zero = 0.46\n",
      "Feature 301: Zeros = 1, Min = 0.000000, Max = 1.812300, Non-zero Skew = 6.59, Non-zero Mean = 0.118462, Non-zero Std = 0.219197, Z-score of Zero = 0.54\n",
      "Feature 338: Zeros = 1, Min = 0.000000, Max = 38.899500, Non-zero Skew = 13.33, Non-zero Mean = 2.328975, Non-zero Std = 1.698956, Z-score of Zero = 1.37\n",
      "Feature 339: Zeros = 1, Min = 0.000000, Max = 196.688000, Non-zero Skew = 27.80, Non-zero Mean = 3.039528, Non-zero Std = 5.646309, Z-score of Zero = 0.54\n",
      "Feature 386: Zeros = 10, Min = 0.000000, Max = 0.207300, Non-zero Skew = 5.84, Non-zero Mean = 0.011563, Non-zero Std = 0.014396, Z-score of Zero = 0.80\n",
      "Feature 413: Zeros = 1, Min = 0.000000, Max = 128.281600, Non-zero Skew = 1.81, Non-zero Mean = 30.931233, Non-zero Std = 18.402815, Z-score of Zero = 1.68\n",
      "Feature 417: Zeros = 8, Min = 0.000000, Max = 9.690000, Non-zero Skew = 1.21, Non-zero Mean = 3.421919, Non-zero Std = 1.008709, Z-score of Zero = 3.39\n",
      "Feature 431: Zeros = 1, Min = 0.000000, Max = 400.000000, Non-zero Skew = 7.38, Non-zero Mean = 18.433379, Non-zero Std = 36.068607, Z-score of Zero = 0.51\n",
      "Feature 432: Zeros = 1, Min = 0.000000, Max = 400.000000, Non-zero Skew = 7.09, Non-zero Mean = 22.372601, Non-zero Std = 36.402653, Z-score of Zero = 0.61\n",
      "Feature 435: Zeros = 1, Min = 0.000000, Max = 400.000000, Non-zero Skew = 8.53, Non-zero Mean = 14.743366, Non-zero Std = 34.117727, Z-score of Zero = 0.43\n",
      "Feature 436: Zeros = 1, Min = 0.000000, Max = 400.000000, Non-zero Skew = 8.65, Non-zero Mean = 9.376657, Non-zero Std = 34.379964, Z-score of Zero = 0.27\n",
      "Feature 437: Zeros = 1, Min = 0.000000, Max = 400.000000, Non-zero Skew = 8.68, Non-zero Mean = 7.518070, Non-zero Std = 34.568334, Z-score of Zero = 0.22\n",
      "Feature 439: Zeros = 1, Min = 0.000000, Max = 851.612900, Non-zero Skew = 10.17, Non-zero Mean = 54.736027, Non-zero Std = 34.090876, Z-score of Zero = 1.61\n",
      "Feature 474: Zeros = 1, Min = 0.000000, Max = 358.950400, Non-zero Skew = 3.43, Non-zero Mean = 39.452137, Non-zero Std = 22.442077, Z-score of Zero = 1.76\n",
      "Feature 475: Zeros = 1, Min = 0.000000, Max = 415.435500, Non-zero Skew = 5.46, Non-zero Mean = 37.661192, Non-zero Std = 24.812556, Z-score of Zero = 1.52\n",
      "Feature 477: Zeros = 1, Min = 0.000000, Max = 274.887100, Non-zero Skew = 5.45, Non-zero Mean = 20.145060, Non-zero Std = 14.935674, Z-score of Zero = 1.35\n",
      "Feature 511: Zeros = 2, Min = 0.000000, Max = 451.485100, Non-zero Skew = 4.12, Non-zero Mean = 55.834863, Non-zero Std = 37.662971, Z-score of Zero = 1.48\n",
      "Feature 520: Zeros = 10, Min = 0.000000, Max = 184.348800, Non-zero Skew = 4.06, Non-zero Mean = 11.867733, Non-zero Std = 15.856059, Z-score of Zero = 0.75\n",
      "Feature 582: Zeros = 8, Min = 0.000000, Max = 737.304800, Non-zero Skew = 2.98, Non-zero Mean = 99.218758, Non-zero Std = 87.366459, Z-score of Zero = 1.14\n",
      "Total features with zeros converted to NaN: 10\n",
      "Features affected: ['4', '8', '22', '23', '26', '27', '28', '113', '129', '417']\n",
      "==================================================\n",
      "\n",
      "\n",
      "Section 8: Imputing NaN values based on original layers...\n",
      "Imputing features in layers 0NA to 794NA using pass/fail means...\n",
      "Imputing features in layers 949NA and 1018NA using the hybrid approach...\n",
      "Verification - Total remaining NaN values in X_indicators: 0\n",
      "\n",
      "Verifying imputation...\n",
      "Feature 1: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 2: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 3: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 4: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 5: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 7: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 8: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 9: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 10: Post-imputation Missing Count = 0, Zero Count = 5\n",
      "Feature 11: Post-imputation Missing Count = 0, Zero Count = 7\n",
      "Feature 12: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 13: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 15: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 16: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 17: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 18: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 19: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 20: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 21: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 22: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 23: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 24: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 25: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 26: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 27: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 28: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 29: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 30: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 31: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 32: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 33: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 34: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 35: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 36: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 37: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 38: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 39: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 40: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 41: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 42: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 44: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 45: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 46: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 47: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 48: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 49: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 51: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 52: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 54: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 55: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 56: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 57: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 58: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 59: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 60: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 61: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 62: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 63: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 64: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 65: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 66: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 67: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 68: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 69: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 71: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 72: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 73: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 74: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 76: Post-imputation Missing Count = 0, Zero Count = 3\n",
      "Feature 77: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 78: Post-imputation Missing Count = 0, Zero Count = 5\n",
      "Feature 79: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 80: Post-imputation Missing Count = 0, Zero Count = 2\n",
      "Feature 81: Post-imputation Missing Count = 0, Zero Count = 2\n",
      "Feature 82: Post-imputation Missing Count = 0, Zero Count = 2\n",
      "Feature 83: Post-imputation Missing Count = 0, Zero Count = 3\n",
      "Feature 84: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 85: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 87: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 88: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 89: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 90: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 91: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 92: Post-imputation Missing Count = 0, Zero Count = 58\n",
      "Feature 93: Post-imputation Missing Count = 0, Zero Count = 27\n",
      "Feature 94: Post-imputation Missing Count = 0, Zero Count = 35\n",
      "Feature 95: Post-imputation Missing Count = 0, Zero Count = 492\n",
      "Feature 96: Post-imputation Missing Count = 0, Zero Count = 677\n",
      "Feature 97: Post-imputation Missing Count = 0, Zero Count = 59\n",
      "Feature 99: Post-imputation Missing Count = 0, Zero Count = 58\n",
      "Feature 100: Post-imputation Missing Count = 0, Zero Count = 67\n",
      "Feature 101: Post-imputation Missing Count = 0, Zero Count = 285\n",
      "Feature 102: Post-imputation Missing Count = 0, Zero Count = 456\n",
      "Feature 103: Post-imputation Missing Count = 0, Zero Count = 59\n",
      "Feature 104: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 105: Post-imputation Missing Count = 0, Zero Count = 113\n",
      "Feature 106: Post-imputation Missing Count = 0, Zero Count = 87\n",
      "Feature 107: Post-imputation Missing Count = 0, Zero Count = 90\n",
      "Feature 108: Post-imputation Missing Count = 0, Zero Count = 59\n",
      "Feature 109: Post-imputation Missing Count = 0, Zero Count = 59\n",
      "Feature 110: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 111: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 112: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 113: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 114: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 116: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 117: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 118: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 119: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 120: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 121: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 122: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 123: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 124: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 125: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 126: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 127: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 128: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 129: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 130: Post-imputation Missing Count = 0, Zero Count = 177\n",
      "Feature 131: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 132: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 133: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 134: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 135: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 136: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 137: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 138: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 139: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 140: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 141: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 143: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 144: Post-imputation Missing Count = 0, Zero Count = 8\n",
      "Feature 145: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 146: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 147: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 148: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 149: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 151: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 152: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 153: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 154: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 155: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 156: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 157: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 160: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 161: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 162: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 163: Post-imputation Missing Count = 0, Zero Count = 5\n",
      "Feature 164: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 165: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 166: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 167: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 168: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 169: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 170: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 171: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 172: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 173: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 174: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 175: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 176: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 177: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 178: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 181: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 182: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 183: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 184: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 185: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 186: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 188: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 189: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 196: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 197: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 198: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 199: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 200: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 201: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 202: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 203: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 204: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 205: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 206: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 208: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 209: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 211: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 212: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 213: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 214: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 215: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 216: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 217: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 218: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 219: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 220: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 222: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 223: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 224: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 225: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 226: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 228: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 229: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 239: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 240: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 245: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 246: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 247: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 248: Post-imputation Missing Count = 0, Zero Count = 10\n",
      "Feature 249: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 251: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 252: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 253: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 254: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 255: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 256: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 268: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 269: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 270: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 271: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 272: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 273: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 274: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 275: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 276: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 278: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 279: Post-imputation Missing Count = 0, Zero Count = 8\n",
      "Feature 280: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 281: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 282: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 283: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 284: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 286: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 287: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 288: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 289: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 290: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 291: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 292: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 295: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 296: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 297: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 298: Post-imputation Missing Count = 0, Zero Count = 5\n",
      "Feature 299: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 300: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 301: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 302: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 303: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 304: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 305: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 306: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 307: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 308: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 309: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 310: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 311: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 312: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 313: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 317: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 318: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 319: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 320: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 321: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 322: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 324: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 325: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 332: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 333: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 334: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 335: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 336: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 337: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 338: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 339: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 340: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 341: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 342: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 344: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 345: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 346: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 347: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 349: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 350: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 351: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 352: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 353: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 354: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 355: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 356: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 357: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 358: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 360: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 361: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 362: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 363: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 364: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 366: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 367: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 368: Post-imputation Missing Count = 0, Zero Count = 58\n",
      "Feature 369: Post-imputation Missing Count = 0, Zero Count = 58\n",
      "Feature 377: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 378: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 383: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 384: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 385: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 386: Post-imputation Missing Count = 0, Zero Count = 10\n",
      "Feature 387: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 389: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 390: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 391: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 392: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 393: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 394: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 406: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 407: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 408: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 409: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 410: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 411: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 412: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 413: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 414: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 416: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 417: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 418: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 419: Post-imputation Missing Count = 0, Zero Count = 512\n",
      "Feature 420: Post-imputation Missing Count = 0, Zero Count = 712\n",
      "Feature 421: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 422: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 424: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 425: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 426: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 427: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 428: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 429: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 430: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 431: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 432: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 433: Post-imputation Missing Count = 0, Zero Count = 57\n",
      "Feature 434: Post-imputation Missing Count = 0, Zero Count = 187\n",
      "Feature 435: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 436: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 437: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 438: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 439: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 440: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 441: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 442: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 443: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 444: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 445: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 446: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 447: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 448: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 449: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 453: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 454: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 455: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 456: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 457: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 458: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 460: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 461: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 468: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 469: Post-imputation Missing Count = 0, Zero Count = 244\n",
      "Feature 470: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 471: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 472: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 473: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 474: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 475: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 476: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 477: Post-imputation Missing Count = 0, Zero Count = 1\n",
      "Feature 478: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 480: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 481: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 483: Post-imputation Missing Count = 0, Zero Count = 482\n",
      "Feature 484: Post-imputation Missing Count = 0, Zero Count = 138\n",
      "Feature 485: Post-imputation Missing Count = 0, Zero Count = 216\n",
      "Feature 486: Post-imputation Missing Count = 0, Zero Count = 184\n",
      "Feature 487: Post-imputation Missing Count = 0, Zero Count = 504\n",
      "Feature 488: Post-imputation Missing Count = 0, Zero Count = 344\n",
      "Feature 489: Post-imputation Missing Count = 0, Zero Count = 229\n",
      "Feature 490: Post-imputation Missing Count = 0, Zero Count = 196\n",
      "Feature 491: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 492: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 494: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 495: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 496: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 497: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 498: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 500: Post-imputation Missing Count = 0, Zero Count = 852\n",
      "Feature 501: Post-imputation Missing Count = 0, Zero Count = 909\n",
      "Feature 511: Post-imputation Missing Count = 0, Zero Count = 2\n",
      "Feature 512: Post-imputation Missing Count = 0, Zero Count = 820\n",
      "Feature 517: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 518: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 519: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 520: Post-imputation Missing Count = 0, Zero Count = 10\n",
      "Feature 521: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 523: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 524: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 525: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 526: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 527: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 528: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 540: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 541: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 542: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 543: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 544: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 545: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 546: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 547: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 548: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 549: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 550: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 551: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 552: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 553: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 554: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 555: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 556: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 557: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 558: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 559: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 560: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 561: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 562: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 563: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 564: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 565: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 566: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 567: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 568: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 569: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 570: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 571: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 572: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 573: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 574: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 575: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 576: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 577: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 578: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 579: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 580: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 581: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 582: Post-imputation Missing Count = 0, Zero Count = 8\n",
      "Feature 583: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 584: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 585: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 586: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 587: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 588: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 589: Post-imputation Missing Count = 0, Zero Count = 0\n",
      "Feature 590: Post-imputation Missing Count = 0, Zero Count = 32\n",
      "Feature proportion_na_features: Post-imputation Missing Count = 0, Zero Count = 0\n"
     ]
    }
   ],
   "source": [
    "# Section 6: Eliminate features with >=1341 zero values AND zero as the minimum value\n",
    "\n",
    "# Threshold for zero values\n",
    "ZERO_THRESHOLD = 1341\n",
    "\n",
    "# Identify features with >=1341 zeros AND zero as the minimum value (excluding indicator columns)\n",
    "features_with_excessive_zeros = []\n",
    "for feature in X_indicators.columns:\n",
    "    # Skip indicator columns\n",
    "    if feature.startswith('missing_'):\n",
    "        continue\n",
    "   \n",
    "    # Count zeros (using == 0 as the threshold for zeros)\n",
    "    zero_count = (X_indicators[feature] == 0).sum()\n",
    "   \n",
    "    # Compute the minimum value (excluding missing values)\n",
    "    min_value = X_indicators[feature].dropna().min() if X_indicators[feature].notna().sum() > 0 else float('inf')\n",
    "   \n",
    "    # Check both conditions: zero_count >= 1341 AND minimum value == 0\n",
    "    if zero_count >= ZERO_THRESHOLD and min_value == 0:\n",
    "        features_with_excessive_zeros.append(feature)\n",
    "        print(f\"Feature {feature}: Zero count = {zero_count}, Minimum value = {min_value:.6f}\", flush=True)\n",
    "\n",
    "# Remove the identified features\n",
    "if features_with_excessive_zeros:\n",
    "    print(f\"Features to remove (zeros >= {ZERO_THRESHOLD} and min == 0): {features_with_excessive_zeros}\", flush=True)\n",
    "    X_indicators.drop(columns=features_with_excessive_zeros, inplace=True)\n",
    "else:\n",
    "    print(f\"No features found with zeros >= {ZERO_THRESHOLD} and minimum value == 0.\", flush=True)\n",
    "\n",
    "# Print updated info about X_indicators\n",
    "print(f\"\\nNumber of features in X_indicators after zero-based removal: {len(X_indicators.columns)}\", flush=True)\n",
    "\n",
    "# Section 7: Convert zeros to NaN based on the three criteria\n",
    "print(\"\\n\" + \"=\"*50, flush=True)\n",
    "print(\"Section 7: Converting zeros to NaN based on criteria...\", flush=True)\n",
    "\n",
    "zero_to_nan_counts = {}\n",
    "for feature in X_indicators.columns:\n",
    "    # Skip 'missing_' indicator columns\n",
    "    if feature.startswith('missing_'):\n",
    "        continue\n",
    "\n",
    "    # Check if the feature has any zeros\n",
    "    zero_mask = (X_indicators[feature] == 0)\n",
    "    total_zeros = zero_mask.sum()\n",
    "    if total_zeros == 0:\n",
    "        continue\n",
    "\n",
    "    # Filter: Only consider features with 0 < total_zeros <= 10\n",
    "    if not (0 < total_zeros <= 10):\n",
    "        continue\n",
    "\n",
    "    # Compute statistics for non-zero values\n",
    "    non_zero_mask = (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "    non_zero_values = X_indicators[feature][non_zero_mask]\n",
    "    if len(non_zero_values) == 0:\n",
    "        continue  # Skip if there are no non-zero values\n",
    "\n",
    "    feature_min = X_indicators[feature].min()\n",
    "    feature_max = X_indicators[feature].max()\n",
    "    non_zero_mean = non_zero_values.mean()\n",
    "    non_zero_std = non_zero_values.std()\n",
    "    non_zero_skew = skew(non_zero_values.dropna()) if len(non_zero_values.dropna()) > 0 else 0\n",
    "\n",
    "    # Debug: Print statistics for features with zeros\n",
    "    z_score = abs(0 - non_zero_mean) / non_zero_std if non_zero_std > 0 else 0\n",
    "    print(f\"Feature {feature}: Zeros = {total_zeros}, Min = {feature_min:.6f}, Max = {feature_max:.6f}, \"\n",
    "          f\"Non-zero Skew = {non_zero_skew:.2f}, Non-zero Mean = {non_zero_mean:.6f}, Non-zero Std = {non_zero_std:.6f}, \"\n",
    "          f\"Z-score of Zero = {z_score:.2f}\", flush=True)\n",
    "\n",
    "    # Criterion 1: Minimum = 0 and significant left skew (skew < -0.5)\n",
    "    criterion_1 = False\n",
    "    if feature_min == 0:\n",
    "        if non_zero_skew < -0.5:\n",
    "            criterion_1 = True\n",
    "\n",
    "    # Criterion 2: Minimum < 0, Maximum = 0, and significant right skew (skew > 0.5)\n",
    "    criterion_2 = False\n",
    "    if feature_min < 0 and feature_max == 0:\n",
    "        if non_zero_skew > 0.5:\n",
    "            criterion_2 = True\n",
    "\n",
    "    # Criterion 3: Zero falls outside non-zero distribution, with exception for abs(maximum) < 0.05\n",
    "    criterion_3 = False\n",
    "    # First, exclude features where Min < 0 and Max > 0 (zero is within range)\n",
    "    if feature_min < 0 and feature_max > 0:\n",
    "        criterion_3 = False\n",
    "    else:\n",
    "        if abs(feature_max) >= 0.05:\n",
    "            if non_zero_std > 0:\n",
    "                z_score = abs(0 - non_zero_mean) / non_zero_std\n",
    "                if z_score > 2.0:  # Relaxed to capture more features\n",
    "                    criterion_3 = True\n",
    "\n",
    "    # Convert zeros to NaN if any criterion is met\n",
    "    if criterion_1 or criterion_2 or criterion_3:\n",
    "        X_indicators.loc[zero_mask, feature] = np.nan\n",
    "        zero_to_nan_counts[feature] = int(total_zeros)\n",
    "\n",
    "print(f\"Total features with zeros converted to NaN: {len(zero_to_nan_counts)}\", flush=True)\n",
    "print(f\"Features affected: {list(zero_to_nan_counts.keys())}\", flush=True)\n",
    "print(\"=\"*50 + \"\\n\", flush=True)\n",
    "\n",
    "# Section 8: Impute NaN values based on original layers\n",
    "print(\"\\nSection 8: Imputing NaN values based on original layers...\", flush=True)\n",
    "\n",
    "# Step 1: Compute original NaN counts (before zero-to-NaN conversion)\n",
    "current_nan_counts = X_indicators.isna().sum().clip(upper=1567)\n",
    "original_nan_counts = current_nan_counts.copy()\n",
    "for feature, zero_count in zero_to_nan_counts.items():\n",
    "    if feature in original_nan_counts:\n",
    "        original_nan_counts[feature] = max(0, current_nan_counts[feature] - zero_count)\n",
    "\n",
    "# Group features by original NaN counts\n",
    "original_nan_count_groups = original_nan_counts.groupby(original_nan_counts).groups\n",
    "\n",
    "# Step 2: Helper function to compute means for imputation\n",
    "def compute_means(feature, pass_mask, fail_mask):\n",
    "    pass_mean = X_indicators[feature][pass_mask].mean() if pass_mask.sum() > 0 else np.nan\n",
    "    fail_mean = X_indicators[feature][fail_mask].mean() if fail_mask.sum() > 0 else np.nan\n",
    "    overall_mask = (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "    overall_mean = X_indicators[feature][overall_mask].mean() if overall_mask.sum() > 0 else 0\n",
    "   \n",
    "    if np.isnan(pass_mean):\n",
    "        pass_mean = overall_mean\n",
    "    if np.isnan(fail_mean):\n",
    "        fail_mean = overall_mean\n",
    "   \n",
    "    return pass_mean, fail_mean\n",
    "\n",
    "# Step 3: Impute NaN values for layers 0NA to 794NA using pass/fail means\n",
    "print(\"Imputing features in layers 0NA to 794NA using pass/fail means...\")\n",
    "for na_count, features in sorted(original_nan_count_groups.items()):\n",
    "    if 0 <= na_count <= 794:\n",
    "        for feature in features:\n",
    "            if X_indicators[feature].isna().any():\n",
    "                pass_mask = (Y == 0) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "                fail_mask = (Y == 1) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "                pass_mean, fail_mean = compute_means(feature, pass_mask, fail_mask)\n",
    "               \n",
    "                nan_pass_mask = (X_indicators[feature].isna()) & (Y == 0)\n",
    "                nan_fail_mask = (X_indicators[feature].isna()) & (Y == 1)\n",
    "               \n",
    "                if not np.isnan(pass_mean):\n",
    "                    X_indicators.loc[nan_pass_mask, feature] = pass_mean\n",
    "                if not np.isnan(fail_mean):\n",
    "                    X_indicators.loc[nan_fail_mask, feature] = fail_mean\n",
    "\n",
    "# Step 4: Impute NaN values for layers 949NA and 1018NA using the hybrid approach\n",
    "print(\"Imputing features in layers 949NA and 1018NA using the hybrid approach...\")\n",
    "for na_count in [949, 1018]:\n",
    "    if na_count in original_nan_count_groups:\n",
    "        features = original_nan_count_groups[na_count]\n",
    "        for feature in features:\n",
    "            if X_indicators[feature].isna().any():\n",
    "                # Part 1: Overall mean for rows < 800\n",
    "                overall_mask = (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "                overall_mean = X_indicators[feature][overall_mask].mean() if overall_mask.sum() > 0 else 0\n",
    "                nan_below_800_mask = (X_indicators[feature].isna()) & (X_indicators.index < 800)\n",
    "                if not np.isnan(overall_mean):\n",
    "                    X_indicators.loc[nan_below_800_mask, feature] = overall_mean\n",
    "               \n",
    "                # Part 2: Pass/fail means for rows >= 800\n",
    "                pass_mask = (Y == 0) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "                fail_mask = (Y == 1) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "                pass_mean, fail_mean = compute_means(feature, pass_mask, fail_mask)\n",
    "               \n",
    "                nan_pass_above_800_mask = (X_indicators[feature].isna()) & (Y == 0) & (X_indicators.index >= 800)\n",
    "                nan_fail_above_800_mask = (X_indicators[feature].isna()) & (Y == 1) & (X_indicators.index >= 800)\n",
    "               \n",
    "                if not np.isnan(pass_mean):\n",
    "                    X_indicators.loc[nan_pass_above_800_mask, feature] = pass_mean\n",
    "                if not np.isnan(fail_mean):\n",
    "                    X_indicators.loc[nan_fail_above_800_mask, feature] = fail_mean\n",
    "\n",
    "# Step 5: Handle remaining features with NaN values\n",
    "remaining_features_with_na = [col for col in X_indicators.columns if X_indicators[col].isna().any()]\n",
    "if remaining_features_with_na:\n",
    "    print(\"Imputing remaining features with NaN values using pass/fail means...\")\n",
    "    for feature in remaining_features_with_na:\n",
    "        pass_mask = (Y == 0) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "        fail_mask = (Y == 1) & (X_indicators[feature] != 0) & (X_indicators[feature].notna())\n",
    "        pass_mean, fail_mean = compute_means(feature, pass_mask, fail_mask)\n",
    "       \n",
    "        nan_pass_mask = (X_indicators[feature].isna()) & (Y == 0)\n",
    "        nan_fail_mask = (X_indicators[feature].isna()) & (Y == 1)\n",
    "       \n",
    "        if not np.isnan(pass_mean):\n",
    "            X_indicators.loc[nan_pass_mask, feature] = pass_mean\n",
    "        if not np.isnan(fail_mean):\n",
    "            X_indicators.loc[nan_fail_mask, feature] = fail_mean\n",
    "\n",
    "# Step 6: Verify no NaN values remain\n",
    "remaining_nans = X_indicators.isna().sum().sum()\n",
    "print(f\"Verification - Total remaining NaN values in X_indicators: {remaining_nans}\")\n",
    "\n",
    "# Section 9: Verify imputation\n",
    "print(\"\\nVerifying imputation...\", flush=True)\n",
    "for feature in X_indicators.columns:\n",
    "    # Skip indicator columns\n",
    "    if feature.startswith('missing_'):\n",
    "        continue\n",
    "    missing_count = X_indicators[feature].isna().sum()\n",
    "    zero_count = (X_indicators[feature] == 0).sum()\n",
    "    print(f\"Feature {feature}: Post-imputation Missing Count = {missing_count}, Zero Count = {zero_count}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e35e309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (SelectKBest): ['15', '22', '23', '27', '29', '33', '34', '39', '41', '57', '59', '60', '64', '65', '66', '69', '71', '77', '80', '91', '96', '101', '104', '112', '113', '122', '123', '124', '125', '126', '127', '128', '130', '131', '134', '160', '161', '164', '165', '166', '167', '181', '184', '197', '198', '200', '201', '206', '211', '248', '281', '295', '296', '299', '300', '301', '317', '320', '338', '349', '366', '431', '432', '435', '436', '437', '438', '453', '456', '461', '469', '470', '472', '478', '511', '512', '520', '543', '544', '548', '551', '552', '554', '555', '557', '558', '563', '566', '568', '570', '574', '576', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520']\n",
      "Shape of X_selected_df: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "# Section 10: Feature Selection\n",
    "\n",
    "# Select top k features using SelectKBest\n",
    "k = 100\n",
    "selector = SelectKBest(score_func=f_classif, k=k)\n",
    "X_selected = selector.fit_transform(X_indicators, Y)\n",
    "selected_features_k = X_indicators.columns[selector.get_support(indices=True)].tolist()\n",
    "X_selected_df = pd.DataFrame(X_selected, columns=selected_features_k, index=X_indicators.index)\n",
    "\n",
    "print(f\"Selected {k} features (SelectKBest): {selected_features_k}\", flush=True)\n",
    "print(f\"Shape of X_selected_df: {X_selected_df.shape}\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a39f49e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (Mutual Info): ['3', '7', '20', '34', '36', '38', '39', '41', '42', '66', '71', '73', '74', '89', '92', '110', '111', '112', '113', '123', '125', '126', '127', '128', '129', '131', '132', '133', '134', '135', '138', '139', '171', '226', '245', '246', '247', '248', '274', '275', '276', '278', '289', '309', '313', '332', '346', '347', '383', '385', '386', '407', '408', '412', '413', '417', '429', '444', '447', '478', '511', '517', '518', '519', '520', '540', '542', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '574', '575', '576', '577', '578', '579', '580', '581', '582', '584']\n",
      "Shape of X_selected_df_mi: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif, SelectKBest\n",
    "\n",
    "# Select top k features using mutual information\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "X_selected_mi = selector_mi.fit_transform(X_indicators, Y)\n",
    "selected_features_mi = X_indicators.columns[selector_mi.get_support(indices=True)].tolist()\n",
    "X_selected_df_mi = pd.DataFrame(X_selected_mi, columns=selected_features_mi, index=X_indicators.index)\n",
    "print(f\"Selected {k} features (Mutual Info): {selected_features_mi}\")\n",
    "print(f\"Shape of X_selected_df_mi: {X_selected_df_mi.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a9003a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (RFE via Logistic Regression): ['15', '20', '21', '27', '32', '33', '35', '36', '37', '38', '48', '59', '60', '65', '66', '76', '78', '82', '84', '87', '97', '99', '100', '103', '109', '112', '118', '122', '125', '127', '129', '130', '131', '133', '141', '143', '146', '147', '151', '157', '173', '175', '176', '178', '185', '196', '203', '205', '212', '214', '215', '217', '218', '219', '248', '269', '270', '278', '286', '291', '306', '311', '313', '317', '319', '322', '334', '337', '339', '341', '349', '350', '352', '356', '391', '407', '412', '417', '425', '438', '444', '446', '447', '449', '455', '456', '458', '476', '517', '525', '546', '547', '553', '557', '558', '564', '565', '567', '572', 'missing_113']\n",
      "Shape of X_selected_df_rfe: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Use logistic regression as the estimator with RFE\n",
    "estimator = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=100, step=1)\n",
    "selector_rfe.fit(X_indicators, Y)\n",
    "selected_features_rfe = X_indicators.columns[selector_rfe.support_].tolist()\n",
    "X_selected_df_rfe = X_indicators[selected_features_rfe]\n",
    "print(f\"Selected {k} features (RFE via Logistic Regression): {selected_features_rfe}\")\n",
    "print(f\"Shape of X_selected_df_rfe: {X_selected_df_rfe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee402f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (RFE via XGBoost): ['1', '11', '17', '22', '30', '34', '49', '60', '65', '68', '73', '74', '79', '80', '82', '85', '89', '92', '93', '96', '97', '103', '104', '113', '116', '118', '119', '121', '125', '126', '128', '130', '134', '139', '141', '147', '151', '156', '164', '173', '185', '189', '197', '215', '219', '239', '240', '248', '249', '253', '280', '284', '292', '296', '299', '321', '346', '347', '362', '363', '368', '386', '394', '417', '424', '427', '429', '431', '434', '438', '469', '470', '478', '489', '512', '520', '524', '547', '548', '549', '550', '552', '555', '556', '558', '562', '563', '564', '565', '567', '568', '569', '570', '576', '580', '581', '582', '588', 'missing_73', 'missing_113']\n",
      "Shape of X_selected_df_xgbrfe: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "# Use XGBoost as the estimator with RFE\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "rfe = RFE(estimator=xgb_model, n_features_to_select=100, step=1)\n",
    "rfe.fit(X_indicators, Y)\n",
    "selected_features_xgbrfe = X_indicators.columns[rfe.support_].tolist()\n",
    "X_selected_df_xgbrfe = X_indicators[selected_features_xgbrfe]\n",
    "print(f\"Selected {k} features (RFE via XGBoost): {selected_features_xgbrfe}\")\n",
    "print(f\"Shape of X_selected_df_xgbrfe: {X_selected_df_xgbrfe.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd37c397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (Random Forest): ['248', '520', '386', '113', '347', 'missing_520', '74', 'missing_73', 'missing_347', 'missing_74', '346', '563', '581', '60', '565', '73', '478', '65', 'missing_346', '580', '427', '349', '566', '567', 'missing_113', '41', '133', '552', '342', '66', '570', '206', '555', '268', '63', '153', '558', '589', 'missing_386', '568', '438', '56', '249', '442', '289', '569', '3', '511', '52', '299', '68', '64', '332', '188', '540', '27', '412', '426', '406', 'missing_248', '154', '29', '164', '288', '5', '383', '121', '461', '208', '474', '28', '204', '108', '81', '495', '302', '353', '148', '578', '10', '445', '79', '518', '125', '338', '22', '71', '421', '524', '37', '200', '411', '120', '362', '39', '166', '122', '139', '112', '430']\n",
      "Shape of X_selected_df_rf: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_indicators, Y)\n",
    "\n",
    "# Get feature importances and select top k\n",
    "importances = pd.Series(rf.feature_importances_, index=X_indicators.columns)\n",
    "top_k_features_rf = importances.nlargest(100).index.tolist()\n",
    "X_selected_df_rf = X_indicators[top_k_features_rf]\n",
    "print(f\"Selected {k} features (Random Forest): {top_k_features_rf}\")\n",
    "print(f\"Shape of X_selected_df_rf: {X_selected_df_rf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1905e468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected 100 features (Lasso): ['2', '3', '4', '15', '16', '22', '23', '24', '25', '41', '47', '52', '56', '60', '63', '65', '68', '69', '71', '72', '73', '74', '89', '91', '116', '134', '136', '137', '138', '139', '141', '152', '153', '160', '162', '163', '184', '186', '188', '189', '201', '205', '209', '224', '226', '251', '253', '269', '275', '295', '296', '297', '298', '341', '364', '414', '419', '420', '424', '426', '429', '432', '433', '434', '437', '439', '440', '457', '461', '468', '469', '472', '473', '474', '478', '483', '484', '485', '486', '487', '488', '489', '490', '491', '495', '497', '500', '501', '511', '512', '520', '542', '548', '556', '562', '563', '570', '573', '578', '582']\n",
      "Shape of X_selected_df_lasso: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "# Fit logistic regression with L1 regularization\n",
    "lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.0249, max_iter=1000)\n",
    "lasso.fit(X_indicators, Y)\n",
    "\n",
    "# Select features with non-zero coefficients\n",
    "selected_features_lasso = X_indicators.columns[lasso.coef_[0] != 0].tolist()\n",
    "X_selected_df_lasso = X_indicators[selected_features_lasso]\n",
    "print(f\"Selected {k} features (Lasso): {selected_features_lasso}\")\n",
    "print(f\"Shape of X_selected_df_lasso: {X_selected_df_lasso.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0ba5e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (Variance Threshold): ['1', '2', '3', '4', '5', '7', '13', '15', '16', '17', '19', '20', '22', '23', '24', '25', '26', '27', '28', '29', '30', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '44', '45', '46', '47', '48', '49', '51', '52', '56', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '71', '72', '73', '74', '84', '89', '91', '97', '99', '111', '112', '116', '118', '121', '123', '124', '125', '126', '127', '128', '129', '130', '134', '135', '136', '137', '138', '139', '140', '141', '143', '149', '151', '152', '153', '155', '156', '160', '161', '162', '163', '164', '165', '166', '167', '168', '170', '171', '176', '178', '181', '182', '183', '184', '185', '186', '188', '189', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '208', '209', '219', '224', '226', '246', '247', '251', '253', '256', '269', '270', '271', '272', '273', '274', '275', '276', '278', '284', '286', '287', '288', '290', '291', '295', '296', '297', '298', '300', '301', '302', '303', '313', '317', '319', '320', '322', '324', '325', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '344', '345', '346', '347', '357', '362', '364', '384', '385', '389', '391', '407', '408', '409', '410', '411', '412', '413', '414', '416', '417', '418', '419', '420', '421', '422', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '449', '453', '454', '455', '456', '457', '458', '460', '461', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '480', '481', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '494', '495', '496', '497', '498', '500', '501', '511', '512', '517', '518', '519', '520', '521', '523', '524', '525', '526', '527', '528', '540', '541', '542', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '560', '562', '563', '564', '565', '567', '569', '570', '571', '572', '573', '574', '575', '577', '578', '582', '586', '590', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520', 'missing_late_utilization']\n",
      "Shape of X_var_df: (1567, 315)\n",
      "Selected 100 features (Variance + SelectKBest): ['15', '22', '23', '27', '29', '32', '33', '34', '39', '41', '60', '64', '65', '66', '69', '71', '91', '112', '116', '123', '124', '125', '126', '127', '128', '130', '134', '139', '160', '161', '164', '165', '166', '167', '176', '181', '182', '184', '189', '197', '198', '200', '201', '204', '206', '295', '296', '300', '301', '302', '317', '320', '334', '338', '342', '412', '417', '418', '424', '431', '432', '434', '435', '436', '437', '438', '453', '456', '461', '469', '470', '472', '476', '478', '485', '489', '511', '512', '520', '546', '548', '551', '552', '554', '555', '557', '558', '563', '570', '574', '578', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520', 'missing_late_utilization']\n",
      "Shape of X_selected_df_kbest_var: (1567, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove features with variance below a threshold\n",
    "selector_var = VarianceThreshold(threshold=0.01)  # Adjust threshold as needed\n",
    "X_var = selector_var.fit_transform(X_indicators)\n",
    "selected_features_var = X_indicators.columns[selector_var.get_support()].tolist()\n",
    "X_var_df = pd.DataFrame(X_var, columns=selected_features_var, index=X_indicators.index)\n",
    "print(f\"Selected features (Variance Threshold): {selected_features_var}\")\n",
    "print(f\"Shape of X_var_df: {X_var_df.shape}\")\n",
    "\n",
    "# Apply SelectKBest on the reduced dataset\n",
    "selector_kbest_var = SelectKBest(score_func=f_classif, k=100)\n",
    "X_selected_var = selector_kbest_var.fit_transform(X_var_df, Y)\n",
    "selected_features_kbest_var = X_var_df.columns[selector_kbest_var.get_support(indices=True)].tolist()\n",
    "X_selected_df_kbest_var = pd.DataFrame(X_selected_var, columns=selected_features_kbest_var, index=X_var_df.index)\n",
    "print(f\"Selected {k} features (Variance + SelectKBest): {selected_features_kbest_var}\")\n",
    "print(f\"Shape of X_selected_df_kbest_var: {X_selected_df_kbest_var.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "28d63345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of Overlaps and Key Feature Retention:\n",
      "                                          Method Pair  Overlap\n",
      "0                           SelectKBest & Mutual Info       31\n",
      "1                                   SelectKBest & RFE       21\n",
      "2                         SelectKBest & Random Forest       42\n",
      "3                                 SelectKBest & Lasso       27\n",
      "4                     SelectKBest & Var + SelectKBest       79\n",
      "5                                   Mutual Info & RFE       27\n",
      "6                         Mutual Info & Random Forest       38\n",
      "7                                 Mutual Info & Lasso       23\n",
      "8                     Mutual Info & Var + SelectKBest       29\n",
      "9                                 RFE & Random Forest       17\n",
      "10                                        RFE & Lasso        7\n",
      "11                            RFE & Var + SelectKBest       23\n",
      "12                              Random Forest & Lasso       25\n",
      "13                  Random Forest & Var + SelectKBest       41\n",
      "14                          Lasso & Var + SelectKBest       35\n",
      "15       SelectKBest retains missing_late_utilization        0\n",
      "16       Mutual Info retains missing_late_utilization        0\n",
      "17               RFE retains missing_late_utilization        0\n",
      "18     Random Forest retains missing_late_utilization        0\n",
      "19             Lasso retains missing_late_utilization        0\n",
      "20  Var + SelectKBest retains missing_late_utiliza...        1\n",
      "21         SelectKBest retains proportion_na_features        0\n",
      "22         Mutual Info retains proportion_na_features        0\n",
      "23                 RFE retains proportion_na_features        0\n",
      "24       Random Forest retains proportion_na_features        0\n",
      "25               Lasso retains proportion_na_features        0\n",
      "26   Var + SelectKBest retains proportion_na_features        0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validated F1-Scores for Each Method:\n",
      "                   F1-Score\n",
      "SelectKBest        0.089737\n",
      "Mutual Info        0.059210\n",
      "RFE                0.102629\n",
      "Random Forest      0.053054\n",
      "Lasso              0.081956\n",
      "Var + SelectKBest  0.086868\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert lists to sets for overlap calculations\n",
    "methods = {\n",
    "    'SelectKBest': set(map(str, selected_features_k)),\n",
    "    'Mutual Info': set(map(str, selected_features_mi)),\n",
    "    'RFE': set(map(str, selected_features_rfe)),\n",
    "    'Random Forest': set(top_k_features_rf),\n",
    "    'Lasso': set(map(str, selected_features_lasso)),\n",
    "    'Var + SelectKBest': set(map(str, selected_features_kbest_var))\n",
    "}\n",
    "\n",
    "# Create a DataFrame to summarize overlaps and key feature retention\n",
    "summary_data = []\n",
    "for method1, method2 in combinations(methods.keys(), 2):\n",
    "    overlap = len(methods[method1] & methods[method2])\n",
    "    summary_data.append({\n",
    "        'Method Pair': f\"{method1} & {method2}\",\n",
    "        'Overlap': overlap\n",
    "    })\n",
    "\n",
    "# Check retention of key features\n",
    "key_features = ['missing_late_utilization', 'proportion_na_features']\n",
    "for key_feature in key_features:\n",
    "    for method, features in methods.items():\n",
    "        summary_data.append({\n",
    "            'Method Pair': f\"{method} retains {key_feature}\",\n",
    "            'Overlap': int(key_feature in features)\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\nSummary of Overlaps and Key Feature Retention:\")\n",
    "print(summary_df)\n",
    "\n",
    "# Visualize overlaps (using a Venn diagram for three methods as an example)\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3(\n",
    "    [methods['SelectKBest'], methods['Mutual Info'], methods['RFE']],\n",
    "    set_labels=('SelectKBest', 'Mutual Info', 'RFE')\n",
    ")\n",
    "plt.title(\"Feature Overlap Between SelectKBest, Mutual Info, and RFE\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model performance with cross-validated F1-scores\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "f1_scores = {}\n",
    "\n",
    "# Prepare datasets for each method\n",
    "X_selected_df_kbest = X_indicators[selected_features_k]\n",
    "X_selected_df_mi = X_indicators[[f for f in selected_features_mi if f in X_indicators.columns]]\n",
    "X_selected_df_rfe = X_indicators[selected_features_rfe]\n",
    "X_selected_df_rf = X_indicators[[f for f in top_k_features_rf if f in X_indicators.columns]]\n",
    "X_selected_df_lasso = X_indicators[[f for f in selected_features_lasso if f in X_indicators.columns]]\n",
    "X_selected_df_var_kbest = X_indicators[selected_features_kbest_var]\n",
    "\n",
    "# Compute F1-scores\n",
    "f1_scores['SelectKBest'] = cross_val_score(model, X_selected_df_kbest, Y, cv=5, scoring='f1').mean()\n",
    "f1_scores['Mutual Info'] = cross_val_score(model, X_selected_df_mi, Y, cv=5, scoring='f1').mean()\n",
    "f1_scores['RFE'] = cross_val_score(model, X_selected_df_rfe, Y, cv=5, scoring='f1').mean()\n",
    "f1_scores['Random Forest'] = cross_val_score(model, X_selected_df_rf, Y, cv=5, scoring='f1').mean()\n",
    "f1_scores['Lasso'] = cross_val_score(model, X_selected_df_lasso, Y, cv=5, scoring='f1').mean()\n",
    "f1_scores['Var + SelectKBest'] = cross_val_score(model, X_selected_df_var_kbest, Y, cv=5, scoring='f1').mean()\n",
    "\n",
    "# Display F1-scores\n",
    "f1_df = pd.DataFrame.from_dict(f1_scores, orient='index', columns=['F1-Score'])\n",
    "print(\"\\nCross-Validated F1-Scores for Each Method:\")\n",
    "print(f1_df)\n",
    "\n",
    "# Plot F1-scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "f1_df['F1-Score'].plot(kind='bar', color='skyblue')\n",
    "plt.title('F1-Scores for Different Feature Selection Methods')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.xlabel('Method')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1996da67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert lists to sets for Venn diagram\n",
    "set_var_kbest = set(map(str, selected_features_kbest_var))\n",
    "set_rfe = set(map(str, selected_features_rfe))\n",
    "set_mi = set(map(str, selected_features_mi))\n",
    "\n",
    "# Create Venn diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3(\n",
    "    [set_var_kbest, set_rfe, set_mi],\n",
    "    set_labels=('Var + SelectKBest', 'RFE', 'Mutual Info')\n",
    ")\n",
    "plt.title(\"Feature Overlap Between Var + SelectKBest, RFE, and Mutual Info\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "750c060f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with class weights: 0.294675\n",
      "\n",
      "Logistic Regression Coefficients (with class weights):\n",
      "         15        20       21        27       32        33        35  \\\n",
      "0 -0.254543  0.938769  0.27085 -0.926835 -0.73913  0.301817  1.104904   \n",
      "\n",
      "         36       37        38        48        59       60        65  \\\n",
      "0 -0.677766  0.63062 -0.053512  1.485018  3.008889  0.13482  0.328346   \n",
      "\n",
      "         66        76        78        82        84        87        97  \\\n",
      "0 -0.153398  0.947906  1.203165  0.044829  0.103942  0.578413 -0.817839   \n",
      "\n",
      "         99       100       103       109       112       118       122  \\\n",
      "0 -0.436105  2.387235 -0.798792  0.278691 -0.450977 -0.236416  1.478248   \n",
      "\n",
      "        125       127       129       130       131       133       141  \\\n",
      "0  0.412778  0.946782 -1.245476  0.516608  0.895354 -0.964758 -0.213122   \n",
      "\n",
      "        143       146       147       151       157      173       175  \\\n",
      "0 -0.295703 -1.087391 -1.008099  0.242902  0.536326  0.36595  0.368626   \n",
      "\n",
      "        176       178       185       196       203       205       212  \\\n",
      "0  0.252858 -0.435193  0.010603  0.305035  0.299419 -0.429113  0.828866   \n",
      "\n",
      "        214       215       217       218      219       248       269  \\\n",
      "0  0.998781 -0.567097 -0.150646  0.557327 -0.06809  1.557446 -0.127091   \n",
      "\n",
      "        270       278       286       291       306       311      313  \\\n",
      "0  0.122399  0.453249 -0.444355 -1.018548 -0.098829  0.113353 -0.28231   \n",
      "\n",
      "        317       319       322       334       337       339      341  \\\n",
      "0 -0.216233  0.473331 -0.843231  0.176038  0.304581 -1.000244  0.84529   \n",
      "\n",
      "        349       350       352       356       391       407       412  \\\n",
      "0  0.254248  0.353928  0.465753  0.263271 -2.762406  0.304488  0.449886   \n",
      "\n",
      "        417       425       438       444       446       447       449  \\\n",
      "0 -0.288848 -0.217654  0.105446  0.698856  0.765056  0.390194 -0.346859   \n",
      "\n",
      "        455       456       458       476      517       525       546  \\\n",
      "0 -0.306904  0.428751  0.310927 -0.318396 -1.28851  0.400225 -0.132931   \n",
      "\n",
      "        547       553       557       558       564       565       567  \\\n",
      "0  0.384831 -0.888975 -0.114984  0.128575 -1.448119  0.923683 -2.448533   \n",
      "\n",
      "       572  missing_113  \n",
      "0 -0.20985     0.075908  \n"
     ]
    }
   ],
   "source": [
    "# Assuming X_indicators and Y are your original data, and selected_features_rfe is the list of 100 features from RFE\n",
    "X_final = X_indicators[selected_features_rfe]\n",
    "\n",
    "# Option 1: Use SMOTE (as you did previously)\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_final_sm, Y_sm = smote.fit_resample(X_final, Y)\n",
    "\n",
    "# Fit logistic regression with sklearn\n",
    "#model_sm = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "#model_sm.fit(X_final_sm, Y_sm)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "#f1_sm = cross_val_score(model_sm, X_final_sm, Y_sm, cv=5, scoring='f1').mean()\n",
    "#rint(f\"F1-score with SMOTE: {f1_sm:.6f}\")\n",
    "\n",
    "# Option 2: Use class weights without SMOTE\n",
    "model_cw = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model_cw.fit(X_final, Y)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "f1_cw = cross_val_score(model_cw, X_final, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with class weights: {f1_cw:.6f}\")\n",
    "\n",
    "# Inspect coefficients (for interpretability, similar to statsmodels summary)\n",
    "coeffs = pd.DataFrame(model_cw.coef_, columns=X_final.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (with class weights):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bd6b12eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1466\n",
      "Method:                           MLE   Df Model:                          100\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.4127\n",
      "Time:                        16:28:05   Log-Likelihood:                -224.68\n",
      "converged:                      False   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.345e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -6.6590      4.087     -1.629      0.103     -14.669       1.351\n",
      "15             -0.3239      0.189     -1.711      0.087      -0.695       0.047\n",
      "20              0.1752      0.443      0.396      0.692      -0.693       1.043\n",
      "21              0.1764      0.158      1.114      0.265      -0.134       0.487\n",
      "27             -0.1906      0.097     -1.963      0.050      -0.381      -0.000\n",
      "32             -0.4730      0.178     -2.661      0.008      -0.821      -0.125\n",
      "33              0.3740      0.297      1.260      0.208      -0.208       0.955\n",
      "35              0.3461   5938.084   5.83e-05      1.000   -1.16e+04    1.16e+04\n",
      "36             -1.1695      2.099     -0.557      0.577      -5.282       2.944\n",
      "37             -0.3472   5938.262  -5.85e-05      1.000   -1.16e+04    1.16e+04\n",
      "38             -0.0011      1.207     -0.001      0.999      -2.367       2.365\n",
      "48              0.4314      0.174      2.476      0.013       0.090       0.773\n",
      "59              0.5339      0.189      2.828      0.005       0.164       0.904\n",
      "60              1.5735      0.256      6.150      0.000       1.072       2.075\n",
      "65              2.1685      0.456      4.761      0.000       1.276       3.061\n",
      "66             -1.6221      0.509     -3.187      0.001      -2.620      -0.625\n",
      "76              0.2180      0.150      1.450      0.147      -0.077       0.513\n",
      "78              0.2782      0.143      1.943      0.052      -0.002       0.559\n",
      "82              0.0628      0.144      0.437      0.662      -0.219       0.344\n",
      "84              0.0167      0.160      0.104      0.917      -0.297       0.330\n",
      "87              0.2107      0.142      1.481      0.139      -0.068       0.490\n",
      "97             -0.1392      0.293     -0.474      0.635      -0.714       0.436\n",
      "99             -0.3032      0.299     -1.015      0.310      -0.889       0.282\n",
      "100             0.3881      0.131      2.966      0.003       0.132       0.644\n",
      "103            -0.1712      0.167     -1.026      0.305      -0.498       0.156\n",
      "109            -0.0981      0.140     -0.702      0.483      -0.372       0.176\n",
      "112            -0.4746      0.248     -1.917      0.055      -0.960       0.011\n",
      "118            -0.9668      0.754     -1.283      0.199      -2.444       0.510\n",
      "122             0.6061      0.443      1.367      0.172      -0.263       1.475\n",
      "125            -0.3980      0.416     -0.956      0.339      -1.214       0.418\n",
      "127             0.1781      0.201      0.884      0.377      -0.217       0.573\n",
      "129            -0.3313      0.241     -1.373      0.170      -0.804       0.142\n",
      "130             0.8221      0.213      3.869      0.000       0.406       1.239\n",
      "131             0.3503      0.210      1.671      0.095      -0.061       0.761\n",
      "133            -0.3678      0.224     -1.640      0.101      -0.807       0.072\n",
      "141            -6.9768     66.579     -0.105      0.917    -137.469     123.515\n",
      "143            -1.4134      0.749     -1.888      0.059      -2.880       0.054\n",
      "146            -0.3817      0.206     -1.857      0.063      -0.785       0.021\n",
      "147            -0.2072      0.182     -1.141      0.254      -0.563       0.149\n",
      "151             0.8833      0.589      1.500      0.134      -0.271       2.037\n",
      "157             0.1376      0.138      0.999      0.318      -0.132       0.407\n",
      "173             1.3666    228.317      0.006      0.995    -446.127     448.861\n",
      "175             6.1898    234.349      0.026      0.979    -453.126     465.506\n",
      "176             2.2777     36.865      0.062      0.951     -69.976      74.531\n",
      "178             3.5834      9.328      0.384      0.701     -14.700      21.867\n",
      "185             0.0470      0.216      0.217      0.828      -0.377       0.471\n",
      "196             0.2488      0.106      2.349      0.019       0.041       0.456\n",
      "203             5.4803      1.501      3.652      0.000       2.539       8.422\n",
      "205           -55.9362    383.401     -0.146      0.884    -807.388     695.516\n",
      "212            -1.1455      0.953     -1.202      0.229      -3.013       0.722\n",
      "214             0.1535      1.213      0.126      0.899      -2.225       2.532\n",
      "215            -0.2678      0.274     -0.976      0.329      -0.806       0.270\n",
      "217             0.1681      0.142      1.181      0.238      -0.111       0.447\n",
      "218             0.0583      0.884      0.066      0.947      -1.674       1.791\n",
      "219            -0.0818      0.158     -0.516      0.606      -0.392       0.229\n",
      "248             0.0557      0.106      0.528      0.597      -0.151       0.263\n",
      "269            -0.6438      0.561     -1.148      0.251      -1.743       0.455\n",
      "270             0.0872      0.147      0.595      0.552      -0.200       0.375\n",
      "278             1.0587      0.773      1.369      0.171      -0.457       2.574\n",
      "286            -0.7538      0.592     -1.274      0.203      -1.914       0.406\n",
      "291            -0.9086      0.691     -1.314      0.189      -2.264       0.446\n",
      "306            -0.0851      0.155     -0.550      0.583      -0.389       0.218\n",
      "311             0.1001      0.561      0.179      0.858      -0.999       1.199\n",
      "313            -0.6371      2.087     -0.305      0.760      -4.728       3.454\n",
      "317            -0.2313      0.187     -1.234      0.217      -0.599       0.136\n",
      "319             0.3864      0.660      0.586      0.558      -0.907       1.680\n",
      "322            -1.6226      1.196     -1.357      0.175      -3.966       0.721\n",
      "334             1.0941      0.608      1.799      0.072      -0.098       2.286\n",
      "337             2.0466      0.925      2.212      0.027       0.234       3.860\n",
      "339            -6.9509      2.064     -3.367      0.001     -10.997      -2.905\n",
      "341            49.4134    351.978      0.140      0.888    -640.452     739.279\n",
      "349             0.2776      0.165      1.680      0.093      -0.046       0.602\n",
      "350             1.4199      0.954      1.488      0.137      -0.450       3.290\n",
      "352            -0.0623      1.187     -0.053      0.958      -2.389       2.264\n",
      "356             0.0690      0.875      0.079      0.937      -1.647       1.785\n",
      "391           -51.5861     37.703     -1.368      0.171    -125.483      22.311\n",
      "407             0.5316      0.578      0.920      0.358      -0.601       1.664\n",
      "412             0.2309      0.147      1.567      0.117      -0.058       0.520\n",
      "417            -0.2419      0.167     -1.452      0.146      -0.568       0.085\n",
      "425            -0.6237      0.814     -0.767      0.443      -2.218       0.971\n",
      "438             0.2862      0.100      2.853      0.004       0.090       0.483\n",
      "444            -2.6493     51.598     -0.051      0.959    -103.779      98.481\n",
      "446            -4.5084     44.966     -0.100      0.920     -92.641      83.624\n",
      "447            -2.2896     36.994     -0.062      0.951     -74.796      70.217\n",
      "449            -3.2408      8.800     -0.368      0.713     -20.488      14.006\n",
      "455            -0.6792      0.633     -1.072      0.284      -1.920       0.562\n",
      "456             0.1104      0.287      0.385      0.700      -0.452       0.673\n",
      "458             1.2982      1.148      1.131      0.258      -0.951       3.547\n",
      "476            -0.2513      0.778     -0.323      0.747      -1.777       1.274\n",
      "517            -7.8958      6.338     -1.246      0.213     -20.319       4.527\n",
      "525             6.9354      6.448      1.076      0.282      -5.702      19.573\n",
      "546            -0.3325      0.184     -1.805      0.071      -0.693       0.029\n",
      "547             0.1569      0.195      0.804      0.421      -0.225       0.539\n",
      "553            -0.0712      0.206     -0.345      0.730      -0.475       0.333\n",
      "557            -0.2497      0.199     -1.253      0.210      -0.640       0.141\n",
      "558             0.3321      0.183      1.810      0.070      -0.027       0.692\n",
      "564            -0.2001      0.173     -1.158      0.247      -0.539       0.138\n",
      "565             2.6669      0.807      3.304      0.001       1.085       4.249\n",
      "567            -2.6913      0.838     -3.212      0.001      -4.334      -1.049\n",
      "572            -0.1265      0.150     -0.846      0.397      -0.420       0.167\n",
      "missing_113     0.0640      0.209      0.307      0.759      -0.345       0.473\n",
      "===============================================================================\n",
      "Significant features (p < 0.05): ['27', '32', '48', '59', '60', '65', '66', '100', '130', '196', '203', '337', '339', '438', '565', '567']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Select RFE features\n",
    "X_final = X_indicators[selected_features_rfe]\n",
    "\n",
    "# Scale the features to prevent numerical instability\n",
    "scaler = StandardScaler()\n",
    "X_final_scaled = scaler.fit_transform(X_final)\n",
    "X_final_scaled = pd.DataFrame(X_final_scaled, columns=X_final.columns)\n",
    "\n",
    "# Add intercept for statsmodels\n",
    "X_final_scaled = sm.add_constant(X_final_scaled)\n",
    "\n",
    "# Fit logistic regression with statsmodels\n",
    "logit_model = sm.Logit(Y, X_final_scaled)\n",
    "result = logit_model.fit(method='lbfgs', maxiter=500)  # Use 'bfgs' optimizer to avoid convergence issues\n",
    "print(result.summary())\n",
    "\n",
    "# Extract p-values\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')  # Remove intercept from feature list\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "34427390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 42 features due to high VIF: ['35', '37', '38', '66', '118', '143', '151', '173', '175', '176', '178', '203', '205', '212', '214', '218', '269', '278', '286', '311', '313', '319', '322', '334', '337', '339', '341', '350', '352', '356', '391', '407', '444', '446', '447', '449', '455', '458', '476', '525', '565', '567']\n"
     ]
    }
   ],
   "source": [
    "# Replace with the feature list for the dataset (e.g., RFE's 100 features)\n",
    "X_selected = X_indicators[selected_features_rfe]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Compute VIF to reduce multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_scaled.columns[1:]  # Exclude 'const'\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) for i in range(1, X_scaled.shape[1])]\n",
    "\n",
    "# Remove features with high VIF (> 10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"]\n",
    "X_scaled_reduced = X_scaled.drop(columns=high_vif_features)\n",
    "print(f\"Removed {len(high_vif_features)} features due to high VIF: {list(high_vif_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d63606c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177538\n",
      "         Iterations 20\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1508\n",
      "Method:                           MLE   Df Model:                           58\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2728\n",
      "Time:                        16:28:42   Log-Likelihood:                -278.20\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.953e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.3285     13.845     -0.890      0.373     -39.464      14.807\n",
      "15             -0.3137      0.130     -2.422      0.015      -0.568      -0.060\n",
      "20              0.2714      0.402      0.676      0.499      -0.516       1.059\n",
      "21              0.0778      0.143      0.543      0.587      -0.203       0.359\n",
      "27             -0.1784      0.077     -2.306      0.021      -0.330      -0.027\n",
      "32             -0.3737      0.147     -2.548      0.011      -0.661      -0.086\n",
      "33              0.5792      0.213      2.713      0.007       0.161       0.998\n",
      "36             -0.9124      0.416     -2.193      0.028      -1.728      -0.097\n",
      "48              0.1298      0.127      1.025      0.306      -0.119       0.378\n",
      "59              0.3831      0.142      2.692      0.007       0.104       0.662\n",
      "60              0.5285      0.139      3.806      0.000       0.256       0.801\n",
      "65              0.3618      0.110      3.275      0.001       0.145       0.578\n",
      "76              0.2356      0.125      1.878      0.060      -0.010       0.481\n",
      "78              0.1249      0.124      1.005      0.315      -0.119       0.368\n",
      "82              0.0598      0.128      0.467      0.641      -0.191       0.311\n",
      "84             -0.0536      0.136     -0.395      0.693      -0.320       0.212\n",
      "87              0.1738      0.125      1.389      0.165      -0.071       0.419\n",
      "97             -0.1405      0.246     -0.571      0.568      -0.623       0.342\n",
      "99             -0.2738      0.251     -1.089      0.276      -0.766       0.219\n",
      "100             0.2617      0.108      2.428      0.015       0.050       0.473\n",
      "103            -0.1076      0.132     -0.816      0.414      -0.366       0.151\n",
      "109             0.0177      0.118      0.150      0.881      -0.214       0.249\n",
      "112            -0.3707      0.219     -1.693      0.090      -0.800       0.058\n",
      "122             0.3734      0.356      1.049      0.294      -0.324       1.071\n",
      "125            -0.1466      0.344     -0.426      0.670      -0.821       0.527\n",
      "127             0.0809      0.160      0.505      0.613      -0.233       0.395\n",
      "129            -0.0283      0.182     -0.156      0.876      -0.384       0.327\n",
      "130             0.6820      0.180      3.795      0.000       0.330       1.034\n",
      "131             0.2988      0.183      1.632      0.103      -0.060       0.658\n",
      "133            -0.3484      0.188     -1.853      0.064      -0.717       0.020\n",
      "141          -162.5908    270.458     -0.601      0.548    -692.679     367.498\n",
      "146            -0.2087      0.171     -1.221      0.222      -0.544       0.126\n",
      "147            -0.1904      0.162     -1.175      0.240      -0.508       0.127\n",
      "157             0.1027      0.161      0.637      0.524      -0.213       0.419\n",
      "185             0.0728      0.147      0.495      0.621      -0.215       0.361\n",
      "196             0.1762      0.083      2.111      0.035       0.013       0.340\n",
      "215            -0.3303      0.234     -1.409      0.159      -0.790       0.129\n",
      "217             0.1111      0.125      0.890      0.374      -0.134       0.356\n",
      "219            -0.1258      0.138     -0.914      0.361      -0.396       0.144\n",
      "248             0.1281      0.087      1.474      0.141      -0.042       0.299\n",
      "270             0.1580      0.125      1.268      0.205      -0.086       0.402\n",
      "291            -0.3188      0.420     -0.759      0.448      -1.142       0.505\n",
      "306             0.0010      0.123      0.008      0.994      -0.241       0.243\n",
      "317            -0.2026      0.154     -1.314      0.189      -0.505       0.100\n",
      "349             0.3315      0.133      2.501      0.012       0.072       0.591\n",
      "412             0.3030      0.129      2.347      0.019       0.050       0.556\n",
      "417            -0.3789      0.150     -2.523      0.012      -0.673      -0.085\n",
      "425            -1.1217      0.763     -1.471      0.141      -2.617       0.373\n",
      "438             0.2171      0.082      2.632      0.008       0.055       0.379\n",
      "456             0.2736      0.133      2.053      0.040       0.012       0.535\n",
      "517            -7.6672      5.394     -1.421      0.155     -18.239       2.905\n",
      "546            -0.2656      0.165     -1.609      0.108      -0.589       0.058\n",
      "547             0.0125      0.170      0.073      0.942      -0.321       0.346\n",
      "553            -0.0029      0.175     -0.016      0.987      -0.345       0.340\n",
      "557            -0.2216      0.168     -1.316      0.188      -0.552       0.108\n",
      "558             0.2864      0.153      1.876      0.061      -0.013       0.586\n",
      "564            -0.1384      0.147     -0.940      0.347      -0.427       0.150\n",
      "572            -0.0246      0.125     -0.197      0.844      -0.269       0.220\n",
      "missing_113    -0.2137      0.158     -1.351      0.177      -0.524       0.096\n",
      "===============================================================================\n",
      "Removing feature 306 with p-value 0.993603\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177538\n",
      "         Iterations 20\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1509\n",
      "Method:                           MLE   Df Model:                           57\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2728\n",
      "Time:                        16:28:42   Log-Likelihood:                -278.20\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.595e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.3319     13.841     -0.891      0.373     -39.460      14.796\n",
      "15             -0.3137      0.129     -2.423      0.015      -0.567      -0.060\n",
      "20              0.2716      0.401      0.677      0.499      -0.515       1.058\n",
      "21              0.0778      0.143      0.543      0.587      -0.203       0.359\n",
      "27             -0.1783      0.077     -2.327      0.020      -0.329      -0.028\n",
      "32             -0.3738      0.147     -2.548      0.011      -0.661      -0.086\n",
      "33              0.5792      0.213      2.714      0.007       0.161       0.997\n",
      "36             -0.9124      0.416     -2.194      0.028      -1.728      -0.097\n",
      "48              0.1298      0.127      1.025      0.306      -0.119       0.378\n",
      "59              0.3831      0.142      2.698      0.007       0.105       0.661\n",
      "60              0.5285      0.139      3.807      0.000       0.256       0.801\n",
      "65              0.3619      0.110      3.280      0.001       0.146       0.578\n",
      "76              0.2357      0.125      1.891      0.059      -0.009       0.480\n",
      "78              0.1249      0.124      1.006      0.315      -0.119       0.368\n",
      "82              0.0599      0.128      0.469      0.639      -0.191       0.310\n",
      "84             -0.0536      0.135     -0.396      0.692      -0.319       0.212\n",
      "87              0.1739      0.125      1.392      0.164      -0.071       0.419\n",
      "97             -0.1404      0.246     -0.571      0.568      -0.622       0.341\n",
      "99             -0.2737      0.251     -1.090      0.276      -0.766       0.218\n",
      "100             0.2617      0.108      2.434      0.015       0.051       0.472\n",
      "103            -0.1077      0.132     -0.818      0.413      -0.366       0.150\n",
      "109             0.0178      0.117      0.152      0.879      -0.212       0.247\n",
      "112            -0.3706      0.218     -1.696      0.090      -0.799       0.058\n",
      "122             0.3734      0.356      1.050      0.294      -0.324       1.071\n",
      "125            -0.1466      0.344     -0.427      0.670      -0.820       0.527\n",
      "127             0.0809      0.160      0.505      0.613      -0.233       0.395\n",
      "129            -0.0283      0.182     -0.156      0.876      -0.384       0.327\n",
      "130             0.6820      0.180      3.795      0.000       0.330       1.034\n",
      "131             0.2988      0.183      1.632      0.103      -0.060       0.658\n",
      "133            -0.3484      0.188     -1.854      0.064      -0.717       0.020\n",
      "141          -162.6570    270.382     -0.602      0.547    -692.596     367.282\n",
      "146            -0.2087      0.171     -1.221      0.222      -0.544       0.126\n",
      "147            -0.1903      0.162     -1.177      0.239      -0.507       0.127\n",
      "157             0.1028      0.161      0.637      0.524      -0.213       0.419\n",
      "185             0.0728      0.147      0.495      0.621      -0.215       0.361\n",
      "196             0.1762      0.083      2.124      0.034       0.014       0.339\n",
      "215            -0.3304      0.234     -1.409      0.159      -0.790       0.129\n",
      "217             0.1110      0.124      0.893      0.372      -0.133       0.355\n",
      "219            -0.1258      0.138     -0.914      0.361      -0.396       0.144\n",
      "248             0.1281      0.087      1.474      0.141      -0.042       0.299\n",
      "270             0.1580      0.125      1.268      0.205      -0.086       0.402\n",
      "291            -0.3189      0.420     -0.759      0.448      -1.142       0.504\n",
      "317            -0.2026      0.154     -1.314      0.189      -0.505       0.100\n",
      "349             0.3315      0.133      2.501      0.012       0.072       0.591\n",
      "412             0.3030      0.129      2.349      0.019       0.050       0.556\n",
      "417            -0.3789      0.150     -2.525      0.012      -0.673      -0.085\n",
      "425            -1.1216      0.763     -1.471      0.141      -2.616       0.373\n",
      "438             0.2170      0.082      2.639      0.008       0.056       0.378\n",
      "456             0.2737      0.133      2.059      0.040       0.013       0.534\n",
      "517            -7.6643      5.381     -1.424      0.154     -18.212       2.883\n",
      "546            -0.2656      0.165     -1.612      0.107      -0.589       0.057\n",
      "547             0.0125      0.170      0.073      0.941      -0.321       0.346\n",
      "553            -0.0029      0.175     -0.016      0.987      -0.345       0.340\n",
      "557            -0.2216      0.168     -1.317      0.188      -0.552       0.108\n",
      "558             0.2865      0.153      1.878      0.060      -0.013       0.585\n",
      "564            -0.1385      0.147     -0.942      0.346      -0.426       0.149\n",
      "572            -0.0246      0.125     -0.197      0.844      -0.269       0.220\n",
      "missing_113    -0.2138      0.157     -1.358      0.174      -0.522       0.095\n",
      "===============================================================================\n",
      "Removing feature 553 with p-value 0.986910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177538\n",
      "         Iterations 20\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1510\n",
      "Method:                           MLE   Df Model:                           56\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2728\n",
      "Time:                        16:28:43   Log-Likelihood:                -278.20\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.842e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.3093     13.762     -0.894      0.371     -39.282      14.663\n",
      "15             -0.3136      0.129     -2.425      0.015      -0.567      -0.060\n",
      "20              0.2718      0.401      0.678      0.498      -0.514       1.058\n",
      "21              0.0778      0.143      0.544      0.587      -0.203       0.359\n",
      "27             -0.1783      0.077     -2.327      0.020      -0.328      -0.028\n",
      "32             -0.3740      0.146     -2.564      0.010      -0.660      -0.088\n",
      "33              0.5791      0.213      2.715      0.007       0.161       0.997\n",
      "36             -0.9122      0.416     -2.195      0.028      -1.727      -0.098\n",
      "48              0.1298      0.127      1.025      0.305      -0.118       0.378\n",
      "59              0.3831      0.142      2.698      0.007       0.105       0.661\n",
      "60              0.5283      0.138      3.820      0.000       0.257       0.799\n",
      "65              0.3618      0.110      3.287      0.001       0.146       0.577\n",
      "76              0.2359      0.124      1.901      0.057      -0.007       0.479\n",
      "78              0.1249      0.124      1.005      0.315      -0.119       0.368\n",
      "82              0.0598      0.128      0.468      0.640      -0.190       0.310\n",
      "84             -0.0538      0.135     -0.398      0.691      -0.319       0.211\n",
      "87              0.1738      0.125      1.392      0.164      -0.071       0.419\n",
      "97             -0.1402      0.246     -0.571      0.568      -0.621       0.341\n",
      "99             -0.2737      0.251     -1.090      0.276      -0.766       0.218\n",
      "100             0.2618      0.107      2.436      0.015       0.051       0.472\n",
      "103            -0.1076      0.131     -0.818      0.413      -0.365       0.150\n",
      "109             0.0178      0.117      0.152      0.879      -0.212       0.247\n",
      "112            -0.3706      0.218     -1.696      0.090      -0.799       0.058\n",
      "122             0.3736      0.356      1.050      0.294      -0.323       1.071\n",
      "125            -0.1468      0.344     -0.427      0.669      -0.820       0.527\n",
      "127             0.0808      0.160      0.505      0.613      -0.233       0.394\n",
      "129            -0.0281      0.181     -0.155      0.877      -0.382       0.326\n",
      "130             0.6819      0.180      3.795      0.000       0.330       1.034\n",
      "131             0.2988      0.183      1.632      0.103      -0.060       0.658\n",
      "133            -0.3486      0.187     -1.860      0.063      -0.716       0.019\n",
      "141          -162.2168    268.834     -0.603      0.546    -689.122     364.688\n",
      "146            -0.2087      0.171     -1.221      0.222      -0.544       0.126\n",
      "147            -0.1903      0.162     -1.177      0.239      -0.507       0.127\n",
      "157             0.1028      0.161      0.637      0.524      -0.213       0.419\n",
      "185             0.0728      0.147      0.496      0.620      -0.215       0.361\n",
      "196             0.1762      0.083      2.125      0.034       0.014       0.339\n",
      "215            -0.3305      0.234     -1.411      0.158      -0.790       0.128\n",
      "217             0.1110      0.124      0.893      0.372      -0.133       0.355\n",
      "219            -0.1259      0.138     -0.916      0.360      -0.396       0.144\n",
      "248             0.1281      0.087      1.474      0.140      -0.042       0.299\n",
      "270             0.1583      0.124      1.279      0.201      -0.084       0.401\n",
      "291            -0.3190      0.420     -0.760      0.447      -1.142       0.504\n",
      "317            -0.2026      0.154     -1.314      0.189      -0.505       0.100\n",
      "349             0.3316      0.132      2.505      0.012       0.072       0.591\n",
      "412             0.3030      0.129      2.349      0.019       0.050       0.556\n",
      "417            -0.3789      0.150     -2.525      0.012      -0.673      -0.085\n",
      "425            -1.1211      0.762     -1.471      0.141      -2.615       0.373\n",
      "438             0.2170      0.082      2.640      0.008       0.056       0.378\n",
      "456             0.2737      0.133      2.059      0.040       0.013       0.534\n",
      "517            -7.6623      5.380     -1.424      0.154     -18.207       2.883\n",
      "546            -0.2656      0.165     -1.613      0.107      -0.588       0.057\n",
      "547             0.0106      0.125      0.085      0.933      -0.235       0.256\n",
      "557            -0.2210      0.164     -1.347      0.178      -0.543       0.101\n",
      "558             0.2858      0.146      1.959      0.050      -0.000       0.572\n",
      "564            -0.1384      0.147     -0.942      0.346      -0.426       0.149\n",
      "572            -0.0246      0.125     -0.198      0.843      -0.269       0.220\n",
      "missing_113    -0.2137      0.157     -1.359      0.174      -0.522       0.095\n",
      "===============================================================================\n",
      "Removing feature 547 with p-value 0.932636\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177541\n",
      "         Iterations 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1511\n",
      "Method:                           MLE   Df Model:                           55\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2728\n",
      "Time:                        16:28:43   Log-Likelihood:                -278.21\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.378e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.2920     13.752     -0.894      0.371     -39.245      14.661\n",
      "15             -0.3137      0.129     -2.427      0.015      -0.567      -0.060\n",
      "20              0.2729      0.401      0.681      0.496      -0.513       1.058\n",
      "21              0.0770      0.143      0.539      0.590      -0.203       0.357\n",
      "27             -0.1782      0.077     -2.327      0.020      -0.328      -0.028\n",
      "32             -0.3738      0.146     -2.563      0.010      -0.660      -0.088\n",
      "33              0.5795      0.213      2.718      0.007       0.162       0.997\n",
      "36             -0.9136      0.415     -2.200      0.028      -1.728      -0.100\n",
      "48              0.1295      0.127      1.023      0.306      -0.119       0.378\n",
      "59              0.3820      0.141      2.701      0.007       0.105       0.659\n",
      "60              0.5274      0.138      3.827      0.000       0.257       0.797\n",
      "65              0.3615      0.110      3.286      0.001       0.146       0.577\n",
      "76              0.2363      0.124      1.905      0.057      -0.007       0.479\n",
      "78              0.1254      0.124      1.011      0.312      -0.118       0.369\n",
      "82              0.0594      0.127      0.466      0.641      -0.190       0.309\n",
      "84             -0.0534      0.135     -0.395      0.693      -0.318       0.211\n",
      "87              0.1727      0.124      1.391      0.164      -0.071       0.416\n",
      "97             -0.1419      0.245     -0.580      0.562      -0.622       0.338\n",
      "99             -0.2744      0.251     -1.094      0.274      -0.766       0.217\n",
      "100             0.2615      0.107      2.433      0.015       0.051       0.472\n",
      "103            -0.1085      0.131     -0.828      0.408      -0.365       0.148\n",
      "109             0.0176      0.117      0.150      0.880      -0.212       0.247\n",
      "112            -0.3705      0.218     -1.697      0.090      -0.799       0.057\n",
      "122             0.3729      0.356      1.048      0.294      -0.324       1.070\n",
      "125            -0.1464      0.344     -0.426      0.670      -0.820       0.527\n",
      "127             0.0808      0.160      0.505      0.614      -0.233       0.394\n",
      "129            -0.0275      0.181     -0.152      0.879      -0.382       0.327\n",
      "130             0.6819      0.180      3.796      0.000       0.330       1.034\n",
      "131             0.2984      0.183      1.631      0.103      -0.060       0.657\n",
      "133            -0.3488      0.187     -1.861      0.063      -0.716       0.018\n",
      "141          -161.9026    268.643     -0.603      0.547    -688.433     364.628\n",
      "146            -0.2086      0.171     -1.220      0.222      -0.544       0.126\n",
      "147            -0.1901      0.162     -1.176      0.240      -0.507       0.127\n",
      "157             0.1025      0.161      0.637      0.524      -0.213       0.418\n",
      "185             0.0731      0.147      0.497      0.619      -0.215       0.361\n",
      "196             0.1760      0.083      2.123      0.034       0.014       0.338\n",
      "215            -0.3284      0.232     -1.413      0.158      -0.784       0.127\n",
      "217             0.1116      0.124      0.899      0.368      -0.132       0.355\n",
      "219            -0.1265      0.137     -0.920      0.357      -0.396       0.143\n",
      "248             0.1278      0.087      1.472      0.141      -0.042       0.298\n",
      "270             0.1583      0.124      1.279      0.201      -0.084       0.401\n",
      "291            -0.3179      0.420     -0.757      0.449      -1.141       0.505\n",
      "317            -0.2036      0.154     -1.324      0.186      -0.505       0.098\n",
      "349             0.3305      0.132      2.510      0.012       0.072       0.589\n",
      "412             0.3028      0.129      2.349      0.019       0.050       0.556\n",
      "417            -0.3789      0.150     -2.525      0.012      -0.673      -0.085\n",
      "425            -1.1218      0.762     -1.472      0.141      -2.615       0.371\n",
      "438             0.2169      0.082      2.638      0.008       0.056       0.378\n",
      "456             0.2739      0.133      2.059      0.039       0.013       0.534\n",
      "517            -7.6236      5.359     -1.423      0.155     -18.127       2.880\n",
      "546            -0.2661      0.165     -1.617      0.106      -0.589       0.056\n",
      "557            -0.2213      0.164     -1.350      0.177      -0.543       0.100\n",
      "558             0.2865      0.146      1.968      0.049       0.001       0.572\n",
      "564            -0.1378      0.147     -0.940      0.347      -0.425       0.150\n",
      "572            -0.0239      0.124     -0.192      0.848      -0.268       0.220\n",
      "missing_113    -0.2138      0.157     -1.359      0.174      -0.522       0.094\n",
      "===============================================================================\n",
      "Removing feature 109 with p-value 0.880388\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177548\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1512\n",
      "Method:                           MLE   Df Model:                           54\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2728\n",
      "Time:                        16:28:43   Log-Likelihood:                -278.22\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.759e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.1001     13.563     -0.892      0.372     -38.684      14.484\n",
      "15             -0.3142      0.129     -2.431      0.015      -0.568      -0.061\n",
      "20              0.2684      0.400      0.672      0.502      -0.515       1.052\n",
      "21              0.0758      0.143      0.531      0.595      -0.204       0.355\n",
      "27             -0.1782      0.077     -2.327      0.020      -0.328      -0.028\n",
      "32             -0.3733      0.146     -2.560      0.010      -0.659      -0.088\n",
      "33              0.5799      0.213      2.719      0.007       0.162       0.998\n",
      "36             -0.9140      0.415     -2.200      0.028      -1.728      -0.100\n",
      "48              0.1298      0.127      1.025      0.305      -0.118       0.378\n",
      "59              0.3819      0.141      2.700      0.007       0.105       0.659\n",
      "60              0.5291      0.137      3.852      0.000       0.260       0.798\n",
      "65              0.3621      0.110      3.296      0.001       0.147       0.577\n",
      "76              0.2362      0.124      1.904      0.057      -0.007       0.479\n",
      "78              0.1253      0.124      1.010      0.312      -0.118       0.368\n",
      "82              0.0601      0.127      0.472      0.637      -0.190       0.310\n",
      "84             -0.0540      0.135     -0.400      0.689      -0.319       0.211\n",
      "87              0.1743      0.124      1.410      0.159      -0.068       0.417\n",
      "97             -0.1390      0.244     -0.570      0.569      -0.617       0.339\n",
      "99             -0.2719      0.250     -1.086      0.277      -0.763       0.219\n",
      "100             0.2610      0.107      2.428      0.015       0.050       0.472\n",
      "103            -0.1029      0.126     -0.819      0.413      -0.349       0.143\n",
      "112            -0.3697      0.218     -1.695      0.090      -0.797       0.058\n",
      "122             0.3764      0.355      1.060      0.289      -0.319       1.072\n",
      "125            -0.1501      0.343     -0.438      0.662      -0.822       0.522\n",
      "127             0.0804      0.160      0.502      0.615      -0.233       0.394\n",
      "129            -0.0260      0.180     -0.144      0.885      -0.379       0.327\n",
      "130             0.6833      0.179      3.808      0.000       0.332       1.035\n",
      "131             0.2979      0.183      1.629      0.103      -0.060       0.656\n",
      "133            -0.3485      0.187     -1.860      0.063      -0.716       0.019\n",
      "141          -158.1183    264.944     -0.597      0.551    -677.399     361.163\n",
      "146            -0.2089      0.171     -1.222      0.222      -0.544       0.126\n",
      "147            -0.1896      0.162     -1.173      0.241      -0.506       0.127\n",
      "157             0.1023      0.161      0.635      0.525      -0.213       0.418\n",
      "185             0.0741      0.147      0.505      0.614      -0.214       0.362\n",
      "196             0.1765      0.083      2.133      0.033       0.014       0.339\n",
      "215            -0.3286      0.233     -1.413      0.158      -0.784       0.127\n",
      "217             0.1102      0.124      0.890      0.373      -0.132       0.353\n",
      "219            -0.1275      0.137     -0.929      0.353      -0.396       0.141\n",
      "248             0.1275      0.087      1.470      0.142      -0.043       0.298\n",
      "270             0.1583      0.124      1.280      0.201      -0.084       0.401\n",
      "291            -0.3209      0.419     -0.766      0.444      -1.142       0.500\n",
      "317            -0.2044      0.154     -1.330      0.184      -0.506       0.097\n",
      "349             0.3307      0.132      2.510      0.012       0.072       0.589\n",
      "412             0.3030      0.129      2.350      0.019       0.050       0.556\n",
      "417            -0.3808      0.150     -2.547      0.011      -0.674      -0.088\n",
      "425            -1.1265      0.762     -1.478      0.139      -2.621       0.368\n",
      "438             0.2166      0.082      2.634      0.008       0.055       0.378\n",
      "456             0.2728      0.133      2.055      0.040       0.013       0.533\n",
      "517            -7.6736      5.350     -1.434      0.152     -18.160       2.813\n",
      "546            -0.2659      0.165     -1.615      0.106      -0.588       0.057\n",
      "557            -0.2240      0.163     -1.375      0.169      -0.543       0.095\n",
      "558             0.2884      0.145      1.989      0.047       0.004       0.573\n",
      "564            -0.1383      0.147     -0.943      0.346      -0.426       0.149\n",
      "572            -0.0245      0.124     -0.197      0.844      -0.268       0.219\n",
      "missing_113    -0.2137      0.157     -1.359      0.174      -0.522       0.094\n",
      "===============================================================================\n",
      "Removing feature 129 with p-value 0.885426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177554\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1513\n",
      "Method:                           MLE   Df Model:                           53\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2727\n",
      "Time:                        16:28:43   Log-Likelihood:                -278.23\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.391e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.0259     13.505     -0.890      0.373     -38.495      14.443\n",
      "15             -0.3148      0.129     -2.436      0.015      -0.568      -0.062\n",
      "20              0.2688      0.400      0.673      0.501      -0.514       1.052\n",
      "21              0.0760      0.143      0.533      0.594      -0.204       0.356\n",
      "27             -0.1780      0.077     -2.325      0.020      -0.328      -0.028\n",
      "32             -0.3719      0.145     -2.557      0.011      -0.657      -0.087\n",
      "33              0.5794      0.213      2.720      0.007       0.162       0.997\n",
      "36             -0.9109      0.414     -2.198      0.028      -1.723      -0.099\n",
      "48              0.1300      0.127      1.027      0.305      -0.118       0.378\n",
      "59              0.3797      0.141      2.702      0.007       0.104       0.655\n",
      "60              0.5269      0.136      3.867      0.000       0.260       0.794\n",
      "65              0.3613      0.110      3.297      0.001       0.147       0.576\n",
      "76              0.2350      0.124      1.897      0.058      -0.008       0.478\n",
      "78              0.1264      0.124      1.021      0.307      -0.116       0.369\n",
      "82              0.0596      0.127      0.468      0.640      -0.190       0.309\n",
      "84             -0.0532      0.135     -0.394      0.693      -0.318       0.211\n",
      "87              0.1736      0.123      1.406      0.160      -0.068       0.416\n",
      "97             -0.1385      0.244     -0.568      0.570      -0.617       0.340\n",
      "99             -0.2704      0.250     -1.081      0.280      -0.760       0.220\n",
      "100             0.2607      0.108      2.424      0.015       0.050       0.472\n",
      "103            -0.1029      0.126     -0.818      0.413      -0.349       0.143\n",
      "112            -0.3720      0.217     -1.712      0.087      -0.798       0.054\n",
      "122             0.3779      0.355      1.065      0.287      -0.318       1.073\n",
      "125            -0.1507      0.343     -0.439      0.661      -0.823       0.522\n",
      "127             0.0660      0.125      0.527      0.599      -0.180       0.312\n",
      "130             0.6777      0.175      3.871      0.000       0.335       1.021\n",
      "131             0.2962      0.182      1.626      0.104      -0.061       0.653\n",
      "133            -0.3565      0.179     -1.992      0.046      -0.707      -0.006\n",
      "141          -156.7238    263.812     -0.594      0.552    -673.786     360.338\n",
      "146            -0.2088      0.171     -1.221      0.222      -0.544       0.126\n",
      "147            -0.1904      0.162     -1.179      0.239      -0.507       0.126\n",
      "157             0.1029      0.161      0.640      0.522      -0.212       0.418\n",
      "185             0.0740      0.147      0.503      0.615      -0.214       0.362\n",
      "196             0.1772      0.083      2.146      0.032       0.015       0.339\n",
      "215            -0.3309      0.233     -1.423      0.155      -0.787       0.125\n",
      "217             0.1105      0.124      0.893      0.372      -0.132       0.353\n",
      "219            -0.1273      0.137     -0.927      0.354      -0.396       0.142\n",
      "248             0.1279      0.087      1.476      0.140      -0.042       0.298\n",
      "270             0.1601      0.123      1.301      0.193      -0.081       0.401\n",
      "291            -0.3197      0.420     -0.762      0.446      -1.142       0.503\n",
      "317            -0.2044      0.154     -1.330      0.184      -0.506       0.097\n",
      "349             0.3308      0.132      2.510      0.012       0.072       0.589\n",
      "412             0.3032      0.129      2.352      0.019       0.051       0.556\n",
      "417            -0.3814      0.149     -2.552      0.011      -0.674      -0.089\n",
      "425            -1.1376      0.759     -1.499      0.134      -2.625       0.350\n",
      "438             0.2174      0.082      2.650      0.008       0.057       0.378\n",
      "456             0.2748      0.132      2.081      0.037       0.016       0.534\n",
      "517            -7.5550      5.287     -1.429      0.153     -17.916       2.806\n",
      "546            -0.2662      0.164     -1.619      0.105      -0.588       0.056\n",
      "557            -0.2240      0.163     -1.375      0.169      -0.543       0.095\n",
      "558             0.2888      0.145      1.992      0.046       0.005       0.573\n",
      "564            -0.1381      0.147     -0.942      0.346      -0.426       0.149\n",
      "572            -0.0238      0.124     -0.192      0.848      -0.267       0.220\n",
      "missing_113    -0.2149      0.157     -1.369      0.171      -0.523       0.093\n",
      "===============================================================================\n",
      "Removing feature 572 with p-value 0.848125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177566\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1514\n",
      "Method:                           MLE   Df Model:                           52\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2727\n",
      "Time:                        16:28:44   Log-Likelihood:                -278.25\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.197e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -11.9347     13.461     -0.887      0.375     -38.318      14.449\n",
      "15             -0.3154      0.129     -2.441      0.015      -0.569      -0.062\n",
      "20              0.2736      0.399      0.686      0.493      -0.508       1.055\n",
      "21              0.0776      0.143      0.545      0.586      -0.202       0.357\n",
      "27             -0.1770      0.076     -2.317      0.021      -0.327      -0.027\n",
      "32             -0.3699      0.145     -2.552      0.011      -0.654      -0.086\n",
      "33              0.5854      0.211      2.776      0.005       0.172       0.999\n",
      "36             -0.9203      0.413     -2.230      0.026      -1.729      -0.112\n",
      "48              0.1310      0.126      1.036      0.300      -0.117       0.379\n",
      "59              0.3772      0.140      2.698      0.007       0.103       0.651\n",
      "60              0.5265      0.136      3.867      0.000       0.260       0.793\n",
      "65              0.3602      0.109      3.294      0.001       0.146       0.575\n",
      "76              0.2349      0.124      1.897      0.058      -0.008       0.478\n",
      "78              0.1262      0.124      1.019      0.308      -0.117       0.369\n",
      "82              0.0636      0.126      0.506      0.613      -0.183       0.310\n",
      "84             -0.0530      0.135     -0.393      0.695      -0.318       0.212\n",
      "87              0.1727      0.123      1.399      0.162      -0.069       0.415\n",
      "97             -0.1424      0.243     -0.586      0.558      -0.619       0.334\n",
      "99             -0.2744      0.249     -1.102      0.271      -0.763       0.214\n",
      "100             0.2585      0.107      2.415      0.016       0.049       0.468\n",
      "103            -0.1039      0.126     -0.828      0.408      -0.350       0.142\n",
      "112            -0.3738      0.217     -1.721      0.085      -0.800       0.052\n",
      "122             0.3819      0.354      1.079      0.280      -0.312       1.075\n",
      "125            -0.1531      0.343     -0.447      0.655      -0.824       0.518\n",
      "127             0.0675      0.125      0.540      0.589      -0.177       0.312\n",
      "130             0.6801      0.175      3.896      0.000       0.338       1.022\n",
      "131             0.2976      0.182      1.635      0.102      -0.059       0.654\n",
      "133            -0.3601      0.178     -2.022      0.043      -0.709      -0.011\n",
      "141          -154.8891    262.948     -0.589      0.556    -670.258     360.480\n",
      "146            -0.2071      0.171     -1.213      0.225      -0.542       0.128\n",
      "147            -0.1923      0.161     -1.192      0.233      -0.509       0.124\n",
      "157             0.1038      0.159      0.653      0.514      -0.208       0.416\n",
      "185             0.0741      0.147      0.504      0.614      -0.214       0.362\n",
      "196             0.1789      0.082      2.180      0.029       0.018       0.340\n",
      "215            -0.3329      0.233     -1.428      0.153      -0.790       0.124\n",
      "217             0.1095      0.124      0.885      0.376      -0.133       0.352\n",
      "219            -0.1264      0.137     -0.922      0.357      -0.395       0.142\n",
      "248             0.1281      0.087      1.479      0.139      -0.042       0.298\n",
      "270             0.1613      0.123      1.311      0.190      -0.080       0.402\n",
      "291            -0.3123      0.417     -0.748      0.454      -1.130       0.506\n",
      "317            -0.2030      0.154     -1.321      0.187      -0.504       0.098\n",
      "349             0.3302      0.132      2.506      0.012       0.072       0.588\n",
      "412             0.3033      0.129      2.353      0.019       0.051       0.556\n",
      "417            -0.3826      0.149     -2.562      0.010      -0.675      -0.090\n",
      "425            -1.1333      0.758     -1.495      0.135      -2.619       0.353\n",
      "438             0.2164      0.082      2.643      0.008       0.056       0.377\n",
      "456             0.2733      0.132      2.074      0.038       0.015       0.532\n",
      "517            -7.6115      5.281     -1.441      0.150     -17.962       2.739\n",
      "546            -0.2649      0.164     -1.612      0.107      -0.587       0.057\n",
      "557            -0.2248      0.163     -1.380      0.168      -0.544       0.095\n",
      "558             0.2898      0.145      2.000      0.046       0.006       0.574\n",
      "564            -0.1378      0.147     -0.940      0.347      -0.425       0.150\n",
      "missing_113    -0.2114      0.156     -1.357      0.175      -0.517       0.094\n",
      "===============================================================================\n",
      "Removing feature 84 with p-value 0.694515\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177615\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1515\n",
      "Method:                           MLE   Df Model:                           51\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2725\n",
      "Time:                        16:28:44   Log-Likelihood:                -278.32\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.209e-21\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -11.7423     13.296     -0.883      0.377     -37.802      14.317\n",
      "15             -0.3154      0.129     -2.443      0.015      -0.568      -0.062\n",
      "20              0.2818      0.398      0.708      0.479      -0.498       1.062\n",
      "21              0.0775      0.143      0.544      0.587      -0.202       0.357\n",
      "27             -0.1776      0.076     -2.323      0.020      -0.327      -0.028\n",
      "32             -0.3715      0.145     -2.561      0.010      -0.656      -0.087\n",
      "33              0.5841      0.211      2.770      0.006       0.171       0.997\n",
      "36             -0.9204      0.413     -2.230      0.026      -1.730      -0.111\n",
      "48              0.1328      0.126      1.051      0.293      -0.115       0.380\n",
      "59              0.3765      0.140      2.691      0.007       0.102       0.651\n",
      "60              0.5278      0.136      3.870      0.000       0.261       0.795\n",
      "65              0.3605      0.109      3.294      0.001       0.146       0.575\n",
      "76              0.2319      0.124      1.877      0.060      -0.010       0.474\n",
      "78              0.1278      0.124      1.034      0.301      -0.114       0.370\n",
      "82              0.0641      0.125      0.512      0.609      -0.182       0.310\n",
      "87              0.1702      0.123      1.383      0.167      -0.071       0.411\n",
      "97             -0.1432      0.243     -0.588      0.556      -0.620       0.334\n",
      "99             -0.2748      0.249     -1.102      0.270      -0.764       0.214\n",
      "100             0.2576      0.107      2.396      0.017       0.047       0.468\n",
      "103            -0.1060      0.126     -0.844      0.399      -0.352       0.140\n",
      "112            -0.3752      0.217     -1.727      0.084      -0.801       0.051\n",
      "122             0.3865      0.355      1.090      0.276      -0.309       1.082\n",
      "125            -0.1516      0.343     -0.441      0.659      -0.825       0.521\n",
      "127             0.0663      0.125      0.531      0.595      -0.178       0.311\n",
      "130             0.6802      0.175      3.896      0.000       0.338       1.022\n",
      "131             0.2957      0.182      1.625      0.104      -0.061       0.652\n",
      "133            -0.3576      0.178     -2.009      0.045      -0.707      -0.009\n",
      "141          -151.2091    259.728     -0.582      0.560    -660.266     357.848\n",
      "146            -0.2065      0.171     -1.210      0.226      -0.541       0.128\n",
      "147            -0.1983      0.161     -1.234      0.217      -0.513       0.117\n",
      "157             0.1037      0.158      0.655      0.513      -0.207       0.414\n",
      "185             0.0748      0.147      0.510      0.610      -0.213       0.363\n",
      "196             0.1780      0.082      2.174      0.030       0.018       0.338\n",
      "215            -0.3359      0.233     -1.444      0.149      -0.792       0.120\n",
      "217             0.1090      0.124      0.881      0.379      -0.134       0.352\n",
      "219            -0.1024      0.123     -0.835      0.404      -0.343       0.138\n",
      "248             0.1294      0.087      1.495      0.135      -0.040       0.299\n",
      "270             0.1601      0.123      1.302      0.193      -0.081       0.401\n",
      "291            -0.3130      0.418     -0.749      0.454      -1.132       0.506\n",
      "317            -0.2033      0.154     -1.323      0.186      -0.504       0.098\n",
      "349             0.3320      0.131      2.525      0.012       0.074       0.590\n",
      "412             0.3038      0.129      2.360      0.018       0.052       0.556\n",
      "417            -0.3850      0.149     -2.584      0.010      -0.677      -0.093\n",
      "425            -1.1393      0.757     -1.504      0.133      -2.624       0.345\n",
      "438             0.2161      0.082      2.647      0.008       0.056       0.376\n",
      "456             0.2761      0.132      2.095      0.036       0.018       0.534\n",
      "517            -7.4049      5.255     -1.409      0.159     -17.704       2.895\n",
      "546            -0.2663      0.164     -1.621      0.105      -0.588       0.056\n",
      "557            -0.2246      0.163     -1.378      0.168      -0.544       0.095\n",
      "558             0.2881      0.145      1.991      0.047       0.004       0.572\n",
      "564            -0.1336      0.146     -0.914      0.361      -0.420       0.153\n",
      "missing_113    -0.2096      0.156     -1.346      0.178      -0.515       0.095\n",
      "===============================================================================\n",
      "Removing feature 125 with p-value 0.658880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177678\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1516\n",
      "Method:                           MLE   Df Model:                           50\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2722\n",
      "Time:                        16:28:44   Log-Likelihood:                -278.42\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.246e-21\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -11.9135     13.475     -0.884      0.377     -38.324      14.497\n",
      "15             -0.3182      0.129     -2.464      0.014      -0.571      -0.065\n",
      "20              0.2899      0.398      0.729      0.466      -0.490       1.070\n",
      "21              0.0797      0.142      0.559      0.576      -0.199       0.359\n",
      "27             -0.1762      0.077     -2.303      0.021      -0.326      -0.026\n",
      "32             -0.3749      0.145     -2.588      0.010      -0.659      -0.091\n",
      "33              0.5769      0.210      2.749      0.006       0.166       0.988\n",
      "36             -0.9079      0.411     -2.209      0.027      -1.713      -0.103\n",
      "48              0.1322      0.126      1.045      0.296      -0.116       0.380\n",
      "59              0.3766      0.140      2.693      0.007       0.102       0.651\n",
      "60              0.5189      0.135      3.849      0.000       0.255       0.783\n",
      "65              0.3555      0.109      3.265      0.001       0.142       0.569\n",
      "76              0.2305      0.123      1.867      0.062      -0.012       0.472\n",
      "78              0.1273      0.124      1.030      0.303      -0.115       0.369\n",
      "82              0.0690      0.125      0.553      0.580      -0.176       0.314\n",
      "87              0.1692      0.123      1.374      0.169      -0.072       0.410\n",
      "97             -0.1407      0.244     -0.577      0.564      -0.618       0.337\n",
      "99             -0.2703      0.249     -1.084      0.278      -0.759       0.219\n",
      "100             0.2561      0.108      2.382      0.017       0.045       0.467\n",
      "103            -0.1022      0.125     -0.816      0.414      -0.348       0.143\n",
      "112            -0.3797      0.217     -1.748      0.081      -0.806       0.046\n",
      "122             0.2450      0.145      1.684      0.092      -0.040       0.530\n",
      "127             0.0724      0.124      0.586      0.558      -0.170       0.315\n",
      "130             0.6559      0.166      3.950      0.000       0.330       0.981\n",
      "131             0.3097      0.180      1.724      0.085      -0.042       0.662\n",
      "133            -0.3473      0.177     -1.964      0.050      -0.694      -0.001\n",
      "141          -154.5456    263.228     -0.587      0.557    -670.464     361.372\n",
      "146            -0.2029      0.170     -1.193      0.233      -0.536       0.131\n",
      "147            -0.2012      0.161     -1.253      0.210      -0.516       0.114\n",
      "157             0.1042      0.158      0.658      0.511      -0.206       0.415\n",
      "185             0.0755      0.149      0.508      0.611      -0.216       0.367\n",
      "196             0.1789      0.082      2.189      0.029       0.019       0.339\n",
      "215            -0.3323      0.232     -1.431      0.152      -0.787       0.123\n",
      "217             0.1029      0.123      0.838      0.402      -0.138       0.343\n",
      "219            -0.1050      0.122     -0.859      0.391      -0.345       0.135\n",
      "248             0.1309      0.087      1.512      0.130      -0.039       0.301\n",
      "270             0.1551      0.122      1.268      0.205      -0.085       0.395\n",
      "291            -0.3044      0.417     -0.730      0.465      -1.122       0.513\n",
      "317            -0.2069      0.153     -1.350      0.177      -0.507       0.094\n",
      "349             0.3298      0.131      2.510      0.012       0.072       0.587\n",
      "412             0.3004      0.129      2.335      0.020       0.048       0.553\n",
      "417            -0.3845      0.149     -2.582      0.010      -0.676      -0.093\n",
      "425            -1.1386      0.757     -1.504      0.133      -2.622       0.345\n",
      "438             0.2175      0.082      2.651      0.008       0.057       0.378\n",
      "456             0.2710      0.131      2.065      0.039       0.014       0.528\n",
      "517            -7.6119      5.237     -1.454      0.146     -17.876       2.652\n",
      "546            -0.2645      0.164     -1.609      0.108      -0.587       0.058\n",
      "557            -0.2251      0.163     -1.381      0.167      -0.545       0.094\n",
      "558             0.2877      0.145      1.987      0.047       0.004       0.572\n",
      "564            -0.1397      0.146     -0.958      0.338      -0.426       0.146\n",
      "missing_113    -0.2054      0.155     -1.322      0.186      -0.510       0.099\n",
      "===============================================================================\n",
      "Removing feature 185 with p-value 0.611180\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177752\n",
      "         Iterations 20\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1517\n",
      "Method:                           MLE   Df Model:                           49\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2719\n",
      "Time:                        16:28:44   Log-Likelihood:                -278.54\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.704e-21\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -11.8941     13.976     -0.851      0.395     -39.286      15.498\n",
      "15             -0.3197      0.129     -2.478      0.013      -0.573      -0.067\n",
      "20              0.2921      0.399      0.732      0.464      -0.490       1.074\n",
      "21              0.0788      0.142      0.554      0.580      -0.200       0.358\n",
      "27             -0.1778      0.076     -2.325      0.020      -0.328      -0.028\n",
      "32             -0.3740      0.145     -2.586      0.010      -0.657      -0.091\n",
      "33              0.5736      0.210      2.732      0.006       0.162       0.985\n",
      "36             -0.9038      0.412     -2.193      0.028      -1.712      -0.096\n",
      "48              0.1421      0.125      1.141      0.254      -0.102       0.386\n",
      "59              0.3848      0.139      2.775      0.006       0.113       0.657\n",
      "60              0.5158      0.134      3.836      0.000       0.252       0.779\n",
      "65              0.3531      0.109      3.249      0.001       0.140       0.566\n",
      "76              0.2357      0.124      1.906      0.057      -0.007       0.478\n",
      "78              0.1274      0.124      1.031      0.302      -0.115       0.369\n",
      "82              0.0695      0.125      0.557      0.578      -0.175       0.314\n",
      "87              0.1675      0.123      1.361      0.174      -0.074       0.409\n",
      "97             -0.1419      0.244     -0.582      0.561      -0.620       0.336\n",
      "99             -0.2717      0.249     -1.089      0.276      -0.761       0.217\n",
      "100             0.2553      0.108      2.375      0.018       0.045       0.466\n",
      "103            -0.0947      0.125     -0.759      0.448      -0.339       0.150\n",
      "112            -0.3768      0.216     -1.743      0.081      -0.800       0.047\n",
      "122             0.2478      0.145      1.705      0.088      -0.037       0.533\n",
      "127             0.0721      0.124      0.583      0.560      -0.170       0.314\n",
      "130             0.6596      0.166      3.978      0.000       0.335       0.985\n",
      "131             0.3118      0.179      1.737      0.082      -0.040       0.664\n",
      "133            -0.3489      0.177     -1.974      0.048      -0.695      -0.002\n",
      "141          -154.0896    273.009     -0.564      0.572    -689.178     380.999\n",
      "146            -0.2030      0.170     -1.193      0.233      -0.536       0.130\n",
      "147            -0.2006      0.160     -1.250      0.211      -0.515       0.114\n",
      "157             0.1063      0.157      0.678      0.498      -0.201       0.413\n",
      "196             0.1824      0.082      2.237      0.025       0.023       0.342\n",
      "215            -0.3247      0.230     -1.413      0.158      -0.775       0.126\n",
      "217             0.1033      0.122      0.844      0.398      -0.136       0.343\n",
      "219            -0.1032      0.122     -0.845      0.398      -0.343       0.136\n",
      "248             0.1326      0.087      1.533      0.125      -0.037       0.302\n",
      "270             0.1528      0.122      1.252      0.211      -0.086       0.392\n",
      "291            -0.3123      0.417     -0.748      0.454      -1.130       0.506\n",
      "317            -0.1838      0.147     -1.250      0.211      -0.472       0.104\n",
      "349             0.3342      0.132      2.530      0.011       0.075       0.593\n",
      "412             0.3023      0.129      2.346      0.019       0.050       0.555\n",
      "417            -0.3845      0.149     -2.582      0.010      -0.676      -0.093\n",
      "425            -1.1566      0.758     -1.526      0.127      -2.642       0.329\n",
      "438             0.2187      0.082      2.673      0.008       0.058       0.379\n",
      "456             0.2707      0.132      2.058      0.040       0.013       0.529\n",
      "517            -7.7968      5.230     -1.491      0.136     -18.048       2.454\n",
      "546            -0.2661      0.165     -1.617      0.106      -0.589       0.056\n",
      "557            -0.2260      0.163     -1.387      0.165      -0.545       0.093\n",
      "558             0.2903      0.145      2.006      0.045       0.007       0.574\n",
      "564            -0.1381      0.146     -0.947      0.344      -0.424       0.148\n",
      "missing_113    -0.2136      0.154     -1.383      0.167      -0.516       0.089\n",
      "===============================================================================\n",
      "Removing feature 21 with p-value 0.579760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177851\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1518\n",
      "Method:                           MLE   Df Model:                           48\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2715\n",
      "Time:                        16:28:44   Log-Likelihood:                -278.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.127e-22\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.2644     13.952     -0.879      0.379     -39.609      15.080\n",
      "15             -0.3210      0.129     -2.492      0.013      -0.573      -0.069\n",
      "20              0.3025      0.399      0.758      0.448      -0.480       1.085\n",
      "27             -0.1808      0.076     -2.374      0.018      -0.330      -0.032\n",
      "32             -0.3791      0.144     -2.630      0.009      -0.662      -0.097\n",
      "33              0.5746      0.210      2.743      0.006       0.164       0.985\n",
      "36             -0.9005      0.411     -2.190      0.029      -1.706      -0.095\n",
      "48              0.1449      0.124      1.164      0.244      -0.099       0.389\n",
      "59              0.3829      0.138      2.765      0.006       0.111       0.654\n",
      "60              0.5126      0.134      3.828      0.000       0.250       0.775\n",
      "65              0.3514      0.108      3.242      0.001       0.139       0.564\n",
      "76              0.2389      0.123      1.935      0.053      -0.003       0.481\n",
      "78              0.1287      0.124      1.042      0.298      -0.113       0.371\n",
      "82              0.0746      0.125      0.598      0.550      -0.170       0.319\n",
      "87              0.1701      0.123      1.386      0.166      -0.070       0.411\n",
      "97             -0.1477      0.243     -0.609      0.543      -0.623       0.328\n",
      "99             -0.2716      0.249     -1.091      0.275      -0.759       0.216\n",
      "100             0.2580      0.107      2.407      0.016       0.048       0.468\n",
      "103            -0.0949      0.124     -0.763      0.445      -0.338       0.149\n",
      "112            -0.3790      0.216     -1.751      0.080      -0.803       0.045\n",
      "122             0.2396      0.144      1.660      0.097      -0.043       0.523\n",
      "127             0.0757      0.123      0.614      0.539      -0.166       0.317\n",
      "130             0.6539      0.165      3.955      0.000       0.330       0.978\n",
      "131             0.3160      0.179      1.763      0.078      -0.035       0.667\n",
      "133            -0.3506      0.177     -1.986      0.047      -0.697      -0.005\n",
      "141          -161.2600    272.533     -0.592      0.554    -695.415     372.895\n",
      "146            -0.2021      0.170     -1.187      0.235      -0.536       0.131\n",
      "147            -0.2029      0.160     -1.265      0.206      -0.517       0.111\n",
      "157             0.0738      0.140      0.527      0.598      -0.200       0.348\n",
      "196             0.1818      0.082      2.228      0.026       0.022       0.342\n",
      "215            -0.3328      0.231     -1.442      0.149      -0.785       0.119\n",
      "217             0.1028      0.122      0.840      0.401      -0.137       0.343\n",
      "219            -0.1023      0.122     -0.838      0.402      -0.342       0.137\n",
      "248             0.1356      0.086      1.576      0.115      -0.033       0.304\n",
      "270             0.1530      0.122      1.255      0.209      -0.086       0.392\n",
      "291            -0.3244      0.420     -0.773      0.439      -1.147       0.498\n",
      "317            -0.1920      0.146     -1.315      0.188      -0.478       0.094\n",
      "349             0.3356      0.132      2.541      0.011       0.077       0.595\n",
      "412             0.3028      0.129      2.350      0.019       0.050       0.555\n",
      "417            -0.3846      0.149     -2.579      0.010      -0.677      -0.092\n",
      "425            -1.1555      0.754     -1.533      0.125      -2.633       0.322\n",
      "438             0.2228      0.082      2.717      0.007       0.062       0.384\n",
      "456             0.2720      0.132      2.068      0.039       0.014       0.530\n",
      "517            -7.8785      5.239     -1.504      0.133     -18.148       2.391\n",
      "546            -0.2622      0.165     -1.593      0.111      -0.585       0.060\n",
      "557            -0.2234      0.163     -1.372      0.170      -0.542       0.096\n",
      "558             0.2877      0.144      1.991      0.046       0.005       0.571\n",
      "564            -0.1363      0.146     -0.936      0.349      -0.422       0.149\n",
      "missing_113    -0.2131      0.154     -1.380      0.167      -0.516       0.089\n",
      "===============================================================================\n",
      "Removing feature 157 with p-value 0.597987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.177918\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1519\n",
      "Method:                           MLE   Df Model:                           47\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2713\n",
      "Time:                        16:28:45   Log-Likelihood:                -278.80\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.660e-22\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -12.2336     13.919     -0.879      0.379     -39.515      15.048\n",
      "15             -0.3240      0.129     -2.513      0.012      -0.577      -0.071\n",
      "20              0.3105      0.399      0.779      0.436      -0.471       1.092\n",
      "27             -0.1794      0.076     -2.356      0.018      -0.329      -0.030\n",
      "32             -0.3790      0.144     -2.629      0.009      -0.662      -0.096\n",
      "33              0.5741      0.209      2.744      0.006       0.164       0.984\n",
      "36             -0.8999      0.410     -2.193      0.028      -1.704      -0.096\n",
      "48              0.1458      0.124      1.172      0.241      -0.098       0.390\n",
      "59              0.3823      0.138      2.765      0.006       0.111       0.653\n",
      "60              0.5106      0.134      3.820      0.000       0.249       0.773\n",
      "65              0.3487      0.108      3.225      0.001       0.137       0.561\n",
      "76              0.2357      0.123      1.912      0.056      -0.006       0.477\n",
      "78              0.1298      0.123      1.051      0.293      -0.112       0.372\n",
      "82              0.0769      0.125      0.617      0.537      -0.167       0.321\n",
      "87              0.1695      0.123      1.382      0.167      -0.071       0.410\n",
      "97             -0.1513      0.242     -0.625      0.532      -0.626       0.323\n",
      "99             -0.2760      0.249     -1.111      0.267      -0.763       0.211\n",
      "100             0.2589      0.107      2.424      0.015       0.050       0.468\n",
      "103            -0.0935      0.124     -0.752      0.452      -0.337       0.150\n",
      "112            -0.3822      0.216     -1.765      0.077      -0.806       0.042\n",
      "122             0.2406      0.144      1.666      0.096      -0.043       0.524\n",
      "127             0.0744      0.123      0.605      0.545      -0.167       0.316\n",
      "130             0.6529      0.166      3.943      0.000       0.328       0.977\n",
      "131             0.3155      0.179      1.762      0.078      -0.035       0.666\n",
      "133            -0.3518      0.176     -1.996      0.046      -0.697      -0.006\n",
      "141          -160.6861    271.908     -0.591      0.555    -693.615     372.243\n",
      "146            -0.2002      0.170     -1.180      0.238      -0.533       0.132\n",
      "147            -0.2034      0.160     -1.270      0.204      -0.517       0.111\n",
      "196             0.1858      0.081      2.286      0.022       0.027       0.345\n",
      "215            -0.3341      0.231     -1.448      0.148      -0.786       0.118\n",
      "217             0.1015      0.123      0.827      0.408      -0.139       0.342\n",
      "219            -0.1016      0.122     -0.832      0.406      -0.341       0.138\n",
      "248             0.1352      0.086      1.569      0.117      -0.034       0.304\n",
      "270             0.1543      0.122      1.265      0.206      -0.085       0.393\n",
      "291            -0.3255      0.419     -0.777      0.437      -1.146       0.495\n",
      "317            -0.1919      0.146     -1.315      0.189      -0.478       0.094\n",
      "349             0.3341      0.132      2.534      0.011       0.076       0.593\n",
      "412             0.3061      0.129      2.376      0.018       0.054       0.559\n",
      "417            -0.3847      0.149     -2.581      0.010      -0.677      -0.093\n",
      "425            -1.1542      0.753     -1.532      0.125      -2.630       0.322\n",
      "438             0.2241      0.082      2.740      0.006       0.064       0.384\n",
      "456             0.2698      0.131      2.054      0.040       0.012       0.527\n",
      "517            -7.7874      5.233     -1.488      0.137     -18.044       2.470\n",
      "546            -0.2619      0.164     -1.593      0.111      -0.584       0.060\n",
      "557            -0.2237      0.163     -1.375      0.169      -0.543       0.095\n",
      "558             0.2878      0.144      1.992      0.046       0.005       0.571\n",
      "564            -0.1358      0.145     -0.934      0.350      -0.421       0.149\n",
      "missing_113    -0.2115      0.154     -1.371      0.170      -0.514       0.091\n",
      "===============================================================================\n",
      "Removing feature 141 with p-value 0.554548\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.178451\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1520\n",
      "Method:                           MLE   Df Model:                           46\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2691\n",
      "Time:                        16:28:45   Log-Likelihood:                -279.63\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.198e-22\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0105      0.266    -15.103      0.000      -4.531      -3.490\n",
      "15             -0.3111      0.127     -2.446      0.014      -0.560      -0.062\n",
      "20              0.3305      0.400      0.825      0.409      -0.454       1.115\n",
      "27             -0.1823      0.076     -2.393      0.017      -0.332      -0.033\n",
      "32             -0.3769      0.144     -2.613      0.009      -0.660      -0.094\n",
      "33              0.5732      0.209      2.749      0.006       0.165       0.982\n",
      "36             -0.8880      0.409     -2.174      0.030      -1.689      -0.087\n",
      "48              0.1447      0.124      1.163      0.245      -0.099       0.389\n",
      "59              0.3805      0.138      2.761      0.006       0.110       0.651\n",
      "60              0.4995      0.132      3.790      0.000       0.241       0.758\n",
      "65              0.3437      0.107      3.199      0.001       0.133       0.554\n",
      "76              0.2384      0.123      1.933      0.053      -0.003       0.480\n",
      "78              0.1285      0.124      1.038      0.299      -0.114       0.371\n",
      "82              0.0838      0.123      0.679      0.497      -0.158       0.326\n",
      "87              0.1734      0.123      1.411      0.158      -0.067       0.414\n",
      "97             -0.1508      0.242     -0.624      0.532      -0.624       0.323\n",
      "99             -0.2739      0.249     -1.100      0.271      -0.762       0.214\n",
      "100             0.2606      0.108      2.424      0.015       0.050       0.471\n",
      "103            -0.0962      0.124     -0.776      0.438      -0.339       0.147\n",
      "112            -0.4029      0.215     -1.874      0.061      -0.824       0.018\n",
      "122             0.2524      0.144      1.752      0.080      -0.030       0.535\n",
      "127             0.0738      0.124      0.597      0.551      -0.169       0.316\n",
      "130             0.6675      0.165      4.035      0.000       0.343       0.992\n",
      "131             0.3164      0.179      1.770      0.077      -0.034       0.667\n",
      "133            -0.3503      0.176     -1.988      0.047      -0.696      -0.005\n",
      "146            -0.1962      0.170     -1.153      0.249      -0.530       0.137\n",
      "147            -0.2039      0.161     -1.268      0.205      -0.519       0.111\n",
      "196             0.1773      0.080      2.218      0.027       0.021       0.334\n",
      "215            -0.3372      0.232     -1.454      0.146      -0.792       0.117\n",
      "217             0.0955      0.123      0.779      0.436      -0.145       0.336\n",
      "219            -0.0946      0.122     -0.777      0.437      -0.333       0.144\n",
      "248             0.1323      0.086      1.535      0.125      -0.037       0.301\n",
      "270             0.1598      0.122      1.310      0.190      -0.079       0.399\n",
      "291            -0.3240      0.422     -0.767      0.443      -1.152       0.504\n",
      "317            -0.1778      0.144     -1.232      0.218      -0.461       0.105\n",
      "349             0.3393      0.131      2.591      0.010       0.083       0.596\n",
      "412             0.2713      0.121      2.249      0.025       0.035       0.508\n",
      "417            -0.3753      0.146     -2.564      0.010      -0.662      -0.088\n",
      "425            -1.1307      0.750     -1.507      0.132      -2.601       0.340\n",
      "438             0.2250      0.082      2.750      0.006       0.065       0.385\n",
      "456             0.2617      0.124      2.105      0.035       0.018       0.505\n",
      "517            -6.9806      5.155     -1.354      0.176     -17.085       3.124\n",
      "546            -0.2700      0.166     -1.630      0.103      -0.595       0.055\n",
      "557            -0.2203      0.162     -1.357      0.175      -0.539       0.098\n",
      "558             0.2846      0.144      1.972      0.049       0.002       0.568\n",
      "564            -0.1512      0.144     -1.051      0.293      -0.433       0.131\n",
      "missing_113    -0.2161      0.154     -1.401      0.161      -0.518       0.086\n",
      "===============================================================================\n",
      "Removing feature 127 with p-value 0.550737\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.178560\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1521\n",
      "Method:                           MLE   Df Model:                           45\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2686\n",
      "Time:                        16:28:45   Log-Likelihood:                -279.80\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.222e-22\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0146      0.265    -15.122      0.000      -4.535      -3.494\n",
      "15             -0.3125      0.127     -2.465      0.014      -0.561      -0.064\n",
      "20              0.3519      0.399      0.882      0.378      -0.430       1.134\n",
      "27             -0.1822      0.076     -2.392      0.017      -0.332      -0.033\n",
      "32             -0.3852      0.143     -2.688      0.007      -0.666      -0.104\n",
      "33              0.5749      0.206      2.785      0.005       0.170       0.979\n",
      "36             -0.8712      0.402     -2.166      0.030      -1.659      -0.083\n",
      "48              0.1462      0.124      1.177      0.239      -0.097       0.390\n",
      "59              0.3805      0.138      2.766      0.006       0.111       0.650\n",
      "60              0.4964      0.131      3.798      0.000       0.240       0.753\n",
      "65              0.3381      0.106      3.182      0.001       0.130       0.546\n",
      "76              0.2431      0.123      1.975      0.048       0.002       0.484\n",
      "78              0.1337      0.123      1.084      0.278      -0.108       0.376\n",
      "82              0.0835      0.123      0.678      0.498      -0.158       0.325\n",
      "87              0.1725      0.123      1.404      0.160      -0.068       0.413\n",
      "97             -0.1548      0.241     -0.644      0.520      -0.626       0.317\n",
      "99             -0.2795      0.248     -1.127      0.260      -0.766       0.207\n",
      "100             0.2641      0.108      2.456      0.014       0.053       0.475\n",
      "103            -0.0933      0.124     -0.754      0.451      -0.336       0.149\n",
      "112            -0.4075      0.215     -1.895      0.058      -0.829       0.014\n",
      "122             0.2570      0.144      1.785      0.074      -0.025       0.539\n",
      "130             0.6833      0.164      4.168      0.000       0.362       1.005\n",
      "131             0.3257      0.179      1.820      0.069      -0.025       0.677\n",
      "133            -0.3304      0.173     -1.907      0.056      -0.670       0.009\n",
      "146            -0.1995      0.170     -1.175      0.240      -0.532       0.133\n",
      "147            -0.1992      0.161     -1.241      0.215      -0.514       0.115\n",
      "196             0.1780      0.080      2.232      0.026       0.022       0.334\n",
      "215            -0.3474      0.231     -1.502      0.133      -0.801       0.106\n",
      "217             0.0983      0.122      0.805      0.421      -0.141       0.338\n",
      "219            -0.0998      0.121     -0.822      0.411      -0.338       0.138\n",
      "248             0.1294      0.086      1.501      0.133      -0.040       0.298\n",
      "270             0.1548      0.122      1.270      0.204      -0.084       0.394\n",
      "291            -0.3480      0.424     -0.821      0.411      -1.178       0.482\n",
      "317            -0.1821      0.144     -1.266      0.205      -0.464       0.100\n",
      "349             0.3449      0.131      2.637      0.008       0.089       0.601\n",
      "412             0.2730      0.121      2.257      0.024       0.036       0.510\n",
      "417            -0.3772      0.147     -2.571      0.010      -0.665      -0.090\n",
      "425            -1.0988      0.743     -1.479      0.139      -2.555       0.357\n",
      "438             0.2259      0.082      2.763      0.006       0.066       0.386\n",
      "456             0.2554      0.124      2.064      0.039       0.013       0.498\n",
      "517            -7.1839      5.145     -1.396      0.163     -17.269       2.901\n",
      "546            -0.2638      0.166     -1.590      0.112      -0.589       0.061\n",
      "557            -0.2205      0.162     -1.359      0.174      -0.539       0.098\n",
      "558             0.2825      0.144      1.961      0.050       0.000       0.565\n",
      "564            -0.1556      0.144     -1.080      0.280      -0.438       0.127\n",
      "missing_113    -0.2165      0.154     -1.406      0.160      -0.518       0.085\n",
      "===============================================================================\n",
      "Removing feature 97 with p-value 0.519796\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.178695\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1522\n",
      "Method:                           MLE   Df Model:                           44\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2681\n",
      "Time:                        16:28:45   Log-Likelihood:                -280.01\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.202e-22\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0295      0.266    -15.160      0.000      -4.550      -3.509\n",
      "15             -0.3106      0.127     -2.451      0.014      -0.559      -0.062\n",
      "20              0.3466      0.400      0.867      0.386      -0.437       1.130\n",
      "27             -0.1837      0.076     -2.415      0.016      -0.333      -0.035\n",
      "32             -0.3872      0.143     -2.708      0.007      -0.668      -0.107\n",
      "33              0.5707      0.207      2.758      0.006       0.165       0.976\n",
      "36             -0.8742      0.405     -2.160      0.031      -1.667      -0.081\n",
      "48              0.1478      0.124      1.189      0.234      -0.096       0.391\n",
      "59              0.3767      0.137      2.748      0.006       0.108       0.645\n",
      "60              0.5083      0.129      3.927      0.000       0.255       0.762\n",
      "65              0.3443      0.106      3.255      0.001       0.137       0.552\n",
      "76              0.2521      0.122      2.063      0.039       0.013       0.492\n",
      "78              0.1323      0.123      1.073      0.283      -0.109       0.374\n",
      "82              0.0771      0.123      0.628      0.530      -0.164       0.318\n",
      "87              0.1775      0.123      1.447      0.148      -0.063       0.418\n",
      "99             -0.1453      0.134     -1.084      0.278      -0.408       0.117\n",
      "100             0.2839      0.102      2.781      0.005       0.084       0.484\n",
      "103            -0.0903      0.124     -0.731      0.465      -0.333       0.152\n",
      "112            -0.4187      0.215     -1.951      0.051      -0.839       0.002\n",
      "122             0.2619      0.143      1.829      0.067      -0.019       0.543\n",
      "130             0.6887      0.164      4.208      0.000       0.368       1.009\n",
      "131             0.3248      0.178      1.822      0.068      -0.025       0.674\n",
      "133            -0.3310      0.173     -1.914      0.056      -0.670       0.008\n",
      "146            -0.1925      0.169     -1.140      0.254      -0.523       0.138\n",
      "147            -0.2027      0.161     -1.261      0.207      -0.518       0.112\n",
      "196             0.1778      0.080      2.231      0.026       0.022       0.334\n",
      "215            -0.3522      0.231     -1.522      0.128      -0.806       0.101\n",
      "217             0.0987      0.122      0.809      0.418      -0.140       0.338\n",
      "219            -0.0984      0.122     -0.810      0.418      -0.337       0.140\n",
      "248             0.1276      0.086      1.482      0.138      -0.041       0.296\n",
      "270             0.1525      0.122      1.254      0.210      -0.086       0.391\n",
      "291            -0.3727      0.420     -0.887      0.375      -1.196       0.451\n",
      "317            -0.1879      0.144     -1.308      0.191      -0.469       0.094\n",
      "349             0.3497      0.130      2.686      0.007       0.094       0.605\n",
      "412             0.2692      0.121      2.222      0.026       0.032       0.507\n",
      "417            -0.3800      0.147     -2.587      0.010      -0.668      -0.092\n",
      "425            -1.1204      0.743     -1.508      0.132      -2.577       0.336\n",
      "438             0.2227      0.082      2.713      0.007       0.062       0.384\n",
      "456             0.2564      0.124      2.072      0.038       0.014       0.499\n",
      "517            -7.4599      5.137     -1.452      0.146     -17.529       2.609\n",
      "546            -0.2623      0.166     -1.581      0.114      -0.588       0.063\n",
      "557            -0.2276      0.162     -1.407      0.160      -0.545       0.090\n",
      "558             0.2897      0.143      2.019      0.043       0.009       0.571\n",
      "564            -0.1592      0.144     -1.108      0.268      -0.441       0.122\n",
      "missing_113    -0.2125      0.154     -1.382      0.167      -0.514       0.089\n",
      "===============================================================================\n",
      "Removing feature 82 with p-value 0.530318\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.178821\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1523\n",
      "Method:                           MLE   Df Model:                           43\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2676\n",
      "Time:                        16:28:45   Log-Likelihood:                -280.21\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.377e-23\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0324      0.266    -15.140      0.000      -4.554      -3.510\n",
      "15             -0.3074      0.127     -2.427      0.015      -0.556      -0.059\n",
      "20              0.3483      0.398      0.875      0.382      -0.432       1.129\n",
      "27             -0.1840      0.076     -2.412      0.016      -0.333      -0.035\n",
      "32             -0.3878      0.143     -2.714      0.007      -0.668      -0.108\n",
      "33              0.5639      0.206      2.732      0.006       0.159       0.968\n",
      "36             -0.8718      0.404     -2.158      0.031      -1.664      -0.080\n",
      "48              0.1467      0.124      1.179      0.238      -0.097       0.391\n",
      "59              0.3762      0.137      2.744      0.006       0.107       0.645\n",
      "60              0.5064      0.129      3.915      0.000       0.253       0.760\n",
      "65              0.3403      0.106      3.217      0.001       0.133       0.548\n",
      "76              0.2505      0.122      2.049      0.040       0.011       0.490\n",
      "78              0.1433      0.122      1.170      0.242      -0.097       0.383\n",
      "87              0.1828      0.122      1.493      0.135      -0.057       0.423\n",
      "99             -0.1412      0.134     -1.055      0.291      -0.403       0.121\n",
      "100             0.2831      0.102      2.780      0.005       0.084       0.483\n",
      "103            -0.0892      0.124     -0.722      0.470      -0.331       0.153\n",
      "112            -0.4230      0.214     -1.978      0.048      -0.842      -0.004\n",
      "122             0.2592      0.143      1.809      0.071      -0.022       0.540\n",
      "130             0.6858      0.164      4.192      0.000       0.365       1.006\n",
      "131             0.3248      0.178      1.825      0.068      -0.024       0.674\n",
      "133            -0.3303      0.173     -1.907      0.056      -0.670       0.009\n",
      "146            -0.1911      0.169     -1.131      0.258      -0.522       0.140\n",
      "147            -0.2056      0.161     -1.280      0.201      -0.521       0.109\n",
      "196             0.1751      0.079      2.208      0.027       0.020       0.331\n",
      "215            -0.3547      0.232     -1.526      0.127      -0.810       0.101\n",
      "217             0.1164      0.118      0.984      0.325      -0.115       0.348\n",
      "219            -0.1025      0.122     -0.842      0.400      -0.341       0.136\n",
      "248             0.1281      0.086      1.490      0.136      -0.040       0.297\n",
      "270             0.1559      0.121      1.284      0.199      -0.082       0.394\n",
      "291            -0.3623      0.422     -0.859      0.390      -1.189       0.464\n",
      "317            -0.1911      0.143     -1.332      0.183      -0.472       0.090\n",
      "349             0.3537      0.131      2.706      0.007       0.097       0.610\n",
      "412             0.2678      0.121      2.212      0.027       0.031       0.505\n",
      "417            -0.3810      0.147     -2.589      0.010      -0.669      -0.093\n",
      "425            -1.1417      0.742     -1.539      0.124      -2.596       0.313\n",
      "438             0.2242      0.082      2.723      0.006       0.063       0.386\n",
      "456             0.2568      0.124      2.078      0.038       0.015       0.499\n",
      "517            -7.6532      5.139     -1.489      0.136     -17.725       2.419\n",
      "546            -0.2648      0.166     -1.595      0.111      -0.590       0.061\n",
      "557            -0.2273      0.162     -1.404      0.160      -0.545       0.090\n",
      "558             0.2931      0.143      2.042      0.041       0.012       0.574\n",
      "564            -0.1596      0.144     -1.110      0.267      -0.441       0.122\n",
      "missing_113    -0.2231      0.153     -1.456      0.146      -0.523       0.077\n",
      "===============================================================================\n",
      "Removing feature 103 with p-value 0.470166\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.178987\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1524\n",
      "Method:                           MLE   Df Model:                           42\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2669\n",
      "Time:                        16:28:45   Log-Likelihood:                -280.47\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.520e-23\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0368      0.266    -15.152      0.000      -4.559      -3.515\n",
      "15             -0.3076      0.127     -2.423      0.015      -0.556      -0.059\n",
      "20              0.3523      0.399      0.884      0.377      -0.429       1.134\n",
      "27             -0.1862      0.077     -2.428      0.015      -0.336      -0.036\n",
      "32             -0.3851      0.143     -2.701      0.007      -0.665      -0.106\n",
      "33              0.5622      0.205      2.738      0.006       0.160       0.965\n",
      "36             -0.8696      0.402     -2.165      0.030      -1.657      -0.082\n",
      "48              0.1519      0.124      1.220      0.222      -0.092       0.396\n",
      "59              0.3774      0.137      2.752      0.006       0.109       0.646\n",
      "60              0.4989      0.128      3.890      0.000       0.248       0.750\n",
      "65              0.3462      0.105      3.290      0.001       0.140       0.552\n",
      "76              0.2516      0.122      2.064      0.039       0.013       0.491\n",
      "78              0.1502      0.122      1.233      0.218      -0.089       0.389\n",
      "87              0.1762      0.122      1.443      0.149      -0.063       0.416\n",
      "99             -0.1399      0.134     -1.042      0.297      -0.403       0.123\n",
      "100             0.2743      0.103      2.658      0.008       0.072       0.477\n",
      "112            -0.4260      0.214     -1.994      0.046      -0.845      -0.007\n",
      "122             0.2659      0.142      1.868      0.062      -0.013       0.545\n",
      "130             0.6932      0.163      4.247      0.000       0.373       1.013\n",
      "131             0.3199      0.178      1.797      0.072      -0.029       0.669\n",
      "133            -0.3332      0.173     -1.922      0.055      -0.673       0.007\n",
      "146            -0.1920      0.169     -1.134      0.257      -0.524       0.140\n",
      "147            -0.2057      0.161     -1.281      0.200      -0.521       0.109\n",
      "196             0.1722      0.079      2.169      0.030       0.017       0.328\n",
      "215            -0.3693      0.233     -1.585      0.113      -0.826       0.087\n",
      "217             0.1143      0.118      0.966      0.334      -0.118       0.346\n",
      "219            -0.1055      0.121     -0.869      0.385      -0.343       0.132\n",
      "248             0.1344      0.085      1.572      0.116      -0.033       0.302\n",
      "270             0.1560      0.121      1.289      0.197      -0.081       0.393\n",
      "291            -0.3653      0.421     -0.869      0.385      -1.190       0.459\n",
      "317            -0.1841      0.142     -1.299      0.194      -0.462       0.094\n",
      "349             0.3617      0.130      2.778      0.005       0.106       0.617\n",
      "412             0.2672      0.121      2.208      0.027       0.030       0.504\n",
      "417            -0.3877      0.147     -2.645      0.008      -0.675      -0.100\n",
      "425            -1.1521      0.743     -1.552      0.121      -2.608       0.303\n",
      "438             0.2257      0.082      2.760      0.006       0.065       0.386\n",
      "456             0.2628      0.123      2.136      0.033       0.022       0.504\n",
      "517            -7.8932      5.127     -1.539      0.124     -17.943       2.156\n",
      "546            -0.2587      0.166     -1.560      0.119      -0.584       0.066\n",
      "557            -0.2273      0.162     -1.405      0.160      -0.544       0.090\n",
      "558             0.2962      0.144      2.062      0.039       0.015       0.578\n",
      "564            -0.1618      0.144     -1.124      0.261      -0.444       0.120\n",
      "missing_113    -0.2310      0.153     -1.513      0.130      -0.530       0.068\n",
      "===============================================================================\n",
      "Removing feature 291 with p-value 0.384978\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179264\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1525\n",
      "Method:                           MLE   Df Model:                           41\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2657\n",
      "Time:                        16:28:45   Log-Likelihood:                -280.91\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.213e-23\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0110      0.264    -15.186      0.000      -4.529      -3.493\n",
      "15             -0.3154      0.127     -2.492      0.013      -0.563      -0.067\n",
      "20              0.4472      0.375      1.194      0.233      -0.287       1.182\n",
      "27             -0.1899      0.076     -2.487      0.013      -0.340      -0.040\n",
      "32             -0.3844      0.142     -2.704      0.007      -0.663      -0.106\n",
      "33              0.5653      0.205      2.754      0.006       0.163       0.968\n",
      "36             -0.8687      0.402     -2.164      0.030      -1.656      -0.082\n",
      "48              0.1562      0.124      1.259      0.208      -0.087       0.399\n",
      "59              0.3683      0.137      2.685      0.007       0.099       0.637\n",
      "60              0.4804      0.125      3.832      0.000       0.235       0.726\n",
      "65              0.3329      0.103      3.218      0.001       0.130       0.536\n",
      "76              0.2431      0.121      2.007      0.045       0.006       0.480\n",
      "78              0.1432      0.122      1.176      0.240      -0.096       0.382\n",
      "87              0.1758      0.122      1.439      0.150      -0.064       0.415\n",
      "99             -0.1301      0.134     -0.972      0.331      -0.393       0.132\n",
      "100             0.2538      0.093      2.721      0.007       0.071       0.437\n",
      "112            -0.4276      0.214     -1.998      0.046      -0.847      -0.008\n",
      "122             0.2572      0.142      1.808      0.071      -0.022       0.536\n",
      "130             0.6968      0.164      4.254      0.000       0.376       1.018\n",
      "131             0.3231      0.178      1.817      0.069      -0.025       0.672\n",
      "133            -0.3527      0.172     -2.053      0.040      -0.689      -0.016\n",
      "146            -0.1812      0.169     -1.072      0.284      -0.512       0.150\n",
      "147            -0.2197      0.160     -1.370      0.171      -0.534       0.095\n",
      "196             0.1694      0.079      2.134      0.033       0.014       0.325\n",
      "215            -0.3323      0.225     -1.476      0.140      -0.774       0.109\n",
      "217             0.1091      0.119      0.920      0.358      -0.123       0.342\n",
      "219            -0.1081      0.121     -0.890      0.374      -0.346       0.130\n",
      "248             0.1389      0.085      1.633      0.102      -0.028       0.306\n",
      "270             0.1581      0.121      1.306      0.191      -0.079       0.395\n",
      "317            -0.1823      0.142     -1.286      0.198      -0.460       0.096\n",
      "349             0.3216      0.122      2.637      0.008       0.083       0.561\n",
      "412             0.2625      0.121      2.177      0.029       0.026       0.499\n",
      "417            -0.3831      0.146     -2.626      0.009      -0.669      -0.097\n",
      "425            -1.1828      0.743     -1.592      0.111      -2.639       0.274\n",
      "438             0.2267      0.082      2.776      0.006       0.067       0.387\n",
      "456             0.2560      0.123      2.077      0.038       0.014       0.498\n",
      "517            -7.7662      5.142     -1.510      0.131     -17.845       2.313\n",
      "546            -0.2781      0.164     -1.692      0.091      -0.600       0.044\n",
      "557            -0.2246      0.162     -1.388      0.165      -0.542       0.092\n",
      "558             0.2965      0.144      2.065      0.039       0.015       0.578\n",
      "564            -0.1623      0.144     -1.124      0.261      -0.445       0.121\n",
      "missing_113    -0.2414      0.153     -1.578      0.115      -0.541       0.058\n",
      "===============================================================================\n",
      "Removing feature 219 with p-value 0.373668\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179521\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1526\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2647\n",
      "Time:                        16:28:46   Log-Likelihood:                -281.31\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.347e-23\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0144      0.264    -15.185      0.000      -4.533      -3.496\n",
      "15             -0.3202      0.126     -2.534      0.011      -0.568      -0.073\n",
      "20              0.4445      0.375      1.184      0.236      -0.291       1.180\n",
      "27             -0.1959      0.076     -2.569      0.010      -0.345      -0.046\n",
      "32             -0.3890      0.142     -2.737      0.006      -0.668      -0.110\n",
      "33              0.5549      0.204      2.718      0.007       0.155       0.955\n",
      "36             -0.8588      0.399     -2.150      0.032      -1.642      -0.076\n",
      "48              0.1568      0.124      1.264      0.206      -0.086       0.400\n",
      "59              0.3683      0.137      2.690      0.007       0.100       0.637\n",
      "60              0.4812      0.125      3.846      0.000       0.236       0.726\n",
      "65              0.3260      0.103      3.162      0.002       0.124       0.528\n",
      "76              0.2526      0.121      2.092      0.036       0.016       0.489\n",
      "78              0.1370      0.122      1.125      0.261      -0.102       0.376\n",
      "87              0.1781      0.122      1.457      0.145      -0.061       0.418\n",
      "99             -0.1287      0.133     -0.967      0.334      -0.390       0.132\n",
      "100             0.2595      0.092      2.820      0.005       0.079       0.440\n",
      "112            -0.4177      0.213     -1.961      0.050      -0.835      -0.000\n",
      "122             0.2595      0.142      1.825      0.068      -0.019       0.538\n",
      "130             0.6954      0.164      4.241      0.000       0.374       1.017\n",
      "131             0.3197      0.178      1.797      0.072      -0.029       0.668\n",
      "133            -0.3485      0.172     -2.031      0.042      -0.685      -0.012\n",
      "146            -0.1814      0.169     -1.076      0.282      -0.512       0.149\n",
      "147            -0.2151      0.160     -1.345      0.179      -0.528       0.098\n",
      "196             0.1711      0.079      2.152      0.031       0.015       0.327\n",
      "215            -0.3416      0.228     -1.500      0.134      -0.788       0.105\n",
      "217             0.1085      0.118      0.917      0.359      -0.123       0.340\n",
      "248             0.1360      0.085      1.598      0.110      -0.031       0.303\n",
      "270             0.1632      0.121      1.350      0.177      -0.074       0.400\n",
      "317            -0.1744      0.141     -1.237      0.216      -0.451       0.102\n",
      "349             0.3241      0.122      2.648      0.008       0.084       0.564\n",
      "412             0.2562      0.121      2.124      0.034       0.020       0.493\n",
      "417            -0.3836      0.146     -2.628      0.009      -0.670      -0.098\n",
      "425            -1.2185      0.743     -1.639      0.101      -2.675       0.239\n",
      "438             0.2240      0.082      2.730      0.006       0.063       0.385\n",
      "456             0.2491      0.123      2.023      0.043       0.008       0.490\n",
      "517            -8.1202      5.134     -1.582      0.114     -18.182       1.942\n",
      "546            -0.2827      0.164     -1.720      0.085      -0.605       0.039\n",
      "557            -0.2223      0.161     -1.377      0.169      -0.539       0.094\n",
      "558             0.2905      0.143      2.031      0.042       0.010       0.571\n",
      "564            -0.1667      0.144     -1.159      0.247      -0.449       0.115\n",
      "missing_113    -0.2484      0.153     -1.628      0.104      -0.547       0.051\n",
      "===============================================================================\n",
      "Removing feature 217 with p-value 0.359224\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179782\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1527\n",
      "Method:                           MLE   Df Model:                           39\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2636\n",
      "Time:                        16:28:46   Log-Likelihood:                -281.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.169e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.0138      0.265    -15.171      0.000      -4.532      -3.495\n",
      "15             -0.3271      0.126     -2.586      0.010      -0.575      -0.079\n",
      "20              0.4441      0.374      1.188      0.235      -0.289       1.177\n",
      "27             -0.2000      0.076     -2.626      0.009      -0.349      -0.051\n",
      "32             -0.3871      0.142     -2.720      0.007      -0.666      -0.108\n",
      "33              0.5645      0.204      2.762      0.006       0.164       0.965\n",
      "36             -0.8747      0.401     -2.179      0.029      -1.661      -0.088\n",
      "48              0.1565      0.124      1.261      0.207      -0.087       0.400\n",
      "59              0.3815      0.136      2.796      0.005       0.114       0.649\n",
      "60              0.4660      0.124      3.767      0.000       0.224       0.708\n",
      "65              0.3217      0.103      3.119      0.002       0.120       0.524\n",
      "76              0.2551      0.121      2.111      0.035       0.018       0.492\n",
      "78              0.1133      0.120      0.946      0.344      -0.121       0.348\n",
      "87              0.1777      0.122      1.454      0.146      -0.062       0.417\n",
      "99             -0.1360      0.133     -1.025      0.305      -0.396       0.124\n",
      "100             0.2571      0.091      2.817      0.005       0.078       0.436\n",
      "112            -0.4277      0.212     -2.022      0.043      -0.842      -0.013\n",
      "122             0.2484      0.141      1.756      0.079      -0.029       0.526\n",
      "130             0.6983      0.164      4.270      0.000       0.378       1.019\n",
      "131             0.3190      0.177      1.802      0.071      -0.028       0.666\n",
      "133            -0.3584      0.171     -2.095      0.036      -0.694      -0.023\n",
      "146            -0.1854      0.169     -1.096      0.273      -0.517       0.146\n",
      "147            -0.2178      0.160     -1.364      0.173      -0.531       0.095\n",
      "196             0.1631      0.079      2.055      0.040       0.008       0.319\n",
      "215            -0.3257      0.223     -1.459      0.145      -0.763       0.112\n",
      "248             0.1334      0.085      1.566      0.117      -0.034       0.300\n",
      "270             0.1612      0.121      1.333      0.183      -0.076       0.398\n",
      "317            -0.1766      0.140     -1.258      0.208      -0.452       0.099\n",
      "349             0.3161      0.122      2.600      0.009       0.078       0.554\n",
      "412             0.2563      0.121      2.120      0.034       0.019       0.493\n",
      "417            -0.3892      0.146     -2.665      0.008      -0.675      -0.103\n",
      "425            -1.2473      0.740     -1.686      0.092      -2.697       0.203\n",
      "438             0.2222      0.082      2.695      0.007       0.061       0.384\n",
      "456             0.2564      0.123      2.087      0.037       0.016       0.497\n",
      "517            -8.0834      5.124     -1.578      0.115     -18.126       1.959\n",
      "546            -0.2984      0.163     -1.831      0.067      -0.618       0.021\n",
      "557            -0.2267      0.162     -1.403      0.160      -0.543       0.090\n",
      "558             0.2932      0.143      2.049      0.040       0.013       0.574\n",
      "564            -0.1786      0.144     -1.244      0.213      -0.460       0.103\n",
      "missing_113    -0.2496      0.152     -1.637      0.102      -0.548       0.049\n",
      "===============================================================================\n",
      "Removing feature 78 with p-value 0.343994\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180066\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1528\n",
      "Method:                           MLE   Df Model:                           38\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2625\n",
      "Time:                        16:28:46   Log-Likelihood:                -282.16\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.046e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9956      0.264    -15.158      0.000      -4.512      -3.479\n",
      "15             -0.3310      0.126     -2.620      0.009      -0.579      -0.083\n",
      "20              0.4268      0.371      1.151      0.250      -0.300       1.154\n",
      "27             -0.2000      0.076     -2.640      0.008      -0.348      -0.052\n",
      "32             -0.3896      0.142     -2.738      0.006      -0.668      -0.111\n",
      "33              0.5708      0.204      2.796      0.005       0.171       0.971\n",
      "36             -0.8877      0.401     -2.216      0.027      -1.673      -0.103\n",
      "48              0.1449      0.123      1.175      0.240      -0.097       0.387\n",
      "59              0.3820      0.136      2.806      0.005       0.115       0.649\n",
      "60              0.4566      0.123      3.723      0.000       0.216       0.697\n",
      "65              0.3133      0.103      3.056      0.002       0.112       0.514\n",
      "76              0.2671      0.120      2.226      0.026       0.032       0.502\n",
      "87              0.1802      0.122      1.476      0.140      -0.059       0.419\n",
      "99             -0.1324      0.132     -1.003      0.316      -0.391       0.126\n",
      "100             0.2489      0.091      2.742      0.006       0.071       0.427\n",
      "112            -0.4253      0.210     -2.022      0.043      -0.838      -0.013\n",
      "122             0.2517      0.142      1.777      0.076      -0.026       0.529\n",
      "130             0.6844      0.163      4.205      0.000       0.365       1.003\n",
      "131             0.3121      0.177      1.765      0.078      -0.035       0.659\n",
      "133            -0.3396      0.170     -1.999      0.046      -0.672      -0.007\n",
      "146            -0.1912      0.170     -1.127      0.260      -0.524       0.141\n",
      "147            -0.2148      0.160     -1.344      0.179      -0.528       0.098\n",
      "196             0.1652      0.079      2.080      0.038       0.009       0.321\n",
      "215            -0.3137      0.220     -1.424      0.154      -0.746       0.118\n",
      "248             0.1364      0.085      1.607      0.108      -0.030       0.303\n",
      "270             0.1641      0.121      1.356      0.175      -0.073       0.401\n",
      "317            -0.1753      0.140     -1.253      0.210      -0.450       0.099\n",
      "349             0.3148      0.121      2.605      0.009       0.078       0.552\n",
      "412             0.2588      0.121      2.143      0.032       0.022       0.496\n",
      "417            -0.3910      0.146     -2.682      0.007      -0.677      -0.105\n",
      "425            -1.2944      0.741     -1.747      0.081      -2.747       0.158\n",
      "438             0.2225      0.082      2.726      0.006       0.063       0.382\n",
      "456             0.2542      0.123      2.074      0.038       0.014       0.494\n",
      "517            -7.6593      5.112     -1.498      0.134     -17.678       2.359\n",
      "546            -0.2930      0.163     -1.796      0.072      -0.613       0.027\n",
      "557            -0.2287      0.161     -1.420      0.156      -0.544       0.087\n",
      "558             0.2932      0.142      2.059      0.040       0.014       0.572\n",
      "564            -0.1808      0.144     -1.258      0.209      -0.463       0.101\n",
      "missing_113    -0.2450      0.153     -1.605      0.109      -0.544       0.054\n",
      "===============================================================================\n",
      "Removing feature 99 with p-value 0.316082\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180382\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1529\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2612\n",
      "Time:                        16:28:46   Log-Likelihood:                -282.66\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.222e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9853      0.262    -15.221      0.000      -4.498      -3.472\n",
      "15             -0.3270      0.126     -2.601      0.009      -0.573      -0.081\n",
      "20              0.4328      0.373      1.160      0.246      -0.298       1.164\n",
      "27             -0.1990      0.076     -2.635      0.008      -0.347      -0.051\n",
      "32             -0.3969      0.142     -2.786      0.005      -0.676      -0.118\n",
      "33              0.5728      0.204      2.802      0.005       0.172       0.973\n",
      "36             -0.8900      0.401     -2.217      0.027      -1.677      -0.103\n",
      "48              0.1504      0.123      1.219      0.223      -0.091       0.392\n",
      "59              0.3776      0.136      2.785      0.005       0.112       0.643\n",
      "60              0.4588      0.123      3.735      0.000       0.218       0.700\n",
      "65              0.3185      0.103      3.102      0.002       0.117       0.520\n",
      "76              0.2654      0.120      2.203      0.028       0.029       0.501\n",
      "87              0.1878      0.122      1.539      0.124      -0.051       0.427\n",
      "100             0.2472      0.092      2.678      0.007       0.066       0.428\n",
      "112            -0.4350      0.210     -2.076      0.038      -0.846      -0.024\n",
      "122             0.2433      0.141      1.723      0.085      -0.033       0.520\n",
      "130             0.6625      0.160      4.146      0.000       0.349       0.976\n",
      "131             0.3220      0.176      1.827      0.068      -0.023       0.667\n",
      "133            -0.3356      0.169     -1.983      0.047      -0.667      -0.004\n",
      "146            -0.1743      0.168     -1.036      0.300      -0.504       0.155\n",
      "147            -0.2140      0.160     -1.341      0.180      -0.527       0.099\n",
      "196             0.1606      0.080      2.018      0.044       0.005       0.316\n",
      "215            -0.3189      0.220     -1.452      0.146      -0.749       0.112\n",
      "248             0.1328      0.085      1.562      0.118      -0.034       0.299\n",
      "270             0.1633      0.121      1.353      0.176      -0.073       0.400\n",
      "317            -0.1691      0.140     -1.208      0.227      -0.444       0.105\n",
      "349             0.3193      0.121      2.642      0.008       0.082       0.556\n",
      "412             0.2551      0.121      2.115      0.034       0.019       0.491\n",
      "417            -0.3936      0.146     -2.699      0.007      -0.679      -0.108\n",
      "425            -1.2644      0.734     -1.722      0.085      -2.704       0.175\n",
      "438             0.2227      0.082      2.712      0.007       0.062       0.384\n",
      "456             0.2512      0.122      2.054      0.040       0.012       0.491\n",
      "517            -7.7725      5.102     -1.523      0.128     -17.772       2.227\n",
      "546            -0.2966      0.162     -1.826      0.068      -0.615       0.022\n",
      "557            -0.2259      0.161     -1.399      0.162      -0.542       0.090\n",
      "558             0.2897      0.142      2.036      0.042       0.011       0.569\n",
      "564            -0.1812      0.144     -1.259      0.208      -0.463       0.101\n",
      "missing_113    -0.2461      0.153     -1.610      0.107      -0.546       0.054\n",
      "===============================================================================\n",
      "Removing feature 146 with p-value 0.300210\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180745\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1530\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2597\n",
      "Time:                        16:28:46   Log-Likelihood:                -283.23\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.165e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9781      0.261    -15.238      0.000      -4.490      -3.466\n",
      "15             -0.3252      0.125     -2.593      0.010      -0.571      -0.079\n",
      "20              0.4545      0.377      1.207      0.227      -0.284       1.193\n",
      "27             -0.1982      0.076     -2.619      0.009      -0.347      -0.050\n",
      "32             -0.3919      0.142     -2.756      0.006      -0.671      -0.113\n",
      "33              0.5696      0.204      2.790      0.005       0.169       0.970\n",
      "36             -0.8745      0.400     -2.184      0.029      -1.659      -0.090\n",
      "48              0.1598      0.123      1.296      0.195      -0.082       0.401\n",
      "59              0.3889      0.135      2.877      0.004       0.124       0.654\n",
      "60              0.4608      0.122      3.770      0.000       0.221       0.700\n",
      "65              0.3238      0.102      3.174      0.002       0.124       0.524\n",
      "76              0.2572      0.120      2.147      0.032       0.022       0.492\n",
      "87              0.1805      0.122      1.486      0.137      -0.058       0.419\n",
      "100             0.2500      0.092      2.707      0.007       0.069       0.431\n",
      "112            -0.4263      0.209     -2.038      0.042      -0.836      -0.016\n",
      "122             0.2497      0.141      1.766      0.077      -0.027       0.527\n",
      "130             0.6621      0.160      4.132      0.000       0.348       0.976\n",
      "131             0.3123      0.177      1.767      0.077      -0.034       0.659\n",
      "133            -0.3150      0.168     -1.871      0.061      -0.645       0.015\n",
      "147            -0.2831      0.143     -1.981      0.048      -0.563      -0.003\n",
      "196             0.1628      0.079      2.049      0.040       0.007       0.318\n",
      "215            -0.3173      0.221     -1.435      0.151      -0.751       0.116\n",
      "248             0.1304      0.085      1.537      0.124      -0.036       0.297\n",
      "270             0.1559      0.120      1.299      0.194      -0.079       0.391\n",
      "317            -0.1681      0.140     -1.198      0.231      -0.443       0.107\n",
      "349             0.3133      0.121      2.585      0.010       0.076       0.551\n",
      "412             0.2485      0.120      2.069      0.039       0.013       0.484\n",
      "417            -0.3988      0.145     -2.745      0.006      -0.683      -0.114\n",
      "425            -1.2629      0.732     -1.726      0.084      -2.697       0.171\n",
      "438             0.2299      0.082      2.812      0.005       0.070       0.390\n",
      "456             0.2517      0.123      2.054      0.040       0.011       0.492\n",
      "517            -7.8790      5.100     -1.545      0.122     -17.875       2.117\n",
      "546            -0.3000      0.163     -1.841      0.066      -0.619       0.019\n",
      "557            -0.2259      0.161     -1.403      0.161      -0.541       0.090\n",
      "558             0.2957      0.142      2.082      0.037       0.017       0.574\n",
      "564            -0.1854      0.144     -1.290      0.197      -0.467       0.096\n",
      "missing_113    -0.2432      0.152     -1.597      0.110      -0.542       0.055\n",
      "===============================================================================\n",
      "Removing feature 317 with p-value 0.230975\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181218\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1531\n",
      "Method:                           MLE   Df Model:                           35\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2577\n",
      "Time:                        16:28:46   Log-Likelihood:                -283.97\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.666e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9544      0.258    -15.312      0.000      -4.461      -3.448\n",
      "15             -0.3213      0.125     -2.564      0.010      -0.567      -0.076\n",
      "20              0.4403      0.375      1.175      0.240      -0.294       1.175\n",
      "27             -0.2047      0.075     -2.717      0.007      -0.352      -0.057\n",
      "32             -0.3946      0.142     -2.780      0.005      -0.673      -0.116\n",
      "33              0.5662      0.203      2.792      0.005       0.169       0.964\n",
      "36             -0.8664      0.398     -2.176      0.030      -1.647      -0.086\n",
      "48              0.1688      0.123      1.375      0.169      -0.072       0.409\n",
      "59              0.4196      0.133      3.166      0.002       0.160       0.679\n",
      "60              0.4712      0.123      3.845      0.000       0.231       0.711\n",
      "65              0.3324      0.102      3.256      0.001       0.132       0.532\n",
      "76              0.2394      0.118      2.034      0.042       0.009       0.470\n",
      "87              0.1682      0.120      1.399      0.162      -0.068       0.404\n",
      "100             0.2437      0.093      2.606      0.009       0.060       0.427\n",
      "112            -0.4369      0.208     -2.101      0.036      -0.845      -0.029\n",
      "122             0.2606      0.141      1.852      0.064      -0.015       0.536\n",
      "130             0.6600      0.160      4.136      0.000       0.347       0.973\n",
      "131             0.3240      0.176      1.838      0.066      -0.022       0.670\n",
      "133            -0.3129      0.168     -1.866      0.062      -0.641       0.016\n",
      "147            -0.2902      0.143     -2.026      0.043      -0.571      -0.009\n",
      "196             0.1746      0.078      2.225      0.026       0.021       0.328\n",
      "215            -0.3489      0.222     -1.572      0.116      -0.784       0.086\n",
      "248             0.1358      0.084      1.608      0.108      -0.030       0.301\n",
      "270             0.1666      0.120      1.393      0.164      -0.068       0.401\n",
      "349             0.3132      0.122      2.572      0.010       0.075       0.552\n",
      "412             0.2599      0.120      2.172      0.030       0.025       0.494\n",
      "417            -0.3935      0.145     -2.718      0.007      -0.677      -0.110\n",
      "425            -1.2552      0.729     -1.721      0.085      -2.684       0.174\n",
      "438             0.2247      0.082      2.738      0.006       0.064       0.386\n",
      "456             0.2575      0.121      2.125      0.034       0.020       0.495\n",
      "517            -7.3531      5.022     -1.464      0.143     -17.196       2.490\n",
      "546            -0.2972      0.163     -1.824      0.068      -0.617       0.022\n",
      "557            -0.2219      0.161     -1.382      0.167      -0.537       0.093\n",
      "558             0.2972      0.142      2.099      0.036       0.020       0.575\n",
      "564            -0.1860      0.144     -1.296      0.195      -0.467       0.095\n",
      "missing_113    -0.2519      0.152     -1.657      0.097      -0.550       0.046\n",
      "===============================================================================\n",
      "Removing feature 20 with p-value 0.239891\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181867\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1532\n",
      "Method:                           MLE   Df Model:                           34\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2551\n",
      "Time:                        16:28:46   Log-Likelihood:                -284.99\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.604e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9253      0.254    -15.427      0.000      -4.424      -3.427\n",
      "15             -0.3169      0.125     -2.538      0.011      -0.562      -0.072\n",
      "27             -0.2047      0.075     -2.718      0.007      -0.352      -0.057\n",
      "32             -0.3947      0.142     -2.778      0.005      -0.673      -0.116\n",
      "33              0.5664      0.201      2.815      0.005       0.172       0.961\n",
      "36             -0.8689      0.394     -2.203      0.028      -1.642      -0.096\n",
      "48              0.1655      0.123      1.350      0.177      -0.075       0.406\n",
      "59              0.4042      0.132      3.072      0.002       0.146       0.662\n",
      "60              0.4653      0.122      3.824      0.000       0.227       0.704\n",
      "65              0.3416      0.102      3.351      0.001       0.142       0.541\n",
      "76              0.2372      0.118      2.016      0.044       0.007       0.468\n",
      "87              0.1790      0.120      1.497      0.134      -0.055       0.413\n",
      "100             0.2411      0.091      2.659      0.008       0.063       0.419\n",
      "112            -0.4312      0.206     -2.089      0.037      -0.836      -0.027\n",
      "122             0.2509      0.140      1.797      0.072      -0.023       0.525\n",
      "130             0.6598      0.159      4.142      0.000       0.348       0.972\n",
      "131             0.3465      0.175      1.978      0.048       0.003       0.690\n",
      "133            -0.3193      0.168     -1.898      0.058      -0.649       0.010\n",
      "147            -0.2955      0.144     -2.050      0.040      -0.578      -0.013\n",
      "196             0.1717      0.078      2.194      0.028       0.018       0.325\n",
      "215            -0.3340      0.219     -1.525      0.127      -0.763       0.095\n",
      "248             0.1332      0.084      1.585      0.113      -0.032       0.298\n",
      "270             0.1773      0.119      1.489      0.137      -0.056       0.411\n",
      "349             0.3071      0.121      2.530      0.011       0.069       0.545\n",
      "412             0.2602      0.120      2.169      0.030       0.025       0.495\n",
      "417            -0.3929      0.145     -2.718      0.007      -0.676      -0.110\n",
      "425            -1.2400      0.724     -1.712      0.087      -2.660       0.180\n",
      "438             0.2221      0.083      2.683      0.007       0.060       0.384\n",
      "456             0.2589      0.121      2.131      0.033       0.021       0.497\n",
      "517            -7.6878      5.003     -1.537      0.124     -17.494       2.119\n",
      "546            -0.3037      0.163     -1.863      0.062      -0.623       0.016\n",
      "557            -0.2154      0.160     -1.345      0.179      -0.529       0.099\n",
      "558             0.2923      0.141      2.074      0.038       0.016       0.569\n",
      "564            -0.2095      0.143     -1.464      0.143      -0.490       0.071\n",
      "missing_113    -0.2728      0.151     -1.804      0.071      -0.569       0.024\n",
      "===============================================================================\n",
      "Removing feature 557 with p-value 0.178684\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182441\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1533\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2527\n",
      "Time:                        16:28:46   Log-Likelihood:                -285.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.388e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.9088      0.253    -15.462      0.000      -4.404      -3.413\n",
      "15             -0.3091      0.125     -2.479      0.013      -0.553      -0.065\n",
      "27             -0.1958      0.075     -2.616      0.009      -0.342      -0.049\n",
      "32             -0.4007      0.142     -2.818      0.005      -0.679      -0.122\n",
      "33              0.5809      0.200      2.904      0.004       0.189       0.973\n",
      "36             -0.9018      0.391     -2.306      0.021      -1.668      -0.135\n",
      "48              0.1565      0.122      1.284      0.199      -0.082       0.395\n",
      "59              0.4004      0.131      3.046      0.002       0.143       0.658\n",
      "60              0.4547      0.121      3.768      0.000       0.218       0.691\n",
      "65              0.3300      0.101      3.252      0.001       0.131       0.529\n",
      "76              0.2361      0.117      2.013      0.044       0.006       0.466\n",
      "87              0.1632      0.119      1.370      0.171      -0.070       0.397\n",
      "100             0.2350      0.093      2.535      0.011       0.053       0.417\n",
      "112            -0.4310      0.207     -2.087      0.037      -0.836      -0.026\n",
      "122             0.2507      0.140      1.789      0.074      -0.024       0.525\n",
      "130             0.6339      0.158      4.019      0.000       0.325       0.943\n",
      "131             0.3458      0.175      1.980      0.048       0.004       0.688\n",
      "133            -0.3191      0.168     -1.899      0.058      -0.648       0.010\n",
      "147            -0.2998      0.144     -2.081      0.037      -0.582      -0.017\n",
      "196             0.1654      0.078      2.119      0.034       0.012       0.318\n",
      "215            -0.3377      0.219     -1.543      0.123      -0.767       0.091\n",
      "248             0.1300      0.084      1.540      0.124      -0.035       0.296\n",
      "270             0.1885      0.118      1.593      0.111      -0.043       0.421\n",
      "349             0.3021      0.121      2.496      0.013       0.065       0.539\n",
      "412             0.2590      0.119      2.169      0.030       0.025       0.493\n",
      "417            -0.3947      0.144     -2.738      0.006      -0.677      -0.112\n",
      "425            -1.2481      0.723     -1.726      0.084      -2.665       0.169\n",
      "438             0.2183      0.080      2.723      0.006       0.061       0.375\n",
      "456             0.2542      0.122      2.089      0.037       0.016       0.493\n",
      "517            -7.6397      5.010     -1.525      0.127     -17.459       2.179\n",
      "546            -0.2967      0.163     -1.824      0.068      -0.615       0.022\n",
      "558             0.1196      0.058      2.054      0.040       0.005       0.234\n",
      "564            -0.2084      0.143     -1.456      0.146      -0.489       0.072\n",
      "missing_113    -0.2692      0.151     -1.786      0.074      -0.565       0.026\n",
      "===============================================================================\n",
      "Removing feature 48 with p-value 0.199172\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182976\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1534\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2505\n",
      "Time:                        16:28:46   Log-Likelihood:                -286.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.131e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.8863      0.250    -15.535      0.000      -4.377      -3.396\n",
      "15             -0.3088      0.125     -2.478      0.013      -0.553      -0.065\n",
      "27             -0.1918      0.076     -2.538      0.011      -0.340      -0.044\n",
      "32             -0.4056      0.142     -2.854      0.004      -0.684      -0.127\n",
      "33              0.5732      0.198      2.898      0.004       0.185       0.961\n",
      "36             -0.8850      0.384     -2.307      0.021      -1.637      -0.133\n",
      "59              0.4223      0.131      3.232      0.001       0.166       0.678\n",
      "60              0.4584      0.120      3.826      0.000       0.224       0.693\n",
      "65              0.3188      0.101      3.158      0.002       0.121       0.517\n",
      "76              0.2386      0.117      2.043      0.041       0.010       0.468\n",
      "87              0.1618      0.119      1.363      0.173      -0.071       0.394\n",
      "100             0.2227      0.093      2.391      0.017       0.040       0.405\n",
      "112            -0.4374      0.207     -2.113      0.035      -0.843      -0.032\n",
      "122             0.2385      0.140      1.707      0.088      -0.035       0.512\n",
      "130             0.6361      0.158      4.024      0.000       0.326       0.946\n",
      "131             0.3477      0.174      1.995      0.046       0.006       0.689\n",
      "133            -0.3262      0.168     -1.942      0.052      -0.655       0.003\n",
      "147            -0.2992      0.143     -2.092      0.036      -0.579      -0.019\n",
      "196             0.1710      0.078      2.187      0.029       0.018       0.324\n",
      "215            -0.3119      0.213     -1.465      0.143      -0.729       0.105\n",
      "248             0.1293      0.085      1.530      0.126      -0.036       0.295\n",
      "270             0.1886      0.118      1.598      0.110      -0.043       0.420\n",
      "349             0.2934      0.120      2.448      0.014       0.058       0.528\n",
      "412             0.2470      0.118      2.088      0.037       0.015       0.479\n",
      "417            -0.3880      0.143     -2.707      0.007      -0.669      -0.107\n",
      "425            -1.2420      0.722     -1.721      0.085      -2.657       0.173\n",
      "438             0.2175      0.081      2.675      0.007       0.058       0.377\n",
      "456             0.2567      0.121      2.114      0.035       0.019       0.495\n",
      "517            -7.4520      5.005     -1.489      0.136     -17.261       2.357\n",
      "546            -0.2811      0.162     -1.732      0.083      -0.599       0.037\n",
      "558             0.1204      0.058      2.073      0.038       0.007       0.234\n",
      "564            -0.2066      0.143     -1.450      0.147      -0.486       0.073\n",
      "missing_113    -0.2702      0.150     -1.797      0.072      -0.565       0.025\n",
      "===============================================================================\n",
      "Removing feature 87 with p-value 0.172975\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183578\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1535\n",
      "Method:                           MLE   Df Model:                           31\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2481\n",
      "Time:                        16:28:46   Log-Likelihood:                -287.67\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.990e-25\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.8925      0.251    -15.510      0.000      -4.384      -3.401\n",
      "15             -0.3099      0.124     -2.491      0.013      -0.554      -0.066\n",
      "27             -0.1899      0.075     -2.519      0.012      -0.338      -0.042\n",
      "32             -0.3951      0.142     -2.788      0.005      -0.673      -0.117\n",
      "33              0.5621      0.195      2.886      0.004       0.180       0.944\n",
      "36             -0.8587      0.374     -2.298      0.022      -1.591      -0.126\n",
      "59              0.4224      0.130      3.237      0.001       0.167       0.678\n",
      "60              0.4461      0.119      3.762      0.000       0.214       0.678\n",
      "65              0.3136      0.100      3.125      0.002       0.117       0.510\n",
      "76              0.2380      0.117      2.030      0.042       0.008       0.468\n",
      "100             0.2151      0.093      2.317      0.020       0.033       0.397\n",
      "112            -0.4395      0.208     -2.111      0.035      -0.848      -0.031\n",
      "122             0.2422      0.140      1.731      0.083      -0.032       0.516\n",
      "130             0.6412      0.158      4.050      0.000       0.331       0.951\n",
      "131             0.3502      0.174      2.007      0.045       0.008       0.692\n",
      "133            -0.3272      0.168     -1.946      0.052      -0.657       0.002\n",
      "147            -0.2993      0.144     -2.077      0.038      -0.582      -0.017\n",
      "196             0.1696      0.079      2.156      0.031       0.015       0.324\n",
      "215            -0.3209      0.214     -1.497      0.134      -0.741       0.099\n",
      "248             0.1384      0.084      1.650      0.099      -0.026       0.303\n",
      "270             0.1975      0.118      1.678      0.093      -0.033       0.428\n",
      "349             0.2905      0.121      2.404      0.016       0.054       0.527\n",
      "412             0.2495      0.118      2.116      0.034       0.018       0.481\n",
      "417            -0.3843      0.143     -2.693      0.007      -0.664      -0.105\n",
      "425            -1.2801      0.718     -1.782      0.075      -2.688       0.128\n",
      "438             0.2109      0.081      2.599      0.009       0.052       0.370\n",
      "456             0.2601      0.121      2.144      0.032       0.022       0.498\n",
      "517            -7.9507      5.014     -1.586      0.113     -17.778       1.876\n",
      "546            -0.2709      0.162     -1.671      0.095      -0.589       0.047\n",
      "558             0.1237      0.058      2.149      0.032       0.011       0.236\n",
      "564            -0.2011      0.142     -1.419      0.156      -0.479       0.077\n",
      "missing_113    -0.2810      0.150     -1.875      0.061      -0.575       0.013\n",
      "===============================================================================\n",
      "Removing feature 564 with p-value 0.155929\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184253\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1536\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2453\n",
      "Time:                        16:28:46   Log-Likelihood:                -288.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.659e-25\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.8791      0.251    -15.472      0.000      -4.371      -3.388\n",
      "15             -0.3173      0.125     -2.548      0.011      -0.561      -0.073\n",
      "27             -0.1824      0.075     -2.448      0.014      -0.328      -0.036\n",
      "32             -0.3939      0.141     -2.795      0.005      -0.670      -0.118\n",
      "33              0.5643      0.195      2.896      0.004       0.182       0.946\n",
      "36             -0.8504      0.375     -2.269      0.023      -1.585      -0.116\n",
      "59              0.4137      0.130      3.171      0.002       0.158       0.669\n",
      "60              0.4415      0.119      3.724      0.000       0.209       0.674\n",
      "65              0.3122      0.100      3.119      0.002       0.116       0.508\n",
      "76              0.2410      0.117      2.067      0.039       0.012       0.470\n",
      "100             0.2187      0.092      2.367      0.018       0.038       0.400\n",
      "112            -0.4608      0.209     -2.206      0.027      -0.870      -0.051\n",
      "122             0.2330      0.139      1.674      0.094      -0.040       0.506\n",
      "130             0.6285      0.158      3.973      0.000       0.318       0.938\n",
      "131             0.3609      0.174      2.069      0.039       0.019       0.703\n",
      "133            -0.3189      0.168     -1.897      0.058      -0.648       0.011\n",
      "147            -0.3064      0.144     -2.133      0.033      -0.588      -0.025\n",
      "196             0.1686      0.078      2.165      0.030       0.016       0.321\n",
      "215            -0.3216      0.213     -1.513      0.130      -0.738       0.095\n",
      "248             0.1346      0.084      1.609      0.108      -0.029       0.299\n",
      "270             0.1955      0.117      1.671      0.095      -0.034       0.425\n",
      "349             0.2955      0.121      2.435      0.015       0.058       0.533\n",
      "412             0.2508      0.119      2.114      0.035       0.018       0.483\n",
      "417            -0.3844      0.142     -2.704      0.007      -0.663      -0.106\n",
      "425            -1.3326      0.726     -1.836      0.066      -2.755       0.090\n",
      "438             0.2135      0.080      2.653      0.008       0.056       0.371\n",
      "456             0.2596      0.119      2.182      0.029       0.026       0.493\n",
      "517            -7.7456      5.019     -1.543      0.123     -17.583       2.091\n",
      "546            -0.2705      0.162     -1.669      0.095      -0.588       0.047\n",
      "558             0.1247      0.058      2.166      0.030       0.012       0.238\n",
      "missing_113    -0.2524      0.149     -1.697      0.090      -0.544       0.039\n",
      "===============================================================================\n",
      "Removing feature 215 with p-value 0.130357\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.185233\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1537\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2413\n",
      "Time:                        16:28:46   Log-Likelihood:                -290.26\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.395e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.8577      0.248    -15.585      0.000      -4.343      -3.373\n",
      "15             -0.3306      0.124     -2.666      0.008      -0.574      -0.088\n",
      "27             -0.1910      0.074     -2.567      0.010      -0.337      -0.045\n",
      "32             -0.4087      0.140     -2.922      0.003      -0.683      -0.135\n",
      "33              0.5778      0.194      2.978      0.003       0.197       0.958\n",
      "36             -0.8629      0.375     -2.304      0.021      -1.597      -0.129\n",
      "59              0.4060      0.130      3.133      0.002       0.152       0.660\n",
      "60              0.4508      0.119      3.794      0.000       0.218       0.684\n",
      "65              0.3225      0.100      3.229      0.001       0.127       0.518\n",
      "76              0.2329      0.115      2.025      0.043       0.007       0.458\n",
      "100             0.2211      0.092      2.412      0.016       0.041       0.401\n",
      "112            -0.4812      0.211     -2.286      0.022      -0.894      -0.069\n",
      "122             0.2513      0.139      1.811      0.070      -0.021       0.523\n",
      "130             0.6130      0.157      3.902      0.000       0.305       0.921\n",
      "131             0.3806      0.174      2.192      0.028       0.040       0.721\n",
      "133            -0.3245      0.167     -1.944      0.052      -0.652       0.003\n",
      "147            -0.3164      0.144     -2.194      0.028      -0.599      -0.034\n",
      "196             0.1729      0.078      2.227      0.026       0.021       0.325\n",
      "248             0.1300      0.083      1.565      0.118      -0.033       0.293\n",
      "270             0.1842      0.116      1.586      0.113      -0.043       0.412\n",
      "349             0.1792      0.095      1.878      0.060      -0.008       0.366\n",
      "412             0.2557      0.119      2.157      0.031       0.023       0.488\n",
      "417            -0.3774      0.141     -2.685      0.007      -0.653      -0.102\n",
      "425            -1.3671      0.720     -1.900      0.057      -2.778       0.043\n",
      "438             0.2191      0.081      2.693      0.007       0.060       0.379\n",
      "456             0.2576      0.119      2.160      0.031       0.024       0.491\n",
      "517            -7.3536      4.982     -1.476      0.140     -17.119       2.411\n",
      "546            -0.2856      0.161     -1.774      0.076      -0.601       0.030\n",
      "558             0.1315      0.058      2.277      0.023       0.018       0.245\n",
      "missing_113    -0.2633      0.147     -1.790      0.073      -0.552       0.025\n",
      "===============================================================================\n",
      "Removing feature 517 with p-value 0.139956\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187078\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1538\n",
      "Method:                           MLE   Df Model:                           28\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2337\n",
      "Time:                        16:28:46   Log-Likelihood:                -293.15\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.402e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.6210      0.198    -18.263      0.000      -4.010      -3.232\n",
      "15             -0.3277      0.123     -2.668      0.008      -0.568      -0.087\n",
      "27             -0.1795      0.074     -2.427      0.015      -0.324      -0.035\n",
      "32             -0.4467      0.138     -3.248      0.001      -0.716      -0.177\n",
      "33              0.5542      0.190      2.915      0.004       0.182       0.927\n",
      "36             -0.8230      0.370     -2.227      0.026      -1.547      -0.099\n",
      "59              0.3978      0.128      3.108      0.002       0.147       0.649\n",
      "60              0.4269      0.116      3.686      0.000       0.200       0.654\n",
      "65              0.3081      0.099      3.118      0.002       0.114       0.502\n",
      "76              0.2496      0.114      2.185      0.029       0.026       0.473\n",
      "100             0.2189      0.094      2.337      0.019       0.035       0.403\n",
      "112            -0.3102      0.117     -2.647      0.008      -0.540      -0.081\n",
      "122             0.2283      0.138      1.653      0.098      -0.042       0.499\n",
      "130             0.5448      0.156      3.493      0.000       0.239       0.851\n",
      "131             0.4489      0.171      2.631      0.009       0.114       0.783\n",
      "133            -0.4247      0.163     -2.605      0.009      -0.744      -0.105\n",
      "147            -0.3267      0.143     -2.280      0.023      -0.607      -0.046\n",
      "196             0.1630      0.076      2.134      0.033       0.013       0.313\n",
      "248             0.1283      0.083      1.540      0.123      -0.035       0.292\n",
      "270             0.1823      0.115      1.581      0.114      -0.044       0.408\n",
      "349             0.1931      0.095      2.037      0.042       0.007       0.379\n",
      "412             0.2502      0.118      2.123      0.034       0.019       0.481\n",
      "417            -0.3879      0.141     -2.755      0.006      -0.664      -0.112\n",
      "425            -1.2717      0.710     -1.790      0.073      -2.664       0.120\n",
      "438             0.2096      0.081      2.590      0.010       0.051       0.368\n",
      "456             0.2249      0.117      1.928      0.054      -0.004       0.454\n",
      "546            -0.2230      0.161     -1.381      0.167      -0.539       0.093\n",
      "558             0.1282      0.058      2.214      0.027       0.015       0.242\n",
      "missing_113    -0.1971      0.140     -1.411      0.158      -0.471       0.077\n",
      "===============================================================================\n",
      "Removing feature 546 with p-value 0.167211\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187722\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1539\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2311\n",
      "Time:                        16:28:46   Log-Likelihood:                -294.16\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.842e-24\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -3.6076      0.197    -18.317      0.000      -3.994      -3.222\n",
      "15             -0.3268      0.123     -2.661      0.008      -0.568      -0.086\n",
      "27             -0.1799      0.074     -2.441      0.015      -0.324      -0.035\n",
      "32             -0.4307      0.136     -3.158      0.002      -0.698      -0.163\n",
      "33              0.5650      0.192      2.942      0.003       0.189       0.942\n",
      "36             -0.8415      0.376     -2.237      0.025      -1.579      -0.104\n",
      "59              0.3884      0.127      3.066      0.002       0.140       0.637\n",
      "60              0.4369      0.117      3.743      0.000       0.208       0.666\n",
      "65              0.3174      0.099      3.203      0.001       0.123       0.512\n",
      "76              0.2503      0.114      2.198      0.028       0.027       0.474\n",
      "100             0.2154      0.089      2.433      0.015       0.042       0.389\n",
      "112            -0.2992      0.117     -2.557      0.011      -0.529      -0.070\n",
      "122             0.2512      0.137      1.837      0.066      -0.017       0.519\n",
      "130             0.5611      0.155      3.621      0.000       0.257       0.865\n",
      "131             0.4038      0.167      2.422      0.015       0.077       0.731\n",
      "133            -0.3505      0.153     -2.290      0.022      -0.650      -0.051\n",
      "147            -0.3311      0.144     -2.302      0.021      -0.613      -0.049\n",
      "196             0.1624      0.076      2.132      0.033       0.013       0.312\n",
      "248             0.1321      0.083      1.588      0.112      -0.031       0.295\n",
      "270             0.1818      0.115      1.580      0.114      -0.044       0.407\n",
      "349             0.1930      0.094      2.057      0.040       0.009       0.377\n",
      "412             0.2429      0.117      2.076      0.038       0.014       0.472\n",
      "417            -0.3855      0.141     -2.735      0.006      -0.662      -0.109\n",
      "425            -1.2890      0.708     -1.820      0.069      -2.677       0.099\n",
      "438             0.2161      0.081      2.667      0.008       0.057       0.375\n",
      "456             0.2234      0.117      1.917      0.055      -0.005       0.452\n",
      "558             0.1291      0.058      2.235      0.025       0.016       0.242\n",
      "missing_113    -0.2060      0.139     -1.479      0.139      -0.479       0.067\n",
      "===============================================================================\n",
      "Removing feature missing_113 with p-value 0.139203\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.188424\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1540\n",
      "Method:                           MLE   Df Model:                           26\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2282\n",
      "Time:                        16:28:46   Log-Likelihood:                -295.26\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.713e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5737      0.193    -18.534      0.000      -3.952      -3.196\n",
      "15            -0.3284      0.122     -2.687      0.007      -0.568      -0.089\n",
      "27            -0.1801      0.073     -2.457      0.014      -0.324      -0.036\n",
      "32            -0.4073      0.135     -3.020      0.003      -0.672      -0.143\n",
      "33             0.5473      0.192      2.844      0.004       0.170       0.924\n",
      "36            -0.8052      0.378     -2.129      0.033      -1.547      -0.064\n",
      "59             0.3829      0.126      3.043      0.002       0.136       0.629\n",
      "60             0.4985      0.115      4.327      0.000       0.273       0.724\n",
      "65             0.3368      0.101      3.349      0.001       0.140       0.534\n",
      "76             0.2377      0.114      2.077      0.038       0.013       0.462\n",
      "100            0.2088      0.087      2.403      0.016       0.038       0.379\n",
      "112           -0.2823      0.115     -2.462      0.014      -0.507      -0.058\n",
      "122            0.2735      0.136      2.014      0.044       0.007       0.540\n",
      "130            0.5647      0.154      3.671      0.000       0.263       0.866\n",
      "131            0.3710      0.166      2.239      0.025       0.046       0.696\n",
      "133           -0.3075      0.149     -2.062      0.039      -0.600      -0.015\n",
      "147           -0.3447      0.144     -2.386      0.017      -0.628      -0.062\n",
      "196            0.1550      0.075      2.059      0.040       0.007       0.303\n",
      "248            0.1363      0.085      1.608      0.108      -0.030       0.302\n",
      "270            0.1874      0.115      1.636      0.102      -0.037       0.412\n",
      "349            0.1928      0.093      2.063      0.039       0.010       0.376\n",
      "412            0.2366      0.116      2.032      0.042       0.008       0.465\n",
      "417           -0.3832      0.140     -2.729      0.006      -0.658      -0.108\n",
      "425           -1.2364      0.697     -1.774      0.076      -2.603       0.130\n",
      "438            0.2124      0.079      2.692      0.007       0.058       0.367\n",
      "456            0.2386      0.116      2.064      0.039       0.012       0.465\n",
      "558            0.1305      0.058      2.260      0.024       0.017       0.244\n",
      "==============================================================================\n",
      "Removing feature 248 with p-value 0.107851\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189199\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1541\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2250\n",
      "Time:                        16:28:46   Log-Likelihood:                -296.47\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.107e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5675      0.193    -18.505      0.000      -3.945      -3.190\n",
      "15            -0.3314      0.122     -2.724      0.006      -0.570      -0.093\n",
      "27            -0.1857      0.073     -2.528      0.011      -0.330      -0.042\n",
      "32            -0.4019      0.134     -3.003      0.003      -0.664      -0.140\n",
      "33             0.5482      0.194      2.829      0.005       0.168       0.928\n",
      "36            -0.8047      0.381     -2.111      0.035      -1.552      -0.057\n",
      "59             0.3903      0.126      3.107      0.002       0.144       0.637\n",
      "60             0.5276      0.116      4.554      0.000       0.301       0.755\n",
      "65             0.3611      0.100      3.597      0.000       0.164       0.558\n",
      "76             0.2329      0.114      2.036      0.042       0.009       0.457\n",
      "100            0.2218      0.087      2.564      0.010       0.052       0.391\n",
      "112           -0.2846      0.115     -2.483      0.013      -0.509      -0.060\n",
      "122            0.2747      0.135      2.028      0.043       0.009       0.540\n",
      "130            0.5806      0.154      3.781      0.000       0.280       0.882\n",
      "131            0.3708      0.165      2.244      0.025       0.047       0.695\n",
      "133           -0.2962      0.149     -1.992      0.046      -0.588      -0.005\n",
      "147           -0.3369      0.144     -2.347      0.019      -0.618      -0.056\n",
      "196            0.1548      0.075      2.068      0.039       0.008       0.302\n",
      "270            0.1944      0.115      1.697      0.090      -0.030       0.419\n",
      "349            0.2036      0.093      2.181      0.029       0.021       0.386\n",
      "412            0.2376      0.117      2.039      0.041       0.009       0.466\n",
      "417           -0.3891      0.141     -2.763      0.006      -0.665      -0.113\n",
      "425           -1.2671      0.698     -1.814      0.070      -2.636       0.102\n",
      "438            0.2097      0.079      2.654      0.008       0.055       0.365\n",
      "456            0.2272      0.116      1.965      0.049       0.001       0.454\n",
      "558            0.1231      0.058      2.133      0.033       0.010       0.236\n",
      "==============================================================================\n",
      "Removing feature 270 with p-value 0.089669\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190097\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1542\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2214\n",
      "Time:                        16:28:46   Log-Likelihood:                -297.88\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.656e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5441      0.190    -18.648      0.000      -3.917      -3.172\n",
      "15            -0.3347      0.121     -2.773      0.006      -0.571      -0.098\n",
      "27            -0.1772      0.073     -2.417      0.016      -0.321      -0.034\n",
      "32            -0.3916      0.133     -2.941      0.003      -0.653      -0.131\n",
      "33             0.5310      0.190      2.791      0.005       0.158       0.904\n",
      "36            -0.7753      0.371     -2.087      0.037      -1.503      -0.047\n",
      "59             0.3989      0.125      3.183      0.001       0.153       0.645\n",
      "60             0.5046      0.113      4.458      0.000       0.283       0.726\n",
      "65             0.3395      0.098      3.462      0.001       0.147       0.532\n",
      "76             0.2453      0.114      2.146      0.032       0.021       0.469\n",
      "100            0.2286      0.086      2.646      0.008       0.059       0.398\n",
      "112           -0.2922      0.115     -2.549      0.011      -0.517      -0.068\n",
      "122            0.2896      0.136      2.135      0.033       0.024       0.556\n",
      "130            0.5571      0.152      3.677      0.000       0.260       0.854\n",
      "131            0.3746      0.165      2.272      0.023       0.051       0.698\n",
      "133           -0.2968      0.149     -1.991      0.047      -0.589      -0.005\n",
      "147           -0.3188      0.141     -2.262      0.024      -0.595      -0.043\n",
      "196            0.1681      0.074      2.268      0.023       0.023       0.313\n",
      "349            0.2032      0.093      2.181      0.029       0.021       0.386\n",
      "412            0.2382      0.117      2.036      0.042       0.009       0.467\n",
      "417           -0.3929      0.141     -2.790      0.005      -0.669      -0.117\n",
      "425           -1.1920      0.691     -1.724      0.085      -2.547       0.163\n",
      "438            0.2188      0.079      2.774      0.006       0.064       0.373\n",
      "456            0.2256      0.115      1.967      0.049       0.001       0.450\n",
      "558            0.1219      0.058      2.096      0.036       0.008       0.236\n",
      "==============================================================================\n",
      "Removing feature 425 with p-value 0.084730\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.191848\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1543\n",
      "Method:                           MLE   Df Model:                           23\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2142\n",
      "Time:                        16:28:46   Log-Likelihood:                -300.63\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.065e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4399      0.175    -19.703      0.000      -3.782      -3.098\n",
      "15            -0.2424      0.110     -2.205      0.027      -0.458      -0.027\n",
      "27            -0.1798      0.074     -2.443      0.015      -0.324      -0.036\n",
      "32            -0.3954      0.132     -2.990      0.003      -0.655      -0.136\n",
      "33             0.5399      0.191      2.827      0.005       0.166       0.914\n",
      "36            -0.7817      0.375     -2.085      0.037      -1.516      -0.047\n",
      "59             0.4058      0.125      3.245      0.001       0.161       0.651\n",
      "60             0.4860      0.112      4.342      0.000       0.267       0.705\n",
      "65             0.3224      0.097      3.331      0.001       0.133       0.512\n",
      "76             0.2338      0.114      2.048      0.041       0.010       0.458\n",
      "100            0.2342      0.086      2.721      0.007       0.066       0.403\n",
      "112           -0.2856      0.115     -2.490      0.013      -0.510      -0.061\n",
      "122            0.3042      0.135      2.255      0.024       0.040       0.569\n",
      "130            0.5704      0.151      3.778      0.000       0.275       0.866\n",
      "131            0.3575      0.164      2.173      0.030       0.035       0.680\n",
      "133           -0.2830      0.148     -1.911      0.056      -0.573       0.007\n",
      "147           -0.3015      0.140     -2.150      0.032      -0.576      -0.027\n",
      "196            0.1674      0.074      2.254      0.024       0.022       0.313\n",
      "349            0.1882      0.091      2.074      0.038       0.010       0.366\n",
      "412            0.2334      0.117      1.996      0.046       0.004       0.463\n",
      "417           -0.3879      0.141     -2.744      0.006      -0.665      -0.111\n",
      "438            0.2246      0.079      2.849      0.004       0.070       0.379\n",
      "456            0.2081      0.115      1.813      0.070      -0.017       0.433\n",
      "558            0.1187      0.058      2.064      0.039       0.006       0.232\n",
      "==============================================================================\n",
      "Removing feature 456 with p-value 0.069877\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192897\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1544\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2099\n",
      "Time:                        16:28:46   Log-Likelihood:                -302.27\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.668e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4231      0.174    -19.703      0.000      -3.764      -3.083\n",
      "15            -0.2422      0.109     -2.222      0.026      -0.456      -0.029\n",
      "27            -0.1765      0.074     -2.396      0.017      -0.321      -0.032\n",
      "32            -0.3827      0.132     -2.907      0.004      -0.641      -0.125\n",
      "33             0.5655      0.195      2.904      0.004       0.184       0.947\n",
      "36            -0.8388      0.388     -2.160      0.031      -1.600      -0.078\n",
      "59             0.3853      0.124      3.099      0.002       0.142       0.629\n",
      "60             0.4809      0.112      4.298      0.000       0.262       0.700\n",
      "65             0.3260      0.097      3.348      0.001       0.135       0.517\n",
      "76             0.2659      0.112      2.366      0.018       0.046       0.486\n",
      "100            0.2427      0.085      2.842      0.004       0.075       0.410\n",
      "112           -0.2743      0.114     -2.414      0.016      -0.497      -0.052\n",
      "122            0.3341      0.133      2.512      0.012       0.073       0.595\n",
      "130            0.5695      0.151      3.776      0.000       0.274       0.865\n",
      "131            0.3314      0.164      2.023      0.043       0.010       0.652\n",
      "133           -0.2504      0.146     -1.710      0.087      -0.537       0.037\n",
      "147           -0.3162      0.141     -2.236      0.025      -0.593      -0.039\n",
      "196            0.1566      0.072      2.170      0.030       0.015       0.298\n",
      "349            0.1860      0.092      2.024      0.043       0.006       0.366\n",
      "412            0.2474      0.116      2.137      0.033       0.021       0.474\n",
      "417           -0.4152      0.140     -2.959      0.003      -0.690      -0.140\n",
      "438            0.2238      0.079      2.845      0.004       0.070       0.378\n",
      "558            0.1161      0.058      2.007      0.045       0.003       0.229\n",
      "==============================================================================\n",
      "Removing feature 133 with p-value 0.087236\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193838\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1545\n",
      "Method:                           MLE   Df Model:                           21\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2060\n",
      "Time:                        16:28:46   Log-Likelihood:                -303.74\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.089e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4190      0.174    -19.667      0.000      -3.760      -3.078\n",
      "15            -0.2422      0.108     -2.233      0.026      -0.455      -0.030\n",
      "27            -0.1818      0.074     -2.472      0.013      -0.326      -0.038\n",
      "32            -0.4181      0.130     -3.226      0.001      -0.672      -0.164\n",
      "33             0.5704      0.196      2.912      0.004       0.187       0.954\n",
      "36            -0.8473      0.392     -2.162      0.031      -1.615      -0.079\n",
      "59             0.3470      0.122      2.855      0.004       0.109       0.585\n",
      "60             0.4420      0.105      4.226      0.000       0.237       0.647\n",
      "65             0.2985      0.094      3.174      0.002       0.114       0.483\n",
      "76             0.2783      0.112      2.489      0.013       0.059       0.497\n",
      "100            0.2362      0.083      2.863      0.004       0.075       0.398\n",
      "112           -0.2950      0.115     -2.576      0.010      -0.519      -0.071\n",
      "122            0.4262      0.125      3.421      0.001       0.182       0.670\n",
      "130            0.6021      0.151      3.988      0.000       0.306       0.898\n",
      "131            0.1869      0.142      1.313      0.189      -0.092       0.466\n",
      "147           -0.3234      0.142     -2.275      0.023      -0.602      -0.045\n",
      "196            0.1515      0.072      2.091      0.037       0.009       0.293\n",
      "349            0.1717      0.090      1.904      0.057      -0.005       0.348\n",
      "412            0.2279      0.115      1.975      0.048       0.002       0.454\n",
      "417           -0.4097      0.139     -2.950      0.003      -0.682      -0.137\n",
      "438            0.2318      0.079      2.919      0.004       0.076       0.387\n",
      "558            0.1147      0.058      1.989      0.047       0.002       0.228\n",
      "==============================================================================\n",
      "Removing feature 131 with p-value 0.189305\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194399\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1546\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2038\n",
      "Time:                        16:28:46   Log-Likelihood:                -304.62\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.632e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4037      0.172    -19.756      0.000      -3.741      -3.066\n",
      "15            -0.2525      0.109     -2.325      0.020      -0.465      -0.040\n",
      "27            -0.1841      0.073     -2.519      0.012      -0.327      -0.041\n",
      "32            -0.4341      0.129     -3.361      0.001      -0.687      -0.181\n",
      "33             0.5831      0.196      2.973      0.003       0.199       0.967\n",
      "36            -0.8832      0.392     -2.251      0.024      -1.652      -0.114\n",
      "59             0.3468      0.122      2.832      0.005       0.107       0.587\n",
      "60             0.4415      0.105      4.189      0.000       0.235       0.648\n",
      "65             0.2972      0.094      3.157      0.002       0.113       0.482\n",
      "76             0.2700      0.111      2.438      0.015       0.053       0.487\n",
      "100            0.2469      0.082      2.998      0.003       0.085       0.408\n",
      "112           -0.2966      0.114     -2.591      0.010      -0.521      -0.072\n",
      "122            0.4436      0.123      3.600      0.000       0.202       0.685\n",
      "130            0.6401      0.151      4.241      0.000       0.344       0.936\n",
      "147           -0.3308      0.142     -2.332      0.020      -0.609      -0.053\n",
      "196            0.1509      0.073      2.080      0.038       0.009       0.293\n",
      "349            0.1702      0.090      1.889      0.059      -0.006       0.347\n",
      "412            0.2221      0.115      1.932      0.053      -0.003       0.447\n",
      "417           -0.3954      0.139     -2.847      0.004      -0.668      -0.123\n",
      "438            0.2360      0.080      2.933      0.003       0.078       0.394\n",
      "558            0.1146      0.057      2.001      0.045       0.002       0.227\n",
      "==============================================================================\n",
      "Removing feature 349 with p-value 0.058830\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.195508\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1547\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1992\n",
      "Time:                        16:28:47   Log-Likelihood:                -306.36\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.461e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4002      0.172    -19.763      0.000      -3.737      -3.063\n",
      "15            -0.2462      0.109     -2.266      0.023      -0.459      -0.033\n",
      "27            -0.1920      0.073     -2.647      0.008      -0.334      -0.050\n",
      "32            -0.4244      0.128     -3.312      0.001      -0.675      -0.173\n",
      "33             0.5874      0.199      2.957      0.003       0.198       0.977\n",
      "36            -0.9036      0.401     -2.253      0.024      -1.690      -0.117\n",
      "59             0.3759      0.122      3.090      0.002       0.137       0.614\n",
      "60             0.4621      0.108      4.273      0.000       0.250       0.674\n",
      "65             0.3147      0.095      3.321      0.001       0.129       0.500\n",
      "76             0.3013      0.107      2.822      0.005       0.092       0.511\n",
      "100            0.2451      0.083      2.964      0.003       0.083       0.407\n",
      "112           -0.3060      0.114     -2.673      0.008      -0.530      -0.082\n",
      "122            0.4728      0.122      3.860      0.000       0.233       0.713\n",
      "130            0.6496      0.150      4.330      0.000       0.356       0.944\n",
      "147           -0.3145      0.139     -2.263      0.024      -0.587      -0.042\n",
      "196            0.1420      0.073      1.952      0.051      -0.001       0.285\n",
      "412            0.2279      0.115      1.987      0.047       0.003       0.453\n",
      "417           -0.4144      0.140     -2.966      0.003      -0.688      -0.141\n",
      "438            0.2361      0.082      2.867      0.004       0.075       0.397\n",
      "558            0.1239      0.057      2.174      0.030       0.012       0.236\n",
      "==============================================================================\n",
      "Removing feature 196 with p-value 0.050995\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196435\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1548\n",
      "Method:                           MLE   Df Model:                           18\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1954\n",
      "Time:                        16:28:47   Log-Likelihood:                -307.81\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.225e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3771      0.170    -19.916      0.000      -3.709      -3.045\n",
      "15            -0.2232      0.108     -2.059      0.039      -0.436      -0.011\n",
      "27            -0.1917      0.073     -2.636      0.008      -0.334      -0.049\n",
      "32            -0.4235      0.127     -3.327      0.001      -0.673      -0.174\n",
      "33             0.5745      0.196      2.931      0.003       0.190       0.959\n",
      "36            -0.8807      0.395     -2.231      0.026      -1.654      -0.107\n",
      "59             0.3721      0.122      3.039      0.002       0.132       0.612\n",
      "60             0.4585      0.109      4.211      0.000       0.245       0.672\n",
      "65             0.3231      0.095      3.404      0.001       0.137       0.509\n",
      "76             0.2954      0.106      2.782      0.005       0.087       0.503\n",
      "100            0.2428      0.082      2.943      0.003       0.081       0.404\n",
      "112           -0.3036      0.114     -2.666      0.008      -0.527      -0.080\n",
      "122            0.4658      0.122      3.826      0.000       0.227       0.704\n",
      "130            0.6374      0.148      4.298      0.000       0.347       0.928\n",
      "147           -0.3139      0.138     -2.271      0.023      -0.585      -0.043\n",
      "412            0.2299      0.114      2.012      0.044       0.006       0.454\n",
      "417           -0.4188      0.140     -2.987      0.003      -0.694      -0.144\n",
      "438            0.2347      0.082      2.859      0.004       0.074       0.396\n",
      "558            0.1209      0.057      2.127      0.033       0.010       0.232\n",
      "==============================================================================\n",
      "All remaining features are significant (p < 0.05). Stopping.\n",
      "Final significant features: ['15', '27', '32', '33', '36', '59', '60', '65', '76', '100', '112', '122', '130', '147', '412', '417', '438', '558']\n"
     ]
    }
   ],
   "source": [
    "# Iterative feature elimination\n",
    "X_current = X_scaled_reduced.copy()\n",
    "features_to_keep = list(X_scaled_reduced.columns)\n",
    "features_to_keep.remove('const')\n",
    "\n",
    "while True:\n",
    "    logit_model = sm.Logit(Y, X_current)\n",
    "    result = logit_model.fit(method='newton', maxiter=1000)\n",
    "    print(result.summary())\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    p_values = p_values.drop('const')\n",
    "\n",
    "    if (p_values >= 0.05).sum() == 0:\n",
    "        print(\"All remaining features are significant (p < 0.05). Stopping.\")\n",
    "        break\n",
    "\n",
    "    max_p_feature = p_values.idxmax()\n",
    "    max_p_value = p_values[max_p_feature]\n",
    "    print(f\"Removing feature {max_p_feature} with p-value {max_p_value:.6f}\")\n",
    "\n",
    "    X_current = X_current.drop(columns=max_p_feature)\n",
    "    features_to_keep.remove(max_p_feature)\n",
    "\n",
    "print(f\"Final significant features: {features_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6bf029bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with significant features: 0.257559\n",
      "\n",
      "Logistic Regression Coefficients (significant features):\n",
      "         15        27        32        33        36        59        60  \\\n",
      "0 -0.105257 -0.595941 -0.937633  0.270992 -0.382298  2.420023  0.082267   \n",
      "\n",
      "         65       76       100       112       122       130       147  \\\n",
      "0  0.088694  1.99683  2.092653 -0.303744  4.018624  0.469543 -1.454052   \n",
      "\n",
      "        412       417       438       558  \n",
      "0  0.419557 -0.383347  0.100608  0.054739  \n"
     ]
    }
   ],
   "source": [
    "# Retrain with sklearn\n",
    "X_final_significant = X_indicators[features_to_keep]\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_final_significant, Y)\n",
    "f1_score = cross_val_score(model, X_final_significant, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with significant features: {f1_score:.6f}\")\n",
    "\n",
    "# Inspect coefficients\n",
    "coeffs = pd.DataFrame(model.coef_, columns=X_final_significant.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (significant features):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd397447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with class weights: 0.174619\n",
      "\n",
      "Logistic Regression Coefficients (with class weights):\n",
      "          1       11        17        22        30        34        49  \\\n",
      "0  0.000146 -0.14625  0.443306  0.001217 -0.160278  0.039618 -0.019802   \n",
      "\n",
      "        60        65        68        73        74        79        80  \\\n",
      "0  0.11072  0.101421 -0.002515  0.011752 -0.150622  0.936032 -0.614199   \n",
      "\n",
      "         82       85        89        92        93        96        97  \\\n",
      "0  0.643028 -0.13619 -0.000808  0.402056 -0.037702 -0.000359 -0.540426   \n",
      "\n",
      "        103       104       113       116       118       119       121  \\\n",
      "0 -1.188117  0.111523  0.377675 -0.003361 -0.292262 -0.004247  0.275743   \n",
      "\n",
      "        125       126       128       130       134      139       141  \\\n",
      "0  1.653249  0.019194 -0.433404  0.441215  0.045672  0.02144 -0.156948   \n",
      "\n",
      "        147       151      156       164       173       185       189  \\\n",
      "0 -1.427343 -0.071913  0.10025 -0.452712  2.401013 -0.146503  0.018227   \n",
      "\n",
      "        197     215      219       239       240       248       249      253  \\\n",
      "0 -0.004125 -0.8454 -0.08167  0.002016  0.067772  0.257412 -0.204379 -0.04565   \n",
      "\n",
      "        280       284       292       296       299       321       346  \\\n",
      "0 -0.095872 -0.198055  0.216304 -0.000074 -0.333084 -0.071778  0.133709   \n",
      "\n",
      "        347       362      363       368       386       394       417  \\\n",
      "0 -0.084306  0.000366 -0.18768  0.068649 -0.094604 -0.035323 -0.342389   \n",
      "\n",
      "        424       427       429       431       434       438      469  \\\n",
      "0  0.018693  0.062402 -0.131206 -0.008996  0.000481  0.107786  0.00014   \n",
      "\n",
      "        470       478       489       512      520       524       547  \\\n",
      "0  0.035558  0.072977 -0.000762  0.000958  0.01325 -0.226995  0.497794   \n",
      "\n",
      "        548       549       550       552       555       556       558  \\\n",
      "0  0.077209 -0.076234 -0.421088 -0.017615  0.178437 -0.001532  0.042639   \n",
      "\n",
      "        562       563       564       565       567       568       569  \\\n",
      "0  0.005576 -0.035937 -2.434992  0.876947 -2.352777  0.165268  0.122325   \n",
      "\n",
      "        570       576       580      581       582       588  missing_73  \\\n",
      "0  0.032382 -1.946398  0.323669  0.08381 -0.002321  0.244854   -0.048299   \n",
      "\n",
      "   missing_113  \n",
      "0     0.234061  \n"
     ]
    }
   ],
   "source": [
    "# Assuming X_indicators and Y are your original data, and selected_features_xgbrfe is the list of 100 features from RFE\n",
    "X_final = X_indicators[selected_features_xgbrfe]\n",
    "\n",
    "# Option 1: Use SMOTE (as you did previously)\n",
    "#smote = SMOTE(random_state=42)\n",
    "#X_final_sm, Y_sm = smote.fit_resample(X_final, Y)\n",
    "\n",
    "# Fit logistic regression with sklearn\n",
    "#model_sm = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "#model_sm.fit(X_final_sm, Y_sm)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "#f1_sm = cross_val_score(model_sm, X_final_sm, Y_sm, cv=5, scoring='f1').mean()\n",
    "#rint(f\"F1-score with SMOTE: {f1_sm:.6f}\")\n",
    "\n",
    "# Option 2: Use class weights without SMOTE\n",
    "model_cw = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model_cw.fit(X_final, Y)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "f1_cw = cross_val_score(model_cw, X_final, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with class weights: {f1_cw:.6f}\")\n",
    "\n",
    "# Inspect coefficients (for interpretability, similar to statsmodels summary)\n",
    "coeffs = pd.DataFrame(model_cw.coef_, columns=X_final.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (with class weights):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dbf0311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1466\n",
      "Method:                           MLE   Df Model:                          100\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.3287\n",
      "Time:                        16:30:00   Log-Likelihood:                -256.83\n",
      "converged:                      False   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.920e-15\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -5.2095      2.934     -1.776      0.076     -10.960       0.541\n",
      "1              -0.0056      0.136     -0.042      0.967      -0.271       0.260\n",
      "11              0.0588      0.136      0.431      0.667      -0.209       0.326\n",
      "17              0.7191      0.632      1.138      0.255      -0.520       1.958\n",
      "22              0.8077      0.225      3.582      0.000       0.366       1.250\n",
      "30             -0.0420      0.149     -0.283      0.777      -0.333       0.249\n",
      "34              0.1805      0.094      1.926      0.054      -0.003       0.364\n",
      "49             -0.0372      0.156     -0.239      0.811      -0.342       0.268\n",
      "60              0.9989      0.251      3.984      0.000       0.507       1.490\n",
      "65              0.5106      0.172      2.968      0.003       0.173       0.848\n",
      "68             -0.5504      0.261     -2.112      0.035      -1.061      -0.040\n",
      "73              0.0920      0.241      0.382      0.703      -0.381       0.565\n",
      "74             -1.7315      0.743     -2.329      0.020      -3.188      -0.274\n",
      "79              0.3220      0.174      1.852      0.064      -0.019       0.663\n",
      "80             -0.2104      0.151     -1.397      0.162      -0.506       0.085\n",
      "82              0.2050      0.130      1.571      0.116      -0.051       0.461\n",
      "85             -0.1778      0.133     -1.341      0.180      -0.438       0.082\n",
      "89              0.1046      0.142      0.738      0.460      -0.173       0.383\n",
      "92             -0.0023      0.167     -0.014      0.989      -0.329       0.324\n",
      "93             -0.0642      0.144     -0.446      0.655      -0.346       0.218\n",
      "96             -0.0352      0.144     -0.244      0.807      -0.317       0.247\n",
      "97             -0.0243      0.147     -0.165      0.869      -0.313       0.265\n",
      "103            -0.1839      0.155     -1.183      0.237      -0.488       0.121\n",
      "104             0.2129      0.143      1.485      0.137      -0.068       0.494\n",
      "113             0.1060      0.371      0.285      0.775      -0.622       0.834\n",
      "116            -0.1144      0.129     -0.889      0.374      -0.367       0.138\n",
      "118            -1.5887      0.726     -2.189      0.029      -3.011      -0.166\n",
      "119            -0.0263      0.139     -0.190      0.850      -0.299       0.246\n",
      "121            -0.0400      0.141     -0.284      0.776      -0.316       0.236\n",
      "125             0.2574      0.174      1.480      0.139      -0.083       0.598\n",
      "126             0.0703      0.207      0.339      0.734      -0.335       0.476\n",
      "128            -0.1057      0.189     -0.559      0.576      -0.477       0.265\n",
      "130             0.4935      0.190      2.599      0.009       0.121       0.866\n",
      "134             0.2039      0.181      1.125      0.261      -0.151       0.559\n",
      "139             0.1764      0.136      1.295      0.195      -0.091       0.443\n",
      "141           -15.1882     55.472     -0.274      0.784    -123.910      93.534\n",
      "147            -0.2984      0.153     -1.945      0.052      -0.599       0.002\n",
      "151            -0.2044      0.147     -1.394      0.163      -0.492       0.083\n",
      "156            13.4812     43.674      0.309      0.758     -72.119      99.081\n",
      "164             0.5559      0.921      0.604      0.546      -1.249       2.361\n",
      "173             0.3543      0.145      2.439      0.015       0.070       0.639\n",
      "185             0.6515      0.938      0.695      0.487      -1.186       2.489\n",
      "189             0.1884      0.142      1.325      0.185      -0.090       0.467\n",
      "197            -0.1688      0.292     -0.578      0.563      -0.741       0.403\n",
      "215            -0.0916      0.154     -0.595      0.552      -0.393       0.210\n",
      "219            -0.1083      0.131     -0.824      0.410      -0.366       0.149\n",
      "239             0.0754      0.129      0.582      0.560      -0.178       0.329\n",
      "240             0.1583      0.155      1.020      0.308      -0.146       0.462\n",
      "248             7.0618      5.660      1.248      0.212      -4.032      18.156\n",
      "249             0.0537      0.224      0.240      0.810      -0.385       0.492\n",
      "253             0.1756      8.438      0.021      0.983     -16.362      16.713\n",
      "280            -0.1119      0.159     -0.702      0.483      -0.424       0.201\n",
      "284            -1.3679      1.012     -1.352      0.176      -3.351       0.615\n",
      "292             0.0588      0.113      0.521      0.602      -0.162       0.280\n",
      "296             0.0797      0.147      0.542      0.588      -0.209       0.368\n",
      "299            -0.6290      0.933     -0.674      0.500      -2.459       1.201\n",
      "321            -0.5469      0.941     -0.581      0.561      -2.392       1.298\n",
      "346             1.1919      0.917      1.300      0.194      -0.605       2.989\n",
      "347            -0.4686      0.877     -0.534      0.593      -2.187       1.250\n",
      "362             0.0215      0.105      0.205      0.838      -0.184       0.227\n",
      "363            -0.0883      0.096     -0.922      0.357      -0.276       0.099\n",
      "368             0.0241      0.126      0.191      0.848      -0.223       0.271\n",
      "386             0.3429      0.496      0.691      0.489      -0.630       1.315\n",
      "394             0.1801      0.161      1.120      0.263      -0.135       0.495\n",
      "417            -0.1952      0.159     -1.232      0.218      -0.506       0.115\n",
      "424             0.2321      0.136      1.710      0.087      -0.034       0.498\n",
      "427             0.0949      0.105      0.908      0.364      -0.110       0.300\n",
      "429           -16.7494     51.714     -0.324      0.746    -118.108      84.609\n",
      "431            -0.3412      0.217     -1.576      0.115      -0.766       0.083\n",
      "434             0.0731      0.128      0.570      0.569      -0.179       0.325\n",
      "438             0.2993      0.118      2.538      0.011       0.068       0.530\n",
      "469             0.0686      0.152      0.452      0.651      -0.229       0.366\n",
      "470             0.1167      0.309      0.378      0.706      -0.489       0.722\n",
      "478             0.5498      0.312      1.762      0.078      -0.062       1.161\n",
      "489            -0.2439      0.130     -1.882      0.060      -0.498       0.010\n",
      "512             0.3175      0.122      2.609      0.009       0.079       0.556\n",
      "520            -7.7647      6.384     -1.216      0.224     -20.277       4.747\n",
      "524            -0.7701      0.444     -1.733      0.083      -1.641       0.101\n",
      "547             0.0924      0.254      0.364      0.716      -0.405       0.589\n",
      "548             0.2875      0.150      1.921      0.055      -0.006       0.581\n",
      "549            -0.0699      0.305     -0.229      0.819      -0.669       0.529\n",
      "550            -0.0685      0.528     -0.130      0.897      -1.103       0.966\n",
      "552            -1.0021      5.772     -0.174      0.862     -12.315      10.311\n",
      "555             0.1364      1.750      0.078      0.938      -3.294       3.567\n",
      "556            -0.1053      0.397     -0.265      0.791      -0.884       0.673\n",
      "558             1.0325      5.851      0.176      0.860     -10.435      12.500\n",
      "562             0.1174      0.122      0.962      0.336      -0.122       0.356\n",
      "563            -0.8053      0.360     -2.236      0.025      -1.511      -0.099\n",
      "564            -0.7607      0.295     -2.578      0.010      -1.339      -0.182\n",
      "565             8.0199      3.888      2.063      0.039       0.399      15.641\n",
      "567            -2.4439      0.807     -3.027      0.002      -4.026      -0.861\n",
      "568             1.1210      0.638      1.758      0.079      -0.129       2.371\n",
      "569            -5.4175      3.776     -1.435      0.151     -12.819       1.984\n",
      "570            -0.8492      0.597     -1.421      0.155      -2.020       0.322\n",
      "576            -0.3356      0.223     -1.503      0.133      -0.773       0.102\n",
      "580             0.8406      0.507      1.658      0.097      -0.153       1.835\n",
      "581            -0.7057      0.533     -1.325      0.185      -1.750       0.338\n",
      "582            -0.1992      0.174     -1.145      0.252      -0.540       0.142\n",
      "588             0.0353      0.198      0.178      0.859      -0.353       0.424\n",
      "missing_73      0.0630      0.155      0.406      0.685      -0.241       0.367\n",
      "missing_113     0.1615      0.193      0.837      0.403      -0.217       0.540\n",
      "===============================================================================\n",
      "Significant features (p < 0.05): ['22', '60', '65', '68', '74', '118', '130', '173', '438', '512', '563', '564', '565', '567']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# Select RFE features\n",
    "X_final = X_indicators[selected_features_xgbrfe]\n",
    "\n",
    "# Scale the features to prevent numerical instability\n",
    "scaler = StandardScaler()\n",
    "X_final_scaled = scaler.fit_transform(X_final)\n",
    "X_final_scaled = pd.DataFrame(X_final_scaled, columns=X_final.columns)\n",
    "\n",
    "# Add intercept for statsmodels\n",
    "X_final_scaled = sm.add_constant(X_final_scaled)\n",
    "\n",
    "# Fit logistic regression with statsmodels\n",
    "logit_model = sm.Logit(Y, X_final_scaled)\n",
    "result = logit_model.fit(method='lbfgs', maxiter=500)  # Use 'bfgs' optimizer to avoid convergence issues\n",
    "print(result.summary())\n",
    "\n",
    "# Extract p-values\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')  # Remove intercept from feature list\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6c01f501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 27 features due to high VIF: ['17', '118', '156', '164', '185', '248', '253', '284', '299', '321', '346', '347', '386', '429', '478', '520', '550', '552', '555', '558', '565', '567', '568', '569', '570', '580', '581']\n"
     ]
    }
   ],
   "source": [
    "# Replace with the feature list for the dataset (e.g., RFE's 100 features)\n",
    "X_selected = X_indicators[selected_features_xgbrfe]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Compute VIF to reduce multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_scaled.columns[1:]  # Exclude 'const'\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) for i in range(1, X_scaled.shape[1])]\n",
    "\n",
    "# Remove features with high VIF (> 10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"]\n",
    "X_scaled_reduced = X_scaled.drop(columns=high_vif_features)\n",
    "print(f\"Removed {len(high_vif_features)} features due to high VIF: {list(high_vif_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a4231e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179038\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1493\n",
      "Method:                           MLE   Df Model:                           73\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:32   Log-Likelihood:                -280.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.471e-14\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7502     13.811     -1.068      0.286     -41.818      12.318\n",
      "1              -0.0052      0.126     -0.041      0.967      -0.252       0.241\n",
      "11              0.0567      0.129      0.439      0.661      -0.196       0.310\n",
      "22              0.6641      0.195      3.413      0.001       0.283       1.045\n",
      "30             -0.0716      0.133     -0.540      0.589      -0.331       0.188\n",
      "34              0.1379      0.089      1.547      0.122      -0.037       0.313\n",
      "49              0.0473      0.149      0.317      0.751      -0.245       0.340\n",
      "60              0.9493      0.219      4.328      0.000       0.519       1.379\n",
      "65              0.5091      0.154      3.305      0.001       0.207       0.811\n",
      "68             -0.3326      0.198     -1.683      0.092      -0.720       0.055\n",
      "73              0.0792      0.223      0.354      0.723      -0.359       0.517\n",
      "74             -0.7694      0.440     -1.748      0.080      -1.632       0.093\n",
      "79              0.3005      0.159      1.891      0.059      -0.011       0.612\n",
      "80             -0.1170      0.139     -0.844      0.398      -0.389       0.155\n",
      "82              0.1894      0.119      1.593      0.111      -0.044       0.422\n",
      "85             -0.1230      0.124     -0.989      0.323      -0.367       0.121\n",
      "89              0.0583      0.129      0.453      0.651      -0.194       0.311\n",
      "92             -0.1215      0.160     -0.761      0.447      -0.435       0.192\n",
      "93             -0.0463      0.133     -0.348      0.728      -0.307       0.215\n",
      "96             -0.0113      0.135     -0.083      0.934      -0.276       0.253\n",
      "97             -0.0043      0.139     -0.031      0.975      -0.277       0.268\n",
      "103            -0.2468      0.150     -1.643      0.100      -0.541       0.048\n",
      "104             0.2280      0.134      1.708      0.088      -0.034       0.490\n",
      "113             0.2405      0.219      1.097      0.273      -0.189       0.670\n",
      "116            -0.1631      0.121     -1.353      0.176      -0.399       0.073\n",
      "119             0.0092      0.130      0.070      0.944      -0.246       0.264\n",
      "121            -0.1142      0.121     -0.940      0.347      -0.352       0.124\n",
      "125             0.2755      0.160      1.718      0.086      -0.039       0.590\n",
      "126             0.0778      0.191      0.406      0.684      -0.297       0.453\n",
      "128            -0.0937      0.178     -0.525      0.599      -0.443       0.256\n",
      "130             0.5243      0.180      2.909      0.004       0.171       0.877\n",
      "134             0.1791      0.165      1.084      0.278      -0.145       0.503\n",
      "139             0.1990      0.130      1.529      0.126      -0.056       0.454\n",
      "141          -216.2113    269.925     -0.801      0.423    -745.255     312.833\n",
      "147            -0.2993      0.138     -2.163      0.031      -0.571      -0.028\n",
      "151            -0.2506      0.134     -1.874      0.061      -0.513       0.012\n",
      "173             0.2477      0.129      1.914      0.056      -0.006       0.501\n",
      "189             0.1260      0.132      0.956      0.339      -0.132       0.384\n",
      "197             0.0338      0.215      0.157      0.875      -0.388       0.455\n",
      "215            -0.1473      0.158     -0.933      0.351      -0.457       0.162\n",
      "219            -0.1171      0.123     -0.956      0.339      -0.357       0.123\n",
      "239             0.0340      0.119      0.284      0.776      -0.200       0.268\n",
      "240             0.1842      0.130      1.419      0.156      -0.070       0.439\n",
      "249             0.0200      0.228      0.087      0.930      -0.427       0.467\n",
      "280            -0.0518      0.145     -0.358      0.721      -0.336       0.232\n",
      "292             0.0222      0.131      0.170      0.865      -0.235       0.279\n",
      "296            -0.0287      0.131     -0.218      0.827      -0.286       0.229\n",
      "362             0.0032      0.090      0.036      0.971      -0.174       0.181\n",
      "363            -0.0430      0.090     -0.475      0.635      -0.220       0.134\n",
      "368             0.0194      0.119      0.163      0.870      -0.213       0.252\n",
      "394             0.1494      0.148      1.011      0.312      -0.140       0.439\n",
      "417            -0.2891      0.151     -1.919      0.055      -0.584       0.006\n",
      "424             0.2208      0.123      1.789      0.074      -0.021       0.463\n",
      "427             0.0235      0.128      0.183      0.855      -0.228       0.275\n",
      "431            -0.2869      0.172     -1.663      0.096      -0.625       0.051\n",
      "434             0.1160      0.119      0.979      0.328      -0.116       0.348\n",
      "438             0.3018      0.114      2.638      0.008       0.078       0.526\n",
      "469            -0.0067      0.142     -0.047      0.962      -0.285       0.272\n",
      "470             0.2622      0.267      0.984      0.325      -0.260       0.785\n",
      "489            -0.2547      0.123     -2.069      0.039      -0.496      -0.013\n",
      "512             0.2545      0.113      2.245      0.025       0.032       0.477\n",
      "524            -0.3017      0.249     -1.212      0.226      -0.790       0.186\n",
      "547             0.0461      0.136      0.339      0.735      -0.220       0.312\n",
      "548             0.2147      0.131      1.634      0.102      -0.043       0.472\n",
      "549            -0.1874      0.129     -1.447      0.148      -0.441       0.066\n",
      "556            -0.0696      0.135     -0.514      0.607      -0.335       0.196\n",
      "562             0.1300      0.114      1.137      0.255      -0.094       0.354\n",
      "563            -0.3165      0.135     -2.351      0.019      -0.580      -0.053\n",
      "564            -0.2486      0.156     -1.593      0.111      -0.554       0.057\n",
      "576            -0.4687      0.219     -2.143      0.032      -0.897      -0.040\n",
      "582            -0.1266      0.156     -0.814      0.416      -0.432       0.178\n",
      "588             0.1539      0.121      1.271      0.204      -0.083       0.391\n",
      "missing_73     -0.0036      0.141     -0.026      0.979      -0.281       0.273\n",
      "missing_113     0.0941      0.174      0.539      0.590      -0.248       0.436\n",
      "===============================================================================\n",
      "Removing feature missing_73 with p-value 0.979427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179038\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1494\n",
      "Method:                           MLE   Df Model:                           72\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:32   Log-Likelihood:                -280.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.453e-14\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7489     13.806     -1.068      0.285     -41.809      12.311\n",
      "1              -0.0053      0.126     -0.042      0.966      -0.252       0.241\n",
      "11              0.0572      0.128      0.447      0.655      -0.194       0.308\n",
      "22              0.6642      0.195      3.414      0.001       0.283       1.045\n",
      "30             -0.0715      0.133     -0.540      0.589      -0.331       0.188\n",
      "34              0.1380      0.089      1.549      0.121      -0.037       0.313\n",
      "49              0.0474      0.149      0.318      0.751      -0.245       0.340\n",
      "60              0.9502      0.217      4.388      0.000       0.526       1.375\n",
      "65              0.5099      0.151      3.376      0.001       0.214       0.806\n",
      "68             -0.3330      0.197     -1.687      0.092      -0.720       0.054\n",
      "73              0.0795      0.223      0.356      0.722      -0.358       0.517\n",
      "74             -0.7708      0.437     -1.763      0.078      -1.628       0.086\n",
      "79              0.2998      0.157      1.914      0.056      -0.007       0.607\n",
      "80             -0.1176      0.137     -0.857      0.392      -0.386       0.151\n",
      "82              0.1892      0.119      1.593      0.111      -0.044       0.422\n",
      "85             -0.1233      0.124     -0.996      0.319      -0.366       0.119\n",
      "89              0.0584      0.129      0.454      0.650      -0.194       0.311\n",
      "92             -0.1217      0.160     -0.762      0.446      -0.435       0.191\n",
      "93             -0.0464      0.133     -0.348      0.728      -0.308       0.215\n",
      "96             -0.0114      0.135     -0.084      0.933      -0.276       0.253\n",
      "97             -0.0043      0.139     -0.031      0.975      -0.277       0.268\n",
      "103            -0.2466      0.150     -1.644      0.100      -0.541       0.047\n",
      "104             0.2280      0.134      1.708      0.088      -0.034       0.490\n",
      "113             0.2405      0.219      1.097      0.273      -0.189       0.670\n",
      "116            -0.1633      0.120     -1.356      0.175      -0.399       0.073\n",
      "119             0.0093      0.130      0.071      0.943      -0.245       0.264\n",
      "121            -0.1141      0.121     -0.940      0.347      -0.352       0.124\n",
      "125             0.2754      0.160      1.718      0.086      -0.039       0.590\n",
      "126             0.0775      0.191      0.406      0.685      -0.297       0.452\n",
      "128            -0.0935      0.178     -0.525      0.600      -0.442       0.256\n",
      "130             0.5244      0.180      2.911      0.004       0.171       0.877\n",
      "134             0.1795      0.164      1.093      0.275      -0.143       0.502\n",
      "139             0.1993      0.129      1.539      0.124      -0.055       0.453\n",
      "141          -216.1848    269.841     -0.801      0.423    -745.063     312.693\n",
      "147            -0.2994      0.138     -2.163      0.031      -0.571      -0.028\n",
      "151            -0.2505      0.134     -1.874      0.061      -0.513       0.012\n",
      "173             0.2480      0.129      1.922      0.055      -0.005       0.501\n",
      "189             0.1261      0.132      0.957      0.338      -0.132       0.384\n",
      "197             0.0337      0.215      0.157      0.876      -0.388       0.456\n",
      "215            -0.1473      0.158     -0.933      0.351      -0.457       0.162\n",
      "219            -0.1170      0.122     -0.955      0.339      -0.357       0.123\n",
      "239             0.0339      0.119      0.284      0.777      -0.200       0.268\n",
      "240             0.1842      0.130      1.419      0.156      -0.070       0.439\n",
      "249             0.0200      0.228      0.088      0.930      -0.427       0.467\n",
      "280            -0.0519      0.145     -0.359      0.720      -0.335       0.232\n",
      "292             0.0225      0.131      0.172      0.863      -0.234       0.279\n",
      "296            -0.0286      0.131     -0.218      0.828      -0.286       0.229\n",
      "362             0.0033      0.090      0.036      0.971      -0.174       0.181\n",
      "363            -0.0430      0.090     -0.475      0.635      -0.220       0.134\n",
      "368             0.0193      0.119      0.163      0.871      -0.213       0.252\n",
      "394             0.1494      0.148      1.011      0.312      -0.140       0.439\n",
      "417            -0.2892      0.151     -1.921      0.055      -0.584       0.006\n",
      "424             0.2207      0.123      1.789      0.074      -0.021       0.463\n",
      "427             0.0235      0.128      0.184      0.854      -0.228       0.275\n",
      "431            -0.2867      0.172     -1.663      0.096      -0.625       0.051\n",
      "434             0.1159      0.118      0.978      0.328      -0.116       0.348\n",
      "438             0.3018      0.114      2.638      0.008       0.078       0.526\n",
      "469            -0.0068      0.142     -0.048      0.962      -0.285       0.272\n",
      "470             0.2625      0.267      0.985      0.325      -0.260       0.785\n",
      "489            -0.2544      0.123     -2.073      0.038      -0.495      -0.014\n",
      "512             0.2547      0.113      2.255      0.024       0.033       0.476\n",
      "524            -0.3015      0.249     -1.211      0.226      -0.789       0.186\n",
      "547             0.0458      0.136      0.338      0.735      -0.220       0.312\n",
      "548             0.2149      0.131      1.637      0.102      -0.042       0.472\n",
      "549            -0.1872      0.129     -1.447      0.148      -0.441       0.066\n",
      "556            -0.0696      0.135     -0.514      0.608      -0.335       0.196\n",
      "562             0.1300      0.114      1.138      0.255      -0.094       0.354\n",
      "563            -0.3164      0.135     -2.352      0.019      -0.580      -0.053\n",
      "564            -0.2487      0.156     -1.594      0.111      -0.554       0.057\n",
      "576            -0.4687      0.219     -2.143      0.032      -0.897      -0.040\n",
      "582            -0.1264      0.155     -0.814      0.416      -0.431       0.178\n",
      "588             0.1538      0.121      1.271      0.204      -0.083       0.391\n",
      "missing_113     0.0943      0.174      0.541      0.588      -0.247       0.436\n",
      "===============================================================================\n",
      "Removing feature 97 with p-value 0.975355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179038\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1495\n",
      "Method:                           MLE   Df Model:                           71\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:32   Log-Likelihood:                -280.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.480e-15\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7439     13.806     -1.068      0.286     -41.803      12.315\n",
      "1              -0.0053      0.126     -0.042      0.966      -0.252       0.241\n",
      "11              0.0570      0.128      0.446      0.656      -0.194       0.308\n",
      "22              0.6643      0.194      3.416      0.001       0.283       1.045\n",
      "30             -0.0716      0.133     -0.540      0.589      -0.331       0.188\n",
      "34              0.1380      0.089      1.550      0.121      -0.037       0.313\n",
      "49              0.0472      0.149      0.317      0.751      -0.245       0.339\n",
      "60              0.9497      0.216      4.400      0.000       0.527       1.373\n",
      "65              0.5094      0.150      3.394      0.001       0.215       0.804\n",
      "68             -0.3325      0.197     -1.689      0.091      -0.718       0.053\n",
      "73              0.0790      0.223      0.355      0.723      -0.357       0.515\n",
      "74             -0.7711      0.437     -1.764      0.078      -1.628       0.085\n",
      "79              0.2996      0.156      1.915      0.055      -0.007       0.606\n",
      "80             -0.1179      0.137     -0.862      0.389      -0.386       0.150\n",
      "82              0.1892      0.119      1.593      0.111      -0.044       0.422\n",
      "85             -0.1238      0.123     -1.006      0.315      -0.365       0.117\n",
      "89              0.0582      0.129      0.453      0.651      -0.194       0.310\n",
      "92             -0.1223      0.158     -0.773      0.439      -0.432       0.188\n",
      "93             -0.0469      0.132     -0.355      0.723      -0.306       0.212\n",
      "96             -0.0113      0.135     -0.084      0.933      -0.276       0.253\n",
      "103            -0.2470      0.149     -1.653      0.098      -0.540       0.046\n",
      "104             0.2284      0.133      1.716      0.086      -0.033       0.489\n",
      "113             0.2403      0.219      1.096      0.273      -0.189       0.670\n",
      "116            -0.1633      0.120     -1.356      0.175      -0.399       0.073\n",
      "119             0.0093      0.130      0.072      0.943      -0.245       0.264\n",
      "121            -0.1140      0.121     -0.939      0.348      -0.352       0.124\n",
      "125             0.2755      0.160      1.719      0.086      -0.039       0.590\n",
      "126             0.0778      0.191      0.407      0.684      -0.296       0.452\n",
      "128            -0.0934      0.178     -0.524      0.600      -0.442       0.256\n",
      "130             0.5250      0.179      2.928      0.003       0.174       0.876\n",
      "134             0.1795      0.164      1.092      0.275      -0.143       0.501\n",
      "139             0.1993      0.129      1.539      0.124      -0.055       0.453\n",
      "141          -216.0838    269.833     -0.801      0.423    -744.947     312.779\n",
      "147            -0.2996      0.138     -2.169      0.030      -0.570      -0.029\n",
      "151            -0.2505      0.134     -1.874      0.061      -0.513       0.012\n",
      "173             0.2481      0.129      1.923      0.054      -0.005       0.501\n",
      "189             0.1262      0.132      0.959      0.338      -0.132       0.384\n",
      "197             0.0336      0.215      0.156      0.876      -0.388       0.456\n",
      "215            -0.1473      0.158     -0.933      0.351      -0.457       0.162\n",
      "219            -0.1173      0.122     -0.961      0.337      -0.357       0.122\n",
      "239             0.0337      0.119      0.282      0.778      -0.200       0.267\n",
      "240             0.1843      0.130      1.420      0.156      -0.070       0.439\n",
      "249             0.0199      0.228      0.087      0.930      -0.427       0.467\n",
      "280            -0.0518      0.145     -0.358      0.720      -0.335       0.232\n",
      "292             0.0223      0.131      0.171      0.864      -0.234       0.278\n",
      "296            -0.0284      0.131     -0.217      0.828      -0.285       0.229\n",
      "362             0.0030      0.090      0.034      0.973      -0.174       0.180\n",
      "363            -0.0431      0.090     -0.476      0.634      -0.220       0.134\n",
      "368             0.0198      0.118      0.168      0.867      -0.211       0.251\n",
      "394             0.1493      0.148      1.011      0.312      -0.140       0.439\n",
      "417            -0.2892      0.151     -1.921      0.055      -0.584       0.006\n",
      "424             0.2208      0.123      1.790      0.073      -0.021       0.463\n",
      "427             0.0234      0.128      0.183      0.855      -0.228       0.275\n",
      "431            -0.2868      0.172     -1.663      0.096      -0.625       0.051\n",
      "434             0.1159      0.118      0.979      0.328      -0.116       0.348\n",
      "438             0.3017      0.114      2.638      0.008       0.078       0.526\n",
      "469            -0.0070      0.142     -0.049      0.961      -0.285       0.271\n",
      "470             0.2626      0.267      0.985      0.325      -0.260       0.785\n",
      "489            -0.2547      0.123     -2.079      0.038      -0.495      -0.015\n",
      "512             0.2546      0.113      2.255      0.024       0.033       0.476\n",
      "524            -0.3015      0.249     -1.212      0.226      -0.789       0.186\n",
      "547             0.0462      0.135      0.342      0.733      -0.219       0.311\n",
      "548             0.2150      0.131      1.638      0.101      -0.042       0.472\n",
      "549            -0.1873      0.129     -1.447      0.148      -0.441       0.066\n",
      "556            -0.0693      0.135     -0.513      0.608      -0.334       0.196\n",
      "562             0.1302      0.114      1.141      0.254      -0.094       0.354\n",
      "563            -0.3163      0.135     -2.352      0.019      -0.580      -0.053\n",
      "564            -0.2487      0.156     -1.594      0.111      -0.554       0.057\n",
      "576            -0.4689      0.219     -2.144      0.032      -0.898      -0.040\n",
      "582            -0.1263      0.155     -0.813      0.416      -0.431       0.178\n",
      "588             0.1539      0.121      1.273      0.203      -0.083       0.391\n",
      "missing_113     0.0941      0.174      0.541      0.589      -0.247       0.435\n",
      "===============================================================================\n",
      "Removing feature 362 with p-value 0.973039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179038\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1496\n",
      "Method:                           MLE   Df Model:                           70\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:33   Log-Likelihood:                -280.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.916e-15\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7392     13.805     -1.068      0.286     -41.796      12.317\n",
      "1              -0.0054      0.126     -0.043      0.966      -0.252       0.241\n",
      "11              0.0571      0.128      0.447      0.655      -0.193       0.308\n",
      "22              0.6640      0.194      3.418      0.001       0.283       1.045\n",
      "30             -0.0716      0.133     -0.540      0.589      -0.331       0.188\n",
      "34              0.1380      0.089      1.549      0.121      -0.037       0.312\n",
      "49              0.0470      0.149      0.316      0.752      -0.245       0.339\n",
      "60              0.9502      0.215      4.414      0.000       0.528       1.372\n",
      "65              0.5096      0.150      3.397      0.001       0.216       0.804\n",
      "68             -0.3326      0.197     -1.689      0.091      -0.718       0.053\n",
      "73              0.0789      0.223      0.354      0.723      -0.357       0.515\n",
      "74             -0.7709      0.437     -1.764      0.078      -1.627       0.086\n",
      "79              0.2995      0.156      1.915      0.055      -0.007       0.606\n",
      "80             -0.1182      0.136     -0.866      0.386      -0.386       0.149\n",
      "82              0.1892      0.119      1.593      0.111      -0.044       0.422\n",
      "85             -0.1239      0.123     -1.008      0.313      -0.365       0.117\n",
      "89              0.0578      0.128      0.451      0.652      -0.193       0.309\n",
      "92             -0.1226      0.158     -0.777      0.437      -0.432       0.187\n",
      "93             -0.0469      0.132     -0.355      0.723      -0.306       0.212\n",
      "96             -0.0113      0.135     -0.084      0.933      -0.276       0.253\n",
      "103            -0.2473      0.149     -1.658      0.097      -0.540       0.045\n",
      "104             0.2283      0.133      1.715      0.086      -0.033       0.489\n",
      "113             0.2400      0.219      1.096      0.273      -0.189       0.669\n",
      "116            -0.1636      0.120     -1.361      0.173      -0.399       0.072\n",
      "119             0.0093      0.130      0.072      0.943      -0.245       0.264\n",
      "121            -0.1139      0.121     -0.939      0.348      -0.352       0.124\n",
      "125             0.2755      0.160      1.719      0.086      -0.039       0.590\n",
      "126             0.0778      0.191      0.407      0.684      -0.296       0.452\n",
      "128            -0.0932      0.178     -0.524      0.600      -0.442       0.256\n",
      "130             0.5248      0.179      2.928      0.003       0.174       0.876\n",
      "134             0.1794      0.164      1.092      0.275      -0.143       0.501\n",
      "139             0.1992      0.129      1.539      0.124      -0.055       0.453\n",
      "141          -215.9940    269.807     -0.801      0.423    -744.805     312.817\n",
      "147            -0.2996      0.138     -2.169      0.030      -0.570      -0.029\n",
      "151            -0.2507      0.134     -1.878      0.060      -0.512       0.011\n",
      "173             0.2480      0.129      1.923      0.054      -0.005       0.501\n",
      "189             0.1265      0.131      0.965      0.334      -0.130       0.383\n",
      "197             0.0338      0.215      0.157      0.875      -0.388       0.456\n",
      "215            -0.1471      0.158     -0.933      0.351      -0.456       0.162\n",
      "219            -0.1175      0.122     -0.964      0.335      -0.356       0.121\n",
      "239             0.0334      0.119      0.281      0.779      -0.200       0.266\n",
      "240             0.1844      0.130      1.422      0.155      -0.070       0.439\n",
      "249             0.0198      0.228      0.087      0.931      -0.428       0.468\n",
      "280            -0.0521      0.144     -0.361      0.718      -0.335       0.231\n",
      "292             0.0223      0.131      0.171      0.864      -0.234       0.279\n",
      "296            -0.0285      0.131     -0.217      0.828      -0.286       0.229\n",
      "363            -0.0430      0.090     -0.475      0.635      -0.220       0.134\n",
      "368             0.0197      0.118      0.167      0.867      -0.211       0.251\n",
      "394             0.1492      0.148      1.010      0.312      -0.140       0.439\n",
      "417            -0.2890      0.150     -1.921      0.055      -0.584       0.006\n",
      "424             0.2208      0.123      1.790      0.073      -0.021       0.463\n",
      "427             0.0234      0.128      0.183      0.855      -0.228       0.275\n",
      "431            -0.2867      0.172     -1.663      0.096      -0.625       0.051\n",
      "434             0.1159      0.118      0.978      0.328      -0.116       0.348\n",
      "438             0.3017      0.114      2.638      0.008       0.078       0.526\n",
      "469            -0.0071      0.142     -0.050      0.960      -0.285       0.271\n",
      "470             0.2622      0.266      0.985      0.325      -0.260       0.784\n",
      "489            -0.2546      0.122     -2.079      0.038      -0.495      -0.015\n",
      "512             0.2547      0.113      2.258      0.024       0.034       0.476\n",
      "524            -0.3018      0.249     -1.213      0.225      -0.789       0.186\n",
      "547             0.0463      0.135      0.343      0.732      -0.218       0.311\n",
      "548             0.2151      0.131      1.639      0.101      -0.042       0.472\n",
      "549            -0.1876      0.129     -1.453      0.146      -0.441       0.065\n",
      "556            -0.0697      0.135     -0.517      0.605      -0.334       0.195\n",
      "562             0.1301      0.114      1.140      0.254      -0.094       0.354\n",
      "563            -0.3164      0.135     -2.352      0.019      -0.580      -0.053\n",
      "564            -0.2488      0.156     -1.596      0.111      -0.554       0.057\n",
      "576            -0.4687      0.219     -2.144      0.032      -0.897      -0.040\n",
      "582            -0.1261      0.155     -0.813      0.416      -0.430       0.178\n",
      "588             0.1541      0.121      1.276      0.202      -0.083       0.391\n",
      "missing_113     0.0942      0.174      0.541      0.588      -0.247       0.435\n",
      "===============================================================================\n",
      "Removing feature 1 with p-value 0.965731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179039\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1497\n",
      "Method:                           MLE   Df Model:                           69\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:33   Log-Likelihood:                -280.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.830e-15\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7535     13.800     -1.069      0.285     -41.801      12.294\n",
      "11              0.0568      0.128      0.445      0.656      -0.193       0.307\n",
      "22              0.6641      0.194      3.418      0.001       0.283       1.045\n",
      "30             -0.0714      0.132     -0.539      0.590      -0.331       0.188\n",
      "34              0.1382      0.089      1.554      0.120      -0.036       0.312\n",
      "49              0.0470      0.149      0.316      0.752      -0.245       0.339\n",
      "60              0.9507      0.215      4.422      0.000       0.529       1.372\n",
      "65              0.5099      0.150      3.402      0.001       0.216       0.804\n",
      "68             -0.3329      0.197     -1.692      0.091      -0.718       0.053\n",
      "73              0.0787      0.223      0.354      0.724      -0.358       0.515\n",
      "74             -0.7701      0.437     -1.764      0.078      -1.626       0.086\n",
      "79              0.2989      0.156      1.920      0.055      -0.006       0.604\n",
      "80             -0.1180      0.136     -0.865      0.387      -0.385       0.149\n",
      "82              0.1889      0.118      1.594      0.111      -0.043       0.421\n",
      "85             -0.1241      0.123     -1.010      0.313      -0.365       0.117\n",
      "89              0.0578      0.128      0.451      0.652      -0.193       0.309\n",
      "92             -0.1228      0.158     -0.778      0.437      -0.432       0.187\n",
      "93             -0.0468      0.132     -0.354      0.723      -0.306       0.212\n",
      "96             -0.0113      0.135     -0.084      0.933      -0.276       0.253\n",
      "103            -0.2469      0.149     -1.659      0.097      -0.538       0.045\n",
      "104             0.2281      0.133      1.715      0.086      -0.033       0.489\n",
      "113             0.2405      0.219      1.099      0.272      -0.188       0.669\n",
      "116            -0.1635      0.120     -1.361      0.174      -0.399       0.072\n",
      "119             0.0097      0.130      0.074      0.941      -0.244       0.264\n",
      "121            -0.1140      0.121     -0.940      0.347      -0.352       0.124\n",
      "125             0.2757      0.160      1.721      0.085      -0.038       0.590\n",
      "126             0.0774      0.191      0.406      0.685      -0.297       0.451\n",
      "128            -0.0931      0.178     -0.523      0.601      -0.442       0.256\n",
      "130             0.5245      0.179      2.929      0.003       0.174       0.876\n",
      "134             0.1794      0.164      1.092      0.275      -0.143       0.501\n",
      "139             0.1991      0.129      1.538      0.124      -0.055       0.453\n",
      "141          -216.2738    269.714     -0.802      0.423    -744.905     312.357\n",
      "147            -0.2997      0.138     -2.169      0.030      -0.570      -0.029\n",
      "151            -0.2511      0.133     -1.884      0.060      -0.512       0.010\n",
      "173             0.2484      0.129      1.929      0.054      -0.004       0.501\n",
      "189             0.1266      0.131      0.966      0.334      -0.130       0.384\n",
      "197             0.0346      0.214      0.161      0.872      -0.386       0.455\n",
      "215            -0.1475      0.157     -0.937      0.349      -0.456       0.161\n",
      "219            -0.1176      0.122     -0.964      0.335      -0.357       0.121\n",
      "239             0.0335      0.119      0.282      0.778      -0.199       0.266\n",
      "240             0.1845      0.130      1.424      0.155      -0.069       0.439\n",
      "249             0.0197      0.228      0.086      0.931      -0.428       0.467\n",
      "280            -0.0520      0.144     -0.360      0.719      -0.335       0.231\n",
      "292             0.0220      0.131      0.168      0.866      -0.234       0.278\n",
      "296            -0.0290      0.131     -0.222      0.825      -0.285       0.227\n",
      "363            -0.0432      0.090     -0.478      0.633      -0.220       0.134\n",
      "368             0.0198      0.118      0.168      0.866      -0.211       0.251\n",
      "394             0.1492      0.148      1.010      0.312      -0.140       0.439\n",
      "417            -0.2887      0.150     -1.920      0.055      -0.583       0.006\n",
      "424             0.2206      0.123      1.791      0.073      -0.021       0.462\n",
      "427             0.0233      0.128      0.181      0.856      -0.228       0.275\n",
      "431            -0.2862      0.172     -1.665      0.096      -0.623       0.051\n",
      "434             0.1156      0.118      0.977      0.328      -0.116       0.348\n",
      "438             0.3017      0.114      2.638      0.008       0.078       0.526\n",
      "469            -0.0070      0.142     -0.050      0.960      -0.285       0.271\n",
      "470             0.2613      0.265      0.984      0.325      -0.259       0.782\n",
      "489            -0.2549      0.122     -2.086      0.037      -0.494      -0.015\n",
      "512             0.2549      0.113      2.263      0.024       0.034       0.476\n",
      "524            -0.3020      0.249     -1.214      0.225      -0.790       0.186\n",
      "547             0.0463      0.135      0.343      0.732      -0.218       0.311\n",
      "548             0.2149      0.131      1.638      0.101      -0.042       0.472\n",
      "549            -0.1871      0.129     -1.454      0.146      -0.439       0.065\n",
      "556            -0.0698      0.135     -0.518      0.605      -0.334       0.194\n",
      "562             0.1296      0.114      1.142      0.254      -0.093       0.352\n",
      "563            -0.3160      0.134     -2.354      0.019      -0.579      -0.053\n",
      "564            -0.2491      0.156     -1.598      0.110      -0.555       0.056\n",
      "576            -0.4687      0.219     -2.144      0.032      -0.897      -0.040\n",
      "582            -0.1259      0.155     -0.812      0.417      -0.430       0.178\n",
      "588             0.1541      0.121      1.276      0.202      -0.083       0.391\n",
      "missing_113     0.0941      0.174      0.540      0.589      -0.247       0.435\n",
      "===============================================================================\n",
      "Removing feature 469 with p-value 0.960491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179040\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1498\n",
      "Method:                           MLE   Df Model:                           68\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:33   Log-Likelihood:                -280.56\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.617e-15\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7152     13.776     -1.068      0.285     -41.716      12.286\n",
      "11              0.0570      0.128      0.447      0.655      -0.193       0.307\n",
      "22              0.6642      0.194      3.419      0.001       0.283       1.045\n",
      "30             -0.0712      0.132     -0.538      0.591      -0.331       0.188\n",
      "34              0.1382      0.089      1.556      0.120      -0.036       0.312\n",
      "49              0.0465      0.149      0.313      0.754      -0.245       0.338\n",
      "60              0.9528      0.211      4.524      0.000       0.540       1.366\n",
      "65              0.5102      0.150      3.409      0.001       0.217       0.804\n",
      "68             -0.3333      0.197     -1.693      0.090      -0.719       0.053\n",
      "73              0.0782      0.222      0.352      0.725      -0.358       0.514\n",
      "74             -0.7696      0.437     -1.762      0.078      -1.626       0.086\n",
      "79              0.2988      0.156      1.919      0.055      -0.006       0.604\n",
      "80             -0.1181      0.136     -0.866      0.387      -0.385       0.149\n",
      "82              0.1887      0.118      1.593      0.111      -0.043       0.421\n",
      "85             -0.1242      0.123     -1.011      0.312      -0.365       0.117\n",
      "89              0.0580      0.128      0.453      0.651      -0.193       0.309\n",
      "92             -0.1224      0.158     -0.777      0.437      -0.431       0.187\n",
      "93             -0.0471      0.132     -0.356      0.722      -0.306       0.212\n",
      "96             -0.0112      0.135     -0.083      0.934      -0.276       0.253\n",
      "103            -0.2466      0.149     -1.658      0.097      -0.538       0.045\n",
      "104             0.2281      0.133      1.715      0.086      -0.033       0.489\n",
      "113             0.2403      0.219      1.099      0.272      -0.188       0.669\n",
      "116            -0.1639      0.120     -1.367      0.172      -0.399       0.071\n",
      "119             0.0095      0.130      0.073      0.942      -0.245       0.264\n",
      "121            -0.1136      0.121     -0.938      0.348      -0.351       0.124\n",
      "125             0.2756      0.160      1.721      0.085      -0.038       0.590\n",
      "126             0.0771      0.191      0.404      0.686      -0.297       0.451\n",
      "128            -0.0928      0.178     -0.522      0.602      -0.441       0.256\n",
      "130             0.5248      0.179      2.933      0.003       0.174       0.876\n",
      "134             0.1796      0.164      1.094      0.274      -0.142       0.501\n",
      "139             0.1992      0.129      1.539      0.124      -0.055       0.453\n",
      "141          -215.5302    269.258     -0.800      0.423    -743.266     312.205\n",
      "147            -0.2994      0.138     -2.169      0.030      -0.570      -0.029\n",
      "151            -0.2509      0.133     -1.883      0.060      -0.512       0.010\n",
      "173             0.2483      0.129      1.928      0.054      -0.004       0.501\n",
      "189             0.1269      0.131      0.970      0.332      -0.130       0.384\n",
      "197             0.0340      0.215      0.158      0.874      -0.386       0.454\n",
      "215            -0.1473      0.157     -0.936      0.349      -0.456       0.161\n",
      "219            -0.1171      0.122     -0.963      0.335      -0.356       0.121\n",
      "239             0.0331      0.119      0.279      0.780      -0.199       0.265\n",
      "240             0.1848      0.130      1.426      0.154      -0.069       0.439\n",
      "249             0.0193      0.229      0.084      0.933      -0.429       0.468\n",
      "280            -0.0524      0.144     -0.364      0.716      -0.335       0.230\n",
      "292             0.0222      0.131      0.170      0.865      -0.234       0.278\n",
      "296            -0.0284      0.130     -0.218      0.827      -0.284       0.227\n",
      "363            -0.0430      0.090     -0.476      0.634      -0.220       0.134\n",
      "368             0.0197      0.118      0.167      0.867      -0.211       0.251\n",
      "394             0.1492      0.148      1.011      0.312      -0.140       0.439\n",
      "417            -0.2892      0.150     -1.928      0.054      -0.583       0.005\n",
      "424             0.2207      0.123      1.792      0.073      -0.021       0.462\n",
      "427             0.0229      0.128      0.179      0.858      -0.228       0.274\n",
      "431            -0.2866      0.172     -1.669      0.095      -0.623       0.050\n",
      "434             0.1155      0.118      0.977      0.329      -0.116       0.347\n",
      "438             0.3015      0.114      2.635      0.008       0.077       0.526\n",
      "470             0.2613      0.266      0.983      0.325      -0.259       0.782\n",
      "489            -0.2546      0.122     -2.086      0.037      -0.494      -0.015\n",
      "512             0.2548      0.113      2.263      0.024       0.034       0.476\n",
      "524            -0.3025      0.248     -1.217      0.223      -0.790       0.185\n",
      "547             0.0463      0.135      0.343      0.732      -0.218       0.311\n",
      "548             0.2150      0.131      1.639      0.101      -0.042       0.472\n",
      "549            -0.1873      0.129     -1.456      0.145      -0.439       0.065\n",
      "556            -0.0702      0.135     -0.521      0.602      -0.334       0.194\n",
      "562             0.1296      0.114      1.141      0.254      -0.093       0.352\n",
      "563            -0.3158      0.134     -2.354      0.019      -0.579      -0.053\n",
      "564            -0.2489      0.156     -1.598      0.110      -0.554       0.056\n",
      "576            -0.4694      0.218     -2.151      0.031      -0.897      -0.042\n",
      "582            -0.1257      0.155     -0.811      0.418      -0.429       0.178\n",
      "588             0.1536      0.120      1.276      0.202      -0.082       0.390\n",
      "missing_113     0.0936      0.174      0.539      0.590      -0.247       0.434\n",
      "===============================================================================\n",
      "Removing feature 119 with p-value 0.941504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179042\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1499\n",
      "Method:                           MLE   Df Model:                           67\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2667\n",
      "Time:                        16:31:33   Log-Likelihood:                -280.56\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.187e-16\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7772     13.757     -1.074      0.283     -41.740      12.185\n",
      "11              0.0572      0.128      0.448      0.654      -0.193       0.307\n",
      "22              0.6643      0.194      3.420      0.001       0.284       1.045\n",
      "30             -0.0714      0.132     -0.540      0.590      -0.331       0.188\n",
      "34              0.1382      0.089      1.555      0.120      -0.036       0.312\n",
      "49              0.0465      0.149      0.313      0.754      -0.245       0.338\n",
      "60              0.9531      0.211      4.525      0.000       0.540       1.366\n",
      "65              0.5104      0.150      3.410      0.001       0.217       0.804\n",
      "68             -0.3334      0.197     -1.692      0.091      -0.720       0.053\n",
      "73              0.0772      0.222      0.348      0.728      -0.358       0.512\n",
      "74             -0.7702      0.437     -1.763      0.078      -1.627       0.086\n",
      "79              0.2992      0.156      1.923      0.054      -0.006       0.604\n",
      "80             -0.1178      0.136     -0.864      0.388      -0.385       0.149\n",
      "82              0.1882      0.118      1.592      0.111      -0.043       0.420\n",
      "85             -0.1241      0.123     -1.010      0.312      -0.365       0.117\n",
      "89              0.0588      0.128      0.461      0.645      -0.191       0.309\n",
      "92             -0.1221      0.158     -0.775      0.438      -0.431       0.187\n",
      "93             -0.0473      0.132     -0.358      0.720      -0.306       0.212\n",
      "96             -0.0116      0.135     -0.086      0.932      -0.276       0.253\n",
      "103            -0.2458      0.148     -1.657      0.097      -0.537       0.045\n",
      "104             0.2278      0.133      1.714      0.087      -0.033       0.488\n",
      "113             0.2407      0.219      1.101      0.271      -0.188       0.669\n",
      "116            -0.1645      0.120     -1.375      0.169      -0.399       0.070\n",
      "121            -0.1138      0.121     -0.941      0.347      -0.351       0.123\n",
      "125             0.2755      0.160      1.720      0.085      -0.038       0.589\n",
      "126             0.0771      0.191      0.404      0.686      -0.297       0.451\n",
      "128            -0.0927      0.178     -0.521      0.602      -0.441       0.256\n",
      "130             0.5237      0.178      2.938      0.003       0.174       0.873\n",
      "134             0.1786      0.164      1.092      0.275      -0.142       0.499\n",
      "139             0.1992      0.129      1.539      0.124      -0.054       0.453\n",
      "141          -216.7549    268.860     -0.806      0.420    -743.711     310.201\n",
      "147            -0.2991      0.138     -2.167      0.030      -0.570      -0.029\n",
      "151            -0.2507      0.133     -1.882      0.060      -0.512       0.010\n",
      "173             0.2484      0.129      1.930      0.054      -0.004       0.501\n",
      "189             0.1265      0.131      0.967      0.333      -0.130       0.383\n",
      "197             0.0341      0.215      0.159      0.874      -0.387       0.455\n",
      "215            -0.1472      0.157     -0.936      0.349      -0.455       0.161\n",
      "219            -0.1171      0.122     -0.963      0.336      -0.355       0.121\n",
      "239             0.0324      0.118      0.274      0.784      -0.199       0.264\n",
      "240             0.1850      0.130      1.429      0.153      -0.069       0.439\n",
      "249             0.0203      0.228      0.089      0.929      -0.427       0.467\n",
      "280            -0.0528      0.144     -0.367      0.714      -0.335       0.229\n",
      "292             0.0222      0.131      0.170      0.865      -0.234       0.279\n",
      "296            -0.0278      0.130     -0.214      0.831      -0.283       0.227\n",
      "363            -0.0428      0.090     -0.474      0.635      -0.220       0.134\n",
      "368             0.0202      0.118      0.172      0.863      -0.210       0.251\n",
      "394             0.1499      0.147      1.017      0.309      -0.139       0.439\n",
      "417            -0.2889      0.150     -1.927      0.054      -0.583       0.005\n",
      "424             0.2205      0.123      1.790      0.073      -0.021       0.462\n",
      "427             0.0220      0.127      0.173      0.862      -0.227       0.271\n",
      "431            -0.2874      0.171     -1.678      0.093      -0.623       0.048\n",
      "434             0.1153      0.118      0.975      0.329      -0.116       0.347\n",
      "438             0.3013      0.114      2.633      0.008       0.077       0.526\n",
      "470             0.2609      0.266      0.981      0.326      -0.260       0.782\n",
      "489            -0.2547      0.122     -2.088      0.037      -0.494      -0.016\n",
      "512             0.2550      0.113      2.265      0.024       0.034       0.476\n",
      "524            -0.3024      0.249     -1.217      0.224      -0.789       0.185\n",
      "547             0.0462      0.135      0.342      0.732      -0.219       0.311\n",
      "548             0.2154      0.131      1.644      0.100      -0.041       0.472\n",
      "549            -0.1871      0.129     -1.455      0.146      -0.439       0.065\n",
      "556            -0.0696      0.134     -0.518      0.605      -0.333       0.194\n",
      "562             0.1292      0.113      1.139      0.255      -0.093       0.352\n",
      "563            -0.3153      0.134     -2.352      0.019      -0.578      -0.053\n",
      "564            -0.2488      0.156     -1.597      0.110      -0.554       0.056\n",
      "576            -0.4688      0.218     -2.151      0.032      -0.896      -0.042\n",
      "582            -0.1267      0.154     -0.820      0.412      -0.429       0.176\n",
      "588             0.1537      0.121      1.276      0.202      -0.082       0.390\n",
      "missing_113     0.0934      0.174      0.538      0.591      -0.247       0.434\n",
      "===============================================================================\n",
      "Removing feature 96 with p-value 0.931522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179044\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1500\n",
      "Method:                           MLE   Df Model:                           66\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2666\n",
      "Time:                        16:31:34   Log-Likelihood:                -280.56\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.182e-16\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.8615     13.729     -1.082      0.279     -41.770      12.047\n",
      "11              0.0570      0.128      0.447      0.655      -0.193       0.307\n",
      "22              0.6643      0.194      3.420      0.001       0.284       1.045\n",
      "30             -0.0714      0.132     -0.539      0.590      -0.331       0.188\n",
      "34              0.1388      0.089      1.566      0.117      -0.035       0.312\n",
      "49              0.0458      0.148      0.309      0.757      -0.245       0.336\n",
      "60              0.9509      0.209      4.548      0.000       0.541       1.361\n",
      "65              0.5090      0.149      3.422      0.001       0.217       0.801\n",
      "68             -0.3325      0.197     -1.690      0.091      -0.718       0.053\n",
      "73              0.0771      0.222      0.347      0.728      -0.358       0.512\n",
      "74             -0.7730      0.436     -1.775      0.076      -1.627       0.081\n",
      "79              0.2995      0.156      1.925      0.054      -0.005       0.604\n",
      "80             -0.1179      0.136     -0.865      0.387      -0.385       0.149\n",
      "82              0.1884      0.118      1.594      0.111      -0.043       0.420\n",
      "85             -0.1240      0.123     -1.009      0.313      -0.365       0.117\n",
      "89              0.0590      0.128      0.463      0.643      -0.191       0.309\n",
      "92             -0.1214      0.157     -0.771      0.441      -0.430       0.187\n",
      "93             -0.0472      0.132     -0.357      0.721      -0.306       0.212\n",
      "103            -0.2458      0.148     -1.658      0.097      -0.536       0.045\n",
      "104             0.2249      0.129      1.749      0.080      -0.027       0.477\n",
      "113             0.2420      0.218      1.109      0.267      -0.186       0.670\n",
      "116            -0.1644      0.120     -1.374      0.169      -0.399       0.070\n",
      "121            -0.1133      0.121     -0.937      0.349      -0.350       0.124\n",
      "125             0.2769      0.159      1.739      0.082      -0.035       0.589\n",
      "126             0.0762      0.190      0.400      0.689      -0.297       0.449\n",
      "128            -0.0906      0.176     -0.514      0.607      -0.436       0.255\n",
      "130             0.5234      0.178      2.937      0.003       0.174       0.873\n",
      "134             0.1780      0.163      1.089      0.276      -0.142       0.498\n",
      "139             0.1994      0.129      1.540      0.124      -0.054       0.453\n",
      "141          -218.4037    268.323     -0.814      0.416    -744.306     307.499\n",
      "147            -0.2987      0.138     -2.165      0.030      -0.569      -0.028\n",
      "151            -0.2506      0.133     -1.881      0.060      -0.512       0.010\n",
      "173             0.2478      0.129      1.928      0.054      -0.004       0.500\n",
      "189             0.1270      0.131      0.973      0.331      -0.129       0.383\n",
      "197             0.0333      0.215      0.155      0.877      -0.387       0.454\n",
      "215            -0.1470      0.157     -0.936      0.349      -0.455       0.161\n",
      "219            -0.1179      0.121     -0.971      0.331      -0.356       0.120\n",
      "239             0.0323      0.118      0.273      0.785      -0.199       0.264\n",
      "240             0.1862      0.129      1.447      0.148      -0.066       0.438\n",
      "249             0.0199      0.228      0.087      0.930      -0.428       0.467\n",
      "280            -0.0527      0.144     -0.366      0.714      -0.335       0.229\n",
      "292             0.0219      0.131      0.167      0.867      -0.235       0.279\n",
      "296            -0.0273      0.130     -0.210      0.833      -0.282       0.227\n",
      "363            -0.0423      0.090     -0.469      0.639      -0.219       0.134\n",
      "368             0.0209      0.118      0.178      0.859      -0.210       0.251\n",
      "394             0.1501      0.147      1.018      0.308      -0.139       0.439\n",
      "417            -0.2880      0.150     -1.926      0.054      -0.581       0.005\n",
      "424             0.2202      0.123      1.788      0.074      -0.021       0.462\n",
      "427             0.0221      0.127      0.174      0.862      -0.227       0.271\n",
      "431            -0.2876      0.171     -1.678      0.093      -0.623       0.048\n",
      "434             0.1157      0.118      0.979      0.328      -0.116       0.347\n",
      "438             0.3013      0.114      2.634      0.008       0.077       0.525\n",
      "470             0.2624      0.265      0.990      0.322      -0.257       0.782\n",
      "489            -0.2539      0.122     -2.088      0.037      -0.492      -0.016\n",
      "512             0.2550      0.113      2.266      0.023       0.034       0.476\n",
      "524            -0.3009      0.248     -1.215      0.224      -0.786       0.185\n",
      "547             0.0468      0.135      0.347      0.729      -0.218       0.311\n",
      "548             0.2150      0.131      1.643      0.100      -0.041       0.471\n",
      "549            -0.1874      0.129     -1.458      0.145      -0.439       0.065\n",
      "556            -0.0693      0.134     -0.516      0.606      -0.332       0.194\n",
      "562             0.1296      0.113      1.142      0.253      -0.093       0.352\n",
      "563            -0.3153      0.134     -2.353      0.019      -0.578      -0.053\n",
      "564            -0.2480      0.155     -1.595      0.111      -0.553       0.057\n",
      "576            -0.4685      0.218     -2.150      0.032      -0.895      -0.041\n",
      "582            -0.1271      0.154     -0.823      0.411      -0.430       0.176\n",
      "588             0.1536      0.121      1.275      0.202      -0.083       0.390\n",
      "missing_113     0.0919      0.173      0.532      0.595      -0.247       0.431\n",
      "===============================================================================\n",
      "Removing feature 249 with p-value 0.930460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179046\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1501\n",
      "Method:                           MLE   Df Model:                           65\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2666\n",
      "Time:                        16:31:34   Log-Likelihood:                -280.57\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.901e-16\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7763     13.687     -1.080      0.280     -41.602      12.049\n",
      "11              0.0570      0.128      0.447      0.655      -0.193       0.307\n",
      "22              0.6642      0.194      3.420      0.001       0.284       1.045\n",
      "30             -0.0711      0.132     -0.537      0.591      -0.331       0.188\n",
      "34              0.1387      0.089      1.566      0.117      -0.035       0.312\n",
      "49              0.0469      0.148      0.317      0.751      -0.243       0.336\n",
      "60              0.9528      0.208      4.577      0.000       0.545       1.361\n",
      "65              0.5101      0.148      3.442      0.001       0.220       0.801\n",
      "68             -0.3343      0.196     -1.707      0.088      -0.718       0.050\n",
      "73              0.0764      0.222      0.344      0.731      -0.359       0.511\n",
      "74             -0.7754      0.435     -1.784      0.074      -1.627       0.077\n",
      "79              0.2992      0.156      1.923      0.055      -0.006       0.604\n",
      "80             -0.1181      0.136     -0.867      0.386      -0.385       0.149\n",
      "82              0.1887      0.118      1.598      0.110      -0.043       0.420\n",
      "85             -0.1234      0.123     -1.006      0.314      -0.364       0.117\n",
      "89              0.0587      0.127      0.460      0.645      -0.191       0.309\n",
      "92             -0.1224      0.157     -0.781      0.435      -0.430       0.185\n",
      "93             -0.0458      0.131     -0.350      0.727      -0.303       0.211\n",
      "103            -0.2477      0.147     -1.690      0.091      -0.535       0.040\n",
      "104             0.2248      0.129      1.748      0.080      -0.027       0.477\n",
      "113             0.2421      0.218      1.110      0.267      -0.185       0.670\n",
      "116            -0.1653      0.119     -1.386      0.166      -0.399       0.068\n",
      "121            -0.1127      0.121     -0.934      0.350      -0.349       0.124\n",
      "125             0.2776      0.159      1.745      0.081      -0.034       0.589\n",
      "126             0.0766      0.190      0.402      0.687      -0.296       0.450\n",
      "128            -0.0903      0.176     -0.513      0.608      -0.436       0.255\n",
      "130             0.5230      0.178      2.935      0.003       0.174       0.872\n",
      "134             0.1770      0.163      1.086      0.278      -0.143       0.497\n",
      "139             0.1985      0.129      1.539      0.124      -0.054       0.451\n",
      "141          -216.7187    267.476     -0.810      0.418    -740.962     307.524\n",
      "147            -0.2990      0.138     -2.167      0.030      -0.569      -0.029\n",
      "151            -0.2511      0.133     -1.887      0.059      -0.512       0.010\n",
      "173             0.2482      0.129      1.932      0.053      -0.004       0.500\n",
      "189             0.1264      0.130      0.970      0.332      -0.129       0.382\n",
      "197             0.0328      0.214      0.153      0.879      -0.387       0.453\n",
      "215            -0.1471      0.157     -0.936      0.349      -0.455       0.161\n",
      "219            -0.1181      0.121     -0.974      0.330      -0.356       0.120\n",
      "239             0.0320      0.118      0.271      0.787      -0.200       0.264\n",
      "240             0.1858      0.129      1.444      0.149      -0.066       0.438\n",
      "280            -0.0523      0.144     -0.363      0.716      -0.334       0.230\n",
      "292             0.0219      0.131      0.167      0.867      -0.235       0.279\n",
      "296            -0.0289      0.129     -0.225      0.822      -0.281       0.223\n",
      "363            -0.0424      0.090     -0.470      0.639      -0.219       0.134\n",
      "368             0.0209      0.118      0.178      0.859      -0.210       0.251\n",
      "394             0.1506      0.147      1.022      0.307      -0.138       0.439\n",
      "417            -0.2873      0.149     -1.924      0.054      -0.580       0.005\n",
      "424             0.2203      0.123      1.788      0.074      -0.021       0.462\n",
      "427             0.0218      0.127      0.172      0.864      -0.228       0.271\n",
      "431            -0.2851      0.169     -1.688      0.091      -0.616       0.046\n",
      "434             0.1154      0.118      0.976      0.329      -0.116       0.347\n",
      "438             0.3012      0.114      2.634      0.008       0.077       0.525\n",
      "470             0.2645      0.265      0.999      0.318      -0.254       0.783\n",
      "489            -0.2538      0.122     -2.088      0.037      -0.492      -0.016\n",
      "512             0.2552      0.113      2.267      0.023       0.035       0.476\n",
      "524            -0.3015      0.248     -1.218      0.223      -0.787       0.184\n",
      "547             0.0470      0.135      0.348      0.728      -0.217       0.311\n",
      "548             0.2152      0.131      1.645      0.100      -0.041       0.472\n",
      "549            -0.1876      0.128     -1.460      0.144      -0.439       0.064\n",
      "556            -0.0696      0.134     -0.518      0.604      -0.333       0.194\n",
      "562             0.1293      0.113      1.141      0.254      -0.093       0.352\n",
      "563            -0.3156      0.134     -2.354      0.019      -0.578      -0.053\n",
      "564            -0.2481      0.155     -1.596      0.111      -0.553       0.057\n",
      "576            -0.4681      0.218     -2.150      0.032      -0.895      -0.041\n",
      "582            -0.1271      0.154     -0.823      0.410      -0.430       0.176\n",
      "588             0.1538      0.120      1.276      0.202      -0.082       0.390\n",
      "missing_113     0.0927      0.173      0.537      0.591      -0.246       0.431\n",
      "===============================================================================\n",
      "Removing feature 197 with p-value 0.878538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179054\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1502\n",
      "Method:                           MLE   Df Model:                           64\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2666\n",
      "Time:                        16:31:34   Log-Likelihood:                -280.58\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.620e-16\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7021     13.672     -1.075      0.282     -41.498      12.094\n",
      "11              0.0582      0.127      0.457      0.648      -0.191       0.308\n",
      "22              0.6638      0.194      3.418      0.001       0.283       1.044\n",
      "30             -0.0698      0.132     -0.528      0.597      -0.329       0.189\n",
      "34              0.1396      0.088      1.582      0.114      -0.033       0.313\n",
      "49              0.0472      0.148      0.320      0.749      -0.242       0.337\n",
      "60              0.9516      0.208      4.578      0.000       0.544       1.359\n",
      "65              0.5123      0.147      3.474      0.001       0.223       0.801\n",
      "68             -0.3272      0.189     -1.731      0.083      -0.698       0.043\n",
      "73              0.0761      0.222      0.343      0.732      -0.359       0.511\n",
      "74             -0.7789      0.431     -1.807      0.071      -1.624       0.066\n",
      "79              0.2983      0.155      1.919      0.055      -0.006       0.603\n",
      "80             -0.1185      0.136     -0.870      0.384      -0.386       0.148\n",
      "82              0.1884      0.118      1.595      0.111      -0.043       0.420\n",
      "85             -0.1229      0.123     -1.002      0.316      -0.363       0.117\n",
      "89              0.0579      0.127      0.455      0.649      -0.192       0.308\n",
      "92             -0.1223      0.157     -0.780      0.435      -0.430       0.185\n",
      "93             -0.0465      0.131     -0.355      0.723      -0.303       0.210\n",
      "103            -0.2472      0.146     -1.688      0.091      -0.534       0.040\n",
      "104             0.2249      0.129      1.750      0.080      -0.027       0.477\n",
      "113             0.2392      0.217      1.101      0.271      -0.187       0.665\n",
      "116            -0.1654      0.119     -1.387      0.165      -0.399       0.068\n",
      "121            -0.1124      0.121     -0.932      0.351      -0.349       0.124\n",
      "125             0.2782      0.159      1.750      0.080      -0.033       0.590\n",
      "126             0.0778      0.190      0.409      0.682      -0.295       0.451\n",
      "128            -0.0907      0.176     -0.515      0.607      -0.436       0.255\n",
      "130             0.5242      0.178      2.946      0.003       0.175       0.873\n",
      "134             0.1752      0.163      1.077      0.281      -0.144       0.494\n",
      "139             0.1985      0.129      1.539      0.124      -0.054       0.451\n",
      "141          -215.2609    267.185     -0.806      0.420    -738.934     308.412\n",
      "147            -0.2985      0.138     -2.164      0.030      -0.569      -0.028\n",
      "151            -0.2508      0.133     -1.884      0.060      -0.512       0.010\n",
      "173             0.2482      0.129      1.931      0.053      -0.004       0.500\n",
      "189             0.1267      0.130      0.971      0.332      -0.129       0.382\n",
      "215            -0.1465      0.157     -0.934      0.350      -0.454       0.161\n",
      "219            -0.1179      0.121     -0.972      0.331      -0.356       0.120\n",
      "239             0.0313      0.118      0.265      0.791      -0.200       0.263\n",
      "240             0.1854      0.129      1.441      0.150      -0.067       0.438\n",
      "280            -0.0522      0.144     -0.363      0.717      -0.334       0.230\n",
      "292             0.0232      0.130      0.178      0.859      -0.232       0.279\n",
      "296            -0.0282      0.129     -0.219      0.827      -0.280       0.224\n",
      "363            -0.0417      0.090     -0.463      0.643      -0.218       0.135\n",
      "368             0.0213      0.117      0.181      0.856      -0.209       0.252\n",
      "394             0.1478      0.146      1.012      0.312      -0.138       0.434\n",
      "417            -0.2888      0.149     -1.938      0.053      -0.581       0.003\n",
      "424             0.2199      0.123      1.784      0.074      -0.022       0.462\n",
      "427             0.0215      0.128      0.169      0.866      -0.228       0.271\n",
      "431            -0.2851      0.169     -1.688      0.091      -0.616       0.046\n",
      "434             0.1140      0.118      0.967      0.333      -0.117       0.345\n",
      "438             0.3005      0.114      2.636      0.008       0.077       0.524\n",
      "470             0.2893      0.205      1.414      0.157      -0.112       0.690\n",
      "489            -0.2542      0.122     -2.091      0.036      -0.492      -0.016\n",
      "512             0.2563      0.112      2.282      0.022       0.036       0.476\n",
      "524            -0.2964      0.243     -1.218      0.223      -0.774       0.181\n",
      "547             0.0464      0.135      0.344      0.731      -0.218       0.311\n",
      "548             0.2145      0.131      1.641      0.101      -0.042       0.471\n",
      "549            -0.1876      0.129     -1.460      0.144      -0.439       0.064\n",
      "556            -0.0700      0.134     -0.521      0.602      -0.333       0.193\n",
      "562             0.1305      0.113      1.154      0.248      -0.091       0.352\n",
      "563            -0.3167      0.134     -2.366      0.018      -0.579      -0.054\n",
      "564            -0.2482      0.156     -1.596      0.111      -0.553       0.057\n",
      "576            -0.4707      0.217     -2.166      0.030      -0.897      -0.045\n",
      "582            -0.1271      0.154     -0.824      0.410      -0.430       0.175\n",
      "588             0.1538      0.120      1.277      0.202      -0.082       0.390\n",
      "missing_113     0.0942      0.172      0.547      0.585      -0.244       0.432\n",
      "===============================================================================\n",
      "Removing feature 427 with p-value 0.865812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179062\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1503\n",
      "Method:                           MLE   Df Model:                           63\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2666\n",
      "Time:                        16:31:35   Log-Likelihood:                -280.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.991e-17\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.5779     13.637     -1.069      0.285     -41.306      12.151\n",
      "11              0.0577      0.127      0.454      0.650      -0.192       0.307\n",
      "22              0.6609      0.193      3.418      0.001       0.282       1.040\n",
      "30             -0.0696      0.132     -0.527      0.598      -0.328       0.189\n",
      "34              0.1400      0.088      1.587      0.112      -0.033       0.313\n",
      "49              0.0464      0.148      0.314      0.753      -0.243       0.336\n",
      "60              0.9570      0.205      4.659      0.000       0.554       1.360\n",
      "65              0.5155      0.146      3.523      0.000       0.229       0.802\n",
      "68             -0.3290      0.189     -1.743      0.081      -0.699       0.041\n",
      "73              0.0757      0.222      0.341      0.733      -0.359       0.510\n",
      "74             -0.7779      0.431     -1.804      0.071      -1.623       0.067\n",
      "79              0.2979      0.155      1.917      0.055      -0.007       0.602\n",
      "80             -0.1189      0.136     -0.873      0.383      -0.386       0.148\n",
      "82              0.1884      0.118      1.595      0.111      -0.043       0.420\n",
      "85             -0.1232      0.123     -1.004      0.315      -0.363       0.117\n",
      "89              0.0589      0.127      0.463      0.644      -0.191       0.308\n",
      "92             -0.1218      0.157     -0.777      0.437      -0.429       0.185\n",
      "93             -0.0459      0.131     -0.350      0.726      -0.302       0.211\n",
      "103            -0.2478      0.146     -1.694      0.090      -0.534       0.039\n",
      "104             0.2241      0.128      1.745      0.081      -0.028       0.476\n",
      "113             0.2400      0.217      1.105      0.269      -0.186       0.666\n",
      "116            -0.1662      0.119     -1.394      0.163      -0.400       0.067\n",
      "121            -0.1121      0.121     -0.929      0.353      -0.348       0.124\n",
      "125             0.2795      0.159      1.760      0.078      -0.032       0.591\n",
      "126             0.0781      0.190      0.411      0.681      -0.295       0.451\n",
      "128            -0.0911      0.176     -0.517      0.605      -0.436       0.254\n",
      "130             0.5244      0.178      2.947      0.003       0.176       0.873\n",
      "134             0.1748      0.163      1.076      0.282      -0.144       0.493\n",
      "139             0.1985      0.129      1.540      0.124      -0.054       0.451\n",
      "141          -212.8403    266.514     -0.799      0.425    -735.199     309.518\n",
      "147            -0.2979      0.138     -2.163      0.031      -0.568      -0.028\n",
      "151            -0.2506      0.133     -1.883      0.060      -0.511       0.010\n",
      "173             0.2489      0.129      1.936      0.053      -0.003       0.501\n",
      "189             0.1261      0.130      0.967      0.334      -0.130       0.382\n",
      "215            -0.1468      0.157     -0.936      0.349      -0.454       0.161\n",
      "219            -0.1183      0.121     -0.975      0.329      -0.356       0.119\n",
      "239             0.0304      0.118      0.258      0.796      -0.201       0.262\n",
      "240             0.1858      0.129      1.443      0.149      -0.067       0.438\n",
      "280            -0.0522      0.144     -0.363      0.717      -0.334       0.230\n",
      "292             0.0234      0.130      0.180      0.857      -0.232       0.279\n",
      "296            -0.0274      0.128     -0.214      0.831      -0.279       0.224\n",
      "363            -0.0416      0.090     -0.463      0.644      -0.218       0.135\n",
      "368             0.0220      0.117      0.188      0.851      -0.208       0.252\n",
      "394             0.1489      0.146      1.021      0.307      -0.137       0.435\n",
      "417            -0.2871      0.149     -1.932      0.053      -0.578       0.004\n",
      "424             0.2197      0.123      1.783      0.075      -0.022       0.461\n",
      "431            -0.2843      0.169     -1.685      0.092      -0.615       0.046\n",
      "434             0.1130      0.118      0.961      0.337      -0.118       0.344\n",
      "438             0.2994      0.114      2.633      0.008       0.077       0.522\n",
      "470             0.2885      0.205      1.410      0.158      -0.112       0.689\n",
      "489            -0.2537      0.122     -2.088      0.037      -0.492      -0.016\n",
      "512             0.2553      0.112      2.276      0.023       0.035       0.475\n",
      "524            -0.2987      0.243     -1.228      0.219      -0.775       0.178\n",
      "547             0.0466      0.135      0.345      0.730      -0.218       0.311\n",
      "548             0.2146      0.131      1.642      0.101      -0.042       0.471\n",
      "549            -0.1873      0.128     -1.458      0.145      -0.439       0.064\n",
      "556            -0.0695      0.134     -0.518      0.604      -0.332       0.193\n",
      "562             0.1313      0.113      1.162      0.245      -0.090       0.353\n",
      "563            -0.3111      0.130     -2.398      0.016      -0.565      -0.057\n",
      "564            -0.2471      0.155     -1.591      0.112      -0.552       0.057\n",
      "576            -0.4725      0.217     -2.173      0.030      -0.899      -0.046\n",
      "582            -0.1255      0.154     -0.815      0.415      -0.427       0.176\n",
      "588             0.1534      0.120      1.274      0.203      -0.083       0.389\n",
      "missing_113     0.0955      0.172      0.555      0.579      -0.242       0.433\n",
      "===============================================================================\n",
      "Removing feature 292 with p-value 0.857225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179071\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1504\n",
      "Method:                           MLE   Df Model:                           62\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2665\n",
      "Time:                        16:31:35   Log-Likelihood:                -280.60\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.956e-17\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.6496     13.631     -1.075      0.282     -41.366      12.067\n",
      "11              0.0568      0.127      0.447      0.655      -0.192       0.306\n",
      "22              0.6616      0.193      3.421      0.001       0.283       1.041\n",
      "30             -0.0694      0.132     -0.525      0.599      -0.328       0.189\n",
      "34              0.1402      0.088      1.590      0.112      -0.033       0.313\n",
      "49              0.0460      0.148      0.312      0.755      -0.243       0.335\n",
      "60              0.9564      0.205      4.655      0.000       0.554       1.359\n",
      "65              0.5144      0.146      3.519      0.000       0.228       0.801\n",
      "68             -0.3285      0.189     -1.740      0.082      -0.699       0.041\n",
      "73              0.0768      0.222      0.346      0.729      -0.358       0.511\n",
      "74             -0.7791      0.431     -1.806      0.071      -1.625       0.066\n",
      "79              0.2976      0.155      1.915      0.055      -0.007       0.602\n",
      "80             -0.1188      0.136     -0.873      0.383      -0.386       0.148\n",
      "82              0.1897      0.118      1.609      0.108      -0.041       0.421\n",
      "85             -0.1221      0.122     -0.997      0.319      -0.362       0.118\n",
      "89              0.0601      0.127      0.473      0.636      -0.189       0.309\n",
      "92             -0.1221      0.157     -0.779      0.436      -0.429       0.185\n",
      "93             -0.0448      0.131     -0.343      0.732      -0.301       0.211\n",
      "103            -0.2468      0.146     -1.688      0.091      -0.533       0.040\n",
      "104             0.2242      0.128      1.747      0.081      -0.027       0.476\n",
      "113             0.2411      0.217      1.111      0.267      -0.184       0.667\n",
      "116            -0.1665      0.119     -1.397      0.162      -0.400       0.067\n",
      "121            -0.1125      0.121     -0.932      0.351      -0.349       0.124\n",
      "125             0.2793      0.159      1.759      0.079      -0.032       0.591\n",
      "126             0.0786      0.190      0.413      0.679      -0.294       0.451\n",
      "128            -0.0906      0.176     -0.515      0.607      -0.436       0.254\n",
      "130             0.5227      0.178      2.943      0.003       0.175       0.871\n",
      "134             0.1743      0.163      1.073      0.283      -0.144       0.493\n",
      "139             0.2009      0.128      1.567      0.117      -0.050       0.452\n",
      "141          -214.2479    266.388     -0.804      0.421    -736.359     307.863\n",
      "147            -0.2986      0.138     -2.169      0.030      -0.568      -0.029\n",
      "151            -0.2508      0.133     -1.884      0.060      -0.512       0.010\n",
      "173             0.2485      0.129      1.934      0.053      -0.003       0.500\n",
      "189             0.1265      0.130      0.970      0.332      -0.129       0.382\n",
      "215            -0.1478      0.157     -0.942      0.346      -0.455       0.160\n",
      "219            -0.1187      0.121     -0.979      0.328      -0.356       0.119\n",
      "239             0.0312      0.118      0.264      0.791      -0.200       0.262\n",
      "240             0.1852      0.129      1.438      0.151      -0.067       0.438\n",
      "280            -0.0533      0.144     -0.371      0.711      -0.335       0.229\n",
      "296            -0.0288      0.128     -0.225      0.822      -0.280       0.223\n",
      "363            -0.0419      0.090     -0.465      0.642      -0.218       0.134\n",
      "368             0.0226      0.117      0.193      0.847      -0.207       0.252\n",
      "394             0.1496      0.146      1.026      0.305      -0.136       0.435\n",
      "417            -0.2876      0.149     -1.936      0.053      -0.579       0.004\n",
      "424             0.2194      0.123      1.781      0.075      -0.022       0.461\n",
      "431            -0.2842      0.169     -1.684      0.092      -0.615       0.047\n",
      "434             0.1140      0.118      0.970      0.332      -0.116       0.344\n",
      "438             0.2992      0.114      2.634      0.008       0.077       0.522\n",
      "470             0.2886      0.204      1.411      0.158      -0.112       0.689\n",
      "489            -0.2553      0.121     -2.106      0.035      -0.493      -0.018\n",
      "512             0.2553      0.112      2.276      0.023       0.035       0.475\n",
      "524            -0.2986      0.243     -1.226      0.220      -0.776       0.179\n",
      "547             0.0471      0.135      0.349      0.727      -0.217       0.312\n",
      "548             0.2141      0.131      1.638      0.101      -0.042       0.470\n",
      "549            -0.1877      0.128     -1.462      0.144      -0.439       0.064\n",
      "556            -0.0702      0.134     -0.523      0.601      -0.333       0.193\n",
      "562             0.1304      0.113      1.156      0.248      -0.091       0.352\n",
      "563            -0.3116      0.130     -2.403      0.016      -0.566      -0.057\n",
      "564            -0.2453      0.155     -1.584      0.113      -0.549       0.058\n",
      "576            -0.4724      0.218     -2.168      0.030      -0.899      -0.045\n",
      "582            -0.1260      0.154     -0.818      0.413      -0.428       0.176\n",
      "588             0.1535      0.120      1.275      0.202      -0.082       0.389\n",
      "missing_113     0.0964      0.172      0.561      0.575      -0.241       0.433\n",
      "===============================================================================\n",
      "Removing feature 368 with p-value 0.846821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179083\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1505\n",
      "Method:                           MLE   Df Model:                           61\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2665\n",
      "Time:                        16:31:35   Log-Likelihood:                -280.62\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.716e-17\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.7291     13.618     -1.082      0.279     -41.420      11.962\n",
      "11              0.0583      0.127      0.460      0.646      -0.190       0.307\n",
      "22              0.6602      0.193      3.417      0.001       0.282       1.039\n",
      "30             -0.0700      0.132     -0.531      0.595      -0.329       0.188\n",
      "34              0.1406      0.088      1.597      0.110      -0.032       0.313\n",
      "49              0.0462      0.148      0.313      0.754      -0.243       0.335\n",
      "60              0.9513      0.204      4.670      0.000       0.552       1.350\n",
      "65              0.5111      0.145      3.521      0.000       0.227       0.796\n",
      "68             -0.3253      0.188     -1.731      0.083      -0.694       0.043\n",
      "73              0.0797      0.221      0.360      0.719      -0.354       0.513\n",
      "74             -0.7760      0.431     -1.802      0.072      -1.620       0.068\n",
      "79              0.2952      0.155      1.907      0.057      -0.008       0.599\n",
      "80             -0.1190      0.136     -0.875      0.382      -0.386       0.148\n",
      "82              0.1893      0.118      1.607      0.108      -0.042       0.420\n",
      "85             -0.1221      0.122     -0.998      0.318      -0.362       0.118\n",
      "89              0.0608      0.127      0.478      0.632      -0.188       0.310\n",
      "92             -0.1238      0.157     -0.790      0.430      -0.431       0.184\n",
      "93             -0.0445      0.131     -0.340      0.734      -0.301       0.212\n",
      "103            -0.2487      0.146     -1.697      0.090      -0.536       0.038\n",
      "104             0.2311      0.125      1.856      0.063      -0.013       0.475\n",
      "113             0.2411      0.217      1.110      0.267      -0.184       0.667\n",
      "116            -0.1667      0.119     -1.399      0.162      -0.400       0.067\n",
      "121            -0.1113      0.120     -0.924      0.356      -0.347       0.125\n",
      "125             0.2794      0.159      1.762      0.078      -0.031       0.590\n",
      "126             0.0797      0.190      0.420      0.675      -0.292       0.452\n",
      "128            -0.0928      0.176     -0.529      0.597      -0.437       0.251\n",
      "130             0.5198      0.177      2.937      0.003       0.173       0.867\n",
      "134             0.1741      0.162      1.073      0.283      -0.144       0.492\n",
      "139             0.1994      0.128      1.559      0.119      -0.051       0.450\n",
      "141          -215.8086    266.134     -0.811      0.417    -737.422     305.805\n",
      "147            -0.2981      0.138     -2.165      0.030      -0.568      -0.028\n",
      "151            -0.2503      0.133     -1.881      0.060      -0.511       0.010\n",
      "173             0.2502      0.128      1.953      0.051      -0.001       0.501\n",
      "189             0.1270      0.130      0.974      0.330      -0.128       0.382\n",
      "215            -0.1469      0.156     -0.939      0.348      -0.454       0.160\n",
      "219            -0.1193      0.121     -0.984      0.325      -0.357       0.118\n",
      "239             0.0316      0.118      0.268      0.788      -0.199       0.263\n",
      "240             0.1850      0.129      1.438      0.151      -0.067       0.437\n",
      "280            -0.0543      0.144     -0.378      0.706      -0.336       0.227\n",
      "296            -0.0289      0.128     -0.225      0.822      -0.280       0.222\n",
      "363            -0.0425      0.090     -0.473      0.636      -0.219       0.134\n",
      "394             0.1519      0.145      1.046      0.296      -0.133       0.437\n",
      "417            -0.2885      0.148     -1.943      0.052      -0.580       0.003\n",
      "424             0.2188      0.123      1.777      0.076      -0.023       0.460\n",
      "431            -0.2832      0.169     -1.679      0.093      -0.614       0.047\n",
      "434             0.1126      0.117      0.959      0.337      -0.117       0.342\n",
      "438             0.2997      0.114      2.636      0.008       0.077       0.522\n",
      "470             0.2869      0.204      1.407      0.160      -0.113       0.687\n",
      "489            -0.2567      0.121     -2.121      0.034      -0.494      -0.019\n",
      "512             0.2559      0.112      2.283      0.022       0.036       0.476\n",
      "524            -0.2970      0.243     -1.220      0.222      -0.774       0.180\n",
      "547             0.0472      0.135      0.349      0.727      -0.217       0.312\n",
      "548             0.2135      0.131      1.635      0.102      -0.042       0.469\n",
      "549            -0.1884      0.128     -1.468      0.142      -0.440       0.063\n",
      "556            -0.0708      0.134     -0.528      0.598      -0.334       0.192\n",
      "562             0.1306      0.113      1.157      0.247      -0.091       0.352\n",
      "563            -0.3099      0.129     -2.396      0.017      -0.563      -0.056\n",
      "564            -0.2466      0.155     -1.592      0.111      -0.550       0.057\n",
      "576            -0.4691      0.217     -2.162      0.031      -0.894      -0.044\n",
      "582            -0.1262      0.154     -0.820      0.412      -0.428       0.175\n",
      "588             0.1534      0.120      1.274      0.203      -0.083       0.389\n",
      "missing_113     0.0943      0.172      0.550      0.582      -0.242       0.431\n",
      "===============================================================================\n",
      "Removing feature 296 with p-value 0.821909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179099\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1506\n",
      "Method:                           MLE   Df Model:                           60\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2664\n",
      "Time:                        16:31:35   Log-Likelihood:                -280.65\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.485e-17\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.4802     13.533     -1.070      0.285     -41.004      12.044\n",
      "11              0.0590      0.127      0.465      0.642      -0.189       0.307\n",
      "22              0.6577      0.193      3.413      0.001       0.280       1.035\n",
      "30             -0.0684      0.132     -0.520      0.603      -0.326       0.190\n",
      "34              0.1412      0.088      1.605      0.109      -0.031       0.314\n",
      "49              0.0472      0.147      0.320      0.749      -0.242       0.336\n",
      "60              0.9504      0.203      4.672      0.000       0.552       1.349\n",
      "65              0.5113      0.145      3.524      0.000       0.227       0.796\n",
      "68             -0.3244      0.190     -1.710      0.087      -0.696       0.047\n",
      "73              0.0779      0.221      0.353      0.724      -0.355       0.511\n",
      "74             -0.7773      0.432     -1.800      0.072      -1.624       0.069\n",
      "79              0.2981      0.154      1.932      0.053      -0.004       0.600\n",
      "80             -0.1204      0.136     -0.885      0.376      -0.387       0.146\n",
      "82              0.1890      0.118      1.604      0.109      -0.042       0.420\n",
      "85             -0.1229      0.122     -1.005      0.315      -0.363       0.117\n",
      "89              0.0609      0.127      0.480      0.632      -0.188       0.310\n",
      "92             -0.1246      0.157     -0.794      0.427      -0.432       0.183\n",
      "93             -0.0428      0.130     -0.328      0.743      -0.298       0.213\n",
      "103            -0.2500      0.146     -1.707      0.088      -0.537       0.037\n",
      "104             0.2307      0.124      1.854      0.064      -0.013       0.475\n",
      "113             0.2363      0.216      1.094      0.274      -0.187       0.660\n",
      "116            -0.1668      0.119     -1.401      0.161      -0.400       0.066\n",
      "121            -0.1109      0.120     -0.921      0.357      -0.347       0.125\n",
      "125             0.2777      0.159      1.752      0.080      -0.033       0.588\n",
      "126             0.0803      0.190      0.423      0.672      -0.292       0.452\n",
      "128            -0.0938      0.176     -0.534      0.593      -0.438       0.250\n",
      "130             0.5190      0.177      2.932      0.003       0.172       0.866\n",
      "134             0.1751      0.162      1.079      0.280      -0.143       0.493\n",
      "139             0.1994      0.128      1.560      0.119      -0.051       0.450\n",
      "141          -210.9493    264.474     -0.798      0.425    -729.309     307.410\n",
      "147            -0.2976      0.138     -2.162      0.031      -0.567      -0.028\n",
      "151            -0.2508      0.133     -1.886      0.059      -0.511       0.010\n",
      "173             0.2509      0.128      1.959      0.050      -0.000       0.502\n",
      "189             0.1268      0.130      0.973      0.330      -0.129       0.382\n",
      "215            -0.1458      0.156     -0.933      0.351      -0.452       0.160\n",
      "219            -0.1183      0.121     -0.977      0.329      -0.356       0.119\n",
      "239             0.0322      0.118      0.274      0.784      -0.199       0.263\n",
      "240             0.1855      0.129      1.441      0.150      -0.067       0.438\n",
      "280            -0.0543      0.144     -0.378      0.706      -0.336       0.228\n",
      "363            -0.0415      0.090     -0.463      0.643      -0.217       0.134\n",
      "394             0.1519      0.145      1.046      0.296      -0.133       0.437\n",
      "417            -0.2896      0.148     -1.951      0.051      -0.580       0.001\n",
      "424             0.2190      0.123      1.778      0.075      -0.022       0.460\n",
      "431            -0.2997      0.152     -1.974      0.048      -0.597      -0.002\n",
      "434             0.1136      0.117      0.969      0.333      -0.116       0.344\n",
      "438             0.2984      0.113      2.645      0.008       0.077       0.520\n",
      "470             0.2858      0.207      1.382      0.167      -0.119       0.691\n",
      "489            -0.2571      0.121     -2.125      0.034      -0.494      -0.020\n",
      "512             0.2554      0.112      2.279      0.023       0.036       0.475\n",
      "524            -0.2944      0.243     -1.211      0.226      -0.771       0.182\n",
      "547             0.0483      0.135      0.358      0.720      -0.216       0.313\n",
      "548             0.2120      0.130      1.626      0.104      -0.044       0.468\n",
      "549            -0.1863      0.128     -1.456      0.145      -0.437       0.064\n",
      "556            -0.0699      0.134     -0.522      0.602      -0.333       0.193\n",
      "562             0.1313      0.113      1.166      0.244      -0.089       0.352\n",
      "563            -0.3103      0.129     -2.400      0.016      -0.564      -0.057\n",
      "564            -0.2473      0.155     -1.596      0.110      -0.551       0.056\n",
      "576            -0.4697      0.216     -2.171      0.030      -0.894      -0.046\n",
      "582            -0.1266      0.154     -0.822      0.411      -0.428       0.175\n",
      "588             0.1534      0.120      1.273      0.203      -0.083       0.389\n",
      "missing_113     0.0943      0.171      0.550      0.582      -0.242       0.430\n",
      "===============================================================================\n",
      "Removing feature 239 with p-value 0.784422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179123\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1507\n",
      "Method:                           MLE   Df Model:                           59\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2663\n",
      "Time:                        16:31:36   Log-Likelihood:                -280.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.116e-18\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.6920     13.543     -1.085      0.278     -41.235      11.851\n",
      "11              0.0615      0.126      0.486      0.627      -0.186       0.309\n",
      "22              0.6601      0.193      3.426      0.001       0.282       1.038\n",
      "30             -0.0676      0.132     -0.514      0.607      -0.325       0.190\n",
      "34              0.1411      0.088      1.604      0.109      -0.031       0.313\n",
      "49              0.0462      0.147      0.314      0.754      -0.243       0.335\n",
      "60              0.9524      0.203      4.686      0.000       0.554       1.351\n",
      "65              0.5139      0.145      3.547      0.000       0.230       0.798\n",
      "68             -0.3239      0.190     -1.707      0.088      -0.696       0.048\n",
      "73              0.0775      0.221      0.351      0.726      -0.355       0.510\n",
      "74             -0.7731      0.432     -1.791      0.073      -1.619       0.073\n",
      "79              0.2969      0.154      1.927      0.054      -0.005       0.599\n",
      "80             -0.1231      0.136     -0.907      0.364      -0.389       0.143\n",
      "82              0.1883      0.118      1.598      0.110      -0.043       0.419\n",
      "85             -0.1236      0.122     -1.010      0.313      -0.363       0.116\n",
      "89              0.0631      0.127      0.498      0.619      -0.185       0.312\n",
      "92             -0.1226      0.157     -0.782      0.434      -0.430       0.185\n",
      "93             -0.0421      0.131     -0.323      0.747      -0.298       0.214\n",
      "103            -0.2488      0.147     -1.696      0.090      -0.536       0.039\n",
      "104             0.2296      0.124      1.844      0.065      -0.014       0.474\n",
      "113             0.2351      0.216      1.089      0.276      -0.188       0.658\n",
      "116            -0.1697      0.119     -1.431      0.152      -0.402       0.063\n",
      "121            -0.1133      0.121     -0.939      0.348      -0.350       0.123\n",
      "125             0.2834      0.157      1.804      0.071      -0.024       0.591\n",
      "126             0.0792      0.190      0.418      0.676      -0.292       0.451\n",
      "128            -0.0968      0.175     -0.553      0.580      -0.440       0.246\n",
      "130             0.5186      0.177      2.929      0.003       0.172       0.866\n",
      "134             0.1718      0.162      1.061      0.289      -0.146       0.489\n",
      "139             0.2003      0.128      1.567      0.117      -0.050       0.451\n",
      "141          -215.0810    264.662     -0.813      0.416    -733.809     303.647\n",
      "147            -0.2983      0.138     -2.169      0.030      -0.568      -0.029\n",
      "151            -0.2533      0.133     -1.909      0.056      -0.513       0.007\n",
      "173             0.2484      0.128      1.946      0.052      -0.002       0.499\n",
      "189             0.1271      0.130      0.975      0.329      -0.128       0.382\n",
      "215            -0.1454      0.156     -0.932      0.351      -0.451       0.160\n",
      "219            -0.1195      0.121     -0.987      0.324      -0.357       0.118\n",
      "240             0.1935      0.127      1.529      0.126      -0.054       0.441\n",
      "280            -0.0549      0.144     -0.381      0.703      -0.337       0.227\n",
      "363            -0.0395      0.089     -0.443      0.658      -0.215       0.136\n",
      "394             0.1561      0.144      1.081      0.280      -0.127       0.439\n",
      "417            -0.2885      0.148     -1.944      0.052      -0.579       0.002\n",
      "424             0.2208      0.123      1.795      0.073      -0.020       0.462\n",
      "431            -0.3001      0.152     -1.976      0.048      -0.598      -0.002\n",
      "434             0.1140      0.117      0.972      0.331      -0.116       0.344\n",
      "438             0.2976      0.112      2.656      0.008       0.078       0.517\n",
      "470             0.2823      0.206      1.368      0.171      -0.122       0.687\n",
      "489            -0.2565      0.121     -2.120      0.034      -0.494      -0.019\n",
      "512             0.2564      0.112      2.290      0.022       0.037       0.476\n",
      "524            -0.2942      0.244     -1.207      0.227      -0.772       0.183\n",
      "547             0.0493      0.135      0.366      0.714      -0.214       0.313\n",
      "548             0.2109      0.130      1.617      0.106      -0.045       0.467\n",
      "549            -0.1885      0.128     -1.476      0.140      -0.439       0.062\n",
      "556            -0.0725      0.134     -0.543      0.587      -0.335       0.189\n",
      "562             0.1327      0.113      1.179      0.239      -0.088       0.353\n",
      "563            -0.3093      0.129     -2.393      0.017      -0.563      -0.056\n",
      "564            -0.2464      0.155     -1.591      0.112      -0.550       0.057\n",
      "576            -0.4713      0.216     -2.183      0.029      -0.894      -0.048\n",
      "582            -0.1281      0.154     -0.832      0.405      -0.430       0.174\n",
      "588             0.1527      0.121      1.266      0.206      -0.084       0.389\n",
      "missing_113     0.0935      0.171      0.546      0.585      -0.242       0.429\n",
      "===============================================================================\n",
      "Removing feature 49 with p-value 0.753792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179155\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1508\n",
      "Method:                           MLE   Df Model:                           58\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2662\n",
      "Time:                        16:31:36   Log-Likelihood:                -280.74\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.439e-18\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.3768     13.481     -1.066      0.286     -40.800      12.046\n",
      "11              0.0594      0.126      0.470      0.638      -0.188       0.307\n",
      "22              0.6664      0.192      3.477      0.001       0.291       1.042\n",
      "30             -0.0692      0.131     -0.526      0.599      -0.327       0.188\n",
      "34              0.1436      0.088      1.635      0.102      -0.029       0.316\n",
      "60              0.9455      0.202      4.686      0.000       0.550       1.341\n",
      "65              0.4998      0.138      3.629      0.000       0.230       0.770\n",
      "68             -0.3226      0.190     -1.701      0.089      -0.694       0.049\n",
      "73              0.0528      0.206      0.256      0.798      -0.352       0.457\n",
      "74             -0.7957      0.425     -1.871      0.061      -1.630       0.038\n",
      "79              0.2959      0.154      1.920      0.055      -0.006       0.598\n",
      "80             -0.1249      0.135     -0.922      0.357      -0.390       0.141\n",
      "82              0.1880      0.118      1.596      0.110      -0.043       0.419\n",
      "85             -0.1223      0.122     -1.000      0.317      -0.362       0.117\n",
      "89              0.0646      0.127      0.509      0.610      -0.184       0.313\n",
      "92             -0.1205      0.156     -0.772      0.440      -0.426       0.186\n",
      "93             -0.0425      0.130     -0.326      0.744      -0.298       0.213\n",
      "103            -0.2462      0.146     -1.689      0.091      -0.532       0.040\n",
      "104             0.2290      0.124      1.842      0.066      -0.015       0.473\n",
      "113             0.2291      0.215      1.066      0.286      -0.192       0.650\n",
      "116            -0.1738      0.118     -1.475      0.140      -0.405       0.057\n",
      "121            -0.1046      0.117     -0.894      0.371      -0.334       0.125\n",
      "125             0.2771      0.156      1.778      0.075      -0.028       0.583\n",
      "126             0.0764      0.189      0.404      0.686      -0.295       0.448\n",
      "128            -0.0928      0.174     -0.532      0.595      -0.435       0.249\n",
      "130             0.5174      0.177      2.924      0.003       0.171       0.864\n",
      "134             0.1753      0.162      1.085      0.278      -0.141       0.492\n",
      "139             0.2003      0.128      1.569      0.117      -0.050       0.451\n",
      "141          -208.9690    263.489     -0.793      0.428    -725.398     307.460\n",
      "147            -0.2975      0.138     -2.162      0.031      -0.567      -0.028\n",
      "151            -0.2527      0.133     -1.903      0.057      -0.513       0.008\n",
      "173             0.2491      0.128      1.953      0.051      -0.001       0.499\n",
      "189             0.1445      0.118      1.228      0.219      -0.086       0.375\n",
      "215            -0.1451      0.156     -0.933      0.351      -0.450       0.160\n",
      "219            -0.1188      0.121     -0.982      0.326      -0.356       0.118\n",
      "240             0.1908      0.126      1.519      0.129      -0.055       0.437\n",
      "280            -0.0566      0.144     -0.394      0.694      -0.339       0.225\n",
      "363            -0.0405      0.089     -0.453      0.650      -0.215       0.135\n",
      "394             0.1618      0.143      1.130      0.259      -0.119       0.443\n",
      "417            -0.2878      0.148     -1.939      0.053      -0.579       0.003\n",
      "424             0.2240      0.122      1.832      0.067      -0.016       0.464\n",
      "431            -0.3035      0.152     -2.003      0.045      -0.600      -0.007\n",
      "434             0.1117      0.117      0.954      0.340      -0.118       0.341\n",
      "438             0.2979      0.113      2.646      0.008       0.077       0.518\n",
      "470             0.2880      0.206      1.400      0.162      -0.115       0.691\n",
      "489            -0.2556      0.121     -2.114      0.035      -0.493      -0.019\n",
      "512             0.2564      0.112      2.291      0.022       0.037       0.476\n",
      "524            -0.2867      0.242     -1.185      0.236      -0.761       0.188\n",
      "547             0.0505      0.135      0.375      0.708      -0.214       0.314\n",
      "548             0.2116      0.131      1.620      0.105      -0.044       0.468\n",
      "549            -0.1875      0.128     -1.468      0.142      -0.438       0.063\n",
      "556            -0.0757      0.133     -0.568      0.570      -0.337       0.186\n",
      "562             0.1336      0.113      1.187      0.235      -0.087       0.354\n",
      "563            -0.3093      0.129     -2.392      0.017      -0.563      -0.056\n",
      "564            -0.2448      0.155     -1.583      0.113      -0.548       0.058\n",
      "576            -0.4665      0.215     -2.170      0.030      -0.888      -0.045\n",
      "582            -0.1242      0.153     -0.811      0.417      -0.424       0.176\n",
      "588             0.1494      0.120      1.243      0.214      -0.086       0.385\n",
      "missing_113     0.0901      0.171      0.527      0.598      -0.245       0.425\n",
      "===============================================================================\n",
      "Removing feature 73 with p-value 0.798182\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179175\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1509\n",
      "Method:                           MLE   Df Model:                           57\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2661\n",
      "Time:                        16:31:36   Log-Likelihood:                -280.77\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.379e-18\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.4751     13.473     -1.074      0.283     -40.881      11.931\n",
      "11              0.0603      0.126      0.478      0.633      -0.187       0.308\n",
      "22              0.6690      0.192      3.491      0.000       0.293       1.045\n",
      "30             -0.0654      0.131     -0.501      0.617      -0.322       0.191\n",
      "34              0.1429      0.088      1.628      0.104      -0.029       0.315\n",
      "60              0.9422      0.201      4.683      0.000       0.548       1.337\n",
      "65              0.4926      0.135      3.658      0.000       0.229       0.756\n",
      "68             -0.3244      0.189     -1.714      0.087      -0.695       0.047\n",
      "74             -0.8325      0.398     -2.089      0.037      -1.613      -0.052\n",
      "79              0.2955      0.154      1.917      0.055      -0.007       0.598\n",
      "80             -0.1211      0.135     -0.900      0.368      -0.385       0.143\n",
      "82              0.1874      0.118      1.592      0.111      -0.043       0.418\n",
      "85             -0.1210      0.122     -0.991      0.322      -0.360       0.118\n",
      "89              0.0635      0.127      0.501      0.616      -0.185       0.312\n",
      "92             -0.1201      0.156     -0.769      0.442      -0.426       0.186\n",
      "93             -0.0420      0.131     -0.322      0.747      -0.298       0.214\n",
      "103            -0.2444      0.146     -1.679      0.093      -0.530       0.041\n",
      "104             0.2274      0.124      1.829      0.067      -0.016       0.471\n",
      "113             0.2292      0.215      1.068      0.285      -0.191       0.650\n",
      "116            -0.1751      0.118     -1.488      0.137      -0.406       0.056\n",
      "121            -0.1037      0.117     -0.887      0.375      -0.333       0.125\n",
      "125             0.2814      0.155      1.815      0.070      -0.023       0.585\n",
      "126             0.0738      0.189      0.390      0.696      -0.297       0.444\n",
      "128            -0.0941      0.174     -0.539      0.590      -0.436       0.248\n",
      "130             0.5147      0.177      2.913      0.004       0.168       0.861\n",
      "134             0.1723      0.161      1.068      0.285      -0.144       0.488\n",
      "139             0.2014      0.128      1.579      0.114      -0.049       0.451\n",
      "141          -210.9137    263.311     -0.801      0.423    -726.994     305.167\n",
      "147            -0.2979      0.137     -2.167      0.030      -0.567      -0.028\n",
      "151            -0.2535      0.133     -1.910      0.056      -0.514       0.007\n",
      "173             0.2498      0.127      1.960      0.050    1.35e-05       0.500\n",
      "189             0.1439      0.118      1.224      0.221      -0.087       0.374\n",
      "215            -0.1446      0.155     -0.931      0.352      -0.449       0.160\n",
      "219            -0.1184      0.121     -0.978      0.328      -0.356       0.119\n",
      "240             0.1896      0.126      1.511      0.131      -0.056       0.436\n",
      "280            -0.0554      0.144     -0.386      0.699      -0.337       0.226\n",
      "363            -0.0386      0.089     -0.434      0.664      -0.213       0.136\n",
      "394             0.1599      0.143      1.119      0.263      -0.120       0.440\n",
      "417            -0.2897      0.148     -1.953      0.051      -0.580       0.001\n",
      "424             0.2224      0.122      1.820      0.069      -0.017       0.462\n",
      "431            -0.3074      0.151     -2.039      0.041      -0.603      -0.012\n",
      "434             0.1126      0.117      0.961      0.337      -0.117       0.342\n",
      "438             0.2967      0.112      2.645      0.008       0.077       0.517\n",
      "470             0.2932      0.205      1.430      0.153      -0.109       0.695\n",
      "489            -0.2556      0.121     -2.114      0.035      -0.493      -0.019\n",
      "512             0.2549      0.112      2.282      0.023       0.036       0.474\n",
      "524            -0.2758      0.238     -1.160      0.246      -0.742       0.190\n",
      "547             0.0514      0.135      0.382      0.703      -0.213       0.315\n",
      "548             0.2114      0.130      1.620      0.105      -0.044       0.467\n",
      "549            -0.1902      0.127     -1.495      0.135      -0.440       0.059\n",
      "556            -0.0751      0.133     -0.562      0.574      -0.337       0.187\n",
      "562             0.1323      0.112      1.177      0.239      -0.088       0.353\n",
      "563            -0.3096      0.129     -2.394      0.017      -0.563      -0.056\n",
      "564            -0.2460      0.155     -1.591      0.112      -0.549       0.057\n",
      "576            -0.4663      0.215     -2.173      0.030      -0.887      -0.046\n",
      "582            -0.1223      0.153     -0.800      0.424      -0.422       0.177\n",
      "588             0.1491      0.120      1.242      0.214      -0.086       0.384\n",
      "missing_113     0.0895      0.171      0.524      0.600      -0.246       0.425\n",
      "===============================================================================\n",
      "Removing feature 93 with p-value 0.747344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179209\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1510\n",
      "Method:                           MLE   Df Model:                           56\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2660\n",
      "Time:                        16:31:37   Log-Likelihood:                -280.82\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.283e-18\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.3740     13.493     -1.065      0.287     -40.820      12.072\n",
      "11              0.0570      0.126      0.453      0.650      -0.190       0.304\n",
      "22              0.6708      0.192      3.502      0.000       0.295       1.046\n",
      "30             -0.0644      0.131     -0.492      0.622      -0.321       0.192\n",
      "34              0.1410      0.088      1.610      0.107      -0.031       0.313\n",
      "60              0.9483      0.201      4.730      0.000       0.555       1.341\n",
      "65              0.4945      0.135      3.671      0.000       0.231       0.759\n",
      "68             -0.3245      0.190     -1.712      0.087      -0.696       0.047\n",
      "74             -0.8376      0.399     -2.101      0.036      -1.619      -0.056\n",
      "79              0.2986      0.154      1.939      0.052      -0.003       0.600\n",
      "80             -0.1231      0.134     -0.916      0.360      -0.387       0.140\n",
      "82              0.1877      0.118      1.595      0.111      -0.043       0.418\n",
      "85             -0.1228      0.122     -1.008      0.314      -0.362       0.116\n",
      "89              0.0618      0.126      0.489      0.625      -0.186       0.310\n",
      "92             -0.1208      0.156     -0.772      0.440      -0.427       0.186\n",
      "103            -0.2464      0.146     -1.687      0.092      -0.533       0.040\n",
      "104             0.2243      0.124      1.806      0.071      -0.019       0.468\n",
      "113             0.2261      0.214      1.054      0.292      -0.194       0.646\n",
      "116            -0.1740      0.118     -1.478      0.139      -0.405       0.057\n",
      "121            -0.1044      0.117     -0.894      0.372      -0.333       0.125\n",
      "125             0.2794      0.155      1.803      0.071      -0.024       0.583\n",
      "126             0.0728      0.189      0.386      0.700      -0.297       0.443\n",
      "128            -0.0960      0.174     -0.551      0.582      -0.438       0.246\n",
      "130             0.5157      0.177      2.918      0.004       0.169       0.862\n",
      "134             0.1730      0.161      1.073      0.283      -0.143       0.489\n",
      "139             0.2003      0.128      1.569      0.117      -0.050       0.450\n",
      "141          -208.9540    263.715     -0.792      0.428    -725.826     307.918\n",
      "147            -0.2976      0.137     -2.169      0.030      -0.566      -0.029\n",
      "151            -0.2505      0.132     -1.892      0.058      -0.510       0.009\n",
      "173             0.2480      0.127      1.947      0.051      -0.002       0.498\n",
      "189             0.1436      0.118      1.222      0.222      -0.087       0.374\n",
      "215            -0.1421      0.155     -0.920      0.358      -0.445       0.161\n",
      "219            -0.1162      0.121     -0.961      0.337      -0.353       0.121\n",
      "240             0.1885      0.125      1.504      0.133      -0.057       0.434\n",
      "280            -0.0545      0.144     -0.380      0.704      -0.336       0.227\n",
      "363            -0.0411      0.089     -0.464      0.643      -0.215       0.133\n",
      "394             0.1621      0.143      1.134      0.257      -0.118       0.442\n",
      "417            -0.2947      0.148     -1.994      0.046      -0.584      -0.005\n",
      "424             0.2229      0.122      1.821      0.069      -0.017       0.463\n",
      "431            -0.3080      0.151     -2.043      0.041      -0.603      -0.013\n",
      "434             0.1122      0.117      0.957      0.339      -0.118       0.342\n",
      "438             0.2966      0.113      2.632      0.008       0.076       0.517\n",
      "470             0.2916      0.205      1.421      0.155      -0.110       0.694\n",
      "489            -0.2560      0.121     -2.115      0.034      -0.493      -0.019\n",
      "512             0.2563      0.112      2.297      0.022       0.038       0.475\n",
      "524            -0.2767      0.238     -1.162      0.245      -0.743       0.190\n",
      "547             0.0474      0.133      0.355      0.723      -0.214       0.309\n",
      "548             0.2117      0.131      1.621      0.105      -0.044       0.468\n",
      "549            -0.1875      0.127     -1.475      0.140      -0.437       0.062\n",
      "556            -0.0753      0.133     -0.564      0.573      -0.337       0.186\n",
      "562             0.1321      0.112      1.176      0.240      -0.088       0.352\n",
      "563            -0.3107      0.129     -2.406      0.016      -0.564      -0.058\n",
      "564            -0.2458      0.155     -1.588      0.112      -0.549       0.057\n",
      "576            -0.4658      0.214     -2.172      0.030      -0.886      -0.045\n",
      "582            -0.1201      0.153     -0.787      0.431      -0.419       0.179\n",
      "588             0.1473      0.120      1.229      0.219      -0.088       0.382\n",
      "missing_113     0.0944      0.170      0.554      0.580      -0.240       0.429\n",
      "===============================================================================\n",
      "Removing feature 547 with p-value 0.722592\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179249\n",
      "         Iterations 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1511\n",
      "Method:                           MLE   Df Model:                           55\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2658\n",
      "Time:                        16:31:37   Log-Likelihood:                -280.88\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.907e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.0430     13.461     -1.043      0.297     -40.427      12.341\n",
      "11              0.0552      0.126      0.439      0.661      -0.191       0.302\n",
      "22              0.6678      0.191      3.488      0.000       0.293       1.043\n",
      "30             -0.0635      0.131     -0.486      0.627      -0.319       0.193\n",
      "34              0.1409      0.087      1.612      0.107      -0.030       0.312\n",
      "60              0.9357      0.197      4.746      0.000       0.549       1.322\n",
      "65              0.4882      0.133      3.660      0.000       0.227       0.750\n",
      "68             -0.3229      0.189     -1.706      0.088      -0.694       0.048\n",
      "74             -0.8333      0.398     -2.092      0.036      -1.614      -0.053\n",
      "79              0.2987      0.154      1.941      0.052      -0.003       0.600\n",
      "80             -0.1215      0.134     -0.906      0.365      -0.385       0.141\n",
      "82              0.1840      0.117      1.571      0.116      -0.046       0.414\n",
      "85             -0.1265      0.121     -1.042      0.297      -0.364       0.111\n",
      "89              0.0599      0.126      0.474      0.636      -0.188       0.308\n",
      "92             -0.1178      0.156     -0.753      0.451      -0.424       0.189\n",
      "103            -0.2475      0.146     -1.693      0.090      -0.534       0.039\n",
      "104             0.2244      0.124      1.808      0.071      -0.019       0.468\n",
      "113             0.2292      0.214      1.070      0.285      -0.191       0.649\n",
      "116            -0.1764      0.117     -1.503      0.133      -0.406       0.054\n",
      "121            -0.1052      0.117     -0.902      0.367      -0.334       0.123\n",
      "125             0.2811      0.155      1.816      0.069      -0.022       0.584\n",
      "126             0.0743      0.189      0.393      0.694      -0.296       0.444\n",
      "128            -0.0971      0.174     -0.557      0.578      -0.439       0.245\n",
      "130             0.5179      0.177      2.932      0.003       0.172       0.864\n",
      "134             0.1709      0.161      1.061      0.289      -0.145       0.487\n",
      "139             0.1984      0.128      1.556      0.120      -0.052       0.448\n",
      "141          -202.4869    263.100     -0.770      0.442    -718.153     313.180\n",
      "147            -0.2970      0.137     -2.166      0.030      -0.566      -0.028\n",
      "151            -0.2517      0.132     -1.900      0.057      -0.511       0.008\n",
      "173             0.2439      0.127      1.922      0.055      -0.005       0.493\n",
      "189             0.1449      0.117      1.233      0.217      -0.085       0.375\n",
      "215            -0.1384      0.153     -0.903      0.367      -0.439       0.162\n",
      "219            -0.1188      0.121     -0.984      0.325      -0.356       0.118\n",
      "240             0.1917      0.124      1.540      0.123      -0.052       0.436\n",
      "280            -0.0533      0.144     -0.371      0.710      -0.335       0.228\n",
      "363            -0.0384      0.088     -0.436      0.663      -0.211       0.134\n",
      "394             0.1612      0.143      1.129      0.259      -0.119       0.441\n",
      "417            -0.2938      0.148     -1.990      0.047      -0.583      -0.004\n",
      "424             0.2212      0.122      1.809      0.070      -0.018       0.461\n",
      "431            -0.3073      0.151     -2.040      0.041      -0.603      -0.012\n",
      "434             0.1149      0.117      0.982      0.326      -0.115       0.344\n",
      "438             0.2952      0.113      2.622      0.009       0.075       0.516\n",
      "470             0.2932      0.205      1.430      0.153      -0.109       0.695\n",
      "489            -0.2536      0.121     -2.098      0.036      -0.490      -0.017\n",
      "512             0.2581      0.112      2.314      0.021       0.040       0.477\n",
      "524            -0.2718      0.238     -1.141      0.254      -0.739       0.195\n",
      "548             0.2204      0.128      1.717      0.086      -0.031       0.472\n",
      "549            -0.1820      0.126     -1.443      0.149      -0.429       0.065\n",
      "556            -0.0561      0.122     -0.459      0.646      -0.296       0.183\n",
      "562             0.1358      0.112      1.212      0.226      -0.084       0.355\n",
      "563            -0.3116      0.129     -2.416      0.016      -0.564      -0.059\n",
      "564            -0.2471      0.155     -1.598      0.110      -0.550       0.056\n",
      "576            -0.4662      0.214     -2.177      0.030      -0.886      -0.046\n",
      "582            -0.1204      0.153     -0.789      0.430      -0.419       0.179\n",
      "588             0.1477      0.120      1.233      0.217      -0.087       0.382\n",
      "missing_113     0.0918      0.170      0.539      0.590      -0.242       0.425\n",
      "===============================================================================\n",
      "Removing feature 280 with p-value 0.710273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179294\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1512\n",
      "Method:                           MLE   Df Model:                           54\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2656\n",
      "Time:                        16:31:37   Log-Likelihood:                -280.95\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.711e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.9889     13.480     -1.038      0.299     -40.409      12.431\n",
      "11              0.0534      0.126      0.426      0.670      -0.193       0.300\n",
      "22              0.6678      0.191      3.488      0.000       0.292       1.043\n",
      "30             -0.0619      0.131     -0.473      0.636      -0.318       0.194\n",
      "34              0.1413      0.087      1.619      0.105      -0.030       0.312\n",
      "60              0.9371      0.197      4.755      0.000       0.551       1.323\n",
      "65              0.4905      0.133      3.684      0.000       0.230       0.751\n",
      "68             -0.3291      0.188     -1.746      0.081      -0.698       0.040\n",
      "74             -0.8208      0.397     -2.070      0.038      -1.598      -0.044\n",
      "79              0.2993      0.154      1.945      0.052      -0.002       0.601\n",
      "80             -0.1205      0.134     -0.898      0.369      -0.384       0.143\n",
      "82              0.1865      0.117      1.594      0.111      -0.043       0.416\n",
      "85             -0.1294      0.121     -1.068      0.286      -0.367       0.108\n",
      "89              0.0600      0.127      0.474      0.635      -0.188       0.308\n",
      "92             -0.1183      0.156     -0.757      0.449      -0.425       0.188\n",
      "103            -0.2475      0.146     -1.691      0.091      -0.534       0.039\n",
      "104             0.2233      0.124      1.805      0.071      -0.019       0.466\n",
      "113             0.2296      0.214      1.074      0.283      -0.189       0.649\n",
      "116            -0.1771      0.117     -1.509      0.131      -0.407       0.053\n",
      "121            -0.1019      0.116     -0.875      0.381      -0.330       0.126\n",
      "125             0.2813      0.155      1.816      0.069      -0.022       0.585\n",
      "126             0.0743      0.189      0.394      0.694      -0.295       0.444\n",
      "128            -0.1000      0.174     -0.574      0.566      -0.441       0.241\n",
      "130             0.5212      0.176      2.955      0.003       0.175       0.867\n",
      "134             0.1737      0.161      1.078      0.281      -0.142       0.489\n",
      "139             0.1972      0.127      1.549      0.121      -0.052       0.447\n",
      "141          -201.4920    263.464     -0.765      0.444    -717.871     314.887\n",
      "147            -0.2956      0.137     -2.165      0.030      -0.563      -0.028\n",
      "151            -0.2541      0.132     -1.923      0.054      -0.513       0.005\n",
      "173             0.2439      0.127      1.922      0.055      -0.005       0.493\n",
      "189             0.1478      0.117      1.261      0.207      -0.082       0.378\n",
      "215            -0.1391      0.154     -0.903      0.367      -0.441       0.163\n",
      "219            -0.1179      0.121     -0.977      0.328      -0.354       0.119\n",
      "240             0.1912      0.125      1.533      0.125      -0.053       0.436\n",
      "363            -0.0392      0.088     -0.444      0.657      -0.212       0.134\n",
      "394             0.1630      0.143      1.142      0.253      -0.117       0.443\n",
      "417            -0.2920      0.147     -1.980      0.048      -0.581      -0.003\n",
      "424             0.2229      0.122      1.828      0.068      -0.016       0.462\n",
      "431            -0.3075      0.151     -2.041      0.041      -0.603      -0.012\n",
      "434             0.1187      0.117      1.018      0.309      -0.110       0.347\n",
      "438             0.2952      0.113      2.618      0.009       0.074       0.516\n",
      "470             0.2909      0.204      1.428      0.153      -0.108       0.690\n",
      "489            -0.2540      0.121     -2.102      0.036      -0.491      -0.017\n",
      "512             0.2572      0.111      2.309      0.021       0.039       0.476\n",
      "524            -0.2679      0.239     -1.123      0.262      -0.736       0.200\n",
      "548             0.2204      0.128      1.717      0.086      -0.031       0.472\n",
      "549            -0.1789      0.126     -1.421      0.155      -0.426       0.068\n",
      "556            -0.0526      0.122     -0.433      0.665      -0.291       0.186\n",
      "562             0.1368      0.112      1.222      0.222      -0.083       0.356\n",
      "563            -0.3103      0.129     -2.409      0.016      -0.563      -0.058\n",
      "564            -0.2468      0.154     -1.599      0.110      -0.549       0.056\n",
      "576            -0.4644      0.213     -2.178      0.029      -0.882      -0.046\n",
      "582            -0.1216      0.153     -0.797      0.426      -0.421       0.177\n",
      "588             0.1477      0.120      1.233      0.218      -0.087       0.383\n",
      "missing_113     0.0855      0.169      0.505      0.614      -0.247       0.418\n",
      "===============================================================================\n",
      "Removing feature 126 with p-value 0.693738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179343\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1513\n",
      "Method:                           MLE   Df Model:                           53\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2654\n",
      "Time:                        16:31:37   Log-Likelihood:                -281.03\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.986e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -14.0151     13.510     -1.037      0.300     -40.494      12.464\n",
      "11              0.0482      0.125      0.386      0.699      -0.196       0.293\n",
      "22              0.6634      0.191      3.471      0.001       0.289       1.038\n",
      "30             -0.0608      0.131     -0.465      0.642      -0.317       0.195\n",
      "34              0.1417      0.087      1.624      0.104      -0.029       0.313\n",
      "60              0.9441      0.197      4.800      0.000       0.559       1.330\n",
      "65              0.4951      0.133      3.726      0.000       0.235       0.756\n",
      "68             -0.3317      0.188     -1.764      0.078      -0.700       0.037\n",
      "74             -0.8127      0.396     -2.053      0.040      -1.589      -0.037\n",
      "79              0.2972      0.154      1.933      0.053      -0.004       0.599\n",
      "80             -0.1243      0.134     -0.929      0.353      -0.387       0.138\n",
      "82              0.1881      0.117      1.611      0.107      -0.041       0.417\n",
      "85             -0.1324      0.121     -1.095      0.273      -0.369       0.104\n",
      "89              0.0582      0.126      0.461      0.645      -0.189       0.306\n",
      "92             -0.1157      0.156     -0.740      0.459      -0.422       0.191\n",
      "103            -0.2443      0.146     -1.676      0.094      -0.530       0.041\n",
      "104             0.2174      0.123      1.768      0.077      -0.024       0.458\n",
      "113             0.2269      0.213      1.064      0.288      -0.191       0.645\n",
      "116            -0.1727      0.117     -1.477      0.140      -0.402       0.056\n",
      "121            -0.1048      0.116     -0.901      0.368      -0.333       0.123\n",
      "125             0.2893      0.154      1.882      0.060      -0.012       0.591\n",
      "128            -0.0602      0.142     -0.424      0.672      -0.339       0.218\n",
      "130             0.4928      0.161      3.065      0.002       0.178       0.808\n",
      "134             0.1726      0.161      1.071      0.284      -0.143       0.488\n",
      "139             0.1975      0.128      1.548      0.122      -0.053       0.447\n",
      "141          -202.0318    264.053     -0.765      0.444    -719.566     315.502\n",
      "147            -0.2944      0.137     -2.151      0.031      -0.563      -0.026\n",
      "151            -0.2520      0.132     -1.909      0.056      -0.511       0.007\n",
      "173             0.2423      0.127      1.910      0.056      -0.006       0.491\n",
      "189             0.1474      0.117      1.258      0.208      -0.082       0.377\n",
      "215            -0.1400      0.153     -0.913      0.361      -0.441       0.161\n",
      "219            -0.1181      0.121     -0.979      0.328      -0.355       0.118\n",
      "240             0.1880      0.124      1.514      0.130      -0.055       0.431\n",
      "363            -0.0402      0.088     -0.456      0.649      -0.213       0.133\n",
      "394             0.1682      0.142      1.183      0.237      -0.111       0.447\n",
      "417            -0.2985      0.147     -2.035      0.042      -0.586      -0.011\n",
      "424             0.2166      0.121      1.790      0.074      -0.021       0.454\n",
      "431            -0.3077      0.151     -2.044      0.041      -0.603      -0.013\n",
      "434             0.1220      0.117      1.047      0.295      -0.106       0.350\n",
      "438             0.2944      0.112      2.622      0.009       0.074       0.514\n",
      "470             0.2882      0.203      1.419      0.156      -0.110       0.686\n",
      "489            -0.2541      0.121     -2.104      0.035      -0.491      -0.017\n",
      "512             0.2555      0.111      2.295      0.022       0.037       0.474\n",
      "524            -0.2695      0.239     -1.130      0.259      -0.737       0.198\n",
      "548             0.2176      0.128      1.696      0.090      -0.034       0.469\n",
      "549            -0.1772      0.126     -1.408      0.159      -0.424       0.069\n",
      "556            -0.0520      0.122     -0.428      0.669      -0.290       0.186\n",
      "562             0.1322      0.111      1.188      0.235      -0.086       0.350\n",
      "563            -0.3065      0.128     -2.389      0.017      -0.558      -0.055\n",
      "564            -0.2470      0.154     -1.603      0.109      -0.549       0.055\n",
      "576            -0.4689      0.213     -2.199      0.028      -0.887      -0.051\n",
      "582            -0.1224      0.153     -0.801      0.423      -0.422       0.177\n",
      "588             0.1489      0.120      1.243      0.214      -0.086       0.384\n",
      "missing_113     0.0905      0.169      0.536      0.592      -0.241       0.422\n",
      "===============================================================================\n",
      "Removing feature 11 with p-value 0.699280\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179391\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1514\n",
      "Method:                           MLE   Df Model:                           52\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2652\n",
      "Time:                        16:31:37   Log-Likelihood:                -281.11\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.051e-19\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.8447     13.457     -1.029      0.304     -40.220      12.530\n",
      "22              0.6673      0.191      3.491      0.000       0.293       1.042\n",
      "30             -0.0623      0.130     -0.477      0.633      -0.318       0.193\n",
      "34              0.1412      0.087      1.620      0.105      -0.030       0.312\n",
      "60              0.9486      0.197      4.826      0.000       0.563       1.334\n",
      "65              0.4946      0.133      3.720      0.000       0.234       0.755\n",
      "68             -0.3293      0.187     -1.758      0.079      -0.697       0.038\n",
      "74             -0.8143      0.396     -2.058      0.040      -1.590      -0.039\n",
      "79              0.2948      0.154      1.920      0.055      -0.006       0.596\n",
      "80             -0.1209      0.134     -0.905      0.365      -0.383       0.141\n",
      "82              0.1886      0.117      1.614      0.107      -0.040       0.418\n",
      "85             -0.1289      0.120     -1.070      0.285      -0.365       0.107\n",
      "89              0.0575      0.126      0.457      0.648      -0.189       0.304\n",
      "92             -0.1196      0.156     -0.766      0.444      -0.426       0.187\n",
      "103            -0.2447      0.146     -1.678      0.093      -0.530       0.041\n",
      "104             0.2145      0.123      1.749      0.080      -0.026       0.455\n",
      "113             0.2178      0.212      1.026      0.305      -0.198       0.634\n",
      "116            -0.1694      0.117     -1.454      0.146      -0.398       0.059\n",
      "121            -0.1049      0.116     -0.901      0.368      -0.333       0.123\n",
      "125             0.2883      0.154      1.875      0.061      -0.013       0.590\n",
      "128            -0.0590      0.142     -0.415      0.678      -0.337       0.219\n",
      "130             0.4956      0.160      3.090      0.002       0.181       0.810\n",
      "134             0.1767      0.161      1.098      0.272      -0.139       0.492\n",
      "139             0.1962      0.128      1.537      0.124      -0.054       0.446\n",
      "141          -198.7135    263.016     -0.756      0.450    -714.216     316.789\n",
      "147            -0.2952      0.137     -2.149      0.032      -0.564      -0.026\n",
      "151            -0.2568      0.131     -1.956      0.051      -0.514       0.001\n",
      "173             0.2416      0.127      1.904      0.057      -0.007       0.490\n",
      "189             0.1477      0.117      1.262      0.207      -0.082       0.377\n",
      "215            -0.1410      0.154     -0.918      0.359      -0.442       0.160\n",
      "219            -0.1203      0.121     -0.998      0.318      -0.357       0.116\n",
      "240             0.1867      0.124      1.503      0.133      -0.057       0.430\n",
      "363            -0.0414      0.088     -0.470      0.638      -0.214       0.131\n",
      "394             0.1663      0.142      1.168      0.243      -0.113       0.445\n",
      "417            -0.2970      0.147     -2.026      0.043      -0.584      -0.010\n",
      "424             0.2220      0.120      1.854      0.064      -0.013       0.457\n",
      "431            -0.3091      0.151     -2.053      0.040      -0.604      -0.014\n",
      "434             0.1225      0.117      1.051      0.293      -0.106       0.351\n",
      "438             0.2967      0.113      2.620      0.009       0.075       0.519\n",
      "470             0.2868      0.202      1.421      0.155      -0.109       0.682\n",
      "489            -0.2548      0.121     -2.110      0.035      -0.492      -0.018\n",
      "512             0.2553      0.111      2.293      0.022       0.037       0.474\n",
      "524            -0.2735      0.239     -1.145      0.252      -0.742       0.195\n",
      "548             0.2182      0.128      1.701      0.089      -0.033       0.470\n",
      "549            -0.1782      0.126     -1.416      0.157      -0.425       0.068\n",
      "556            -0.0483      0.121     -0.399      0.690      -0.286       0.189\n",
      "562             0.1318      0.111      1.184      0.236      -0.086       0.350\n",
      "563            -0.3067      0.128     -2.387      0.017      -0.558      -0.055\n",
      "564            -0.2480      0.154     -1.609      0.108      -0.550       0.054\n",
      "576            -0.4737      0.213     -2.220      0.026      -0.892      -0.056\n",
      "582            -0.1215      0.153     -0.796      0.426      -0.421       0.178\n",
      "588             0.1517      0.119      1.271      0.204      -0.082       0.386\n",
      "missing_113     0.0930      0.169      0.550      0.582      -0.238       0.424\n",
      "===============================================================================\n",
      "Removing feature 556 with p-value 0.690125\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179442\n",
      "         Iterations 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1515\n",
      "Method:                           MLE   Df Model:                           51\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2650\n",
      "Time:                        16:31:38   Log-Likelihood:                -281.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.542e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.6141     13.433     -1.013      0.311     -39.942      12.714\n",
      "22              0.6686      0.191      3.504      0.000       0.295       1.043\n",
      "30             -0.0663      0.130     -0.510      0.610      -0.321       0.188\n",
      "34              0.1409      0.087      1.618      0.106      -0.030       0.312\n",
      "60              0.9443      0.196      4.816      0.000       0.560       1.329\n",
      "65              0.4923      0.133      3.707      0.000       0.232       0.753\n",
      "68             -0.3288      0.188     -1.750      0.080      -0.697       0.039\n",
      "74             -0.8203      0.397     -2.067      0.039      -1.598      -0.042\n",
      "79              0.2976      0.153      1.940      0.052      -0.003       0.598\n",
      "80             -0.1235      0.134     -0.924      0.355      -0.385       0.138\n",
      "82              0.1852      0.117      1.589      0.112      -0.043       0.414\n",
      "85             -0.1309      0.120     -1.089      0.276      -0.367       0.105\n",
      "89              0.0584      0.126      0.464      0.643      -0.188       0.305\n",
      "92             -0.1200      0.156     -0.769      0.442      -0.426       0.186\n",
      "103            -0.2410      0.146     -1.655      0.098      -0.526       0.044\n",
      "104             0.2140      0.123      1.744      0.081      -0.027       0.454\n",
      "113             0.2222      0.212      1.046      0.296      -0.194       0.638\n",
      "116            -0.1701      0.116     -1.462      0.144      -0.398       0.058\n",
      "121            -0.1047      0.116     -0.899      0.369      -0.333       0.124\n",
      "125             0.2882      0.154      1.875      0.061      -0.013       0.589\n",
      "128            -0.0574      0.142     -0.404      0.686      -0.336       0.221\n",
      "130             0.4934      0.160      3.075      0.002       0.179       0.808\n",
      "134             0.1748      0.161      1.088      0.276      -0.140       0.490\n",
      "139             0.1964      0.128      1.538      0.124      -0.054       0.447\n",
      "141          -194.2337    262.551     -0.740      0.459    -708.824     320.357\n",
      "147            -0.2950      0.137     -2.148      0.032      -0.564      -0.026\n",
      "151            -0.2560      0.131     -1.950      0.051      -0.513       0.001\n",
      "173             0.2379      0.126      1.882      0.060      -0.010       0.486\n",
      "189             0.1456      0.117      1.246      0.213      -0.083       0.375\n",
      "215            -0.1408      0.154     -0.914      0.361      -0.443       0.161\n",
      "219            -0.1219      0.121     -1.010      0.312      -0.358       0.115\n",
      "240             0.1883      0.124      1.520      0.128      -0.054       0.431\n",
      "363            -0.0389      0.088     -0.443      0.658      -0.211       0.133\n",
      "394             0.1630      0.142      1.148      0.251      -0.115       0.441\n",
      "417            -0.2966      0.146     -2.025      0.043      -0.584      -0.010\n",
      "424             0.2245      0.120      1.876      0.061      -0.010       0.459\n",
      "431            -0.3101      0.150     -2.062      0.039      -0.605      -0.015\n",
      "434             0.1242      0.116      1.067      0.286      -0.104       0.352\n",
      "438             0.2959      0.113      2.626      0.009       0.075       0.517\n",
      "470             0.2891      0.203      1.427      0.154      -0.108       0.686\n",
      "489            -0.2522      0.120     -2.093      0.036      -0.488      -0.016\n",
      "512             0.2537      0.111      2.279      0.023       0.036       0.472\n",
      "524            -0.2697      0.239     -1.128      0.259      -0.738       0.199\n",
      "548             0.2115      0.127      1.670      0.095      -0.037       0.460\n",
      "549            -0.1799      0.126     -1.432      0.152      -0.426       0.066\n",
      "562             0.1321      0.111      1.188      0.235      -0.086       0.350\n",
      "563            -0.3056      0.128     -2.381      0.017      -0.557      -0.054\n",
      "564            -0.2458      0.154     -1.596      0.110      -0.548       0.056\n",
      "576            -0.4745      0.214     -2.220      0.026      -0.893      -0.056\n",
      "582            -0.1224      0.153     -0.801      0.423      -0.422       0.177\n",
      "588             0.1543      0.119      1.296      0.195      -0.079       0.388\n",
      "missing_113     0.0938      0.169      0.555      0.579      -0.237       0.425\n",
      "===============================================================================\n",
      "Removing feature 128 with p-value 0.685854\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179495\n",
      "         Iterations 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1516\n",
      "Method:                           MLE   Df Model:                           50\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2648\n",
      "Time:                        16:31:38   Log-Likelihood:                -281.27\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.898e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.5674     13.400     -1.012      0.311     -39.832      12.697\n",
      "22              0.6692      0.191      3.509      0.000       0.295       1.043\n",
      "30             -0.0648      0.130     -0.499      0.618      -0.320       0.190\n",
      "34              0.1431      0.087      1.644      0.100      -0.027       0.314\n",
      "60              0.9556      0.194      4.914      0.000       0.574       1.337\n",
      "65              0.4959      0.133      3.740      0.000       0.236       0.756\n",
      "68             -0.3330      0.187     -1.779      0.075      -0.700       0.034\n",
      "74             -0.8200      0.397     -2.065      0.039      -1.598      -0.042\n",
      "79              0.2968      0.154      1.932      0.053      -0.004       0.598\n",
      "80             -0.1207      0.133     -0.905      0.366      -0.382       0.141\n",
      "82              0.1849      0.116      1.587      0.112      -0.043       0.413\n",
      "85             -0.1268      0.120     -1.060      0.289      -0.361       0.108\n",
      "89              0.0550      0.126      0.438      0.661      -0.191       0.301\n",
      "92             -0.1210      0.157     -0.773      0.440      -0.428       0.186\n",
      "103            -0.2406      0.146     -1.643      0.100      -0.528       0.046\n",
      "104             0.2159      0.123      1.757      0.079      -0.025       0.457\n",
      "113             0.2242      0.212      1.056      0.291      -0.192       0.640\n",
      "116            -0.1702      0.116     -1.462      0.144      -0.398       0.058\n",
      "121            -0.1034      0.116     -0.891      0.373      -0.331       0.124\n",
      "125             0.2879      0.153      1.877      0.061      -0.013       0.589\n",
      "130             0.5060      0.158      3.205      0.001       0.197       0.815\n",
      "134             0.1779      0.161      1.108      0.268      -0.137       0.493\n",
      "139             0.1953      0.127      1.532      0.126      -0.055       0.445\n",
      "141          -193.2976    261.913     -0.738      0.461    -706.637     320.042\n",
      "147            -0.2945      0.137     -2.148      0.032      -0.563      -0.026\n",
      "151            -0.2516      0.131     -1.926      0.054      -0.508       0.004\n",
      "173             0.2426      0.126      1.929      0.054      -0.004       0.489\n",
      "189             0.1478      0.117      1.267      0.205      -0.081       0.376\n",
      "215            -0.1444      0.154     -0.936      0.349      -0.447       0.158\n",
      "219            -0.1209      0.120     -1.005      0.315      -0.357       0.115\n",
      "240             0.1841      0.125      1.479      0.139      -0.060       0.428\n",
      "363            -0.0381      0.088     -0.434      0.664      -0.210       0.134\n",
      "394             0.1584      0.142      1.119      0.263      -0.119       0.436\n",
      "417            -0.2970      0.146     -2.028      0.043      -0.584      -0.010\n",
      "424             0.2238      0.119      1.875      0.061      -0.010       0.458\n",
      "431            -0.3081      0.150     -2.050      0.040      -0.603      -0.014\n",
      "434             0.1237      0.116      1.063      0.288      -0.104       0.352\n",
      "438             0.2952      0.113      2.617      0.009       0.074       0.516\n",
      "470             0.2862      0.201      1.421      0.155      -0.109       0.681\n",
      "489            -0.2535      0.120     -2.105      0.035      -0.490      -0.017\n",
      "512             0.2532      0.111      2.275      0.023       0.035       0.471\n",
      "524            -0.2705      0.240     -1.126      0.260      -0.741       0.200\n",
      "548             0.2137      0.126      1.690      0.091      -0.034       0.462\n",
      "549            -0.1838      0.125     -1.467      0.142      -0.429       0.062\n",
      "562             0.1333      0.111      1.200      0.230      -0.084       0.351\n",
      "563            -0.3054      0.128     -2.380      0.017      -0.557      -0.054\n",
      "564            -0.2493      0.154     -1.620      0.105      -0.551       0.052\n",
      "576            -0.4754      0.214     -2.219      0.026      -0.895      -0.055\n",
      "582            -0.1252      0.153     -0.820      0.412      -0.424       0.174\n",
      "588             0.1542      0.119      1.296      0.195      -0.079       0.388\n",
      "missing_113     0.1003      0.168      0.596      0.551      -0.230       0.430\n",
      "===============================================================================\n",
      "Removing feature 363 with p-value 0.664332\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179558\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1517\n",
      "Method:                           MLE   Df Model:                           49\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2645\n",
      "Time:                        16:31:38   Log-Likelihood:                -281.37\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.521e-20\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.1347     13.300     -0.988      0.323     -39.203      12.933\n",
      "22              0.6674      0.191      3.498      0.000       0.293       1.041\n",
      "30             -0.0655      0.130     -0.505      0.614      -0.320       0.189\n",
      "34              0.1440      0.087      1.658      0.097      -0.026       0.314\n",
      "60              0.9570      0.195      4.918      0.000       0.576       1.338\n",
      "65              0.5006      0.132      3.785      0.000       0.241       0.760\n",
      "68             -0.3411      0.187     -1.828      0.068      -0.707       0.025\n",
      "74             -0.8095      0.397     -2.041      0.041      -1.587      -0.032\n",
      "79              0.2940      0.154      1.915      0.056      -0.007       0.595\n",
      "80             -0.1246      0.133     -0.935      0.350      -0.386       0.136\n",
      "82              0.1836      0.116      1.579      0.114      -0.044       0.412\n",
      "85             -0.1239      0.119     -1.038      0.299      -0.358       0.110\n",
      "89              0.0578      0.126      0.460      0.646      -0.188       0.304\n",
      "92             -0.1182      0.156     -0.756      0.450      -0.425       0.188\n",
      "103            -0.2403      0.146     -1.645      0.100      -0.527       0.046\n",
      "104             0.2136      0.123      1.735      0.083      -0.028       0.455\n",
      "113             0.2228      0.213      1.048      0.295      -0.194       0.639\n",
      "116            -0.1713      0.116     -1.471      0.141      -0.400       0.057\n",
      "121            -0.1046      0.116     -0.901      0.367      -0.332       0.123\n",
      "125             0.2860      0.153      1.864      0.062      -0.015       0.587\n",
      "130             0.5055      0.158      3.204      0.001       0.196       0.815\n",
      "134             0.1802      0.160      1.123      0.262      -0.134       0.495\n",
      "139             0.1917      0.127      1.510      0.131      -0.057       0.441\n",
      "141          -184.8610    259.966     -0.711      0.477    -694.384     324.662\n",
      "147            -0.2906      0.136     -2.130      0.033      -0.558      -0.023\n",
      "151            -0.2541      0.130     -1.948      0.051      -0.510       0.002\n",
      "173             0.2399      0.126      1.910      0.056      -0.006       0.486\n",
      "189             0.1496      0.117      1.283      0.200      -0.079       0.378\n",
      "215            -0.1435      0.154     -0.930      0.352      -0.446       0.159\n",
      "219            -0.1173      0.120     -0.978      0.328      -0.352       0.118\n",
      "240             0.1849      0.125      1.483      0.138      -0.059       0.429\n",
      "394             0.1620      0.141      1.146      0.252      -0.115       0.439\n",
      "417            -0.2980      0.146     -2.034      0.042      -0.585      -0.011\n",
      "424             0.2274      0.119      1.916      0.055      -0.005       0.460\n",
      "431            -0.3164      0.149     -2.123      0.034      -0.608      -0.024\n",
      "434             0.1244      0.116      1.069      0.285      -0.104       0.352\n",
      "438             0.2964      0.113      2.627      0.009       0.075       0.518\n",
      "470             0.2891      0.202      1.433      0.152      -0.106       0.685\n",
      "489            -0.2549      0.120     -2.118      0.034      -0.491      -0.019\n",
      "512             0.2529      0.111      2.272      0.023       0.035       0.471\n",
      "524            -0.2728      0.241     -1.131      0.258      -0.746       0.200\n",
      "548             0.2150      0.126      1.701      0.089      -0.033       0.463\n",
      "549            -0.1846      0.125     -1.474      0.140      -0.430       0.061\n",
      "562             0.1334      0.111      1.201      0.230      -0.084       0.351\n",
      "563            -0.3047      0.128     -2.376      0.018      -0.556      -0.053\n",
      "564            -0.2481      0.154     -1.613      0.107      -0.550       0.053\n",
      "576            -0.4783      0.214     -2.232      0.026      -0.898      -0.058\n",
      "582            -0.1238      0.153     -0.810      0.418      -0.423       0.176\n",
      "588             0.1549      0.119      1.302      0.193      -0.078       0.388\n",
      "missing_113     0.1008      0.168      0.599      0.549      -0.229       0.431\n",
      "===============================================================================\n",
      "Removing feature 89 with p-value 0.645544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179626\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1518\n",
      "Method:                           MLE   Df Model:                           48\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2643\n",
      "Time:                        16:31:38   Log-Likelihood:                -281.47\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.949e-21\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.0502     13.343     -0.978      0.328     -39.201      13.101\n",
      "22              0.6649      0.191      3.488      0.000       0.291       1.038\n",
      "30             -0.0634      0.130     -0.488      0.625      -0.318       0.191\n",
      "34              0.1427      0.087      1.641      0.101      -0.028       0.313\n",
      "60              0.9573      0.195      4.920      0.000       0.576       1.339\n",
      "65              0.4979      0.132      3.764      0.000       0.239       0.757\n",
      "68             -0.3450      0.187     -1.843      0.065      -0.712       0.022\n",
      "74             -0.8169      0.397     -2.059      0.039      -1.595      -0.039\n",
      "79              0.2912      0.153      1.901      0.057      -0.009       0.591\n",
      "80             -0.1233      0.133     -0.927      0.354      -0.384       0.137\n",
      "82              0.1896      0.116      1.640      0.101      -0.037       0.416\n",
      "85             -0.1271      0.119     -1.067      0.286      -0.361       0.106\n",
      "92             -0.1150      0.156     -0.739      0.460      -0.420       0.190\n",
      "103            -0.2379      0.145     -1.643      0.100      -0.522       0.046\n",
      "104             0.2161      0.123      1.755      0.079      -0.025       0.457\n",
      "113             0.2239      0.212      1.056      0.291      -0.192       0.640\n",
      "116            -0.1640      0.115     -1.421      0.155      -0.390       0.062\n",
      "121            -0.1075      0.115     -0.932      0.352      -0.334       0.119\n",
      "125             0.2898      0.153      1.894      0.058      -0.010       0.590\n",
      "130             0.5086      0.158      3.223      0.001       0.199       0.818\n",
      "134             0.1808      0.160      1.129      0.259      -0.133       0.495\n",
      "139             0.1906      0.127      1.502      0.133      -0.058       0.439\n",
      "141          -183.2078    260.796     -0.702      0.482    -694.358     327.942\n",
      "147            -0.2880      0.136     -2.115      0.034      -0.555      -0.021\n",
      "151            -0.2540      0.131     -1.944      0.052      -0.510       0.002\n",
      "173             0.2384      0.126      1.895      0.058      -0.008       0.485\n",
      "189             0.1496      0.117      1.282      0.200      -0.079       0.378\n",
      "215            -0.1439      0.154     -0.935      0.350      -0.446       0.158\n",
      "219            -0.1163      0.120     -0.970      0.332      -0.351       0.119\n",
      "240             0.1824      0.125      1.464      0.143      -0.062       0.427\n",
      "394             0.1613      0.141      1.142      0.253      -0.115       0.438\n",
      "417            -0.2987      0.146     -2.039      0.041      -0.586      -0.012\n",
      "424             0.2292      0.119      1.929      0.054      -0.004       0.462\n",
      "431            -0.3133      0.149     -2.107      0.035      -0.605      -0.022\n",
      "434             0.1268      0.116      1.092      0.275      -0.101       0.355\n",
      "438             0.3006      0.113      2.660      0.008       0.079       0.522\n",
      "470             0.2943      0.202      1.455      0.146      -0.102       0.691\n",
      "489            -0.2540      0.120     -2.114      0.035      -0.490      -0.018\n",
      "512             0.2527      0.111      2.271      0.023       0.035       0.471\n",
      "524            -0.2702      0.241     -1.121      0.262      -0.743       0.202\n",
      "548             0.2137      0.126      1.696      0.090      -0.033       0.461\n",
      "549            -0.1879      0.125     -1.504      0.133      -0.433       0.057\n",
      "562             0.1357      0.111      1.222      0.222      -0.082       0.353\n",
      "563            -0.3071      0.128     -2.397      0.017      -0.558      -0.056\n",
      "564            -0.2480      0.154     -1.613      0.107      -0.549       0.053\n",
      "576            -0.4776      0.214     -2.232      0.026      -0.897      -0.058\n",
      "582            -0.1216      0.153     -0.797      0.425      -0.421       0.177\n",
      "588             0.1487      0.119      1.253      0.210      -0.084       0.381\n",
      "missing_113     0.1016      0.168      0.603      0.546      -0.229       0.432\n",
      "===============================================================================\n",
      "Removing feature 30 with p-value 0.625373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179701\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1519\n",
      "Method:                           MLE   Df Model:                           47\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2640\n",
      "Time:                        16:31:39   Log-Likelihood:                -281.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.156e-21\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const         -13.4925     13.277     -1.016      0.310     -39.515      12.530\n",
      "22              0.6715      0.190      3.532      0.000       0.299       1.044\n",
      "34              0.1477      0.086      1.715      0.086      -0.021       0.316\n",
      "60              0.9430      0.192      4.902      0.000       0.566       1.320\n",
      "65              0.4898      0.131      3.736      0.000       0.233       0.747\n",
      "68             -0.3499      0.187     -1.872      0.061      -0.716       0.016\n",
      "74             -0.7990      0.395     -2.022      0.043      -1.573      -0.025\n",
      "79              0.2962      0.153      1.941      0.052      -0.003       0.595\n",
      "80             -0.1271      0.133     -0.959      0.338      -0.387       0.133\n",
      "82              0.1895      0.116      1.637      0.102      -0.037       0.416\n",
      "85             -0.1244      0.119     -1.045      0.296      -0.358       0.109\n",
      "92             -0.1177      0.156     -0.756      0.449      -0.423       0.187\n",
      "103            -0.2424      0.145     -1.674      0.094      -0.526       0.041\n",
      "104             0.2174      0.123      1.762      0.078      -0.024       0.459\n",
      "113             0.2265      0.212      1.069      0.285      -0.189       0.642\n",
      "116            -0.1615      0.115     -1.404      0.160      -0.387       0.064\n",
      "121            -0.1123      0.115     -0.979      0.328      -0.337       0.112\n",
      "125             0.2955      0.152      1.940      0.052      -0.003       0.594\n",
      "130             0.5068      0.157      3.220      0.001       0.198       0.815\n",
      "134             0.1789      0.160      1.118      0.264      -0.135       0.493\n",
      "139             0.1901      0.127      1.499      0.134      -0.058       0.439\n",
      "141          -191.8430    259.501     -0.739      0.460    -700.455     316.769\n",
      "147            -0.2942      0.136     -2.166      0.030      -0.560      -0.028\n",
      "151            -0.2508      0.131     -1.921      0.055      -0.507       0.005\n",
      "173             0.2362      0.126      1.878      0.060      -0.010       0.483\n",
      "189             0.1501      0.117      1.287      0.198      -0.078       0.379\n",
      "215            -0.1434      0.152     -0.941      0.346      -0.442       0.155\n",
      "219            -0.1212      0.119     -1.015      0.310      -0.355       0.113\n",
      "240             0.1861      0.123      1.511      0.131      -0.055       0.428\n",
      "394             0.1655      0.141      1.173      0.241      -0.111       0.442\n",
      "417            -0.3039      0.146     -2.079      0.038      -0.590      -0.017\n",
      "424             0.2304      0.119      1.942      0.052      -0.002       0.463\n",
      "431            -0.3131      0.149     -2.107      0.035      -0.604      -0.022\n",
      "434             0.1284      0.116      1.105      0.269      -0.099       0.356\n",
      "438             0.3029      0.113      2.672      0.008       0.081       0.525\n",
      "470             0.3009      0.202      1.490      0.136      -0.095       0.697\n",
      "489            -0.2571      0.120     -2.142      0.032      -0.492      -0.022\n",
      "512             0.2531      0.111      2.276      0.023       0.035       0.471\n",
      "524            -0.2675      0.242     -1.107      0.268      -0.741       0.206\n",
      "548             0.2087      0.126      1.660      0.097      -0.038       0.455\n",
      "549            -0.1856      0.125     -1.488      0.137      -0.430       0.059\n",
      "562             0.1340      0.111      1.208      0.227      -0.083       0.352\n",
      "563            -0.3059      0.128     -2.387      0.017      -0.557      -0.055\n",
      "564            -0.2430      0.153     -1.585      0.113      -0.543       0.058\n",
      "576            -0.4797      0.214     -2.237      0.025      -0.900      -0.059\n",
      "582            -0.1190      0.152     -0.784      0.433      -0.417       0.179\n",
      "588             0.1478      0.119      1.244      0.213      -0.085       0.381\n",
      "missing_113     0.1005      0.169      0.596      0.551      -0.230       0.431\n",
      "===============================================================================\n",
      "Removing feature missing_113 with p-value 0.551189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179815\n",
      "         Iterations 19\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1520\n",
      "Method:                           MLE   Df Model:                           46\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2635\n",
      "Time:                        16:31:39   Log-Likelihood:                -281.77\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.256e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const        -12.9388     13.211     -0.979      0.327     -38.832      12.954\n",
      "22             0.6698      0.190      3.526      0.000       0.298       1.042\n",
      "34             0.1435      0.085      1.679      0.093      -0.024       0.311\n",
      "60             0.8942      0.172      5.184      0.000       0.556       1.232\n",
      "65             0.4676      0.125      3.739      0.000       0.222       0.713\n",
      "68            -0.3351      0.186     -1.804      0.071      -0.699       0.029\n",
      "74            -0.8120      0.393     -2.066      0.039      -1.582      -0.042\n",
      "79             0.3105      0.150      2.071      0.038       0.017       0.604\n",
      "80            -0.1341      0.132     -1.017      0.309      -0.393       0.124\n",
      "82             0.1798      0.115      1.565      0.118      -0.045       0.405\n",
      "85            -0.1199      0.119     -1.010      0.312      -0.353       0.113\n",
      "92            -0.1093      0.156     -0.702      0.482      -0.414       0.196\n",
      "103           -0.2325      0.145     -1.604      0.109      -0.517       0.052\n",
      "104            0.2253      0.122      1.852      0.064      -0.013       0.464\n",
      "113            0.2343      0.210      1.118      0.263      -0.176       0.645\n",
      "116           -0.1570      0.115     -1.366      0.172      -0.382       0.068\n",
      "121           -0.1095      0.114     -0.958      0.338      -0.334       0.115\n",
      "125            0.2891      0.152      1.902      0.057      -0.009       0.587\n",
      "130            0.5121      0.157      3.255      0.001       0.204       0.820\n",
      "134            0.1792      0.160      1.118      0.264      -0.135       0.493\n",
      "139            0.1929      0.127      1.521      0.128      -0.056       0.441\n",
      "141         -181.0850    258.223     -0.701      0.483    -687.193     325.023\n",
      "147           -0.2870      0.135     -2.124      0.034      -0.552      -0.022\n",
      "151           -0.2492      0.131     -1.907      0.057      -0.505       0.007\n",
      "173            0.2302      0.125      1.837      0.066      -0.015       0.476\n",
      "189            0.1527      0.117      1.308      0.191      -0.076       0.382\n",
      "215           -0.1394      0.151     -0.921      0.357      -0.436       0.157\n",
      "219           -0.1195      0.119     -1.001      0.317      -0.354       0.114\n",
      "240            0.1838      0.125      1.476      0.140      -0.060       0.428\n",
      "394            0.1555      0.140      1.112      0.266      -0.118       0.430\n",
      "417           -0.3069      0.146     -2.096      0.036      -0.594      -0.020\n",
      "424            0.2286      0.118      1.934      0.053      -0.003       0.460\n",
      "431           -0.3110      0.148     -2.097      0.036      -0.602      -0.020\n",
      "434            0.1311      0.116      1.127      0.260      -0.097       0.359\n",
      "438            0.3043      0.115      2.654      0.008       0.080       0.529\n",
      "470            0.3132      0.202      1.551      0.121      -0.083       0.709\n",
      "489           -0.2562      0.120     -2.133      0.033      -0.492      -0.021\n",
      "512            0.2535      0.111      2.283      0.022       0.036       0.471\n",
      "524           -0.2547      0.240     -1.060      0.289      -0.726       0.216\n",
      "548            0.2041      0.125      1.628      0.104      -0.042       0.450\n",
      "549           -0.1907      0.124     -1.533      0.125      -0.434       0.053\n",
      "562            0.1350      0.111      1.217      0.224      -0.082       0.353\n",
      "563           -0.3004      0.128     -2.345      0.019      -0.551      -0.049\n",
      "564           -0.2550      0.152     -1.674      0.094      -0.554       0.044\n",
      "576           -0.4783      0.215     -2.225      0.026      -0.900      -0.057\n",
      "582           -0.1196      0.153     -0.783      0.434      -0.419       0.180\n",
      "588            0.1528      0.118      1.290      0.197      -0.079       0.385\n",
      "==============================================================================\n",
      "Removing feature 141 with p-value 0.483132\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180358\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1521\n",
      "Method:                           MLE   Df Model:                           45\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2613\n",
      "Time:                        16:31:39   Log-Likelihood:                -282.62\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.058e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6932      0.195    -18.946      0.000      -4.075      -3.311\n",
      "22             0.6702      0.190      3.525      0.000       0.298       1.043\n",
      "34             0.1504      0.085      1.770      0.077      -0.016       0.317\n",
      "60             0.8875      0.172      5.157      0.000       0.550       1.225\n",
      "65             0.4659      0.125      3.721      0.000       0.220       0.711\n",
      "68            -0.3273      0.182     -1.794      0.073      -0.685       0.030\n",
      "74            -0.8035      0.390     -2.059      0.039      -1.568      -0.039\n",
      "79             0.3065      0.150      2.049      0.040       0.013       0.600\n",
      "80            -0.1308      0.132     -0.990      0.322      -0.390       0.128\n",
      "82             0.1864      0.114      1.632      0.103      -0.038       0.410\n",
      "85            -0.1239      0.119     -1.041      0.298      -0.357       0.109\n",
      "92            -0.1080      0.155     -0.697      0.486      -0.412       0.196\n",
      "103           -0.2352      0.145     -1.621      0.105      -0.520       0.049\n",
      "104            0.2158      0.121      1.776      0.076      -0.022       0.454\n",
      "113            0.2266      0.210      1.082      0.279      -0.184       0.637\n",
      "116           -0.1644      0.115     -1.432      0.152      -0.390       0.061\n",
      "121           -0.1080      0.114     -0.945      0.345      -0.332       0.116\n",
      "125            0.3054      0.151      2.017      0.044       0.009       0.602\n",
      "130            0.5222      0.158      3.315      0.001       0.213       0.831\n",
      "134            0.1787      0.160      1.115      0.265      -0.135       0.493\n",
      "139            0.1661      0.121      1.374      0.169      -0.071       0.403\n",
      "147           -0.2832      0.136     -2.085      0.037      -0.549      -0.017\n",
      "151           -0.2443      0.130     -1.883      0.060      -0.499       0.010\n",
      "173            0.2350      0.125      1.881      0.060      -0.010       0.480\n",
      "189            0.1550      0.117      1.328      0.184      -0.074       0.384\n",
      "215           -0.1401      0.153     -0.913      0.361      -0.441       0.161\n",
      "219           -0.1170      0.119     -0.983      0.325      -0.350       0.116\n",
      "240            0.1866      0.125      1.499      0.134      -0.057       0.431\n",
      "394            0.1586      0.140      1.131      0.258      -0.116       0.433\n",
      "417           -0.3001      0.144     -2.077      0.038      -0.583      -0.017\n",
      "424            0.2231      0.118      1.894      0.058      -0.008       0.454\n",
      "431           -0.3121      0.149     -2.098      0.036      -0.604      -0.020\n",
      "434            0.1321      0.116      1.134      0.257      -0.096       0.360\n",
      "438            0.3037      0.114      2.655      0.008       0.080       0.528\n",
      "470            0.3037      0.196      1.547      0.122      -0.081       0.689\n",
      "489           -0.2530      0.120     -2.111      0.035      -0.488      -0.018\n",
      "512            0.2600      0.111      2.352      0.019       0.043       0.477\n",
      "524           -0.2522      0.239     -1.056      0.291      -0.720       0.216\n",
      "548            0.2036      0.125      1.627      0.104      -0.042       0.449\n",
      "549           -0.1892      0.124     -1.524      0.128      -0.433       0.054\n",
      "562            0.1409      0.111      1.274      0.203      -0.076       0.358\n",
      "563           -0.3027      0.128     -2.363      0.018      -0.554      -0.052\n",
      "564           -0.2730      0.152     -1.801      0.072      -0.570       0.024\n",
      "576           -0.4754      0.214     -2.224      0.026      -0.894      -0.056\n",
      "582           -0.1217      0.150     -0.811      0.418      -0.416       0.173\n",
      "588            0.1531      0.119      1.291      0.197      -0.079       0.385\n",
      "==============================================================================\n",
      "Removing feature 92 with p-value 0.485902\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180512\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1522\n",
      "Method:                           MLE   Df Model:                           44\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2606\n",
      "Time:                        16:31:39   Log-Likelihood:                -282.86\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.156e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6909      0.195    -18.954      0.000      -4.073      -3.309\n",
      "22             0.6652      0.190      3.505      0.000       0.293       1.037\n",
      "34             0.1480      0.085      1.734      0.083      -0.019       0.315\n",
      "60             0.8868      0.172      5.152      0.000       0.549       1.224\n",
      "65             0.4735      0.125      3.795      0.000       0.229       0.718\n",
      "68            -0.3203      0.183     -1.755      0.079      -0.678       0.037\n",
      "74            -0.7919      0.392     -2.020      0.043      -1.560      -0.024\n",
      "79             0.3122      0.149      2.090      0.037       0.019       0.605\n",
      "80            -0.1339      0.132     -1.015      0.310      -0.392       0.125\n",
      "82             0.1874      0.114      1.644      0.100      -0.036       0.411\n",
      "85            -0.1204      0.119     -1.013      0.311      -0.354       0.113\n",
      "103           -0.1794      0.123     -1.464      0.143      -0.420       0.061\n",
      "104            0.2117      0.121      1.743      0.081      -0.026       0.450\n",
      "113            0.2322      0.210      1.106      0.269      -0.179       0.644\n",
      "116           -0.1687      0.115     -1.473      0.141      -0.393       0.056\n",
      "121           -0.1111      0.115     -0.968      0.333      -0.336       0.114\n",
      "125            0.3131      0.151      2.075      0.038       0.017       0.609\n",
      "130            0.5299      0.158      3.361      0.001       0.221       0.839\n",
      "134            0.1706      0.160      1.068      0.286      -0.143       0.484\n",
      "139            0.1674      0.121      1.387      0.166      -0.069       0.404\n",
      "147           -0.2840      0.136     -2.081      0.037      -0.551      -0.017\n",
      "151           -0.2525      0.129     -1.956      0.050      -0.506       0.001\n",
      "173            0.2330      0.125      1.859      0.063      -0.013       0.479\n",
      "189            0.1570      0.117      1.345      0.179      -0.072       0.386\n",
      "215           -0.1388      0.153     -0.905      0.366      -0.440       0.162\n",
      "219           -0.1178      0.119     -0.990      0.322      -0.351       0.115\n",
      "240            0.1907      0.123      1.550      0.121      -0.050       0.432\n",
      "394            0.1625      0.140      1.157      0.247      -0.113       0.438\n",
      "417           -0.3031      0.145     -2.096      0.036      -0.587      -0.020\n",
      "424            0.2271      0.118      1.929      0.054      -0.004       0.458\n",
      "431           -0.3082      0.148     -2.078      0.038      -0.599      -0.018\n",
      "434            0.1353      0.116      1.163      0.245      -0.093       0.363\n",
      "438            0.3012      0.114      2.633      0.008       0.077       0.525\n",
      "470            0.2940      0.196      1.499      0.134      -0.091       0.678\n",
      "489           -0.2502      0.119     -2.094      0.036      -0.484      -0.016\n",
      "512            0.2573      0.110      2.330      0.020       0.041       0.474\n",
      "524           -0.2579      0.239     -1.077      0.281      -0.727       0.211\n",
      "548            0.2037      0.125      1.627      0.104      -0.042       0.449\n",
      "549           -0.1891      0.124     -1.522      0.128      -0.433       0.054\n",
      "562            0.1413      0.111      1.277      0.202      -0.076       0.358\n",
      "563           -0.2972      0.128     -2.326      0.020      -0.548      -0.047\n",
      "564           -0.2749      0.151     -1.819      0.069      -0.571       0.021\n",
      "576           -0.4778      0.214     -2.229      0.026      -0.898      -0.058\n",
      "582           -0.1192      0.150     -0.794      0.427      -0.413       0.175\n",
      "588            0.1508      0.118      1.279      0.201      -0.080       0.382\n",
      "==============================================================================\n",
      "Removing feature 582 with p-value 0.427001\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180731\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1523\n",
      "Method:                           MLE   Df Model:                           43\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2597\n",
      "Time:                        16:31:39   Log-Likelihood:                -283.21\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.970e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6856      0.194    -18.980      0.000      -4.066      -3.305\n",
      "22             0.6724      0.190      3.547      0.000       0.301       1.044\n",
      "34             0.1506      0.085      1.769      0.077      -0.016       0.318\n",
      "60             0.8909      0.172      5.177      0.000       0.554       1.228\n",
      "65             0.4745      0.125      3.798      0.000       0.230       0.719\n",
      "68            -0.3186      0.182     -1.754      0.079      -0.674       0.037\n",
      "74            -0.7815      0.391     -1.996      0.046      -1.549      -0.014\n",
      "79             0.3089      0.149      2.076      0.038       0.017       0.600\n",
      "80            -0.1399      0.132     -1.063      0.288      -0.398       0.118\n",
      "82             0.1888      0.114      1.655      0.098      -0.035       0.412\n",
      "85            -0.1173      0.119     -0.986      0.324      -0.351       0.116\n",
      "103           -0.1842      0.122     -1.510      0.131      -0.423       0.055\n",
      "104            0.2093      0.121      1.724      0.085      -0.029       0.447\n",
      "113            0.2222      0.209      1.061      0.289      -0.188       0.633\n",
      "116           -0.1706      0.114     -1.494      0.135      -0.394       0.053\n",
      "121           -0.1112      0.115     -0.966      0.334      -0.337       0.114\n",
      "125            0.3150      0.151      2.087      0.037       0.019       0.611\n",
      "130            0.5299      0.158      3.360      0.001       0.221       0.839\n",
      "134            0.1714      0.160      1.074      0.283      -0.141       0.484\n",
      "139            0.1694      0.121      1.400      0.162      -0.068       0.407\n",
      "147           -0.2834      0.135     -2.096      0.036      -0.548      -0.018\n",
      "151           -0.2485      0.129     -1.923      0.054      -0.502       0.005\n",
      "173            0.2406      0.125      1.923      0.055      -0.005       0.486\n",
      "189            0.1579      0.117      1.353      0.176      -0.071       0.387\n",
      "215           -0.1343      0.152     -0.885      0.376      -0.432       0.163\n",
      "219           -0.1234      0.119     -1.039      0.299      -0.356       0.109\n",
      "240            0.1941      0.122      1.585      0.113      -0.046       0.434\n",
      "394            0.1663      0.140      1.186      0.236      -0.109       0.441\n",
      "417           -0.3084      0.145     -2.134      0.033      -0.592      -0.025\n",
      "424            0.2251      0.118      1.914      0.056      -0.005       0.456\n",
      "431           -0.3160      0.147     -2.146      0.032      -0.605      -0.027\n",
      "434            0.1291      0.116      1.110      0.267      -0.099       0.357\n",
      "438            0.2979      0.114      2.621      0.009       0.075       0.521\n",
      "470            0.2935      0.195      1.508      0.132      -0.088       0.675\n",
      "489           -0.2541      0.120     -2.126      0.034      -0.488      -0.020\n",
      "512            0.2545      0.110      2.305      0.021       0.038       0.471\n",
      "524           -0.2664      0.239     -1.113      0.266      -0.736       0.203\n",
      "548            0.2064      0.125      1.647      0.100      -0.039       0.452\n",
      "549           -0.1909      0.124     -1.541      0.123      -0.434       0.052\n",
      "562            0.1408      0.111      1.270      0.204      -0.076       0.358\n",
      "563           -0.2953      0.128     -2.316      0.021      -0.545      -0.045\n",
      "564           -0.2783      0.151     -1.840      0.066      -0.575       0.018\n",
      "576           -0.4872      0.215     -2.264      0.024      -0.909      -0.065\n",
      "588            0.1222      0.114      1.076      0.282      -0.100       0.345\n",
      "==============================================================================\n",
      "Removing feature 215 with p-value 0.376333\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181038\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1524\n",
      "Method:                           MLE   Df Model:                           42\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2585\n",
      "Time:                        16:31:39   Log-Likelihood:                -283.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.650e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6840      0.194    -18.982      0.000      -4.064      -3.304\n",
      "22             0.6753      0.189      3.566      0.000       0.304       1.047\n",
      "34             0.1555      0.085      1.832      0.067      -0.011       0.322\n",
      "60             0.8963      0.171      5.256      0.000       0.562       1.230\n",
      "65             0.4780      0.125      3.831      0.000       0.233       0.723\n",
      "68            -0.3187      0.182     -1.748      0.080      -0.676       0.039\n",
      "74            -0.7798      0.392     -1.989      0.047      -1.548      -0.011\n",
      "79             0.3163      0.148      2.131      0.033       0.025       0.607\n",
      "80            -0.1529      0.127     -1.200      0.230      -0.403       0.097\n",
      "82             0.1893      0.114      1.661      0.097      -0.034       0.413\n",
      "85            -0.1164      0.119     -0.976      0.329      -0.350       0.117\n",
      "103           -0.1843      0.122     -1.515      0.130      -0.423       0.054\n",
      "104            0.2132      0.120      1.770      0.077      -0.023       0.449\n",
      "113            0.2354      0.209      1.125      0.261      -0.175       0.646\n",
      "116           -0.1641      0.113     -1.452      0.146      -0.386       0.057\n",
      "121           -0.1107      0.116     -0.958      0.338      -0.337       0.116\n",
      "125            0.3214      0.151      2.136      0.033       0.026       0.616\n",
      "130            0.5274      0.158      3.344      0.001       0.218       0.837\n",
      "134            0.1643      0.159      1.030      0.303      -0.148       0.477\n",
      "139            0.1712      0.121      1.416      0.157      -0.066       0.408\n",
      "147           -0.2926      0.136     -2.150      0.032      -0.559      -0.026\n",
      "151           -0.2553      0.129     -1.980      0.048      -0.508      -0.003\n",
      "173            0.2394      0.125      1.912      0.056      -0.006       0.485\n",
      "189            0.1573      0.116      1.352      0.176      -0.071       0.385\n",
      "219           -0.1241      0.119     -1.045      0.296      -0.357       0.109\n",
      "240            0.1926      0.122      1.577      0.115      -0.047       0.432\n",
      "394            0.1699      0.140      1.214      0.225      -0.104       0.444\n",
      "417           -0.3064      0.144     -2.126      0.034      -0.589      -0.024\n",
      "424            0.2291      0.117      1.955      0.051      -0.001       0.459\n",
      "431           -0.3135      0.147     -2.128      0.033      -0.602      -0.025\n",
      "434            0.1348      0.116      1.161      0.246      -0.093       0.363\n",
      "438            0.2981      0.113      2.636      0.008       0.076       0.520\n",
      "470            0.2885      0.194      1.485      0.138      -0.092       0.669\n",
      "489           -0.2554      0.120     -2.137      0.033      -0.490      -0.021\n",
      "512            0.2558      0.111      2.314      0.021       0.039       0.472\n",
      "524           -0.2621      0.240     -1.092      0.275      -0.733       0.208\n",
      "548            0.2113      0.125      1.686      0.092      -0.034       0.457\n",
      "549           -0.1901      0.124     -1.532      0.126      -0.433       0.053\n",
      "562            0.1491      0.111      1.347      0.178      -0.068       0.366\n",
      "563           -0.2926      0.127     -2.296      0.022      -0.542      -0.043\n",
      "564           -0.2743      0.151     -1.812      0.070      -0.571       0.022\n",
      "576           -0.4863      0.215     -2.261      0.024      -0.908      -0.065\n",
      "588            0.1191      0.114      1.049      0.294      -0.103       0.342\n",
      "==============================================================================\n",
      "Removing feature 121 with p-value 0.338002\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181331\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1525\n",
      "Method:                           MLE   Df Model:                           41\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2573\n",
      "Time:                        16:31:40   Log-Likelihood:                -284.15\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.029e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6792      0.194    -18.978      0.000      -4.059      -3.299\n",
      "22             0.6657      0.189      3.529      0.000       0.296       1.035\n",
      "34             0.1570      0.085      1.850      0.064      -0.009       0.323\n",
      "60             0.8924      0.170      5.239      0.000       0.559       1.226\n",
      "65             0.4818      0.125      3.860      0.000       0.237       0.726\n",
      "68            -0.3176      0.183     -1.735      0.083      -0.676       0.041\n",
      "74            -0.7893      0.395     -1.999      0.046      -1.563      -0.016\n",
      "79             0.3201      0.148      2.168      0.030       0.031       0.609\n",
      "80            -0.1564      0.127     -1.233      0.218      -0.405       0.092\n",
      "82             0.1863      0.114      1.637      0.102      -0.037       0.409\n",
      "85            -0.1123      0.119     -0.944      0.345      -0.345       0.121\n",
      "103           -0.1789      0.122     -1.470      0.141      -0.417       0.060\n",
      "104            0.2154      0.120      1.792      0.073      -0.020       0.451\n",
      "113            0.2485      0.210      1.184      0.236      -0.163       0.660\n",
      "116           -0.1632      0.113     -1.446      0.148      -0.384       0.058\n",
      "125            0.3230      0.150      2.148      0.032       0.028       0.618\n",
      "130            0.5252      0.158      3.333      0.001       0.216       0.834\n",
      "134            0.1654      0.159      1.038      0.299      -0.147       0.478\n",
      "139            0.1674      0.121      1.382      0.167      -0.070       0.405\n",
      "147           -0.2859      0.136     -2.103      0.035      -0.552      -0.020\n",
      "151           -0.2499      0.129     -1.943      0.052      -0.502       0.002\n",
      "173            0.2522      0.124      2.027      0.043       0.008       0.496\n",
      "189            0.1591      0.116      1.369      0.171      -0.069       0.387\n",
      "219           -0.1245      0.119     -1.049      0.294      -0.357       0.108\n",
      "240            0.1970      0.121      1.626      0.104      -0.040       0.434\n",
      "394            0.1401      0.135      1.039      0.299      -0.124       0.404\n",
      "417           -0.3062      0.144     -2.128      0.033      -0.588      -0.024\n",
      "424            0.2267      0.118      1.929      0.054      -0.004       0.457\n",
      "431           -0.3110      0.147     -2.115      0.034      -0.599      -0.023\n",
      "434            0.1358      0.116      1.169      0.242      -0.092       0.364\n",
      "438            0.2873      0.111      2.585      0.010       0.069       0.505\n",
      "470            0.2912      0.194      1.502      0.133      -0.089       0.671\n",
      "489           -0.2541      0.119     -2.130      0.033      -0.488      -0.020\n",
      "512            0.2592      0.110      2.352      0.019       0.043       0.475\n",
      "524           -0.2577      0.241     -1.068      0.286      -0.731       0.215\n",
      "548            0.2115      0.125      1.687      0.092      -0.034       0.457\n",
      "549           -0.1799      0.123     -1.458      0.145      -0.422       0.062\n",
      "562            0.1486      0.110      1.346      0.178      -0.068       0.365\n",
      "563           -0.2943      0.127     -2.309      0.021      -0.544      -0.044\n",
      "564           -0.2737      0.151     -1.808      0.071      -0.570       0.023\n",
      "576           -0.4853      0.216     -2.250      0.024      -0.908      -0.063\n",
      "588            0.1172      0.113      1.035      0.301      -0.105       0.339\n",
      "==============================================================================\n",
      "Removing feature 85 with p-value 0.344989\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181617\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1526\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2561\n",
      "Time:                        16:31:40   Log-Likelihood:                -284.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.937e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6685      0.193    -19.044      0.000      -4.046      -3.291\n",
      "22             0.6652      0.188      3.534      0.000       0.296       1.034\n",
      "34             0.1577      0.085      1.861      0.063      -0.008       0.324\n",
      "60             0.8932      0.171      5.238      0.000       0.559       1.227\n",
      "65             0.4809      0.125      3.846      0.000       0.236       0.726\n",
      "68            -0.3212      0.186     -1.729      0.084      -0.685       0.043\n",
      "74            -0.7917      0.398     -1.988      0.047      -1.572      -0.011\n",
      "79             0.3254      0.148      2.204      0.028       0.036       0.615\n",
      "80            -0.1562      0.127     -1.232      0.218      -0.405       0.092\n",
      "82             0.1841      0.114      1.620      0.105      -0.039       0.407\n",
      "103           -0.1819      0.122     -1.494      0.135      -0.420       0.057\n",
      "104            0.2110      0.120      1.753      0.080      -0.025       0.447\n",
      "113            0.2416      0.210      1.148      0.251      -0.171       0.654\n",
      "116           -0.1550      0.112     -1.378      0.168      -0.375       0.065\n",
      "125            0.3238      0.150      2.155      0.031       0.029       0.618\n",
      "130            0.5260      0.158      3.336      0.001       0.217       0.835\n",
      "134            0.1609      0.159      1.012      0.311      -0.151       0.472\n",
      "139            0.1727      0.121      1.428      0.153      -0.064       0.410\n",
      "147           -0.2905      0.136     -2.134      0.033      -0.557      -0.024\n",
      "151           -0.2495      0.129     -1.936      0.053      -0.502       0.003\n",
      "173            0.2441      0.124      1.965      0.049       0.001       0.488\n",
      "189            0.1500      0.116      1.298      0.194      -0.076       0.376\n",
      "219           -0.1233      0.119     -1.040      0.298      -0.356       0.109\n",
      "240            0.2016      0.121      1.670      0.095      -0.035       0.438\n",
      "394            0.1477      0.135      1.094      0.274      -0.117       0.412\n",
      "417           -0.3045      0.143     -2.125      0.034      -0.585      -0.024\n",
      "424            0.2276      0.118      1.934      0.053      -0.003       0.458\n",
      "431           -0.3106      0.147     -2.112      0.035      -0.599      -0.022\n",
      "434            0.1329      0.116      1.143      0.253      -0.095       0.361\n",
      "438            0.2868      0.112      2.550      0.011       0.066       0.507\n",
      "470            0.2904      0.195      1.492      0.136      -0.091       0.672\n",
      "489           -0.2547      0.119     -2.135      0.033      -0.488      -0.021\n",
      "512            0.2517      0.110      2.288      0.022       0.036       0.467\n",
      "524           -0.2556      0.243     -1.052      0.293      -0.732       0.221\n",
      "548            0.2100      0.125      1.675      0.094      -0.036       0.456\n",
      "549           -0.1813      0.123     -1.475      0.140      -0.422       0.060\n",
      "562            0.1473      0.110      1.335      0.182      -0.069       0.364\n",
      "563           -0.2963      0.127     -2.331      0.020      -0.545      -0.047\n",
      "564           -0.2840      0.151     -1.884      0.060      -0.580       0.011\n",
      "576           -0.4803      0.214     -2.244      0.025      -0.900      -0.061\n",
      "588            0.1166      0.113      1.032      0.302      -0.105       0.338\n",
      "==============================================================================\n",
      "Removing feature 134 with p-value 0.311381\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181943\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1527\n",
      "Method:                           MLE   Df Model:                           39\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2548\n",
      "Time:                        16:31:40   Log-Likelihood:                -285.10\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.292e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6509      0.190    -19.176      0.000      -4.024      -3.278\n",
      "22             0.6584      0.188      3.507      0.000       0.290       1.026\n",
      "34             0.1515      0.085      1.787      0.074      -0.015       0.318\n",
      "60             0.8979      0.170      5.283      0.000       0.565       1.231\n",
      "65             0.4921      0.125      3.950      0.000       0.248       0.736\n",
      "68            -0.3170      0.185     -1.712      0.087      -0.680       0.046\n",
      "74            -0.7929      0.400     -1.985      0.047      -1.576      -0.010\n",
      "79             0.3251      0.147      2.209      0.027       0.037       0.613\n",
      "80            -0.1452      0.126     -1.150      0.250      -0.392       0.102\n",
      "82             0.1758      0.114      1.544      0.123      -0.047       0.399\n",
      "103           -0.1752      0.122     -1.441      0.150      -0.413       0.063\n",
      "104            0.2110      0.120      1.756      0.079      -0.025       0.447\n",
      "113            0.2435      0.210      1.157      0.247      -0.169       0.656\n",
      "116           -0.1581      0.112     -1.407      0.159      -0.378       0.062\n",
      "125            0.4133      0.121      3.406      0.001       0.175       0.651\n",
      "130            0.5042      0.156      3.235      0.001       0.199       0.810\n",
      "139            0.1665      0.121      1.377      0.169      -0.070       0.404\n",
      "147           -0.2850      0.136     -2.089      0.037      -0.552      -0.018\n",
      "151           -0.2512      0.129     -1.952      0.051      -0.503       0.001\n",
      "173            0.2411      0.124      1.944      0.052      -0.002       0.484\n",
      "189            0.1455      0.115      1.264      0.206      -0.080       0.371\n",
      "219           -0.1294      0.118     -1.096      0.273      -0.361       0.102\n",
      "240            0.2044      0.119      1.723      0.085      -0.028       0.437\n",
      "394            0.1329      0.135      0.988      0.323      -0.131       0.397\n",
      "417           -0.2993      0.143     -2.088      0.037      -0.580      -0.018\n",
      "424            0.2306      0.118      1.957      0.050      -0.000       0.462\n",
      "431           -0.3132      0.147     -2.134      0.033      -0.601      -0.026\n",
      "434            0.1383      0.116      1.193      0.233      -0.089       0.365\n",
      "438            0.2886      0.112      2.567      0.010       0.068       0.509\n",
      "470            0.2756      0.193      1.431      0.152      -0.102       0.653\n",
      "489           -0.2485      0.119     -2.093      0.036      -0.481      -0.016\n",
      "512            0.2488      0.110      2.266      0.023       0.034       0.464\n",
      "524           -0.2590      0.245     -1.056      0.291      -0.740       0.222\n",
      "548            0.2033      0.124      1.636      0.102      -0.040       0.447\n",
      "549           -0.1874      0.123     -1.523      0.128      -0.429       0.054\n",
      "562            0.1513      0.110      1.372      0.170      -0.065       0.367\n",
      "563           -0.3073      0.127     -2.427      0.015      -0.556      -0.059\n",
      "564           -0.2874      0.151     -1.906      0.057      -0.583       0.008\n",
      "576           -0.4765      0.213     -2.241      0.025      -0.893      -0.060\n",
      "588            0.1068      0.114      0.940      0.347      -0.116       0.329\n",
      "==============================================================================\n",
      "Removing feature 588 with p-value 0.347339\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182213\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1528\n",
      "Method:                           MLE   Df Model:                           38\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2537\n",
      "Time:                        16:31:40   Log-Likelihood:                -285.53\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.959e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6480      0.190    -19.194      0.000      -4.020      -3.275\n",
      "22             0.6687      0.188      3.562      0.000       0.301       1.037\n",
      "34             0.1504      0.085      1.775      0.076      -0.016       0.316\n",
      "60             0.8981      0.170      5.286      0.000       0.565       1.231\n",
      "65             0.4994      0.124      4.017      0.000       0.256       0.743\n",
      "68            -0.3144      0.182     -1.730      0.084      -0.671       0.042\n",
      "74            -0.7893      0.396     -1.991      0.046      -1.566      -0.012\n",
      "79             0.3203      0.147      2.179      0.029       0.032       0.608\n",
      "80            -0.1384      0.126     -1.101      0.271      -0.385       0.108\n",
      "82             0.1816      0.114      1.594      0.111      -0.042       0.405\n",
      "103           -0.1661      0.122     -1.367      0.172      -0.404       0.072\n",
      "104            0.2100      0.120      1.754      0.079      -0.025       0.445\n",
      "113            0.2392      0.210      1.138      0.255      -0.173       0.651\n",
      "116           -0.1485      0.112     -1.325      0.185      -0.368       0.071\n",
      "125            0.4190      0.121      3.454      0.001       0.181       0.657\n",
      "130            0.5040      0.155      3.244      0.001       0.200       0.808\n",
      "139            0.1560      0.120      1.303      0.193      -0.079       0.391\n",
      "147           -0.2814      0.136     -2.068      0.039      -0.548      -0.015\n",
      "151           -0.2462      0.129     -1.913      0.056      -0.498       0.006\n",
      "173            0.2506      0.124      2.021      0.043       0.008       0.494\n",
      "189            0.1453      0.115      1.264      0.206      -0.080       0.371\n",
      "219           -0.1340      0.118     -1.137      0.255      -0.365       0.097\n",
      "240            0.2043      0.117      1.740      0.082      -0.026       0.434\n",
      "394            0.1324      0.135      0.981      0.327      -0.132       0.397\n",
      "417           -0.2925      0.143     -2.047      0.041      -0.573      -0.012\n",
      "424            0.2235      0.118      1.902      0.057      -0.007       0.454\n",
      "431           -0.3167      0.147     -2.155      0.031      -0.605      -0.029\n",
      "434            0.1397      0.116      1.203      0.229      -0.088       0.367\n",
      "438            0.2918      0.113      2.578      0.010       0.070       0.514\n",
      "470            0.2728      0.188      1.453      0.146      -0.095       0.641\n",
      "489           -0.2475      0.119     -2.085      0.037      -0.480      -0.015\n",
      "512            0.2505      0.110      2.283      0.022       0.035       0.466\n",
      "524           -0.2635      0.242     -1.090      0.276      -0.737       0.210\n",
      "548            0.2123      0.124      1.710      0.087      -0.031       0.456\n",
      "549           -0.1918      0.123     -1.559      0.119      -0.433       0.049\n",
      "562            0.1491      0.110      1.353      0.176      -0.067       0.365\n",
      "563           -0.3153      0.126     -2.501      0.012      -0.562      -0.068\n",
      "564           -0.2779      0.150     -1.849      0.065      -0.573       0.017\n",
      "576           -0.4856      0.213     -2.280      0.023      -0.903      -0.068\n",
      "==============================================================================\n",
      "Removing feature 394 with p-value 0.326751\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182516\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1529\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2524\n",
      "Time:                        16:31:40   Log-Likelihood:                -286.00\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.071e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6330      0.188    -19.284      0.000      -4.002      -3.264\n",
      "22             0.6404      0.185      3.454      0.001       0.277       1.004\n",
      "34             0.1444      0.085      1.701      0.089      -0.022       0.311\n",
      "60             0.8952      0.171      5.245      0.000       0.561       1.230\n",
      "65             0.4982      0.125      3.994      0.000       0.254       0.743\n",
      "68            -0.3100      0.181     -1.713      0.087      -0.665       0.045\n",
      "74            -0.7636      0.396     -1.930      0.054      -1.539       0.012\n",
      "79             0.2941      0.145      2.025      0.043       0.009       0.579\n",
      "80            -0.1439      0.125     -1.148      0.251      -0.390       0.102\n",
      "82             0.1771      0.113      1.564      0.118      -0.045       0.399\n",
      "103           -0.1570      0.123     -1.281      0.200      -0.397       0.083\n",
      "104            0.2147      0.120      1.788      0.074      -0.021       0.450\n",
      "113            0.2374      0.211      1.124      0.261      -0.177       0.651\n",
      "116           -0.1445      0.112     -1.293      0.196      -0.364       0.075\n",
      "125            0.4118      0.121      3.413      0.001       0.175       0.648\n",
      "130            0.5150      0.154      3.341      0.001       0.213       0.817\n",
      "139            0.1542      0.120      1.289      0.198      -0.080       0.389\n",
      "147           -0.2661      0.134     -1.981      0.048      -0.529      -0.003\n",
      "151           -0.2396      0.128     -1.865      0.062      -0.491       0.012\n",
      "173            0.2520      0.124      2.032      0.042       0.009       0.495\n",
      "189            0.1465      0.115      1.272      0.203      -0.079       0.372\n",
      "219           -0.1334      0.118     -1.133      0.257      -0.364       0.097\n",
      "240            0.2137      0.116      1.840      0.066      -0.014       0.441\n",
      "417           -0.2890      0.143     -2.027      0.043      -0.568      -0.010\n",
      "424            0.2127      0.118      1.805      0.071      -0.018       0.444\n",
      "431           -0.3058      0.146     -2.090      0.037      -0.592      -0.019\n",
      "434            0.1345      0.115      1.165      0.244      -0.092       0.361\n",
      "438            0.2717      0.108      2.513      0.012       0.060       0.484\n",
      "470            0.2642      0.185      1.431      0.152      -0.098       0.626\n",
      "489           -0.2423      0.118     -2.047      0.041      -0.474      -0.010\n",
      "512            0.2569      0.110      2.344      0.019       0.042       0.472\n",
      "524           -0.2740      0.243     -1.126      0.260      -0.751       0.203\n",
      "548            0.2113      0.124      1.704      0.088      -0.032       0.454\n",
      "549           -0.1846      0.123     -1.506      0.132      -0.425       0.056\n",
      "562            0.1449      0.110      1.319      0.187      -0.070       0.360\n",
      "563           -0.3192      0.126     -2.538      0.011      -0.566      -0.073\n",
      "564           -0.2783      0.150     -1.854      0.064      -0.572       0.016\n",
      "576           -0.4757      0.214     -2.226      0.026      -0.895      -0.057\n",
      "==============================================================================\n",
      "Removing feature 113 with p-value 0.261146\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182936\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1530\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2507\n",
      "Time:                        16:31:40   Log-Likelihood:                -286.66\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.717e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6214      0.187    -19.324      0.000      -3.989      -3.254\n",
      "22             0.6434      0.186      3.465      0.001       0.280       1.007\n",
      "34             0.1319      0.083      1.580      0.114      -0.032       0.295\n",
      "60             0.9396      0.166      5.646      0.000       0.613       1.266\n",
      "65             0.5281      0.122      4.334      0.000       0.289       0.767\n",
      "68            -0.3218      0.179     -1.795      0.073      -0.673       0.030\n",
      "74            -0.7564      0.399     -1.895      0.058      -1.539       0.026\n",
      "79             0.2939      0.145      2.027      0.043       0.010       0.578\n",
      "80            -0.1274      0.124     -1.026      0.305      -0.371       0.116\n",
      "82             0.1681      0.113      1.489      0.136      -0.053       0.389\n",
      "103           -0.1611      0.123     -1.312      0.190      -0.402       0.080\n",
      "104            0.2136      0.120      1.773      0.076      -0.023       0.450\n",
      "116           -0.1436      0.112     -1.287      0.198      -0.362       0.075\n",
      "125            0.4157      0.121      3.449      0.001       0.180       0.652\n",
      "130            0.5146      0.155      3.330      0.001       0.212       0.818\n",
      "139            0.1594      0.119      1.335      0.182      -0.075       0.393\n",
      "147           -0.2675      0.134     -1.991      0.046      -0.531      -0.004\n",
      "151           -0.2441      0.129     -1.888      0.059      -0.497       0.009\n",
      "173            0.2509      0.124      2.022      0.043       0.008       0.494\n",
      "189            0.1429      0.115      1.239      0.215      -0.083       0.369\n",
      "219           -0.1314      0.118     -1.116      0.265      -0.362       0.099\n",
      "240            0.2153      0.116      1.857      0.063      -0.012       0.443\n",
      "417           -0.2946      0.143     -2.065      0.039      -0.574      -0.015\n",
      "424            0.2107      0.119      1.766      0.077      -0.023       0.445\n",
      "431           -0.3186      0.145     -2.197      0.028      -0.603      -0.034\n",
      "434            0.1367      0.115      1.184      0.237      -0.090       0.363\n",
      "438            0.2646      0.106      2.503      0.012       0.057       0.472\n",
      "470            0.2652      0.183      1.446      0.148      -0.094       0.625\n",
      "489           -0.2347      0.118     -1.990      0.047      -0.466      -0.003\n",
      "512            0.2545      0.109      2.329      0.020       0.040       0.469\n",
      "524           -0.2997      0.240     -1.246      0.213      -0.771       0.172\n",
      "548            0.2106      0.123      1.706      0.088      -0.031       0.453\n",
      "549           -0.1845      0.123     -1.504      0.133      -0.425       0.056\n",
      "562            0.1409      0.110      1.285      0.199      -0.074       0.356\n",
      "563           -0.3179      0.126     -2.525      0.012      -0.565      -0.071\n",
      "564           -0.2809      0.150     -1.872      0.061      -0.575       0.013\n",
      "576           -0.4766      0.213     -2.233      0.026      -0.895      -0.058\n",
      "==============================================================================\n",
      "Removing feature 80 with p-value 0.304900\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183276\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1531\n",
      "Method:                           MLE   Df Model:                           35\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2493\n",
      "Time:                        16:31:40   Log-Likelihood:                -287.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.439e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6142      0.187    -19.350      0.000      -3.980      -3.248\n",
      "22             0.6387      0.185      3.446      0.001       0.275       1.002\n",
      "34             0.1296      0.084      1.546      0.122      -0.035       0.294\n",
      "60             0.8725      0.153      5.713      0.000       0.573       1.172\n",
      "65             0.4878      0.115      4.247      0.000       0.263       0.713\n",
      "68            -0.3071      0.179     -1.715      0.086      -0.658       0.044\n",
      "74            -0.7759      0.395     -1.963      0.050      -1.550      -0.001\n",
      "79             0.3160      0.143      2.206      0.027       0.035       0.597\n",
      "82             0.1643      0.113      1.453      0.146      -0.057       0.386\n",
      "103           -0.1627      0.122     -1.332      0.183      -0.402       0.077\n",
      "104            0.2147      0.121      1.780      0.075      -0.022       0.451\n",
      "116           -0.1474      0.112     -1.319      0.187      -0.367       0.072\n",
      "125            0.4114      0.121      3.411      0.001       0.175       0.648\n",
      "130            0.5178      0.155      3.347      0.001       0.215       0.821\n",
      "139            0.1399      0.117      1.193      0.233      -0.090       0.370\n",
      "147           -0.2694      0.135     -1.990      0.047      -0.535      -0.004\n",
      "151           -0.2430      0.129     -1.887      0.059      -0.496       0.009\n",
      "173            0.2530      0.124      2.041      0.041       0.010       0.496\n",
      "189            0.1481      0.115      1.290      0.197      -0.077       0.373\n",
      "219           -0.1277      0.118     -1.085      0.278      -0.359       0.103\n",
      "240            0.2094      0.115      1.820      0.069      -0.016       0.435\n",
      "417           -0.2982      0.141     -2.109      0.035      -0.575      -0.021\n",
      "424            0.2086      0.120      1.742      0.081      -0.026       0.443\n",
      "431           -0.3203      0.145     -2.208      0.027      -0.605      -0.036\n",
      "434            0.1336      0.115      1.157      0.247      -0.093       0.360\n",
      "438            0.2662      0.106      2.518      0.012       0.059       0.473\n",
      "470            0.2854      0.184      1.548      0.122      -0.076       0.647\n",
      "489           -0.2381      0.118     -2.018      0.044      -0.469      -0.007\n",
      "512            0.2548      0.109      2.335      0.020       0.041       0.469\n",
      "524           -0.2679      0.236     -1.137      0.256      -0.730       0.194\n",
      "548            0.2223      0.122      1.821      0.069      -0.017       0.462\n",
      "549           -0.1931      0.122     -1.577      0.115      -0.433       0.047\n",
      "562            0.1356      0.110      1.237      0.216      -0.079       0.351\n",
      "563           -0.3137      0.126     -2.496      0.013      -0.560      -0.067\n",
      "564           -0.2792      0.150     -1.864      0.062      -0.573       0.014\n",
      "576           -0.4823      0.214     -2.251      0.024      -0.902      -0.062\n",
      "==============================================================================\n",
      "Removing feature 219 with p-value 0.278110\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183660\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1532\n",
      "Method:                           MLE   Df Model:                           34\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2477\n",
      "Time:                        16:31:40   Log-Likelihood:                -287.80\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.677e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6038      0.185    -19.433      0.000      -3.967      -3.240\n",
      "22             0.6347      0.185      3.438      0.001       0.273       0.997\n",
      "34             0.1282      0.083      1.538      0.124      -0.035       0.292\n",
      "60             0.8675      0.152      5.696      0.000       0.569       1.166\n",
      "65             0.4788      0.115      4.177      0.000       0.254       0.704\n",
      "68            -0.3155      0.178     -1.772      0.076      -0.664       0.034\n",
      "74            -0.7933      0.393     -2.019      0.043      -1.563      -0.023\n",
      "79             0.3193      0.144      2.223      0.026       0.038       0.601\n",
      "82             0.1708      0.113      1.516      0.129      -0.050       0.392\n",
      "103           -0.1682      0.122     -1.379      0.168      -0.407       0.071\n",
      "104            0.2093      0.122      1.721      0.085      -0.029       0.448\n",
      "116           -0.1520      0.112     -1.357      0.175      -0.372       0.068\n",
      "125            0.4083      0.120      3.397      0.001       0.173       0.644\n",
      "130            0.5102      0.154      3.313      0.001       0.208       0.812\n",
      "139            0.1311      0.117      1.122      0.262      -0.098       0.360\n",
      "147           -0.2621      0.135     -1.939      0.053      -0.527       0.003\n",
      "151           -0.2412      0.129     -1.872      0.061      -0.494       0.011\n",
      "173            0.2531      0.124      2.042      0.041       0.010       0.496\n",
      "189            0.1468      0.115      1.281      0.200      -0.078       0.371\n",
      "240            0.1997      0.113      1.760      0.078      -0.023       0.422\n",
      "417           -0.2952      0.141     -2.088      0.037      -0.572      -0.018\n",
      "424            0.2132      0.120      1.775      0.076      -0.022       0.449\n",
      "431           -0.3115      0.144     -2.158      0.031      -0.595      -0.029\n",
      "434            0.1341      0.116      1.161      0.246      -0.092       0.360\n",
      "438            0.2649      0.106      2.488      0.013       0.056       0.474\n",
      "470            0.3051      0.185      1.647      0.100      -0.058       0.668\n",
      "489           -0.2390      0.118     -2.027      0.043      -0.470      -0.008\n",
      "512            0.2501      0.109      2.295      0.022       0.036       0.464\n",
      "524           -0.2643      0.234     -1.132      0.258      -0.722       0.193\n",
      "548            0.2197      0.121      1.817      0.069      -0.017       0.457\n",
      "549           -0.1921      0.122     -1.568      0.117      -0.432       0.048\n",
      "562            0.1360      0.110      1.241      0.214      -0.079       0.351\n",
      "563           -0.3040      0.125     -2.434      0.015      -0.549      -0.059\n",
      "564           -0.2725      0.149     -1.832      0.067      -0.564       0.019\n",
      "576           -0.4899      0.214     -2.284      0.022      -0.910      -0.070\n",
      "==============================================================================\n",
      "Removing feature 139 with p-value 0.262019\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184051\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1533\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2461\n",
      "Time:                        16:31:40   Log-Likelihood:                -288.41\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.154e-23\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5960      0.185    -19.437      0.000      -3.959      -3.233\n",
      "22             0.6383      0.184      3.472      0.001       0.278       0.999\n",
      "34             0.1269      0.083      1.527      0.127      -0.036       0.290\n",
      "60             0.8863      0.151      5.874      0.000       0.591       1.182\n",
      "65             0.4937      0.114      4.337      0.000       0.271       0.717\n",
      "68            -0.3251      0.179     -1.817      0.069      -0.676       0.026\n",
      "74            -0.7845      0.393     -1.996      0.046      -1.555      -0.014\n",
      "79             0.3148      0.144      2.190      0.029       0.033       0.597\n",
      "82             0.1647      0.113      1.462      0.144      -0.056       0.385\n",
      "103           -0.1647      0.121     -1.367      0.172      -0.401       0.071\n",
      "104            0.2137      0.121      1.765      0.078      -0.024       0.451\n",
      "116           -0.1511      0.112     -1.347      0.178      -0.371       0.069\n",
      "125            0.4060      0.120      3.394      0.001       0.172       0.640\n",
      "130            0.5081      0.154      3.296      0.001       0.206       0.810\n",
      "147           -0.2524      0.135     -1.863      0.062      -0.518       0.013\n",
      "151           -0.2444      0.129     -1.893      0.058      -0.497       0.009\n",
      "173            0.2657      0.124      2.144      0.032       0.023       0.509\n",
      "189            0.1393      0.114      1.220      0.222      -0.084       0.363\n",
      "240            0.1976      0.114      1.738      0.082      -0.025       0.421\n",
      "417           -0.2574      0.135     -1.901      0.057      -0.523       0.008\n",
      "424            0.2187      0.119      1.836      0.066      -0.015       0.452\n",
      "431           -0.3154      0.144     -2.185      0.029      -0.598      -0.032\n",
      "434            0.1313      0.115      1.140      0.254      -0.094       0.357\n",
      "438            0.2675      0.107      2.502      0.012       0.058       0.477\n",
      "470            0.2987      0.186      1.609      0.108      -0.065       0.663\n",
      "489           -0.2342      0.118     -1.992      0.046      -0.465      -0.004\n",
      "512            0.2504      0.109      2.296      0.022       0.037       0.464\n",
      "524           -0.2756      0.235     -1.174      0.240      -0.736       0.185\n",
      "548            0.2215      0.121      1.830      0.067      -0.016       0.459\n",
      "549           -0.1839      0.122     -1.509      0.131      -0.423       0.055\n",
      "562            0.1386      0.109      1.270      0.204      -0.075       0.352\n",
      "563           -0.3007      0.125     -2.413      0.016      -0.545      -0.056\n",
      "564           -0.2735      0.148     -1.842      0.065      -0.564       0.018\n",
      "576           -0.5087      0.216     -2.354      0.019      -0.932      -0.085\n",
      "==============================================================================\n",
      "Removing feature 434 with p-value 0.254083\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184454\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1534\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2445\n",
      "Time:                        16:31:40   Log-Likelihood:                -289.04\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.974e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5878      0.184    -19.487      0.000      -3.949      -3.227\n",
      "22             0.6883      0.178      3.876      0.000       0.340       1.036\n",
      "34             0.1229      0.083      1.481      0.139      -0.040       0.286\n",
      "60             0.8827      0.150      5.871      0.000       0.588       1.177\n",
      "65             0.4993      0.114      4.385      0.000       0.276       0.722\n",
      "68            -0.3366      0.175     -1.924      0.054      -0.679       0.006\n",
      "74            -0.7690      0.389     -1.975      0.048      -1.532      -0.006\n",
      "79             0.3330      0.143      2.325      0.020       0.052       0.614\n",
      "82             0.1620      0.113      1.437      0.151      -0.059       0.383\n",
      "103           -0.1705      0.120     -1.422      0.155      -0.406       0.065\n",
      "104            0.2103      0.121      1.736      0.082      -0.027       0.448\n",
      "116           -0.1601      0.112     -1.434      0.152      -0.379       0.059\n",
      "125            0.4062      0.120      3.391      0.001       0.171       0.641\n",
      "130            0.5141      0.154      3.337      0.001       0.212       0.816\n",
      "147           -0.2564      0.136     -1.878      0.060      -0.524       0.011\n",
      "151           -0.2451      0.129     -1.897      0.058      -0.498       0.008\n",
      "173            0.2600      0.124      2.102      0.036       0.018       0.502\n",
      "189            0.1311      0.114      1.152      0.249      -0.092       0.354\n",
      "240            0.1986      0.114      1.739      0.082      -0.025       0.422\n",
      "417           -0.2519      0.135     -1.871      0.061      -0.516       0.012\n",
      "424            0.2210      0.120      1.841      0.066      -0.014       0.456\n",
      "431           -0.3391      0.143     -2.376      0.018      -0.619      -0.059\n",
      "438            0.2728      0.106      2.573      0.010       0.065       0.481\n",
      "470            0.3106      0.181      1.717      0.086      -0.044       0.665\n",
      "489           -0.2337      0.118     -1.982      0.047      -0.465      -0.003\n",
      "512            0.2532      0.109      2.323      0.020       0.040       0.467\n",
      "524           -0.2861      0.231     -1.236      0.216      -0.739       0.167\n",
      "548            0.2181      0.121      1.807      0.071      -0.018       0.455\n",
      "549           -0.1901      0.122     -1.558      0.119      -0.429       0.049\n",
      "562            0.1397      0.109      1.278      0.201      -0.075       0.354\n",
      "563           -0.3002      0.125     -2.405      0.016      -0.545      -0.056\n",
      "564           -0.2686      0.148     -1.812      0.070      -0.559       0.022\n",
      "576           -0.5069      0.217     -2.340      0.019      -0.932      -0.082\n",
      "==============================================================================\n",
      "Removing feature 189 with p-value 0.249346\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184871\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1535\n",
      "Method:                           MLE   Df Model:                           31\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2428\n",
      "Time:                        16:31:40   Log-Likelihood:                -289.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.567e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5817      0.184    -19.497      0.000      -3.942      -3.222\n",
      "22             0.6893      0.177      3.891      0.000       0.342       1.036\n",
      "34             0.1182      0.083      1.427      0.153      -0.044       0.280\n",
      "60             0.9062      0.149      6.065      0.000       0.613       1.199\n",
      "65             0.5165      0.113      4.575      0.000       0.295       0.738\n",
      "68            -0.3409      0.176     -1.939      0.053      -0.685       0.004\n",
      "74            -0.7487      0.388     -1.929      0.054      -1.510       0.012\n",
      "79             0.3476      0.143      2.434      0.015       0.068       0.628\n",
      "82             0.1590      0.112      1.413      0.158      -0.062       0.379\n",
      "103           -0.1725      0.120     -1.443      0.149      -0.407       0.062\n",
      "104            0.2079      0.122      1.708      0.088      -0.031       0.446\n",
      "116           -0.1588      0.112     -1.422      0.155      -0.378       0.060\n",
      "125            0.4120      0.120      3.443      0.001       0.177       0.647\n",
      "130            0.5162      0.154      3.345      0.001       0.214       0.819\n",
      "147           -0.2599      0.136     -1.905      0.057      -0.527       0.007\n",
      "151           -0.2469      0.129     -1.907      0.056      -0.501       0.007\n",
      "173            0.2573      0.123      2.085      0.037       0.015       0.499\n",
      "240            0.2047      0.111      1.839      0.066      -0.013       0.423\n",
      "417           -0.2376      0.134     -1.775      0.076      -0.500       0.025\n",
      "424            0.2212      0.121      1.834      0.067      -0.015       0.458\n",
      "431           -0.3446      0.142     -2.427      0.015      -0.623      -0.066\n",
      "438            0.2695      0.107      2.516      0.012       0.060       0.480\n",
      "470            0.3028      0.183      1.659      0.097      -0.055       0.661\n",
      "489           -0.2332      0.118     -1.984      0.047      -0.464      -0.003\n",
      "512            0.2586      0.109      2.376      0.017       0.045       0.472\n",
      "524           -0.3031      0.230     -1.316      0.188      -0.755       0.148\n",
      "548            0.2187      0.121      1.810      0.070      -0.018       0.455\n",
      "549           -0.1871      0.122     -1.532      0.126      -0.426       0.052\n",
      "562            0.1369      0.110      1.248      0.212      -0.078       0.352\n",
      "563           -0.2955      0.124     -2.376      0.017      -0.539      -0.052\n",
      "564           -0.2689      0.148     -1.815      0.070      -0.559       0.022\n",
      "576           -0.5190      0.217     -2.393      0.017      -0.944      -0.094\n",
      "==============================================================================\n",
      "Removing feature 562 with p-value 0.212162\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.185357\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1536\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2408\n",
      "Time:                        16:31:40   Log-Likelihood:                -290.46\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.216e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5681      0.182    -19.589      0.000      -3.925      -3.211\n",
      "22             0.6722      0.176      3.814      0.000       0.327       1.018\n",
      "34             0.1105      0.083      1.338      0.181      -0.051       0.272\n",
      "60             0.9050      0.150      6.045      0.000       0.612       1.198\n",
      "65             0.5159      0.113      4.576      0.000       0.295       0.737\n",
      "68            -0.3405      0.175     -1.947      0.052      -0.683       0.002\n",
      "74            -0.7405      0.392     -1.887      0.059      -1.510       0.029\n",
      "79             0.3629      0.142      2.548      0.011       0.084       0.642\n",
      "82             0.1595      0.113      1.414      0.157      -0.062       0.381\n",
      "103           -0.1713      0.118     -1.446      0.148      -0.404       0.061\n",
      "104            0.2160      0.122      1.774      0.076      -0.023       0.455\n",
      "116           -0.1578      0.111     -1.420      0.156      -0.376       0.060\n",
      "125            0.4147      0.119      3.471      0.001       0.181       0.649\n",
      "130            0.5099      0.153      3.323      0.001       0.209       0.811\n",
      "147           -0.2542      0.136     -1.864      0.062      -0.522       0.013\n",
      "151           -0.2454      0.129     -1.899      0.058      -0.499       0.008\n",
      "173            0.2486      0.123      2.019      0.043       0.007       0.490\n",
      "240            0.1961      0.111      1.772      0.076      -0.021       0.413\n",
      "417           -0.2355      0.134     -1.760      0.078      -0.498       0.027\n",
      "424            0.2246      0.119      1.887      0.059      -0.009       0.458\n",
      "431           -0.3382      0.141     -2.398      0.016      -0.615      -0.062\n",
      "438            0.2766      0.110      2.507      0.012       0.060       0.493\n",
      "470            0.3105      0.182      1.710      0.087      -0.045       0.666\n",
      "489           -0.2240      0.117     -1.915      0.055      -0.453       0.005\n",
      "512            0.2505      0.109      2.306      0.021       0.038       0.463\n",
      "524           -0.3150      0.231     -1.361      0.174      -0.769       0.139\n",
      "548            0.2270      0.121      1.872      0.061      -0.011       0.465\n",
      "549           -0.1868      0.122     -1.531      0.126      -0.426       0.052\n",
      "563           -0.2967      0.124     -2.392      0.017      -0.540      -0.054\n",
      "564           -0.2583      0.148     -1.749      0.080      -0.548       0.031\n",
      "576           -0.5222      0.215     -2.424      0.015      -0.944      -0.100\n",
      "==============================================================================\n",
      "Removing feature 34 with p-value 0.180924\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.185861\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1537\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2387\n",
      "Time:                        16:31:40   Log-Likelihood:                -291.24\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.235e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5653      0.181    -19.652      0.000      -3.921      -3.210\n",
      "22             0.6760      0.176      3.843      0.000       0.331       1.021\n",
      "60             0.9303      0.150      6.197      0.000       0.636       1.225\n",
      "65             0.5129      0.113      4.543      0.000       0.292       0.734\n",
      "68            -0.2961      0.171     -1.730      0.084      -0.632       0.039\n",
      "74            -0.8574      0.383     -2.238      0.025      -1.608      -0.107\n",
      "79             0.3715      0.142      2.613      0.009       0.093       0.650\n",
      "82             0.1556      0.113      1.381      0.167      -0.065       0.376\n",
      "103           -0.1643      0.119     -1.380      0.168      -0.398       0.069\n",
      "104            0.2077      0.123      1.686      0.092      -0.034       0.449\n",
      "116           -0.1499      0.111     -1.350      0.177      -0.367       0.068\n",
      "125            0.4275      0.119      3.594      0.000       0.194       0.661\n",
      "130            0.5103      0.153      3.328      0.001       0.210       0.811\n",
      "147           -0.2499      0.135     -1.844      0.065      -0.515       0.016\n",
      "151           -0.2481      0.129     -1.925      0.054      -0.501       0.005\n",
      "173            0.2165      0.120      1.801      0.072      -0.019       0.452\n",
      "240            0.1940      0.111      1.743      0.081      -0.024       0.412\n",
      "417           -0.2293      0.133     -1.720      0.085      -0.491       0.032\n",
      "424            0.2284      0.118      1.928      0.054      -0.004       0.461\n",
      "431           -0.3441      0.141     -2.443      0.015      -0.620      -0.068\n",
      "438            0.2774      0.110      2.511      0.012       0.061       0.494\n",
      "470            0.3068      0.183      1.680      0.093      -0.051       0.665\n",
      "489           -0.2327      0.117     -1.996      0.046      -0.461      -0.004\n",
      "512            0.2513      0.108      2.320      0.020       0.039       0.464\n",
      "524           -0.3216      0.230     -1.397      0.162      -0.773       0.130\n",
      "548            0.2226      0.120      1.853      0.064      -0.013       0.458\n",
      "549           -0.1833      0.122     -1.504      0.133      -0.422       0.056\n",
      "563           -0.2994      0.124     -2.424      0.015      -0.541      -0.057\n",
      "564           -0.2591      0.147     -1.764      0.078      -0.547       0.029\n",
      "576           -0.5173      0.213     -2.424      0.015      -0.936      -0.099\n",
      "==============================================================================\n",
      "Removing feature 116 with p-value 0.177063\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.186439\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1538\n",
      "Method:                           MLE   Df Model:                           28\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2364\n",
      "Time:                        16:31:40   Log-Likelihood:                -292.15\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.716e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5547      0.180    -19.703      0.000      -3.908      -3.201\n",
      "22             0.6706      0.176      3.819      0.000       0.326       1.015\n",
      "60             0.9366      0.150      6.252      0.000       0.643       1.230\n",
      "65             0.5117      0.113      4.536      0.000       0.291       0.733\n",
      "68            -0.3056      0.173     -1.770      0.077      -0.644       0.033\n",
      "74            -0.8519      0.386     -2.205      0.027      -1.609      -0.095\n",
      "79             0.3729      0.142      2.626      0.009       0.095       0.651\n",
      "82             0.1643      0.113      1.458      0.145      -0.057       0.385\n",
      "103           -0.1590      0.119     -1.332      0.183      -0.393       0.075\n",
      "104            0.2026      0.123      1.646      0.100      -0.039       0.444\n",
      "125            0.4338      0.119      3.650      0.000       0.201       0.667\n",
      "130            0.5180      0.153      3.377      0.001       0.217       0.819\n",
      "147           -0.2500      0.136     -1.840      0.066      -0.516       0.016\n",
      "151           -0.2472      0.129     -1.922      0.055      -0.499       0.005\n",
      "173            0.2130      0.120      1.769      0.077      -0.023       0.449\n",
      "240            0.1931      0.112      1.727      0.084      -0.026       0.412\n",
      "417           -0.2376      0.133     -1.786      0.074      -0.498       0.023\n",
      "424            0.2153      0.118      1.819      0.069      -0.017       0.447\n",
      "431           -0.3359      0.140     -2.399      0.016      -0.610      -0.061\n",
      "438            0.2801      0.111      2.534      0.011       0.063       0.497\n",
      "470            0.3092      0.186      1.663      0.096      -0.055       0.673\n",
      "489           -0.2287      0.116     -1.966      0.049      -0.457      -0.001\n",
      "512            0.2491      0.108      2.305      0.021       0.037       0.461\n",
      "524           -0.3239      0.232     -1.397      0.162      -0.778       0.131\n",
      "548            0.2181      0.120      1.814      0.070      -0.018       0.454\n",
      "549           -0.1772      0.121     -1.461      0.144      -0.415       0.061\n",
      "563           -0.3022      0.123     -2.449      0.014      -0.544      -0.060\n",
      "564           -0.2568      0.146     -1.754      0.080      -0.544       0.030\n",
      "576           -0.5196      0.212     -2.448      0.014      -0.936      -0.104\n",
      "==============================================================================\n",
      "Removing feature 103 with p-value 0.183016\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187002\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1539\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2341\n",
      "Time:                        16:31:40   Log-Likelihood:                -293.03\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.209e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5392      0.179    -19.793      0.000      -3.890      -3.189\n",
      "22             0.6700      0.175      3.834      0.000       0.327       1.013\n",
      "60             0.9216      0.149      6.206      0.000       0.631       1.213\n",
      "65             0.5259      0.113      4.664      0.000       0.305       0.747\n",
      "68            -0.2980      0.174     -1.708      0.088      -0.640       0.044\n",
      "74            -0.8074      0.381     -2.117      0.034      -1.555      -0.060\n",
      "79             0.3641      0.141      2.575      0.010       0.087       0.641\n",
      "82             0.1653      0.113      1.461      0.144      -0.056       0.387\n",
      "104            0.1951      0.124      1.568      0.117      -0.049       0.439\n",
      "125            0.4444      0.119      3.745      0.000       0.212       0.677\n",
      "130            0.5261      0.153      3.437      0.001       0.226       0.826\n",
      "147           -0.2538      0.137     -1.854      0.064      -0.522       0.014\n",
      "151           -0.2483      0.129     -1.932      0.053      -0.500       0.004\n",
      "173            0.2119      0.120      1.767      0.077      -0.023       0.447\n",
      "240            0.1830      0.111      1.655      0.098      -0.034       0.400\n",
      "417           -0.2514      0.133     -1.895      0.058      -0.511       0.009\n",
      "424            0.2173      0.119      1.833      0.067      -0.015       0.450\n",
      "431           -0.3324      0.140     -2.383      0.017      -0.606      -0.059\n",
      "438            0.2843      0.109      2.605      0.009       0.070       0.498\n",
      "470            0.2995      0.193      1.548      0.122      -0.080       0.679\n",
      "489           -0.2315      0.116     -1.990      0.047      -0.459      -0.004\n",
      "512            0.2422      0.108      2.240      0.025       0.030       0.454\n",
      "524           -0.3338      0.229     -1.458      0.145      -0.783       0.115\n",
      "548            0.2178      0.121      1.807      0.071      -0.018       0.454\n",
      "549           -0.1883      0.121     -1.552      0.121      -0.426       0.049\n",
      "563           -0.3040      0.124     -2.461      0.014      -0.546      -0.062\n",
      "564           -0.2554      0.147     -1.744      0.081      -0.543       0.032\n",
      "576           -0.5299      0.212     -2.503      0.012      -0.945      -0.115\n",
      "==============================================================================\n",
      "Removing feature 524 with p-value 0.144760\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187825\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1540\n",
      "Method:                           MLE   Df Model:                           26\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2307\n",
      "Time:                        16:31:40   Log-Likelihood:                -294.32\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.535e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5459      0.180    -19.723      0.000      -3.898      -3.194\n",
      "22             0.6592      0.174      3.789      0.000       0.318       1.000\n",
      "60             0.8716      0.144      6.068      0.000       0.590       1.153\n",
      "65             0.4918      0.110      4.481      0.000       0.277       0.707\n",
      "68            -0.3208      0.167     -1.915      0.055      -0.649       0.007\n",
      "74            -1.1857      0.318     -3.733      0.000      -1.808      -0.563\n",
      "79             0.3501      0.141      2.484      0.013       0.074       0.626\n",
      "82             0.1624      0.113      1.434      0.152      -0.060       0.384\n",
      "104            0.2205      0.121      1.823      0.068      -0.017       0.458\n",
      "125            0.4466      0.118      3.771      0.000       0.214       0.679\n",
      "130            0.5220      0.153      3.418      0.001       0.223       0.821\n",
      "147           -0.2418      0.134     -1.799      0.072      -0.505       0.022\n",
      "151           -0.2515      0.128     -1.962      0.050      -0.503      -0.000\n",
      "173            0.1879      0.118      1.588      0.112      -0.044       0.420\n",
      "240            0.1835      0.111      1.655      0.098      -0.034       0.401\n",
      "417           -0.2546      0.133     -1.918      0.055      -0.515       0.006\n",
      "424            0.2170      0.119      1.826      0.068      -0.016       0.450\n",
      "431           -0.3245      0.139     -2.334      0.020      -0.597      -0.052\n",
      "438            0.2790      0.108      2.583      0.010       0.067       0.491\n",
      "470            0.2993      0.204      1.469      0.142      -0.100       0.699\n",
      "489           -0.2464      0.116     -2.130      0.033      -0.473      -0.020\n",
      "512            0.2381      0.108      2.207      0.027       0.027       0.450\n",
      "548            0.2238      0.120      1.866      0.062      -0.011       0.459\n",
      "549           -0.1866      0.121     -1.543      0.123      -0.424       0.050\n",
      "563           -0.3150      0.123     -2.554      0.011      -0.557      -0.073\n",
      "564           -0.2596      0.146     -1.778      0.075      -0.546       0.026\n",
      "576           -0.4971      0.211     -2.354      0.019      -0.911      -0.083\n",
      "==============================================================================\n",
      "Removing feature 82 with p-value 0.151541\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.188495\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1541\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2279\n",
      "Time:                        16:31:40   Log-Likelihood:                -295.37\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.341e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5441      0.180    -19.660      0.000      -3.897      -3.191\n",
      "22             0.6642      0.174      3.820      0.000       0.323       1.005\n",
      "60             0.8868      0.143      6.190      0.000       0.606       1.168\n",
      "65             0.4996      0.110      4.546      0.000       0.284       0.715\n",
      "68            -0.3312      0.167     -1.985      0.047      -0.658      -0.004\n",
      "74            -1.1786      0.317     -3.716      0.000      -1.800      -0.557\n",
      "79             0.3482      0.142      2.457      0.014       0.070       0.626\n",
      "104            0.2100      0.121      1.740      0.082      -0.027       0.447\n",
      "125            0.4348      0.118      3.689      0.000       0.204       0.666\n",
      "130            0.5092      0.153      3.334      0.001       0.210       0.808\n",
      "147           -0.2516      0.135     -1.865      0.062      -0.516       0.013\n",
      "151           -0.2476      0.127     -1.942      0.052      -0.497       0.002\n",
      "173            0.1946      0.118      1.649      0.099      -0.037       0.426\n",
      "240            0.1843      0.110      1.674      0.094      -0.032       0.400\n",
      "417           -0.2569      0.133     -1.933      0.053      -0.517       0.004\n",
      "424            0.2247      0.118      1.909      0.056      -0.006       0.455\n",
      "431           -0.3282      0.139     -2.357      0.018      -0.601      -0.055\n",
      "438            0.2819      0.108      2.606      0.009       0.070       0.494\n",
      "470            0.2930      0.202      1.454      0.146      -0.102       0.688\n",
      "489           -0.2174      0.116     -1.869      0.062      -0.445       0.011\n",
      "512            0.2469      0.107      2.305      0.021       0.037       0.457\n",
      "548            0.2291      0.121      1.886      0.059      -0.009       0.467\n",
      "549           -0.1843      0.122     -1.516      0.129      -0.423       0.054\n",
      "563           -0.3193      0.123     -2.590      0.010      -0.561      -0.078\n",
      "564           -0.2686      0.145     -1.847      0.065      -0.554       0.016\n",
      "576           -0.5092      0.212     -2.401      0.016      -0.925      -0.094\n",
      "==============================================================================\n",
      "Removing feature 470 with p-value 0.145954\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189273\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1542\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2247\n",
      "Time:                        16:31:40   Log-Likelihood:                -296.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.480e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5334      0.180    -19.637      0.000      -3.886      -3.181\n",
      "22             0.6794      0.173      3.927      0.000       0.340       1.018\n",
      "60             0.9068      0.143      6.333      0.000       0.626       1.187\n",
      "65             0.5114      0.110      4.667      0.000       0.297       0.726\n",
      "68            -0.1498      0.115     -1.299      0.194      -0.376       0.076\n",
      "74            -1.0112      0.299     -3.380      0.001      -1.598      -0.425\n",
      "79             0.3545      0.142      2.505      0.012       0.077       0.632\n",
      "104            0.1969      0.121      1.626      0.104      -0.040       0.434\n",
      "125            0.4354      0.118      3.675      0.000       0.203       0.668\n",
      "130            0.5253      0.152      3.448      0.001       0.227       0.824\n",
      "147           -0.2618      0.135     -1.943      0.052      -0.526       0.002\n",
      "151           -0.2556      0.127     -2.009      0.045      -0.505      -0.006\n",
      "173            0.2143      0.118      1.818      0.069      -0.017       0.445\n",
      "240            0.1893      0.109      1.735      0.083      -0.025       0.403\n",
      "417           -0.2438      0.132     -1.847      0.065      -0.502       0.015\n",
      "424            0.2158      0.118      1.830      0.067      -0.015       0.447\n",
      "431           -0.3433      0.138     -2.479      0.013      -0.615      -0.072\n",
      "438            0.2809      0.107      2.617      0.009       0.071       0.491\n",
      "489           -0.2174      0.116     -1.871      0.061      -0.445       0.010\n",
      "512            0.2527      0.106      2.376      0.017       0.044       0.461\n",
      "548            0.2293      0.121      1.890      0.059      -0.009       0.467\n",
      "549           -0.1933      0.121     -1.596      0.110      -0.431       0.044\n",
      "563           -0.3088      0.123     -2.513      0.012      -0.550      -0.068\n",
      "564           -0.2719      0.145     -1.872      0.061      -0.557       0.013\n",
      "576           -0.5121      0.212     -2.420      0.016      -0.927      -0.097\n",
      "==============================================================================\n",
      "Removing feature 68 with p-value 0.193973\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189667\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1543\n",
      "Method:                           MLE   Df Model:                           23\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2231\n",
      "Time:                        16:31:40   Log-Likelihood:                -297.21\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.534e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5309      0.180    -19.663      0.000      -3.883      -3.179\n",
      "22             0.6824      0.173      3.938      0.000       0.343       1.022\n",
      "60             0.8517      0.135      6.319      0.000       0.587       1.116\n",
      "65             0.4696      0.103      4.542      0.000       0.267       0.672\n",
      "74            -1.2179      0.270     -4.516      0.000      -1.746      -0.689\n",
      "79             0.3327      0.140      2.372      0.018       0.058       0.608\n",
      "104            0.2090      0.119      1.755      0.079      -0.024       0.442\n",
      "125            0.4257      0.118      3.611      0.000       0.195       0.657\n",
      "130            0.5239      0.152      3.454      0.001       0.227       0.821\n",
      "147           -0.2666      0.136     -1.967      0.049      -0.532      -0.001\n",
      "151           -0.2577      0.127     -2.029      0.042      -0.507      -0.009\n",
      "173            0.2116      0.118      1.791      0.073      -0.020       0.443\n",
      "240            0.1824      0.109      1.670      0.095      -0.032       0.396\n",
      "417           -0.2501      0.132     -1.896      0.058      -0.509       0.008\n",
      "424            0.2207      0.117      1.886      0.059      -0.009       0.450\n",
      "431           -0.3397      0.139     -2.451      0.014      -0.611      -0.068\n",
      "438            0.2841      0.108      2.640      0.008       0.073       0.495\n",
      "489           -0.2149      0.116     -1.853      0.064      -0.442       0.012\n",
      "512            0.2514      0.106      2.368      0.018       0.043       0.460\n",
      "548            0.2317      0.121      1.913      0.056      -0.006       0.469\n",
      "549           -0.1894      0.121     -1.566      0.117      -0.426       0.048\n",
      "563           -0.2987      0.123     -2.433      0.015      -0.539      -0.058\n",
      "564           -0.2621      0.144     -1.818      0.069      -0.545       0.020\n",
      "576           -0.5067      0.210     -2.409      0.016      -0.919      -0.094\n",
      "==============================================================================\n",
      "Removing feature 549 with p-value 0.117282\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190478\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1544\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2198\n",
      "Time:                        16:31:40   Log-Likelihood:                -298.48\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.663e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5201      0.179    -19.635      0.000      -3.871      -3.169\n",
      "22             0.6919      0.173      3.998      0.000       0.353       1.031\n",
      "60             0.8381      0.134      6.257      0.000       0.576       1.101\n",
      "65             0.4545      0.102      4.438      0.000       0.254       0.655\n",
      "74            -1.1830      0.268     -4.412      0.000      -1.709      -0.657\n",
      "79             0.3736      0.137      2.719      0.007       0.104       0.643\n",
      "104            0.1944      0.119      1.627      0.104      -0.040       0.429\n",
      "125            0.4275      0.118      3.631      0.000       0.197       0.658\n",
      "130            0.5269      0.153      3.441      0.001       0.227       0.827\n",
      "147           -0.2540      0.133     -1.903      0.057      -0.516       0.008\n",
      "151           -0.2480      0.127     -1.960      0.050      -0.496    2.05e-05\n",
      "173            0.2055      0.118      1.744      0.081      -0.026       0.437\n",
      "240            0.1755      0.109      1.606      0.108      -0.039       0.390\n",
      "417           -0.2774      0.131     -2.123      0.034      -0.534      -0.021\n",
      "424            0.2178      0.115      1.886      0.059      -0.008       0.444\n",
      "431           -0.3480      0.138     -2.519      0.012      -0.619      -0.077\n",
      "438            0.2762      0.109      2.524      0.012       0.062       0.491\n",
      "489           -0.2338      0.115     -2.030      0.042      -0.460      -0.008\n",
      "512            0.2501      0.106      2.356      0.018       0.042       0.458\n",
      "548            0.2096      0.120      1.746      0.081      -0.026       0.445\n",
      "563           -0.2884      0.122     -2.360      0.018      -0.528      -0.049\n",
      "564           -0.2671      0.144     -1.851      0.064      -0.550       0.016\n",
      "576           -0.5005      0.209     -2.390      0.017      -0.911      -0.090\n",
      "==============================================================================\n",
      "Removing feature 240 with p-value 0.108382\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.191188\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1545\n",
      "Method:                           MLE   Df Model:                           21\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2169\n",
      "Time:                        16:31:40   Log-Likelihood:                -299.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.549e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5055      0.178    -19.678      0.000      -3.855      -3.156\n",
      "22             0.6740      0.172      3.925      0.000       0.337       1.011\n",
      "60             0.8149      0.132      6.161      0.000       0.556       1.074\n",
      "65             0.4323      0.101      4.280      0.000       0.234       0.630\n",
      "74            -1.1559      0.267     -4.323      0.000      -1.680      -0.632\n",
      "79             0.3660      0.136      2.684      0.007       0.099       0.633\n",
      "104            0.1756      0.119      1.479      0.139      -0.057       0.408\n",
      "125            0.4263      0.117      3.645      0.000       0.197       0.656\n",
      "130            0.5163      0.153      3.380      0.001       0.217       0.816\n",
      "147           -0.2594      0.133     -1.946      0.052      -0.521       0.002\n",
      "151           -0.2405      0.126     -1.908      0.056      -0.488       0.007\n",
      "173            0.2089      0.117      1.780      0.075      -0.021       0.439\n",
      "417           -0.2704      0.130     -2.074      0.038      -0.526      -0.015\n",
      "424            0.2126      0.115      1.843      0.065      -0.014       0.439\n",
      "431           -0.3345      0.137     -2.445      0.014      -0.603      -0.066\n",
      "438            0.2741      0.109      2.514      0.012       0.060       0.488\n",
      "489           -0.2228      0.115     -1.942      0.052      -0.448       0.002\n",
      "512            0.2352      0.106      2.217      0.027       0.027       0.443\n",
      "548            0.2044      0.120      1.705      0.088      -0.031       0.439\n",
      "563           -0.2865      0.123     -2.338      0.019      -0.527      -0.046\n",
      "564           -0.2556      0.144     -1.781      0.075      -0.537       0.026\n",
      "576           -0.4996      0.209     -2.386      0.017      -0.910      -0.089\n",
      "==============================================================================\n",
      "Removing feature 104 with p-value 0.139269\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.191841\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1546\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2142\n",
      "Time:                        16:31:40   Log-Likelihood:                -300.62\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.314e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4979      0.177    -19.711      0.000      -3.846      -3.150\n",
      "22             0.6739      0.172      3.926      0.000       0.337       1.010\n",
      "60             0.8814      0.126      7.004      0.000       0.635       1.128\n",
      "65             0.4749      0.098      4.843      0.000       0.283       0.667\n",
      "74            -1.2570      0.258     -4.880      0.000      -1.762      -0.752\n",
      "79             0.3594      0.137      2.621      0.009       0.091       0.628\n",
      "125            0.4307      0.117      3.696      0.000       0.202       0.659\n",
      "130            0.5464      0.151      3.622      0.000       0.251       0.842\n",
      "147           -0.2586      0.133     -1.939      0.052      -0.520       0.003\n",
      "151           -0.2376      0.126     -1.884      0.060      -0.485       0.010\n",
      "173            0.2107      0.117      1.800      0.072      -0.019       0.440\n",
      "417           -0.2687      0.130     -2.063      0.039      -0.524      -0.013\n",
      "424            0.2305      0.113      2.048      0.041       0.010       0.451\n",
      "431           -0.3270      0.137     -2.389      0.017      -0.595      -0.059\n",
      "438            0.2642      0.109      2.431      0.015       0.051       0.477\n",
      "489           -0.2222      0.115     -1.940      0.052      -0.447       0.002\n",
      "512            0.2375      0.106      2.242      0.025       0.030       0.445\n",
      "548            0.2096      0.120      1.747      0.081      -0.026       0.445\n",
      "563           -0.2908      0.122     -2.380      0.017      -0.530      -0.051\n",
      "564           -0.2592      0.142     -1.824      0.068      -0.538       0.019\n",
      "576           -0.5085      0.210     -2.424      0.015      -0.920      -0.097\n",
      "==============================================================================\n",
      "Removing feature 548 with p-value 0.080607\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192843\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1547\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2101\n",
      "Time:                        16:31:41   Log-Likelihood:                -302.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.793e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4703      0.174    -19.912      0.000      -3.812      -3.129\n",
      "22             0.6521      0.171      3.817      0.000       0.317       0.987\n",
      "60             0.8762      0.125      6.994      0.000       0.631       1.122\n",
      "65             0.4718      0.098      4.800      0.000       0.279       0.664\n",
      "74            -1.2471      0.256     -4.870      0.000      -1.749      -0.745\n",
      "79             0.3319      0.136      2.440      0.015       0.065       0.598\n",
      "125            0.4278      0.115      3.715      0.000       0.202       0.653\n",
      "130            0.5419      0.150      3.614      0.000       0.248       0.836\n",
      "147           -0.2654      0.135     -1.973      0.049      -0.529      -0.002\n",
      "151           -0.2395      0.127     -1.888      0.059      -0.488       0.009\n",
      "173            0.2025      0.117      1.738      0.082      -0.026       0.431\n",
      "417           -0.2748      0.129     -2.123      0.034      -0.528      -0.021\n",
      "424            0.2329      0.113      2.057      0.040       0.011       0.455\n",
      "431           -0.3015      0.135     -2.231      0.026      -0.566      -0.037\n",
      "438            0.2695      0.108      2.502      0.012       0.058       0.481\n",
      "489           -0.2150      0.114     -1.879      0.060      -0.439       0.009\n",
      "512            0.2351      0.106      2.225      0.026       0.028       0.442\n",
      "563           -0.2943      0.122     -2.419      0.016      -0.533      -0.056\n",
      "564           -0.2411      0.141     -1.707      0.088      -0.518       0.036\n",
      "576           -0.4905      0.206     -2.385      0.017      -0.894      -0.087\n",
      "==============================================================================\n",
      "Removing feature 564 with p-value 0.087911\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193836\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1548\n",
      "Method:                           MLE   Df Model:                           18\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2061\n",
      "Time:                        16:31:41   Log-Likelihood:                -303.74\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.387e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4487      0.172    -20.010      0.000      -3.786      -3.111\n",
      "22             0.6523      0.170      3.828      0.000       0.318       0.986\n",
      "60             0.8601      0.124      6.956      0.000       0.618       1.103\n",
      "65             0.4648      0.098      4.765      0.000       0.274       0.656\n",
      "74            -1.2477      0.255     -4.894      0.000      -1.747      -0.748\n",
      "79             0.3514      0.135      2.605      0.009       0.087       0.616\n",
      "125            0.4194      0.114      3.664      0.000       0.195       0.644\n",
      "130            0.5431      0.152      3.584      0.000       0.246       0.840\n",
      "147           -0.2706      0.134     -2.013      0.044      -0.534      -0.007\n",
      "151           -0.2329      0.127     -1.838      0.066      -0.481       0.015\n",
      "173            0.2000      0.116      1.721      0.085      -0.028       0.428\n",
      "417           -0.2759      0.129     -2.131      0.033      -0.530      -0.022\n",
      "424            0.2239      0.110      2.027      0.043       0.007       0.440\n",
      "431           -0.3042      0.135     -2.257      0.024      -0.568      -0.040\n",
      "438            0.2670      0.106      2.528      0.011       0.060       0.474\n",
      "489           -0.2200      0.115     -1.920      0.055      -0.445       0.005\n",
      "512            0.2334      0.105      2.219      0.027       0.027       0.440\n",
      "563           -0.2483      0.115     -2.156      0.031      -0.474      -0.023\n",
      "576           -0.5049      0.206     -2.453      0.014      -0.908      -0.101\n",
      "==============================================================================\n",
      "Removing feature 173 with p-value 0.085212\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194795\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1549\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2021\n",
      "Time:                        16:31:41   Log-Likelihood:                -305.24\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.988e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4289      0.171    -20.096      0.000      -3.763      -3.094\n",
      "22             0.6308      0.169      3.733      0.000       0.300       0.962\n",
      "60             0.8547      0.124      6.919      0.000       0.613       1.097\n",
      "65             0.4502      0.097      4.644      0.000       0.260       0.640\n",
      "74            -1.2500      0.258     -4.840      0.000      -1.756      -0.744\n",
      "79             0.3396      0.134      2.536      0.011       0.077       0.602\n",
      "125            0.4212      0.114      3.690      0.000       0.198       0.645\n",
      "130            0.5430      0.152      3.575      0.000       0.245       0.841\n",
      "147           -0.2703      0.134     -2.013      0.044      -0.533      -0.007\n",
      "151           -0.2261      0.127     -1.783      0.075      -0.475       0.022\n",
      "417           -0.2834      0.130     -2.184      0.029      -0.538      -0.029\n",
      "424            0.2204      0.110      1.996      0.046       0.004       0.437\n",
      "431           -0.2896      0.134     -2.165      0.030      -0.552      -0.027\n",
      "438            0.2751      0.104      2.647      0.008       0.071       0.479\n",
      "489           -0.2108      0.115     -1.841      0.066      -0.435       0.014\n",
      "512            0.2274      0.105      2.172      0.030       0.022       0.432\n",
      "563           -0.2558      0.115     -2.224      0.026      -0.481      -0.030\n",
      "576           -0.5013      0.203     -2.475      0.013      -0.898      -0.104\n",
      "==============================================================================\n",
      "Removing feature 151 with p-value 0.074552\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.195857\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1550\n",
      "Method:                           MLE   Df Model:                           16\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1978\n",
      "Time:                        16:31:41   Log-Likelihood:                -306.91\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.272e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4077      0.169    -20.223      0.000      -3.738      -3.077\n",
      "22             0.6236      0.168      3.711      0.000       0.294       0.953\n",
      "60             0.8470      0.123      6.890      0.000       0.606       1.088\n",
      "65             0.4578      0.097      4.731      0.000       0.268       0.647\n",
      "74            -1.2369      0.257     -4.817      0.000      -1.740      -0.734\n",
      "79             0.3324      0.134      2.490      0.013       0.071       0.594\n",
      "125            0.4308      0.114      3.776      0.000       0.207       0.654\n",
      "130            0.5245      0.151      3.479      0.001       0.229       0.820\n",
      "147           -0.2641      0.134     -1.973      0.048      -0.526      -0.002\n",
      "417           -0.2877      0.129     -2.232      0.026      -0.540      -0.035\n",
      "424            0.1227      0.105      1.167      0.243      -0.083       0.329\n",
      "431           -0.2914      0.134     -2.180      0.029      -0.553      -0.029\n",
      "438            0.2707      0.105      2.590      0.010       0.066       0.476\n",
      "489           -0.2067      0.114     -1.805      0.071      -0.431       0.018\n",
      "512            0.2266      0.105      2.168      0.030       0.022       0.431\n",
      "563           -0.2572      0.115     -2.242      0.025      -0.482      -0.032\n",
      "576           -0.4998      0.202     -2.469      0.014      -0.896      -0.103\n",
      "==============================================================================\n",
      "Removing feature 424 with p-value 0.243305\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196263\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1551\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1961\n",
      "Time:                        16:31:41   Log-Likelihood:                -307.54\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.350e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4011      0.168    -20.257      0.000      -3.730      -3.072\n",
      "22             0.6184      0.168      3.685      0.000       0.289       0.947\n",
      "60             0.8624      0.122      7.042      0.000       0.622       1.102\n",
      "65             0.4616      0.097      4.771      0.000       0.272       0.651\n",
      "74            -1.2676      0.256     -4.961      0.000      -1.768      -0.767\n",
      "79             0.3404      0.133      2.553      0.011       0.079       0.602\n",
      "125            0.4196      0.114      3.686      0.000       0.196       0.643\n",
      "130            0.5286      0.150      3.515      0.000       0.234       0.823\n",
      "147           -0.2726      0.133     -2.044      0.041      -0.534      -0.011\n",
      "417           -0.2791      0.129     -2.167      0.030      -0.532      -0.027\n",
      "431           -0.2864      0.133     -2.147      0.032      -0.548      -0.025\n",
      "438            0.2708      0.105      2.582      0.010       0.065       0.476\n",
      "489           -0.2100      0.115     -1.830      0.067      -0.435       0.015\n",
      "512            0.2199      0.104      2.111      0.035       0.016       0.424\n",
      "563           -0.2636      0.114     -2.307      0.021      -0.488      -0.040\n",
      "576           -0.5006      0.202     -2.483      0.013      -0.896      -0.105\n",
      "==============================================================================\n",
      "Removing feature 489 with p-value 0.067179\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197361\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1552\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1916\n",
      "Time:                        16:31:41   Log-Likelihood:                -309.26\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.410e-24\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3771      0.165    -20.418      0.000      -3.701      -3.053\n",
      "22             0.6020      0.167      3.610      0.000       0.275       0.929\n",
      "60             0.8584      0.122      7.027      0.000       0.619       1.098\n",
      "65             0.4707      0.096      4.907      0.000       0.283       0.659\n",
      "74            -1.2738      0.255     -4.996      0.000      -1.774      -0.774\n",
      "79             0.3519      0.134      2.634      0.008       0.090       0.614\n",
      "125            0.4095      0.114      3.595      0.000       0.186       0.633\n",
      "130            0.5314      0.151      3.519      0.000       0.235       0.827\n",
      "147           -0.2772      0.134     -2.069      0.039      -0.540      -0.015\n",
      "417           -0.2774      0.128     -2.162      0.031      -0.529      -0.026\n",
      "431           -0.2754      0.133     -2.075      0.038      -0.536      -0.015\n",
      "438            0.2746      0.104      2.649      0.008       0.071       0.478\n",
      "512            0.2150      0.104      2.068      0.039       0.011       0.419\n",
      "563           -0.2576      0.114     -2.263      0.024      -0.481      -0.034\n",
      "576           -0.4930      0.198     -2.491      0.013      -0.881      -0.105\n",
      "==============================================================================\n",
      "All remaining features are significant (p < 0.05). Stopping.\n",
      "Final significant features: ['22', '60', '65', '74', '79', '125', '130', '147', '417', '431', '438', '512', '563', '576']\n"
     ]
    }
   ],
   "source": [
    "# Iterative feature elimination\n",
    "X_current = X_scaled_reduced.copy()\n",
    "features_to_keep = list(X_scaled_reduced.columns)\n",
    "features_to_keep.remove('const')\n",
    "\n",
    "while True:\n",
    "    logit_model = sm.Logit(Y, X_current)\n",
    "    result = logit_model.fit(method='newton', maxiter=1000)\n",
    "    print(result.summary())\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    p_values = p_values.drop('const')\n",
    "\n",
    "    if (p_values >= 0.05).sum() == 0:\n",
    "        print(\"All remaining features are significant (p < 0.05). Stopping.\")\n",
    "        break\n",
    "\n",
    "    max_p_feature = p_values.idxmax()\n",
    "    max_p_value = p_values[max_p_feature]\n",
    "    print(f\"Removing feature {max_p_feature} with p-value {max_p_value:.6f}\")\n",
    "\n",
    "    X_current = X_current.drop(columns=max_p_feature)\n",
    "    features_to_keep.remove(max_p_feature)\n",
    "\n",
    "print(f\"Final significant features: {features_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fe568fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with significant features: 0.260411\n",
      "\n",
      "Logistic Regression Coefficients (significant features):\n",
      "         22        60        65        74        79       125       130  \\\n",
      "0  0.001013  0.087659  0.088105 -0.065985  2.204859  2.603606  0.380113   \n",
      "\n",
      "        147       417       431       438       512       563       576  \n",
      "0 -1.526034 -0.245161 -0.007673  0.111757  0.000521 -0.023783 -3.035525  \n"
     ]
    }
   ],
   "source": [
    "# Retrain with sklearn\n",
    "X_final_significant = X_indicators[features_to_keep]\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_final_significant, Y)\n",
    "f1_score = cross_val_score(model, X_final_significant, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with significant features: {f1_score:.6f}\")\n",
    "\n",
    "# Inspect coefficients\n",
    "coeffs = pd.DataFrame(model.coef_, columns=X_final_significant.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (significant features):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dcd8641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1466\n",
      "Method:                           MLE   Df Model:                          100\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.3064\n",
      "Time:                        16:33:28   Log-Likelihood:                -265.34\n",
      "converged:                      False   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.243e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -8.4647     10.719     -0.790      0.430     -29.474      12.544\n",
      "2             -0.1823      0.125     -1.462      0.144      -0.427       0.062\n",
      "3             -0.1528      0.151     -1.011      0.312      -0.449       0.143\n",
      "4             -0.0562      0.168     -0.334      0.738      -0.386       0.274\n",
      "15            -0.4012      0.150     -2.673      0.008      -0.695      -0.107\n",
      "16             0.1810      0.282      0.642      0.521      -0.372       0.734\n",
      "22             0.3480      0.250      1.389      0.165      -0.143       0.839\n",
      "23            -0.3966      0.282     -1.407      0.160      -0.949       0.156\n",
      "24            -0.0433      0.137     -0.317      0.751      -0.311       0.224\n",
      "25             0.0599      0.140      0.427      0.669      -0.215       0.335\n",
      "41            -0.0826      0.122     -0.680      0.497      -0.321       0.156\n",
      "47             0.7301      0.795      0.919      0.358      -0.828       2.288\n",
      "52             0.2078      0.398      0.522      0.602      -0.573       0.988\n",
      "56            -0.1136      0.174     -0.653      0.513      -0.454       0.227\n",
      "60             1.1734      0.199      5.882      0.000       0.782       1.564\n",
      "63            -0.5445      0.365     -1.493      0.135      -1.259       0.170\n",
      "65             0.3603      0.390      0.924      0.355      -0.404       1.124\n",
      "68             7.1902    254.574      0.028      0.977    -491.765     506.145\n",
      "69            -0.2714      0.273     -0.993      0.321      -0.807       0.264\n",
      "71            -0.5397      0.763     -0.707      0.479      -2.036       0.956\n",
      "72             0.2872      0.133      2.166      0.030       0.027       0.547\n",
      "73             0.4446      0.385      1.155      0.248      -0.310       1.199\n",
      "74            -0.5751      0.627     -0.917      0.359      -1.804       0.654\n",
      "89             0.1012      0.136      0.745      0.457      -0.165       0.368\n",
      "91            -0.1596      0.177     -0.903      0.366      -0.506       0.187\n",
      "116           -0.0959      0.177     -0.541      0.589      -0.443       0.252\n",
      "134            0.2399      0.143      1.683      0.092      -0.040       0.519\n",
      "136            0.0327      0.147      0.222      0.824      -0.256       0.321\n",
      "137           -0.0707      0.170     -0.416      0.677      -0.403       0.262\n",
      "138            0.1315      0.183      0.717      0.473      -0.228       0.491\n",
      "139            0.1295      0.140      0.925      0.355      -0.145       0.404\n",
      "141          -69.4400    204.457     -0.340      0.734    -470.168     331.288\n",
      "152           -2.8026      1.048     -2.675      0.007      -4.856      -0.749\n",
      "153           -1.5016     21.269     -0.071      0.944     -43.189      40.186\n",
      "160            0.3248      0.974      0.334      0.739      -1.584       2.233\n",
      "162           -1.7726      1.407     -1.260      0.208      -4.530       0.985\n",
      "163           -0.7054      0.786     -0.897      0.370      -2.247       0.836\n",
      "184            0.5381      0.236      2.277      0.023       0.075       1.001\n",
      "186           -0.1020      0.433     -0.236      0.814      -0.950       0.746\n",
      "188           -0.5635      0.416     -1.354      0.176      -1.379       0.252\n",
      "189           -0.0798      0.385     -0.207      0.836      -0.835       0.675\n",
      "201           -0.2838      0.331     -0.857      0.391      -0.933       0.365\n",
      "205          -79.7929    574.248     -0.139      0.889   -1205.299    1045.713\n",
      "209           -0.2047      0.133     -1.534      0.125      -0.466       0.057\n",
      "224           -0.0775      0.108     -0.719      0.472      -0.289       0.134\n",
      "226           -0.2590      0.537     -0.483      0.629      -1.311       0.793\n",
      "251           -0.1655      0.376     -0.440      0.660      -0.903       0.572\n",
      "253           -7.4954      7.982     -0.939      0.348     -23.139       8.148\n",
      "269            0.1935      0.135      1.439      0.150      -0.070       0.457\n",
      "275            0.1695      0.148      1.148      0.251      -0.120       0.459\n",
      "295           -0.2080      1.088     -0.191      0.848      -2.340       1.924\n",
      "296           -0.7724      0.362     -2.136      0.033      -1.481      -0.064\n",
      "297            1.6610      1.376      1.207      0.227      -1.036       4.358\n",
      "298            0.6343      0.779      0.815      0.415      -0.892       2.160\n",
      "341           64.9533    738.588      0.088      0.930   -1382.652    1512.558\n",
      "364            0.3211      0.450      0.714      0.475      -0.560       1.202\n",
      "414           -0.5759      0.496     -1.162      0.245      -1.548       0.396\n",
      "419           -0.0666      0.130     -0.510      0.610      -0.322       0.189\n",
      "420            0.1054      0.128      0.826      0.409      -0.145       0.356\n",
      "424            0.2040      0.133      1.532      0.126      -0.057       0.465\n",
      "426            0.9154      2.022      0.453      0.651      -3.048       4.879\n",
      "429           -0.9812      0.655     -1.497      0.134      -2.266       0.303\n",
      "432            2.2558      0.744      3.031      0.002       0.797       3.714\n",
      "433           -0.0778      0.175     -0.445      0.657      -0.421       0.265\n",
      "434            0.1841      0.126      1.456      0.146      -0.064       0.432\n",
      "437           -2.1487      0.655     -3.278      0.001      -3.433      -0.864\n",
      "439            0.2352      0.140      1.679      0.093      -0.039       0.510\n",
      "440           -0.2945      0.200     -1.470      0.141      -0.687       0.098\n",
      "457           -0.1017      0.270     -0.377      0.706      -0.630       0.427\n",
      "461            0.5179      0.359      1.443      0.149      -0.186       1.221\n",
      "468            0.1101      0.102      1.084      0.278      -0.089       0.309\n",
      "469           -0.0083      0.157     -0.053      0.958      -0.315       0.299\n",
      "472            0.3564      0.291      1.223      0.221      -0.215       0.928\n",
      "473            0.2700      0.246      1.099      0.272      -0.212       0.752\n",
      "474            0.0136      0.177      0.077      0.939      -0.333       0.361\n",
      "478            0.8024      0.336      2.391      0.017       0.145       1.460\n",
      "483           -0.0318      0.129     -0.246      0.806      -0.285       0.222\n",
      "484            0.0963      0.136      0.706      0.480      -0.171       0.364\n",
      "485           -0.2509      0.137     -1.838      0.066      -0.519       0.017\n",
      "486           -0.1672      0.136     -1.228      0.219      -0.434       0.100\n",
      "487            0.3322      0.135      2.461      0.014       0.068       0.597\n",
      "488            0.1973      0.133      1.482      0.138      -0.064       0.458\n",
      "489           -0.2787      0.131     -2.127      0.033      -0.536      -0.022\n",
      "490            0.2027      0.127      1.595      0.111      -0.046       0.452\n",
      "491           -0.1137      0.132     -0.860      0.390      -0.373       0.145\n",
      "495            0.0858      0.086      0.993      0.321      -0.083       0.255\n",
      "497           -0.1225      0.143     -0.859      0.390      -0.402       0.157\n",
      "500           -0.1152      0.128     -0.902      0.367      -0.365       0.135\n",
      "501            0.2035      0.121      1.689      0.091      -0.033       0.440\n",
      "511            0.1084      0.101      1.070      0.285      -0.090       0.307\n",
      "512            0.2905      0.120      2.427      0.015       0.056       0.525\n",
      "520            0.0616      0.104      0.595      0.552      -0.141       0.265\n",
      "542            0.2920      0.126      2.325      0.020       0.046       0.538\n",
      "548            0.2146      0.142      1.507      0.132      -0.065       0.494\n",
      "556           -0.0056      0.136     -0.041      0.967      -0.271       0.260\n",
      "562            0.0424      0.120      0.354      0.724      -0.193       0.278\n",
      "563           -0.1580      0.130     -1.219      0.223      -0.412       0.096\n",
      "570            0.1938      0.117      1.663      0.096      -0.035       0.422\n",
      "573            0.3986      0.268      1.486      0.137      -0.127       0.924\n",
      "578           -0.5985      0.308     -1.945      0.052      -1.202       0.004\n",
      "582           -0.1916      0.147     -1.308      0.191      -0.479       0.096\n",
      "==============================================================================\n",
      "Significant features (p < 0.05): ['15', '60', '72', '152', '184', '296', '432', '437', '478', '487', '489', '512', '542']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\discrete\\discrete_model.py:2385: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-X))\n"
     ]
    }
   ],
   "source": [
    "# Subset to Lasso features\n",
    "X_lasso = X_indicators[selected_features_lasso]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_lasso_scaled = scaler.fit_transform(X_lasso)\n",
    "X_lasso_scaled = pd.DataFrame(X_lasso_scaled, columns=X_lasso.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_lasso_scaled = sm.add_constant(X_lasso_scaled)\n",
    "\n",
    "# Fit logistic regression with 'lbfgs'\n",
    "logit_model = sm.Logit(Y, X_lasso_scaled)\n",
    "result = logit_model.fit(method='lbfgs', maxiter=500)\n",
    "print(result.summary())\n",
    "\n",
    "# Extract significant features\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a74c7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for infinite values:\n",
      "2      0\n",
      "3      0\n",
      "4      0\n",
      "15     0\n",
      "16     0\n",
      "22     0\n",
      "23     0\n",
      "24     0\n",
      "25     0\n",
      "41     0\n",
      "47     0\n",
      "52     0\n",
      "56     0\n",
      "60     0\n",
      "63     0\n",
      "65     0\n",
      "68     0\n",
      "69     0\n",
      "71     0\n",
      "72     0\n",
      "73     0\n",
      "74     0\n",
      "89     0\n",
      "91     0\n",
      "116    0\n",
      "134    0\n",
      "136    0\n",
      "137    0\n",
      "138    0\n",
      "139    0\n",
      "141    0\n",
      "152    0\n",
      "153    0\n",
      "160    0\n",
      "162    0\n",
      "163    0\n",
      "184    0\n",
      "186    0\n",
      "188    0\n",
      "189    0\n",
      "201    0\n",
      "205    0\n",
      "209    0\n",
      "224    0\n",
      "226    0\n",
      "251    0\n",
      "253    0\n",
      "269    0\n",
      "275    0\n",
      "295    0\n",
      "296    0\n",
      "297    0\n",
      "298    0\n",
      "341    0\n",
      "364    0\n",
      "414    0\n",
      "419    0\n",
      "420    0\n",
      "424    0\n",
      "426    0\n",
      "429    0\n",
      "432    0\n",
      "433    0\n",
      "434    0\n",
      "437    0\n",
      "439    0\n",
      "440    0\n",
      "457    0\n",
      "461    0\n",
      "468    0\n",
      "469    0\n",
      "472    0\n",
      "473    0\n",
      "474    0\n",
      "478    0\n",
      "483    0\n",
      "484    0\n",
      "485    0\n",
      "486    0\n",
      "487    0\n",
      "488    0\n",
      "489    0\n",
      "490    0\n",
      "491    0\n",
      "495    0\n",
      "497    0\n",
      "500    0\n",
      "501    0\n",
      "511    0\n",
      "512    0\n",
      "520    0\n",
      "542    0\n",
      "548    0\n",
      "556    0\n",
      "562    0\n",
      "563    0\n",
      "570    0\n",
      "573    0\n",
      "578    0\n",
      "582    0\n",
      "dtype: int64\n",
      "Checking for extreme values (abs > 10):\n",
      "2       0\n",
      "3       0\n",
      "4       0\n",
      "15      0\n",
      "16      2\n",
      "22      0\n",
      "23      0\n",
      "24      0\n",
      "25      0\n",
      "41      0\n",
      "47      0\n",
      "52      0\n",
      "56      0\n",
      "60      1\n",
      "63      1\n",
      "65      0\n",
      "68      4\n",
      "69      1\n",
      "71      1\n",
      "72      0\n",
      "73      4\n",
      "74      6\n",
      "89      0\n",
      "91      0\n",
      "116     0\n",
      "134     0\n",
      "136     1\n",
      "137     0\n",
      "138     0\n",
      "139     0\n",
      "141     4\n",
      "152     6\n",
      "153     1\n",
      "160     0\n",
      "162     0\n",
      "163     0\n",
      "184     0\n",
      "186     1\n",
      "188     1\n",
      "189     0\n",
      "201     3\n",
      "205     5\n",
      "209     0\n",
      "224     1\n",
      "226     0\n",
      "251     2\n",
      "253     1\n",
      "269     0\n",
      "275     0\n",
      "295     0\n",
      "296     0\n",
      "297     0\n",
      "298     0\n",
      "341     5\n",
      "364     0\n",
      "414     4\n",
      "419     0\n",
      "420     0\n",
      "424     1\n",
      "426     1\n",
      "429    11\n",
      "432     7\n",
      "433     0\n",
      "434     0\n",
      "437     7\n",
      "439     2\n",
      "440     4\n",
      "457     2\n",
      "461     0\n",
      "468    11\n",
      "469     0\n",
      "472     5\n",
      "473     0\n",
      "474     1\n",
      "478     4\n",
      "483     0\n",
      "484     0\n",
      "485     0\n",
      "486     0\n",
      "487     0\n",
      "488     0\n",
      "489     0\n",
      "490     0\n",
      "491     0\n",
      "495     6\n",
      "497     0\n",
      "500     0\n",
      "501     0\n",
      "511     2\n",
      "512     0\n",
      "520     1\n",
      "542     0\n",
      "548     0\n",
      "556     0\n",
      "562     0\n",
      "563     0\n",
      "570     0\n",
      "573     0\n",
      "578     0\n",
      "582     2\n",
      "dtype: int64\n",
      "   Feature         VIF\n",
      "0        2    1.103925\n",
      "1        3    1.389423\n",
      "2        4    1.546096\n",
      "3       15    1.229607\n",
      "4       16    1.526949\n",
      "5       22    3.340913\n",
      "6       23    4.530370\n",
      "7       24    1.138408\n",
      "8       25    1.171319\n",
      "9       41    1.134614\n",
      "10      47   39.043028\n",
      "11      52    9.220319\n",
      "12      56    2.172992\n",
      "13      60    2.926809\n",
      "14      63    8.394799\n",
      "15      65    9.531057\n",
      "16      68   52.990384\n",
      "17      69    4.058116\n",
      "18      71   35.732335\n",
      "19      72    1.242004\n",
      "20      73    9.405889\n",
      "21      74    9.142281\n",
      "22      89    1.100680\n",
      "23      91    2.070865\n",
      "24     116    2.111080\n",
      "25     134    1.130004\n",
      "26     136    1.088788\n",
      "27     137    1.898956\n",
      "28     138    1.973903\n",
      "29     139    1.413443\n",
      "30     141   10.109238\n",
      "31     152    1.732126\n",
      "32     153   22.765322\n",
      "33     160  106.492871\n",
      "34     162  107.734795\n",
      "35     163   46.641375\n",
      "36     184    2.933642\n",
      "37     186    5.246807\n",
      "38     188    6.628796\n",
      "39     189    9.710980\n",
      "40     201    6.486963\n",
      "41     205  116.523841\n",
      "42     209    1.157089\n",
      "43     224    1.240252\n",
      "44     226   16.304812\n",
      "45     251    1.958955\n",
      "46     253    1.037493\n",
      "47     269    1.260205\n",
      "48     275    1.287186\n",
      "49     295  127.160132\n",
      "50     296   14.130216\n",
      "51     297  105.516696\n",
      "52     298   46.647142\n",
      "53     341  178.441417\n",
      "54     364   15.367293\n",
      "55     414   10.420315\n",
      "56     419    1.053294\n",
      "57     420    1.066477\n",
      "58     424    1.310871\n",
      "59     426   22.875058\n",
      "60     429    1.081655\n",
      "61     432   62.014168\n",
      "62     433    1.533044\n",
      "63     434    1.267011\n",
      "64     437   41.738258\n",
      "65     439    2.017690\n",
      "66     440    2.076459\n",
      "67     457    3.474124\n",
      "68     461    7.739794\n",
      "69     468    1.213740\n",
      "70     469    1.185836\n",
      "71     472    4.457828\n",
      "72     473    3.892000\n",
      "73     474    2.316965\n",
      "74     478   10.373852\n",
      "75     483    1.064206\n",
      "76     484    1.115224\n",
      "77     485    1.112752\n",
      "78     486    1.086868\n",
      "79     487    1.078670\n",
      "80     488    1.143928\n",
      "81     489    1.072951\n",
      "82     490    1.067683\n",
      "83     491    1.066521\n",
      "84     495    1.239070\n",
      "85     497    1.088410\n",
      "86     500    1.065973\n",
      "87     501    1.064557\n",
      "88     511    1.215135\n",
      "89     512    1.056669\n",
      "90     520    1.143800\n",
      "91     542    1.112648\n",
      "92     548    1.135386\n",
      "93     556    1.206402\n",
      "94     562    1.097286\n",
      "95     563    1.086649\n",
      "96     570    1.082718\n",
      "97     573    4.292829\n",
      "98     578    4.365921\n",
      "99     582    1.129403\n",
      "Removed 21 features due to high VIF: ['47', '68', '71', '141', '153', '160', '162', '163', '205', '226', '295', '296', '297', '298', '341', '364', '414', '426', '432', '437', '478']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179890\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1487\n",
      "Method:                           MLE   Df Model:                           79\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2632\n",
      "Time:                        16:33:44   Log-Likelihood:                -281.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.202e-12\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1125      0.316    -13.023      0.000      -4.731      -3.494\n",
      "2             -0.1453      0.119     -1.221      0.222      -0.378       0.088\n",
      "3             -0.1090      0.138     -0.791      0.429      -0.379       0.161\n",
      "4             -0.0486      0.158     -0.308      0.758      -0.358       0.261\n",
      "15            -0.3546      0.141     -2.514      0.012      -0.631      -0.078\n",
      "16             0.3103      0.260      1.193      0.233      -0.199       0.820\n",
      "22             0.2990      0.189      1.580      0.114      -0.072       0.670\n",
      "23             0.0415      0.183      0.227      0.821      -0.317       0.400\n",
      "24            -0.0646      0.126     -0.514      0.608      -0.311       0.182\n",
      "25            -0.0106      0.133     -0.080      0.937      -0.271       0.250\n",
      "41            -0.0372      0.117     -0.317      0.751      -0.267       0.193\n",
      "52             0.2121      0.378      0.561      0.575      -0.529       0.954\n",
      "56            -0.0665      0.146     -0.455      0.649      -0.353       0.220\n",
      "60             0.9613      0.164      5.856      0.000       0.640       1.283\n",
      "63            -0.4018      0.243     -1.657      0.098      -0.877       0.074\n",
      "65             0.6252      0.146      4.291      0.000       0.340       0.911\n",
      "69            -0.2922      0.174     -1.678      0.093      -0.634       0.049\n",
      "72             0.2331      0.117      1.994      0.046       0.004       0.462\n",
      "73             0.5035      0.326      1.546      0.122      -0.135       1.142\n",
      "74            -0.9466      0.505     -1.875      0.061      -1.936       0.043\n",
      "89             0.0512      0.127      0.402      0.687      -0.198       0.300\n",
      "91            -0.1805      0.168     -1.076      0.282      -0.509       0.148\n",
      "116           -0.0470      0.168     -0.279      0.780      -0.377       0.283\n",
      "134            0.2230      0.137      1.628      0.103      -0.045       0.491\n",
      "136           -0.0047      0.147     -0.032      0.975      -0.293       0.284\n",
      "137           -0.0909      0.160     -0.569      0.570      -0.404       0.222\n",
      "138            0.1058      0.172      0.614      0.539      -0.232       0.444\n",
      "139            0.1173      0.128      0.916      0.359      -0.134       0.368\n",
      "152           -2.0170      0.950     -2.124      0.034      -3.878      -0.156\n",
      "184            0.4187      0.220      1.906      0.057      -0.012       0.849\n",
      "186           -0.0497      0.402     -0.124      0.902      -0.838       0.738\n",
      "188           -0.4635      0.383     -1.212      0.226      -1.213       0.286\n",
      "189           -0.0506      0.363     -0.139      0.889      -0.763       0.661\n",
      "201           -0.0359      0.291     -0.123      0.902      -0.606       0.534\n",
      "209           -0.2564      0.119     -2.148      0.032      -0.490      -0.022\n",
      "224           -0.0411      0.097     -0.424      0.672      -0.231       0.149\n",
      "251           -0.0565      0.150     -0.376      0.707      -0.351       0.238\n",
      "253           -7.5751      7.440     -1.018      0.309     -22.157       7.006\n",
      "269            0.1647      0.128      1.289      0.197      -0.086       0.415\n",
      "275            0.0455      0.139      0.327      0.743      -0.227       0.318\n",
      "419           -0.0974      0.124     -0.783      0.434      -0.341       0.147\n",
      "420            0.0746      0.123      0.606      0.545      -0.167       0.316\n",
      "424            0.1287      0.130      0.986      0.324      -0.127       0.384\n",
      "429           -1.0076      0.637     -1.582      0.114      -2.256       0.241\n",
      "433           -0.1157      0.141     -0.821      0.412      -0.392       0.161\n",
      "434            0.2286      0.118      1.939      0.053      -0.002       0.460\n",
      "439            0.2290      0.136      1.687      0.092      -0.037       0.495\n",
      "440           -0.2102      0.181     -1.161      0.245      -0.565       0.144\n",
      "457           -0.0598      0.231     -0.259      0.796      -0.513       0.393\n",
      "461            0.4098      0.335      1.222      0.222      -0.247       1.067\n",
      "468            0.0966      0.093      1.037      0.300      -0.086       0.279\n",
      "469           -0.0958      0.150     -0.639      0.523      -0.389       0.198\n",
      "472            0.2086      0.196      1.064      0.287      -0.176       0.593\n",
      "473            0.2537      0.202      1.255      0.209      -0.142       0.650\n",
      "474            0.1442      0.149      0.967      0.333      -0.148       0.436\n",
      "483           -0.0251      0.122     -0.205      0.838      -0.265       0.215\n",
      "484            0.0530      0.129      0.411      0.681      -0.200       0.306\n",
      "485           -0.2061      0.129     -1.598      0.110      -0.459       0.047\n",
      "486           -0.1415      0.132     -1.072      0.284      -0.400       0.117\n",
      "487            0.2793      0.127      2.195      0.028       0.030       0.529\n",
      "488            0.1139      0.127      0.894      0.371      -0.136       0.364\n",
      "489           -0.2628      0.124     -2.116      0.034      -0.506      -0.019\n",
      "490            0.1652      0.120      1.376      0.169      -0.070       0.400\n",
      "491           -0.1217      0.130     -0.939      0.348      -0.376       0.132\n",
      "495            0.0648      0.082      0.794      0.427      -0.095       0.225\n",
      "497           -0.0833      0.135     -0.618      0.536      -0.347       0.181\n",
      "500           -0.1081      0.123     -0.880      0.379      -0.349       0.133\n",
      "501            0.1458      0.117      1.247      0.212      -0.083       0.375\n",
      "511            0.1343      0.098      1.370      0.171      -0.058       0.327\n",
      "512            0.2860      0.114      2.506      0.012       0.062       0.510\n",
      "520            0.0757      0.099      0.763      0.446      -0.119       0.270\n",
      "542            0.2530      0.118      2.145      0.032       0.022       0.484\n",
      "548            0.1859      0.132      1.410      0.159      -0.073       0.444\n",
      "556            0.0102      0.126      0.081      0.935      -0.236       0.256\n",
      "562            0.0345      0.113      0.304      0.761      -0.188       0.257\n",
      "563           -0.1816      0.124     -1.460      0.144      -0.425       0.062\n",
      "570            0.2114      0.112      1.885      0.059      -0.008       0.431\n",
      "573            0.3917      0.258      1.519      0.129      -0.114       0.897\n",
      "578           -0.6647      0.299     -2.225      0.026      -1.250      -0.079\n",
      "582           -0.1563      0.141     -1.107      0.268      -0.433       0.121\n",
      "==============================================================================\n",
      "Significant features (p < 0.05): ['15', '60', '65', '72', '152', '209', '487', '489', '512', '542', '578']\n"
     ]
    }
   ],
   "source": [
    "X_lasso = X_indicators[selected_features_lasso]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_lasso_scaled = scaler.fit_transform(X_lasso)\n",
    "X_lasso_scaled = pd.DataFrame(X_lasso_scaled, columns=X_lasso.columns)\n",
    "\n",
    "# Check for extreme values or infinities\n",
    "print(\"Checking for infinite values:\")\n",
    "print(np.isinf(X_lasso_scaled).sum())\n",
    "print(\"Checking for extreme values (abs > 10):\")\n",
    "print((X_lasso_scaled.abs() > 10).sum())\n",
    "\n",
    "# Add intercept\n",
    "X_lasso_scaled = sm.add_constant(X_lasso_scaled)\n",
    "\n",
    "# Compute VIF to further reduce multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_lasso_scaled.columns[1:]  # Exclude 'const'\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_lasso_scaled.values, i) for i in range(1, X_lasso_scaled.shape[1])]\n",
    "\n",
    "print(vif_data)\n",
    "\n",
    "# Remove features with high VIF (> 10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"]\n",
    "X_lasso_scaled_reduced = X_lasso_scaled.drop(columns=high_vif_features)\n",
    "print(f\"Removed {len(high_vif_features)} features due to high VIF: {list(high_vif_features)}\")\n",
    "\n",
    "# Retry with 'newton' optimizer\n",
    "logit_model = sm.Logit(Y, X_lasso_scaled_reduced)\n",
    "result = logit_model.fit(method='newton', maxiter=1000)\n",
    "print(result.summary())\n",
    "\n",
    "# Extract significant features\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "da4c3dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179890\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1487\n",
      "Method:                           MLE   Df Model:                           79\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2632\n",
      "Time:                        16:34:55   Log-Likelihood:                -281.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.202e-12\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1125      0.316    -13.023      0.000      -4.731      -3.494\n",
      "2             -0.1453      0.119     -1.221      0.222      -0.378       0.088\n",
      "3             -0.1090      0.138     -0.791      0.429      -0.379       0.161\n",
      "4             -0.0486      0.158     -0.308      0.758      -0.358       0.261\n",
      "15            -0.3546      0.141     -2.514      0.012      -0.631      -0.078\n",
      "16             0.3103      0.260      1.193      0.233      -0.199       0.820\n",
      "22             0.2990      0.189      1.580      0.114      -0.072       0.670\n",
      "23             0.0415      0.183      0.227      0.821      -0.317       0.400\n",
      "24            -0.0646      0.126     -0.514      0.608      -0.311       0.182\n",
      "25            -0.0106      0.133     -0.080      0.937      -0.271       0.250\n",
      "41            -0.0372      0.117     -0.317      0.751      -0.267       0.193\n",
      "52             0.2121      0.378      0.561      0.575      -0.529       0.954\n",
      "56            -0.0665      0.146     -0.455      0.649      -0.353       0.220\n",
      "60             0.9613      0.164      5.856      0.000       0.640       1.283\n",
      "63            -0.4018      0.243     -1.657      0.098      -0.877       0.074\n",
      "65             0.6252      0.146      4.291      0.000       0.340       0.911\n",
      "69            -0.2922      0.174     -1.678      0.093      -0.634       0.049\n",
      "72             0.2331      0.117      1.994      0.046       0.004       0.462\n",
      "73             0.5035      0.326      1.546      0.122      -0.135       1.142\n",
      "74            -0.9466      0.505     -1.875      0.061      -1.936       0.043\n",
      "89             0.0512      0.127      0.402      0.687      -0.198       0.300\n",
      "91            -0.1805      0.168     -1.076      0.282      -0.509       0.148\n",
      "116           -0.0470      0.168     -0.279      0.780      -0.377       0.283\n",
      "134            0.2230      0.137      1.628      0.103      -0.045       0.491\n",
      "136           -0.0047      0.147     -0.032      0.975      -0.293       0.284\n",
      "137           -0.0909      0.160     -0.569      0.570      -0.404       0.222\n",
      "138            0.1058      0.172      0.614      0.539      -0.232       0.444\n",
      "139            0.1173      0.128      0.916      0.359      -0.134       0.368\n",
      "152           -2.0170      0.950     -2.124      0.034      -3.878      -0.156\n",
      "184            0.4187      0.220      1.906      0.057      -0.012       0.849\n",
      "186           -0.0497      0.402     -0.124      0.902      -0.838       0.738\n",
      "188           -0.4635      0.383     -1.212      0.226      -1.213       0.286\n",
      "189           -0.0506      0.363     -0.139      0.889      -0.763       0.661\n",
      "201           -0.0359      0.291     -0.123      0.902      -0.606       0.534\n",
      "209           -0.2564      0.119     -2.148      0.032      -0.490      -0.022\n",
      "224           -0.0411      0.097     -0.424      0.672      -0.231       0.149\n",
      "251           -0.0565      0.150     -0.376      0.707      -0.351       0.238\n",
      "253           -7.5751      7.440     -1.018      0.309     -22.157       7.006\n",
      "269            0.1647      0.128      1.289      0.197      -0.086       0.415\n",
      "275            0.0455      0.139      0.327      0.743      -0.227       0.318\n",
      "419           -0.0974      0.124     -0.783      0.434      -0.341       0.147\n",
      "420            0.0746      0.123      0.606      0.545      -0.167       0.316\n",
      "424            0.1287      0.130      0.986      0.324      -0.127       0.384\n",
      "429           -1.0076      0.637     -1.582      0.114      -2.256       0.241\n",
      "433           -0.1157      0.141     -0.821      0.412      -0.392       0.161\n",
      "434            0.2286      0.118      1.939      0.053      -0.002       0.460\n",
      "439            0.2290      0.136      1.687      0.092      -0.037       0.495\n",
      "440           -0.2102      0.181     -1.161      0.245      -0.565       0.144\n",
      "457           -0.0598      0.231     -0.259      0.796      -0.513       0.393\n",
      "461            0.4098      0.335      1.222      0.222      -0.247       1.067\n",
      "468            0.0966      0.093      1.037      0.300      -0.086       0.279\n",
      "469           -0.0958      0.150     -0.639      0.523      -0.389       0.198\n",
      "472            0.2086      0.196      1.064      0.287      -0.176       0.593\n",
      "473            0.2537      0.202      1.255      0.209      -0.142       0.650\n",
      "474            0.1442      0.149      0.967      0.333      -0.148       0.436\n",
      "483           -0.0251      0.122     -0.205      0.838      -0.265       0.215\n",
      "484            0.0530      0.129      0.411      0.681      -0.200       0.306\n",
      "485           -0.2061      0.129     -1.598      0.110      -0.459       0.047\n",
      "486           -0.1415      0.132     -1.072      0.284      -0.400       0.117\n",
      "487            0.2793      0.127      2.195      0.028       0.030       0.529\n",
      "488            0.1139      0.127      0.894      0.371      -0.136       0.364\n",
      "489           -0.2628      0.124     -2.116      0.034      -0.506      -0.019\n",
      "490            0.1652      0.120      1.376      0.169      -0.070       0.400\n",
      "491           -0.1217      0.130     -0.939      0.348      -0.376       0.132\n",
      "495            0.0648      0.082      0.794      0.427      -0.095       0.225\n",
      "497           -0.0833      0.135     -0.618      0.536      -0.347       0.181\n",
      "500           -0.1081      0.123     -0.880      0.379      -0.349       0.133\n",
      "501            0.1458      0.117      1.247      0.212      -0.083       0.375\n",
      "511            0.1343      0.098      1.370      0.171      -0.058       0.327\n",
      "512            0.2860      0.114      2.506      0.012       0.062       0.510\n",
      "520            0.0757      0.099      0.763      0.446      -0.119       0.270\n",
      "542            0.2530      0.118      2.145      0.032       0.022       0.484\n",
      "548            0.1859      0.132      1.410      0.159      -0.073       0.444\n",
      "556            0.0102      0.126      0.081      0.935      -0.236       0.256\n",
      "562            0.0345      0.113      0.304      0.761      -0.188       0.257\n",
      "563           -0.1816      0.124     -1.460      0.144      -0.425       0.062\n",
      "570            0.2114      0.112      1.885      0.059      -0.008       0.431\n",
      "573            0.3917      0.258      1.519      0.129      -0.114       0.897\n",
      "578           -0.6647      0.299     -2.225      0.026      -1.250      -0.079\n",
      "582           -0.1563      0.141     -1.107      0.268      -0.433       0.121\n",
      "==============================================================================\n",
      "Removing feature 136 with p-value 0.974529\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179891\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1488\n",
      "Method:                           MLE   Df Model:                           78\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2632\n",
      "Time:                        16:34:55   Log-Likelihood:                -281.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.400e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1127      0.316    -13.026      0.000      -4.731      -3.494\n",
      "2             -0.1454      0.119     -1.224      0.221      -0.378       0.087\n",
      "3             -0.1089      0.138     -0.791      0.429      -0.379       0.161\n",
      "4             -0.0487      0.158     -0.309      0.757      -0.358       0.260\n",
      "15            -0.3547      0.141     -2.514      0.012      -0.631      -0.078\n",
      "16             0.3106      0.260      1.195      0.232      -0.199       0.820\n",
      "22             0.2991      0.189      1.581      0.114      -0.072       0.670\n",
      "23             0.0414      0.183      0.226      0.821      -0.318       0.400\n",
      "24            -0.0648      0.126     -0.516      0.606      -0.311       0.181\n",
      "25            -0.0106      0.133     -0.080      0.937      -0.271       0.250\n",
      "41            -0.0373      0.117     -0.318      0.750      -0.267       0.192\n",
      "52             0.2116      0.378      0.560      0.575      -0.529       0.952\n",
      "56            -0.0666      0.146     -0.455      0.649      -0.353       0.220\n",
      "60             0.9617      0.164      5.872      0.000       0.641       1.283\n",
      "63            -0.4015      0.242     -1.657      0.098      -0.876       0.073\n",
      "65             0.6254      0.145      4.299      0.000       0.340       0.911\n",
      "69            -0.2920      0.174     -1.678      0.093      -0.633       0.049\n",
      "72             0.2333      0.117      1.999      0.046       0.005       0.462\n",
      "73             0.5033      0.326      1.545      0.122      -0.135       1.142\n",
      "74            -0.9474      0.504     -1.878      0.060      -1.936       0.041\n",
      "89             0.0514      0.127      0.406      0.685      -0.197       0.300\n",
      "91            -0.1801      0.167     -1.077      0.282      -0.508       0.148\n",
      "116           -0.0473      0.168     -0.281      0.779      -0.377       0.282\n",
      "134            0.2229      0.137      1.628      0.104      -0.046       0.491\n",
      "137           -0.0905      0.159     -0.568      0.570      -0.403       0.222\n",
      "138            0.1061      0.172      0.616      0.538      -0.231       0.443\n",
      "139            0.1174      0.128      0.918      0.359      -0.133       0.368\n",
      "152           -2.0181      0.949     -2.127      0.033      -3.878      -0.158\n",
      "184            0.4188      0.220      1.906      0.057      -0.012       0.849\n",
      "186           -0.0489      0.401     -0.122      0.903      -0.835       0.737\n",
      "188           -0.4631      0.382     -1.211      0.226      -1.212       0.286\n",
      "189           -0.0500      0.363     -0.138      0.890      -0.761       0.661\n",
      "201           -0.0361      0.291     -0.124      0.901      -0.606       0.534\n",
      "209           -0.2564      0.119     -2.148      0.032      -0.490      -0.022\n",
      "224           -0.0411      0.097     -0.424      0.671      -0.231       0.149\n",
      "251           -0.0564      0.150     -0.375      0.708      -0.351       0.238\n",
      "253           -7.5756      7.440     -1.018      0.309     -22.157       7.006\n",
      "269            0.1643      0.127      1.293      0.196      -0.085       0.413\n",
      "275            0.0451      0.138      0.326      0.745      -0.226       0.316\n",
      "419           -0.0974      0.124     -0.782      0.434      -0.341       0.147\n",
      "420            0.0745      0.123      0.606      0.545      -0.167       0.316\n",
      "424            0.1290      0.130      0.990      0.322      -0.126       0.384\n",
      "429           -1.0079      0.637     -1.583      0.113      -2.256       0.240\n",
      "433           -0.1158      0.141     -0.821      0.412      -0.392       0.161\n",
      "434            0.2287      0.118      1.942      0.052      -0.002       0.460\n",
      "439            0.2290      0.136      1.687      0.092      -0.037       0.495\n",
      "440           -0.2102      0.181     -1.162      0.245      -0.565       0.144\n",
      "457           -0.0599      0.231     -0.259      0.795      -0.513       0.393\n",
      "461            0.4095      0.335      1.222      0.222      -0.247       1.066\n",
      "468            0.0966      0.093      1.037      0.300      -0.086       0.279\n",
      "469           -0.0960      0.150     -0.641      0.521      -0.389       0.197\n",
      "472            0.2086      0.196      1.064      0.287      -0.176       0.593\n",
      "473            0.2537      0.202      1.255      0.210      -0.143       0.650\n",
      "474            0.1441      0.149      0.967      0.333      -0.148       0.436\n",
      "483           -0.0252      0.122     -0.206      0.837      -0.265       0.215\n",
      "484            0.0532      0.129      0.413      0.680      -0.199       0.306\n",
      "485           -0.2061      0.129     -1.598      0.110      -0.459       0.047\n",
      "486           -0.1412      0.132     -1.072      0.284      -0.399       0.117\n",
      "487            0.2794      0.127      2.195      0.028       0.030       0.529\n",
      "488            0.1139      0.127      0.894      0.371      -0.136       0.364\n",
      "489           -0.2628      0.124     -2.116      0.034      -0.506      -0.019\n",
      "490            0.1653      0.120      1.379      0.168      -0.070       0.400\n",
      "491           -0.1214      0.129     -0.938      0.348      -0.375       0.132\n",
      "495            0.0649      0.082      0.795      0.427      -0.095       0.225\n",
      "497           -0.0836      0.134     -0.624      0.533      -0.346       0.179\n",
      "500           -0.1079      0.123     -0.880      0.379      -0.348       0.132\n",
      "501            0.1458      0.117      1.247      0.212      -0.083       0.375\n",
      "511            0.1343      0.098      1.370      0.171      -0.058       0.327\n",
      "512            0.2862      0.114      2.510      0.012       0.063       0.510\n",
      "520            0.0755      0.099      0.762      0.446      -0.119       0.270\n",
      "542            0.2530      0.118      2.145      0.032       0.022       0.484\n",
      "548            0.1859      0.132      1.409      0.159      -0.073       0.444\n",
      "556            0.0103      0.126      0.082      0.935      -0.236       0.256\n",
      "562            0.0347      0.113      0.307      0.759      -0.187       0.257\n",
      "563           -0.1816      0.124     -1.460      0.144      -0.425       0.062\n",
      "570            0.2116      0.112      1.890      0.059      -0.008       0.431\n",
      "573            0.3920      0.258      1.521      0.128      -0.113       0.897\n",
      "578           -0.6647      0.299     -2.226      0.026      -1.250      -0.079\n",
      "582           -0.1565      0.141     -1.108      0.268      -0.433       0.120\n",
      "==============================================================================\n",
      "Removing feature 25 with p-value 0.936593\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179893\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1489\n",
      "Method:                           MLE   Df Model:                           77\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2632\n",
      "Time:                        16:34:56   Log-Likelihood:                -281.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.535e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1130      0.316    -13.028      0.000      -4.732      -3.494\n",
      "2             -0.1452      0.119     -1.223      0.222      -0.378       0.088\n",
      "3             -0.1090      0.138     -0.792      0.429      -0.379       0.161\n",
      "4             -0.0484      0.158     -0.307      0.759      -0.357       0.261\n",
      "15            -0.3553      0.141     -2.522      0.012      -0.631      -0.079\n",
      "16             0.3117      0.259      1.202      0.229      -0.197       0.820\n",
      "22             0.3006      0.188      1.596      0.111      -0.069       0.670\n",
      "23             0.0422      0.183      0.231      0.818      -0.316       0.401\n",
      "24            -0.0658      0.125     -0.527      0.598      -0.311       0.179\n",
      "41            -0.0377      0.117     -0.322      0.747      -0.267       0.192\n",
      "52             0.2106      0.378      0.558      0.577      -0.529       0.951\n",
      "56            -0.0665      0.146     -0.455      0.649      -0.353       0.220\n",
      "60             0.9617      0.164      5.872      0.000       0.641       1.283\n",
      "63            -0.4017      0.242     -1.658      0.097      -0.877       0.073\n",
      "65             0.6258      0.145      4.303      0.000       0.341       0.911\n",
      "69            -0.2914      0.174     -1.676      0.094      -0.632       0.049\n",
      "72             0.2328      0.117      1.998      0.046       0.004       0.461\n",
      "73             0.5048      0.325      1.553      0.121      -0.132       1.142\n",
      "74            -0.9470      0.504     -1.877      0.060      -1.936       0.042\n",
      "89             0.0507      0.127      0.401      0.689      -0.197       0.299\n",
      "91            -0.1808      0.167     -1.083      0.279      -0.508       0.147\n",
      "116           -0.0462      0.168     -0.276      0.783      -0.375       0.282\n",
      "134            0.2230      0.137      1.629      0.103      -0.045       0.491\n",
      "137           -0.0907      0.159     -0.569      0.569      -0.403       0.222\n",
      "138            0.1057      0.172      0.614      0.539      -0.232       0.443\n",
      "139            0.1170      0.128      0.915      0.360      -0.134       0.368\n",
      "152           -2.0140      0.947     -2.126      0.033      -3.871      -0.158\n",
      "184            0.4182      0.219      1.905      0.057      -0.012       0.848\n",
      "186           -0.0493      0.401     -0.123      0.902      -0.835       0.736\n",
      "188           -0.4613      0.382     -1.209      0.227      -1.209       0.287\n",
      "189           -0.0492      0.362     -0.136      0.892      -0.760       0.661\n",
      "201           -0.0357      0.291     -0.123      0.902      -0.605       0.534\n",
      "209           -0.2566      0.119     -2.150      0.032      -0.490      -0.023\n",
      "224           -0.0415      0.097     -0.429      0.668      -0.231       0.148\n",
      "251           -0.0565      0.150     -0.376      0.707      -0.351       0.238\n",
      "253           -7.5998      7.434     -1.022      0.307     -22.169       6.970\n",
      "269            0.1632      0.126      1.292      0.196      -0.084       0.411\n",
      "275            0.0450      0.138      0.325      0.745      -0.226       0.316\n",
      "419           -0.0973      0.124     -0.782      0.434      -0.341       0.147\n",
      "420            0.0740      0.123      0.602      0.547      -0.167       0.315\n",
      "424            0.1293      0.130      0.993      0.321      -0.126       0.384\n",
      "429           -1.0092      0.637     -1.584      0.113      -2.258       0.239\n",
      "433           -0.1160      0.141     -0.822      0.411      -0.393       0.161\n",
      "434            0.2286      0.118      1.942      0.052      -0.002       0.459\n",
      "439            0.2290      0.136      1.688      0.091      -0.037       0.495\n",
      "440           -0.2105      0.181     -1.165      0.244      -0.565       0.144\n",
      "457           -0.0592      0.231     -0.257      0.797      -0.511       0.393\n",
      "461            0.4083      0.334      1.221      0.222      -0.247       1.064\n",
      "468            0.0969      0.093      1.040      0.298      -0.086       0.279\n",
      "469           -0.0954      0.149     -0.638      0.523      -0.388       0.198\n",
      "472            0.2092      0.196      1.068      0.286      -0.175       0.593\n",
      "473            0.2536      0.202      1.255      0.210      -0.143       0.650\n",
      "474            0.1437      0.149      0.965      0.335      -0.148       0.435\n",
      "483           -0.0253      0.122     -0.206      0.837      -0.265       0.215\n",
      "484            0.0525      0.129      0.408      0.683      -0.200       0.305\n",
      "485           -0.2066      0.129     -1.604      0.109      -0.459       0.046\n",
      "486           -0.1414      0.132     -1.073      0.283      -0.400       0.117\n",
      "487            0.2795      0.127      2.197      0.028       0.030       0.529\n",
      "488            0.1150      0.127      0.908      0.364      -0.133       0.363\n",
      "489           -0.2621      0.124     -2.115      0.034      -0.505      -0.019\n",
      "490            0.1657      0.120      1.383      0.167      -0.069       0.401\n",
      "491           -0.1216      0.129     -0.939      0.348      -0.375       0.132\n",
      "495            0.0649      0.082      0.795      0.426      -0.095       0.225\n",
      "497           -0.0833      0.134     -0.621      0.534      -0.346       0.179\n",
      "500           -0.1083      0.123     -0.884      0.377      -0.348       0.132\n",
      "501            0.1457      0.117      1.246      0.213      -0.083       0.375\n",
      "511            0.1347      0.098      1.375      0.169      -0.057       0.327\n",
      "512            0.2862      0.114      2.511      0.012       0.063       0.510\n",
      "520            0.0755      0.099      0.763      0.446      -0.119       0.270\n",
      "542            0.2520      0.117      2.148      0.032       0.022       0.482\n",
      "548            0.1866      0.132      1.417      0.157      -0.072       0.445\n",
      "556            0.0100      0.125      0.080      0.936      -0.236       0.256\n",
      "562            0.0352      0.113      0.311      0.756      -0.186       0.257\n",
      "563           -0.1816      0.124     -1.460      0.144      -0.425       0.062\n",
      "570            0.2121      0.112      1.897      0.058      -0.007       0.431\n",
      "573            0.3936      0.257      1.532      0.126      -0.110       0.897\n",
      "578           -0.6663      0.298     -2.235      0.025      -1.251      -0.082\n",
      "582           -0.1565      0.141     -1.108      0.268      -0.433       0.120\n",
      "==============================================================================\n",
      "Removing feature 556 with p-value 0.936479\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179895\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1490\n",
      "Method:                           MLE   Df Model:                           76\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2632\n",
      "Time:                        16:34:56   Log-Likelihood:                -281.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.762e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1139      0.316    -13.038      0.000      -4.732      -3.495\n",
      "2             -0.1453      0.119     -1.224      0.221      -0.378       0.087\n",
      "3             -0.1081      0.137     -0.787      0.431      -0.377       0.161\n",
      "4             -0.0488      0.158     -0.309      0.757      -0.358       0.260\n",
      "15            -0.3559      0.141     -2.530      0.011      -0.632      -0.080\n",
      "16             0.3121      0.259      1.204      0.229      -0.196       0.820\n",
      "22             0.2997      0.188      1.594      0.111      -0.069       0.668\n",
      "23             0.0419      0.183      0.229      0.819      -0.317       0.400\n",
      "24            -0.0650      0.125     -0.522      0.602      -0.309       0.179\n",
      "41            -0.0376      0.117     -0.321      0.748      -0.267       0.192\n",
      "52             0.2091      0.377      0.554      0.579      -0.530       0.948\n",
      "56            -0.0660      0.146     -0.452      0.651      -0.352       0.220\n",
      "60             0.9623      0.164      5.884      0.000       0.642       1.283\n",
      "63            -0.4007      0.242     -1.656      0.098      -0.875       0.074\n",
      "65             0.6262      0.145      4.308      0.000       0.341       0.911\n",
      "69            -0.2910      0.174     -1.675      0.094      -0.632       0.050\n",
      "72             0.2318      0.116      2.001      0.045       0.005       0.459\n",
      "73             0.5044      0.325      1.551      0.121      -0.133       1.142\n",
      "74            -0.9484      0.504     -1.882      0.060      -1.936       0.039\n",
      "89             0.0506      0.127      0.400      0.689      -0.197       0.299\n",
      "91            -0.1804      0.167     -1.081      0.280      -0.508       0.147\n",
      "116           -0.0466      0.168     -0.278      0.781      -0.375       0.282\n",
      "134            0.2234      0.137      1.631      0.103      -0.045       0.492\n",
      "137           -0.0896      0.159     -0.564      0.573      -0.401       0.222\n",
      "138            0.1066      0.172      0.621      0.535      -0.230       0.443\n",
      "139            0.1173      0.128      0.917      0.359      -0.133       0.368\n",
      "152           -2.0161      0.947     -2.129      0.033      -3.872      -0.160\n",
      "184            0.4196      0.219      1.918      0.055      -0.009       0.849\n",
      "186           -0.0503      0.401     -0.125      0.900      -0.836       0.735\n",
      "188           -0.4633      0.381     -1.216      0.224      -1.210       0.283\n",
      "189           -0.0476      0.362     -0.131      0.895      -0.757       0.662\n",
      "201           -0.0359      0.291     -0.124      0.902      -0.606       0.534\n",
      "209           -0.2567      0.119     -2.152      0.031      -0.490      -0.023\n",
      "224           -0.0421      0.097     -0.435      0.663      -0.232       0.148\n",
      "251           -0.0569      0.150     -0.379      0.705      -0.352       0.238\n",
      "253           -7.6360      7.420     -1.029      0.303     -22.179       6.907\n",
      "269            0.1634      0.126      1.294      0.196      -0.084       0.411\n",
      "275            0.0445      0.138      0.322      0.747      -0.226       0.315\n",
      "419           -0.0969      0.124     -0.779      0.436      -0.341       0.147\n",
      "420            0.0748      0.122      0.611      0.541      -0.165       0.315\n",
      "424            0.1285      0.130      0.990      0.322      -0.126       0.383\n",
      "429           -1.0083      0.636     -1.584      0.113      -2.256       0.239\n",
      "433           -0.1156      0.141     -0.821      0.412      -0.392       0.161\n",
      "434            0.2277      0.117      1.943      0.052      -0.002       0.457\n",
      "439            0.2295      0.136      1.693      0.090      -0.036       0.495\n",
      "440           -0.2107      0.181     -1.165      0.244      -0.565       0.144\n",
      "457           -0.0596      0.231     -0.258      0.796      -0.512       0.393\n",
      "461            0.4072      0.334      1.217      0.223      -0.248       1.063\n",
      "468            0.0968      0.093      1.039      0.299      -0.086       0.279\n",
      "469           -0.0955      0.150     -0.638      0.523      -0.388       0.198\n",
      "472            0.2089      0.196      1.066      0.286      -0.175       0.593\n",
      "473            0.2533      0.202      1.254      0.210      -0.143       0.649\n",
      "474            0.1433      0.149      0.963      0.335      -0.148       0.435\n",
      "483           -0.0256      0.122     -0.210      0.834      -0.265       0.214\n",
      "484            0.0518      0.128      0.404      0.686      -0.200       0.303\n",
      "485           -0.2065      0.129     -1.603      0.109      -0.459       0.046\n",
      "486           -0.1413      0.132     -1.073      0.283      -0.399       0.117\n",
      "487            0.2795      0.127      2.197      0.028       0.030       0.529\n",
      "488            0.1151      0.127      0.909      0.363      -0.133       0.363\n",
      "489           -0.2623      0.124     -2.117      0.034      -0.505      -0.019\n",
      "490            0.1653      0.120      1.381      0.167      -0.069       0.400\n",
      "491           -0.1215      0.129     -0.939      0.348      -0.375       0.132\n",
      "495            0.0651      0.082      0.799      0.425      -0.095       0.225\n",
      "497           -0.0837      0.134     -0.625      0.532      -0.346       0.179\n",
      "500           -0.1088      0.122     -0.888      0.374      -0.349       0.131\n",
      "501            0.1457      0.117      1.246      0.213      -0.083       0.375\n",
      "511            0.1349      0.098      1.377      0.168      -0.057       0.327\n",
      "512            0.2864      0.114      2.514      0.012       0.063       0.510\n",
      "520            0.0749      0.099      0.758      0.448      -0.119       0.268\n",
      "542            0.2513      0.117      2.148      0.032       0.022       0.481\n",
      "548            0.1880      0.130      1.441      0.150      -0.068       0.444\n",
      "562            0.0353      0.113      0.312      0.755      -0.186       0.257\n",
      "563           -0.1819      0.124     -1.463      0.143      -0.426       0.062\n",
      "570            0.2117      0.112      1.895      0.058      -0.007       0.431\n",
      "573            0.3926      0.257      1.530      0.126      -0.110       0.896\n",
      "578           -0.6662      0.298     -2.235      0.025      -1.250      -0.082\n",
      "582           -0.1563      0.141     -1.107      0.268      -0.433       0.120\n",
      "==============================================================================\n",
      "Removing feature 201 with p-value 0.901677\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179899\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1491\n",
      "Method:                           MLE   Df Model:                           75\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2631\n",
      "Time:                        16:34:56   Log-Likelihood:                -281.90\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.675e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1143      0.316    -13.039      0.000      -4.733      -3.496\n",
      "2             -0.1448      0.119     -1.220      0.222      -0.377       0.088\n",
      "3             -0.1079      0.137     -0.785      0.432      -0.377       0.161\n",
      "4             -0.0483      0.158     -0.306      0.759      -0.357       0.261\n",
      "15            -0.3552      0.141     -2.528      0.011      -0.631      -0.080\n",
      "16             0.3127      0.259      1.206      0.228      -0.195       0.821\n",
      "22             0.2992      0.188      1.592      0.111      -0.069       0.668\n",
      "23             0.0416      0.183      0.227      0.820      -0.317       0.400\n",
      "24            -0.0648      0.125     -0.520      0.603      -0.309       0.179\n",
      "41            -0.0368      0.117     -0.315      0.753      -0.266       0.192\n",
      "52             0.1961      0.363      0.541      0.589      -0.515       0.907\n",
      "56            -0.0663      0.146     -0.454      0.650      -0.353       0.220\n",
      "60             0.9643      0.163      5.920      0.000       0.645       1.283\n",
      "63            -0.3887      0.222     -1.754      0.079      -0.823       0.046\n",
      "65             0.6188      0.133      4.670      0.000       0.359       0.879\n",
      "69            -0.2814      0.155     -1.811      0.070      -0.586       0.023\n",
      "72             0.2337      0.115      2.036      0.042       0.009       0.459\n",
      "73             0.4988      0.322      1.550      0.121      -0.132       1.129\n",
      "74            -0.9643      0.487     -1.978      0.048      -1.920      -0.009\n",
      "89             0.0502      0.126      0.397      0.691      -0.198       0.298\n",
      "91            -0.1800      0.167     -1.078      0.281      -0.507       0.147\n",
      "116           -0.0464      0.168     -0.277      0.782      -0.375       0.282\n",
      "134            0.2239      0.137      1.636      0.102      -0.044       0.492\n",
      "137           -0.0892      0.159     -0.562      0.574      -0.400       0.222\n",
      "138            0.1059      0.172      0.617      0.537      -0.230       0.442\n",
      "139            0.1176      0.128      0.920      0.357      -0.133       0.368\n",
      "152           -2.0144      0.947     -2.128      0.033      -3.870      -0.159\n",
      "184            0.4152      0.216      1.923      0.055      -0.008       0.838\n",
      "186           -0.0503      0.401     -0.125      0.900      -0.836       0.735\n",
      "188           -0.4626      0.381     -1.215      0.224      -1.209       0.284\n",
      "189           -0.0429      0.360     -0.119      0.905      -0.749       0.663\n",
      "209           -0.2572      0.119     -2.158      0.031      -0.491      -0.024\n",
      "224           -0.0426      0.097     -0.441      0.659      -0.232       0.147\n",
      "251           -0.0556      0.150     -0.371      0.710      -0.349       0.238\n",
      "253           -7.6708      7.414     -1.035      0.301     -22.202       6.861\n",
      "269            0.1633      0.126      1.292      0.196      -0.084       0.411\n",
      "275            0.0458      0.138      0.333      0.739      -0.224       0.316\n",
      "419           -0.0971      0.124     -0.781      0.435      -0.341       0.147\n",
      "420            0.0743      0.122      0.607      0.544      -0.166       0.314\n",
      "424            0.1285      0.130      0.990      0.322      -0.126       0.383\n",
      "429           -1.0081      0.637     -1.582      0.114      -2.257       0.241\n",
      "433           -0.1151      0.141     -0.817      0.414      -0.391       0.161\n",
      "434            0.2276      0.117      1.942      0.052      -0.002       0.457\n",
      "439            0.2298      0.136      1.694      0.090      -0.036       0.496\n",
      "440           -0.2113      0.181     -1.167      0.243      -0.566       0.143\n",
      "457           -0.0586      0.231     -0.254      0.799      -0.510       0.393\n",
      "461            0.4020      0.332      1.211      0.226      -0.249       1.053\n",
      "468            0.0991      0.091      1.085      0.278      -0.080       0.278\n",
      "469           -0.0948      0.149     -0.634      0.526      -0.388       0.198\n",
      "472            0.2063      0.195      1.059      0.289      -0.175       0.588\n",
      "473            0.2379      0.159      1.492      0.136      -0.075       0.550\n",
      "474            0.1414      0.148      0.957      0.339      -0.148       0.431\n",
      "483           -0.0257      0.122     -0.210      0.833      -0.265       0.214\n",
      "484            0.0523      0.128      0.407      0.684      -0.199       0.304\n",
      "485           -0.2058      0.129     -1.598      0.110      -0.458       0.047\n",
      "486           -0.1408      0.132     -1.070      0.285      -0.399       0.117\n",
      "487            0.2793      0.127      2.196      0.028       0.030       0.529\n",
      "488            0.1160      0.126      0.918      0.359      -0.132       0.364\n",
      "489           -0.2619      0.124     -2.114      0.034      -0.505      -0.019\n",
      "490            0.1647      0.120      1.376      0.169      -0.070       0.399\n",
      "491           -0.1215      0.129     -0.939      0.348      -0.375       0.132\n",
      "495            0.0649      0.082      0.795      0.426      -0.095       0.225\n",
      "497           -0.0830      0.134     -0.621      0.535      -0.345       0.179\n",
      "500           -0.1090      0.122     -0.890      0.373      -0.349       0.131\n",
      "501            0.1452      0.117      1.243      0.214      -0.084       0.374\n",
      "511            0.1356      0.098      1.387      0.165      -0.056       0.327\n",
      "512            0.2868      0.114      2.517      0.012       0.063       0.510\n",
      "520            0.0747      0.099      0.757      0.449      -0.119       0.268\n",
      "542            0.2512      0.117      2.148      0.032       0.022       0.480\n",
      "548            0.1887      0.130      1.448      0.148      -0.067       0.444\n",
      "562            0.0354      0.113      0.313      0.754      -0.186       0.257\n",
      "563           -0.1815      0.124     -1.461      0.144      -0.425       0.062\n",
      "570            0.2115      0.112      1.892      0.059      -0.008       0.431\n",
      "573            0.3925      0.257      1.529      0.126      -0.111       0.896\n",
      "578           -0.6669      0.298     -2.237      0.025      -1.251      -0.083\n",
      "582           -0.1555      0.141     -1.103      0.270      -0.432       0.121\n",
      "==============================================================================\n",
      "Removing feature 189 with p-value 0.905197\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179904\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1492\n",
      "Method:                           MLE   Df Model:                           74\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2631\n",
      "Time:                        16:34:57   Log-Likelihood:                -281.91\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.009e-13\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1134      0.315    -13.042      0.000      -4.732      -3.495\n",
      "2             -0.1449      0.119     -1.222      0.222      -0.377       0.088\n",
      "3             -0.1096      0.137     -0.802      0.422      -0.377       0.158\n",
      "4             -0.0473      0.158     -0.300      0.764      -0.356       0.261\n",
      "15            -0.3551      0.141     -2.527      0.012      -0.631      -0.080\n",
      "16             0.3131      0.259      1.208      0.227      -0.195       0.821\n",
      "22             0.3003      0.188      1.600      0.110      -0.068       0.668\n",
      "23             0.0422      0.183      0.231      0.817      -0.316       0.400\n",
      "24            -0.0657      0.124     -0.528      0.597      -0.309       0.178\n",
      "41            -0.0368      0.117     -0.315      0.753      -0.266       0.192\n",
      "52             0.1568      0.152      1.032      0.302      -0.141       0.455\n",
      "56            -0.0689      0.144     -0.477      0.634      -0.352       0.214\n",
      "60             0.9637      0.163      5.919      0.000       0.645       1.283\n",
      "63            -0.3914      0.221     -1.774      0.076      -0.824       0.041\n",
      "65             0.6177      0.132      4.672      0.000       0.359       0.877\n",
      "69            -0.2816      0.155     -1.811      0.070      -0.586       0.023\n",
      "72             0.2336      0.115      2.034      0.042       0.009       0.459\n",
      "73             0.5016      0.321      1.563      0.118      -0.127       1.130\n",
      "74            -0.9569      0.484     -1.979      0.048      -1.905      -0.009\n",
      "89             0.0500      0.126      0.396      0.692      -0.198       0.298\n",
      "91            -0.1788      0.167     -1.073      0.283      -0.505       0.148\n",
      "116           -0.0467      0.168     -0.279      0.781      -0.375       0.282\n",
      "134            0.2263      0.135      1.670      0.095      -0.039       0.492\n",
      "137           -0.0897      0.159     -0.565      0.572      -0.401       0.221\n",
      "138            0.1052      0.171      0.614      0.540      -0.231       0.441\n",
      "139            0.1187      0.128      0.931      0.352      -0.131       0.369\n",
      "152           -2.0135      0.947     -2.127      0.033      -3.869      -0.158\n",
      "184            0.4168      0.215      1.937      0.053      -0.005       0.839\n",
      "186           -0.0499      0.401     -0.125      0.901      -0.835       0.735\n",
      "188           -0.4651      0.380     -1.225      0.221      -1.209       0.279\n",
      "209           -0.2568      0.119     -2.155      0.031      -0.490      -0.023\n",
      "224           -0.0434      0.097     -0.450      0.653      -0.233       0.146\n",
      "251           -0.0547      0.150     -0.365      0.715      -0.348       0.238\n",
      "253           -7.6132      7.396     -1.029      0.303     -22.109       6.882\n",
      "269            0.1634      0.126      1.294      0.196      -0.084       0.411\n",
      "275            0.0451      0.138      0.328      0.743      -0.225       0.315\n",
      "419           -0.0969      0.124     -0.780      0.436      -0.341       0.147\n",
      "420            0.0750      0.122      0.614      0.540      -0.165       0.315\n",
      "424            0.1304      0.129      1.012      0.311      -0.122       0.383\n",
      "429           -1.0107      0.637     -1.586      0.113      -2.260       0.238\n",
      "433           -0.1149      0.141     -0.816      0.415      -0.391       0.161\n",
      "434            0.2272      0.117      1.939      0.052      -0.002       0.457\n",
      "439            0.2296      0.136      1.689      0.091      -0.037       0.496\n",
      "440           -0.2112      0.181     -1.166      0.244      -0.566       0.144\n",
      "457           -0.0499      0.214     -0.233      0.816      -0.470       0.370\n",
      "461            0.3667      0.149      2.466      0.014       0.075       0.658\n",
      "468            0.0983      0.091      1.079      0.281      -0.080       0.277\n",
      "469           -0.0958      0.149     -0.641      0.521      -0.388       0.197\n",
      "472            0.2048      0.194      1.054      0.292      -0.176       0.586\n",
      "473            0.2384      0.160      1.494      0.135      -0.074       0.551\n",
      "474            0.1402      0.148      0.950      0.342      -0.149       0.429\n",
      "483           -0.0263      0.122     -0.215      0.829      -0.266       0.213\n",
      "484            0.0514      0.128      0.401      0.688      -0.200       0.303\n",
      "485           -0.2063      0.129     -1.604      0.109      -0.458       0.046\n",
      "486           -0.1401      0.131     -1.066      0.287      -0.398       0.118\n",
      "487            0.2801      0.127      2.205      0.027       0.031       0.529\n",
      "488            0.1162      0.126      0.919      0.358      -0.132       0.364\n",
      "489           -0.2634      0.123     -2.138      0.032      -0.505      -0.022\n",
      "490            0.1650      0.120      1.379      0.168      -0.069       0.399\n",
      "491           -0.1224      0.129     -0.947      0.344      -0.376       0.131\n",
      "495            0.0645      0.081      0.791      0.429      -0.095       0.224\n",
      "497           -0.0829      0.134     -0.620      0.535      -0.345       0.179\n",
      "500           -0.1102      0.122     -0.903      0.366      -0.349       0.129\n",
      "501            0.1461      0.117      1.254      0.210      -0.082       0.375\n",
      "511            0.1365      0.097      1.401      0.161      -0.054       0.327\n",
      "512            0.2866      0.114      2.516      0.012       0.063       0.510\n",
      "520            0.0754      0.099      0.765      0.444      -0.118       0.269\n",
      "542            0.2522      0.117      2.162      0.031       0.024       0.481\n",
      "548            0.1886      0.130      1.447      0.148      -0.067       0.444\n",
      "562            0.0352      0.113      0.311      0.756      -0.186       0.257\n",
      "563           -0.1822      0.124     -1.468      0.142      -0.425       0.061\n",
      "570            0.2111      0.112      1.889      0.059      -0.008       0.430\n",
      "573            0.3918      0.257      1.527      0.127      -0.111       0.895\n",
      "578           -0.6665      0.298     -2.235      0.025      -1.251      -0.082\n",
      "582           -0.1553      0.141     -1.102      0.270      -0.431       0.121\n",
      "==============================================================================\n",
      "Removing feature 186 with p-value 0.900878\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179909\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1493\n",
      "Method:                           MLE   Df Model:                           73\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2631\n",
      "Time:                        16:34:57   Log-Likelihood:                -281.92\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.043e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1129      0.315    -13.038      0.000      -4.731      -3.495\n",
      "2             -0.1445      0.119     -1.218      0.223      -0.377       0.088\n",
      "3             -0.1106      0.136     -0.811      0.418      -0.378       0.157\n",
      "4             -0.0470      0.158     -0.299      0.765      -0.356       0.262\n",
      "15            -0.3558      0.140     -2.533      0.011      -0.631      -0.081\n",
      "16             0.3119      0.259      1.204      0.229      -0.196       0.820\n",
      "22             0.2997      0.187      1.599      0.110      -0.068       0.667\n",
      "23             0.0408      0.182      0.224      0.823      -0.316       0.398\n",
      "24            -0.0658      0.124     -0.529      0.597      -0.310       0.178\n",
      "41            -0.0367      0.117     -0.314      0.753      -0.266       0.192\n",
      "52             0.1630      0.143      1.137      0.255      -0.118       0.444\n",
      "56            -0.0685      0.144     -0.474      0.635      -0.351       0.214\n",
      "60             0.9640      0.163      5.920      0.000       0.645       1.283\n",
      "63            -0.3954      0.218     -1.811      0.070      -0.823       0.032\n",
      "65             0.6175      0.132      4.669      0.000       0.358       0.877\n",
      "69            -0.2840      0.154     -1.840      0.066      -0.586       0.018\n",
      "72             0.2348      0.114      2.051      0.040       0.010       0.459\n",
      "73             0.5031      0.320      1.570      0.116      -0.125       1.131\n",
      "74            -0.9534      0.483     -1.976      0.048      -1.899      -0.008\n",
      "89             0.0500      0.126      0.396      0.692      -0.198       0.298\n",
      "91            -0.1786      0.167     -1.073      0.283      -0.505       0.148\n",
      "116           -0.0477      0.167     -0.285      0.775      -0.376       0.280\n",
      "134            0.2262      0.135      1.671      0.095      -0.039       0.492\n",
      "137           -0.0905      0.158     -0.571      0.568      -0.401       0.220\n",
      "138            0.1063      0.171      0.621      0.535      -0.229       0.442\n",
      "139            0.1191      0.127      0.935      0.350      -0.131       0.369\n",
      "152           -2.0201      0.946     -2.136      0.033      -3.874      -0.166\n",
      "184            0.4116      0.211      1.954      0.051      -0.001       0.824\n",
      "188           -0.4597      0.376     -1.223      0.221      -1.196       0.277\n",
      "209           -0.2570      0.119     -2.157      0.031      -0.490      -0.024\n",
      "224           -0.0436      0.097     -0.451      0.652      -0.233       0.146\n",
      "251           -0.0558      0.149     -0.374      0.709      -0.348       0.237\n",
      "253           -7.6281      7.396     -1.031      0.302     -22.124       6.868\n",
      "269            0.1633      0.126      1.292      0.196      -0.084       0.411\n",
      "275            0.0451      0.138      0.327      0.743      -0.225       0.315\n",
      "419           -0.0961      0.124     -0.774      0.439      -0.339       0.147\n",
      "420            0.0748      0.122      0.612      0.540      -0.165       0.314\n",
      "424            0.1307      0.129      1.015      0.310      -0.122       0.383\n",
      "429           -1.0131      0.638     -1.588      0.112      -2.263       0.237\n",
      "433           -0.1145      0.141     -0.813      0.416      -0.391       0.162\n",
      "434            0.2275      0.117      1.941      0.052      -0.002       0.457\n",
      "439            0.2279      0.136      1.678      0.093      -0.038       0.494\n",
      "440           -0.2105      0.181     -1.161      0.246      -0.566       0.145\n",
      "457           -0.0529      0.211     -0.250      0.802      -0.467       0.361\n",
      "461            0.3664      0.148      2.469      0.014       0.076       0.657\n",
      "468            0.0988      0.091      1.086      0.278      -0.080       0.277\n",
      "469           -0.0960      0.149     -0.643      0.520      -0.389       0.197\n",
      "472            0.2058      0.194      1.061      0.289      -0.175       0.586\n",
      "473            0.2395      0.159      1.504      0.133      -0.073       0.552\n",
      "474            0.1387      0.147      0.944      0.345      -0.149       0.427\n",
      "483           -0.0269      0.122     -0.221      0.825      -0.266       0.212\n",
      "484            0.0520      0.128      0.406      0.685      -0.199       0.303\n",
      "485           -0.2074      0.128     -1.616      0.106      -0.459       0.044\n",
      "486           -0.1392      0.131     -1.061      0.289      -0.396       0.118\n",
      "487            0.2794      0.127      2.201      0.028       0.031       0.528\n",
      "488            0.1155      0.126      0.915      0.360      -0.132       0.363\n",
      "489           -0.2637      0.123     -2.140      0.032      -0.505      -0.022\n",
      "490            0.1654      0.120      1.383      0.167      -0.069       0.400\n",
      "491           -0.1224      0.129     -0.947      0.344      -0.376       0.131\n",
      "495            0.0645      0.081      0.791      0.429      -0.095       0.224\n",
      "497           -0.0822      0.134     -0.615      0.538      -0.344       0.180\n",
      "500           -0.1105      0.122     -0.906      0.365      -0.350       0.129\n",
      "501            0.1468      0.116      1.261      0.207      -0.081       0.375\n",
      "511            0.1367      0.097      1.403      0.161      -0.054       0.328\n",
      "512            0.2868      0.114      2.517      0.012       0.063       0.510\n",
      "520            0.0746      0.098      0.758      0.448      -0.118       0.267\n",
      "542            0.2509      0.116      2.159      0.031       0.023       0.479\n",
      "548            0.1884      0.130      1.446      0.148      -0.067       0.444\n",
      "562            0.0353      0.113      0.313      0.754      -0.186       0.257\n",
      "563           -0.1817      0.124     -1.464      0.143      -0.425       0.061\n",
      "570            0.2115      0.112      1.894      0.058      -0.007       0.430\n",
      "573            0.3915      0.256      1.528      0.127      -0.111       0.894\n",
      "578           -0.6661      0.298     -2.236      0.025      -1.250      -0.082\n",
      "582           -0.1563      0.141     -1.112      0.266      -0.432       0.119\n",
      "==============================================================================\n",
      "Removing feature 483 with p-value 0.825463\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179925\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1494\n",
      "Method:                           MLE   Df Model:                           72\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2630\n",
      "Time:                        16:34:57   Log-Likelihood:                -281.94\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.632e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1103      0.315    -13.039      0.000      -4.728      -3.492\n",
      "2             -0.1450      0.119     -1.223      0.222      -0.377       0.087\n",
      "3             -0.1105      0.137     -0.809      0.418      -0.378       0.157\n",
      "4             -0.0484      0.157     -0.308      0.758      -0.357       0.260\n",
      "15            -0.3560      0.140     -2.534      0.011      -0.631      -0.081\n",
      "16             0.3113      0.259      1.202      0.229      -0.196       0.819\n",
      "22             0.3008      0.187      1.607      0.108      -0.066       0.668\n",
      "23             0.0416      0.182      0.229      0.819      -0.315       0.398\n",
      "24            -0.0652      0.124     -0.524      0.600      -0.309       0.179\n",
      "41            -0.0371      0.117     -0.317      0.751      -0.266       0.192\n",
      "52             0.1603      0.143      1.122      0.262      -0.120       0.440\n",
      "56            -0.0673      0.144     -0.467      0.641      -0.350       0.215\n",
      "60             0.9644      0.163      5.919      0.000       0.645       1.284\n",
      "63            -0.3952      0.218     -1.811      0.070      -0.823       0.032\n",
      "65             0.6175      0.132      4.667      0.000       0.358       0.877\n",
      "69            -0.2834      0.154     -1.838      0.066      -0.586       0.019\n",
      "72             0.2350      0.114      2.053      0.040       0.011       0.459\n",
      "73             0.5042      0.320      1.574      0.116      -0.124       1.132\n",
      "74            -0.9533      0.482     -1.978      0.048      -1.898      -0.009\n",
      "89             0.0489      0.126      0.387      0.699      -0.199       0.296\n",
      "91            -0.1814      0.166     -1.093      0.274      -0.507       0.144\n",
      "116           -0.0452      0.167     -0.271      0.786      -0.372       0.282\n",
      "134            0.2269      0.135      1.677      0.094      -0.038       0.492\n",
      "137           -0.0903      0.158     -0.570      0.569      -0.401       0.220\n",
      "138            0.1048      0.171      0.613      0.540      -0.230       0.440\n",
      "139            0.1198      0.127      0.940      0.347      -0.130       0.370\n",
      "152           -2.0161      0.946     -2.132      0.033      -3.870      -0.162\n",
      "184            0.4111      0.211      1.951      0.051      -0.002       0.824\n",
      "188           -0.4570      0.376     -1.216      0.224      -1.193       0.279\n",
      "209           -0.2570      0.119     -2.158      0.031      -0.491      -0.024\n",
      "224           -0.0429      0.097     -0.444      0.657      -0.232       0.146\n",
      "251           -0.0553      0.149     -0.371      0.711      -0.348       0.237\n",
      "253           -7.5575      7.391     -1.022      0.307     -22.044       6.929\n",
      "269            0.1620      0.126      1.284      0.199      -0.085       0.409\n",
      "275            0.0470      0.137      0.342      0.732      -0.222       0.316\n",
      "419           -0.0960      0.124     -0.774      0.439      -0.339       0.147\n",
      "420            0.0738      0.122      0.604      0.546      -0.166       0.313\n",
      "424            0.1309      0.129      1.016      0.310      -0.122       0.383\n",
      "429           -1.0060      0.636     -1.582      0.114      -2.252       0.240\n",
      "433           -0.1166      0.141     -0.830      0.407      -0.392       0.159\n",
      "434            0.2286      0.117      1.955      0.051      -0.001       0.458\n",
      "439            0.2283      0.136      1.684      0.092      -0.037       0.494\n",
      "440           -0.2115      0.182     -1.163      0.245      -0.568       0.145\n",
      "457           -0.0560      0.212     -0.264      0.791      -0.471       0.359\n",
      "461            0.3670      0.148      2.472      0.013       0.076       0.658\n",
      "468            0.1006      0.091      1.110      0.267      -0.077       0.278\n",
      "469           -0.0944      0.149     -0.634      0.526      -0.386       0.197\n",
      "472            0.2039      0.193      1.057      0.290      -0.174       0.582\n",
      "473            0.2383      0.159      1.495      0.135      -0.074       0.551\n",
      "474            0.1400      0.147      0.955      0.340      -0.147       0.427\n",
      "484            0.0528      0.128      0.413      0.680      -0.198       0.304\n",
      "485           -0.2061      0.128     -1.608      0.108      -0.457       0.045\n",
      "486           -0.1382      0.131     -1.054      0.292      -0.395       0.119\n",
      "487            0.2810      0.127      2.218      0.027       0.033       0.529\n",
      "488            0.1166      0.126      0.924      0.356      -0.131       0.364\n",
      "489           -0.2644      0.123     -2.147      0.032      -0.506      -0.023\n",
      "490            0.1639      0.119      1.372      0.170      -0.070       0.398\n",
      "491           -0.1218      0.129     -0.943      0.346      -0.375       0.132\n",
      "495            0.0652      0.081      0.800      0.424      -0.094       0.225\n",
      "497           -0.0827      0.134     -0.620      0.536      -0.345       0.179\n",
      "500           -0.1106      0.122     -0.907      0.364      -0.350       0.128\n",
      "501            0.1467      0.116      1.261      0.207      -0.081       0.375\n",
      "511            0.1362      0.098      1.396      0.163      -0.055       0.327\n",
      "512            0.2863      0.114      2.512      0.012       0.063       0.510\n",
      "520            0.0736      0.098      0.750      0.453      -0.119       0.266\n",
      "542            0.2511      0.116      2.161      0.031       0.023       0.479\n",
      "548            0.1882      0.130      1.444      0.149      -0.067       0.444\n",
      "562            0.0338      0.113      0.300      0.764      -0.187       0.255\n",
      "563           -0.1839      0.124     -1.487      0.137      -0.426       0.059\n",
      "570            0.2109      0.112      1.891      0.059      -0.008       0.430\n",
      "573            0.3908      0.256      1.526      0.127      -0.111       0.893\n",
      "578           -0.6700      0.298     -2.250      0.024      -1.254      -0.086\n",
      "582           -0.1556      0.140     -1.109      0.267      -0.431       0.119\n",
      "==============================================================================\n",
      "Removing feature 23 with p-value 0.819153\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179941\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1495\n",
      "Method:                           MLE   Df Model:                           71\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2630\n",
      "Time:                        16:34:57   Log-Likelihood:                -281.97\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.171e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1119      0.315    -13.049      0.000      -4.730      -3.494\n",
      "2             -0.1453      0.119     -1.226      0.220      -0.378       0.087\n",
      "3             -0.1098      0.137     -0.804      0.421      -0.378       0.158\n",
      "4             -0.0509      0.157     -0.324      0.746      -0.358       0.257\n",
      "15            -0.3558      0.140     -2.533      0.011      -0.631      -0.080\n",
      "16             0.3141      0.259      1.213      0.225      -0.193       0.821\n",
      "22             0.2657      0.107      2.481      0.013       0.056       0.476\n",
      "24            -0.0663      0.124     -0.534      0.593      -0.310       0.177\n",
      "41            -0.0368      0.117     -0.315      0.753      -0.266       0.192\n",
      "52             0.1580      0.143      1.108      0.268      -0.122       0.438\n",
      "56            -0.0689      0.144     -0.478      0.632      -0.352       0.214\n",
      "60             0.9646      0.163      5.918      0.000       0.645       1.284\n",
      "63            -0.3963      0.218     -1.816      0.069      -0.824       0.031\n",
      "65             0.6154      0.132      4.660      0.000       0.357       0.874\n",
      "69            -0.2818      0.154     -1.828      0.068      -0.584       0.020\n",
      "72             0.2362      0.114      2.066      0.039       0.012       0.460\n",
      "73             0.5106      0.319      1.599      0.110      -0.115       1.136\n",
      "74            -0.9481      0.481     -1.970      0.049      -1.892      -0.005\n",
      "89             0.0473      0.126      0.375      0.708      -0.200       0.294\n",
      "91            -0.1819      0.166     -1.095      0.273      -0.507       0.144\n",
      "116           -0.0458      0.167     -0.274      0.784      -0.373       0.281\n",
      "134            0.2308      0.134      1.719      0.086      -0.032       0.494\n",
      "137           -0.0910      0.158     -0.574      0.566      -0.402       0.220\n",
      "138            0.1040      0.171      0.608      0.543      -0.231       0.439\n",
      "139            0.1185      0.127      0.931      0.352      -0.131       0.368\n",
      "152           -2.0167      0.945     -2.134      0.033      -3.869      -0.164\n",
      "184            0.4120      0.211      1.954      0.051      -0.001       0.825\n",
      "188           -0.4633      0.375     -1.235      0.217      -1.199       0.272\n",
      "209           -0.2580      0.119     -2.167      0.030      -0.491      -0.025\n",
      "224           -0.0444      0.096     -0.461      0.645      -0.233       0.144\n",
      "251           -0.0566      0.149     -0.379      0.704      -0.349       0.236\n",
      "253           -7.6974      7.372     -1.044      0.296     -22.146       6.751\n",
      "269            0.1605      0.126      1.274      0.203      -0.086       0.407\n",
      "275            0.0491      0.137      0.359      0.720      -0.219       0.317\n",
      "419           -0.0964      0.124     -0.777      0.437      -0.340       0.147\n",
      "420            0.0744      0.122      0.610      0.542      -0.165       0.314\n",
      "424            0.1292      0.129      1.004      0.315      -0.123       0.381\n",
      "429           -1.0034      0.633     -1.584      0.113      -2.245       0.238\n",
      "433           -0.1185      0.141     -0.842      0.400      -0.394       0.157\n",
      "434            0.2286      0.117      1.955      0.051      -0.001       0.458\n",
      "439            0.2272      0.135      1.680      0.093      -0.038       0.492\n",
      "440           -0.2085      0.181     -1.150      0.250      -0.564       0.147\n",
      "457           -0.0572      0.213     -0.269      0.788      -0.474       0.360\n",
      "461            0.3685      0.148      2.485      0.013       0.078       0.659\n",
      "468            0.1002      0.091      1.104      0.269      -0.078       0.278\n",
      "469           -0.0963      0.149     -0.648      0.517      -0.388       0.195\n",
      "472            0.2084      0.191      1.091      0.275      -0.166       0.583\n",
      "473            0.2374      0.160      1.488      0.137      -0.075       0.550\n",
      "474            0.1365      0.146      0.936      0.349      -0.149       0.422\n",
      "484            0.0524      0.128      0.410      0.682      -0.198       0.303\n",
      "485           -0.2036      0.128     -1.595      0.111      -0.454       0.047\n",
      "486           -0.1390      0.131     -1.061      0.289      -0.396       0.118\n",
      "487            0.2803      0.127      2.213      0.027       0.032       0.528\n",
      "488            0.1166      0.126      0.924      0.356      -0.131       0.364\n",
      "489           -0.2640      0.123     -2.145      0.032      -0.505      -0.023\n",
      "490            0.1650      0.119      1.384      0.166      -0.069       0.399\n",
      "491           -0.1232      0.129     -0.953      0.340      -0.376       0.130\n",
      "495            0.0658      0.081      0.809      0.419      -0.094       0.225\n",
      "497           -0.0853      0.133     -0.639      0.523      -0.347       0.176\n",
      "500           -0.1103      0.122     -0.905      0.366      -0.349       0.129\n",
      "501            0.1438      0.116      1.243      0.214      -0.083       0.371\n",
      "511            0.1391      0.097      1.440      0.150      -0.050       0.329\n",
      "512            0.2882      0.114      2.536      0.011       0.065       0.511\n",
      "520            0.0725      0.098      0.739      0.460      -0.120       0.265\n",
      "542            0.2519      0.116      2.168      0.030       0.024       0.480\n",
      "548            0.1889      0.130      1.450      0.147      -0.066       0.444\n",
      "562            0.0308      0.112      0.275      0.784      -0.189       0.251\n",
      "563           -0.1828      0.124     -1.479      0.139      -0.425       0.059\n",
      "570            0.2097      0.111      1.883      0.060      -0.009       0.428\n",
      "573            0.3930      0.256      1.534      0.125      -0.109       0.895\n",
      "578           -0.6710      0.298     -2.251      0.024      -1.255      -0.087\n",
      "582           -0.1564      0.140     -1.115      0.265      -0.431       0.118\n",
      "==============================================================================\n",
      "Removing feature 457 with p-value 0.788077\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179965\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1496\n",
      "Method:                           MLE   Df Model:                           70\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2629\n",
      "Time:                        16:34:58   Log-Likelihood:                -282.01\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.299e-14\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1112      0.315    -13.064      0.000      -4.728      -3.494\n",
      "2             -0.1439      0.118     -1.214      0.225      -0.376       0.088\n",
      "3             -0.1093      0.137     -0.800      0.424      -0.377       0.158\n",
      "4             -0.0489      0.156     -0.313      0.754      -0.356       0.258\n",
      "15            -0.3559      0.140     -2.535      0.011      -0.631      -0.081\n",
      "16             0.3209      0.257      1.248      0.212      -0.183       0.825\n",
      "22             0.2655      0.107      2.481      0.013       0.056       0.475\n",
      "24            -0.0663      0.124     -0.533      0.594      -0.310       0.177\n",
      "41            -0.0390      0.117     -0.335      0.738      -0.268       0.189\n",
      "52             0.1669      0.138      1.209      0.227      -0.104       0.438\n",
      "56            -0.0750      0.142     -0.527      0.598      -0.354       0.204\n",
      "60             0.9693      0.162      5.974      0.000       0.651       1.287\n",
      "63            -0.3972      0.218     -1.821      0.069      -0.825       0.030\n",
      "65             0.6174      0.132      4.686      0.000       0.359       0.876\n",
      "69            -0.2820      0.154     -1.827      0.068      -0.585       0.020\n",
      "72             0.2355      0.114      2.058      0.040       0.011       0.460\n",
      "73             0.5052      0.318      1.588      0.112      -0.118       1.129\n",
      "74            -0.9536      0.481     -1.984      0.047      -1.896      -0.012\n",
      "89             0.0447      0.126      0.355      0.722      -0.202       0.291\n",
      "91            -0.1853      0.166     -1.118      0.264      -0.510       0.140\n",
      "116           -0.0439      0.167     -0.263      0.792      -0.371       0.283\n",
      "134            0.2286      0.134      1.708      0.088      -0.034       0.491\n",
      "137           -0.0912      0.158     -0.575      0.565      -0.402       0.219\n",
      "138            0.1039      0.171      0.608      0.543      -0.231       0.439\n",
      "139            0.1165      0.127      0.917      0.359      -0.133       0.366\n",
      "152           -2.0122      0.943     -2.133      0.033      -3.861      -0.163\n",
      "184            0.4179      0.208      2.013      0.044       0.011       0.825\n",
      "188           -0.4877      0.359     -1.357      0.175      -1.192       0.217\n",
      "209           -0.2586      0.119     -2.172      0.030      -0.492      -0.025\n",
      "224           -0.0439      0.096     -0.455      0.649      -0.233       0.145\n",
      "251           -0.0576      0.149     -0.386      0.699      -0.350       0.235\n",
      "253           -7.7735      7.370     -1.055      0.292     -22.219       6.672\n",
      "269            0.1618      0.126      1.285      0.199      -0.085       0.409\n",
      "275            0.0486      0.137      0.355      0.723      -0.220       0.317\n",
      "419           -0.0983      0.124     -0.793      0.428      -0.341       0.145\n",
      "420            0.0736      0.122      0.604      0.546      -0.166       0.313\n",
      "424            0.1292      0.128      1.006      0.314      -0.122       0.381\n",
      "429           -0.9908      0.630     -1.573      0.116      -2.225       0.243\n",
      "433           -0.1172      0.140     -0.835      0.404      -0.392       0.158\n",
      "434            0.2294      0.117      1.966      0.049       0.001       0.458\n",
      "439            0.2258      0.135      1.667      0.095      -0.040       0.491\n",
      "440           -0.2080      0.182     -1.142      0.254      -0.565       0.149\n",
      "461            0.3478      0.128      2.715      0.007       0.097       0.599\n",
      "468            0.0994      0.091      1.097      0.273      -0.078       0.277\n",
      "469           -0.0958      0.149     -0.644      0.520      -0.387       0.196\n",
      "472            0.2079      0.191      1.087      0.277      -0.167       0.583\n",
      "473            0.2411      0.159      1.519      0.129      -0.070       0.552\n",
      "474            0.1326      0.145      0.912      0.362      -0.152       0.417\n",
      "484            0.0521      0.128      0.407      0.684      -0.199       0.303\n",
      "485           -0.2025      0.128     -1.587      0.112      -0.452       0.048\n",
      "486           -0.1418      0.131     -1.084      0.279      -0.398       0.115\n",
      "487            0.2800      0.127      2.210      0.027       0.032       0.528\n",
      "488            0.1158      0.126      0.917      0.359      -0.132       0.363\n",
      "489           -0.2652      0.123     -2.157      0.031      -0.506      -0.024\n",
      "490            0.1659      0.119      1.391      0.164      -0.068       0.400\n",
      "491           -0.1234      0.129     -0.955      0.340      -0.377       0.130\n",
      "495            0.0663      0.081      0.815      0.415      -0.093       0.226\n",
      "497           -0.0874      0.133     -0.656      0.512      -0.348       0.174\n",
      "500           -0.1088      0.122     -0.894      0.372      -0.347       0.130\n",
      "501            0.1424      0.116      1.231      0.218      -0.084       0.369\n",
      "511            0.1391      0.097      1.440      0.150      -0.050       0.328\n",
      "512            0.2880      0.114      2.534      0.011       0.065       0.511\n",
      "520            0.0706      0.098      0.722      0.470      -0.121       0.262\n",
      "542            0.2515      0.116      2.165      0.030       0.024       0.479\n",
      "548            0.1863      0.130      1.434      0.152      -0.068       0.441\n",
      "562            0.0321      0.112      0.287      0.774      -0.187       0.252\n",
      "563           -0.1832      0.124     -1.483      0.138      -0.425       0.059\n",
      "570            0.2075      0.111      1.870      0.061      -0.010       0.425\n",
      "573            0.3922      0.256      1.531      0.126      -0.110       0.894\n",
      "578           -0.6707      0.298     -2.251      0.024      -1.255      -0.087\n",
      "582           -0.1583      0.140     -1.130      0.259      -0.433       0.116\n",
      "==============================================================================\n",
      "Removing feature 116 with p-value 0.792460\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.179987\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1497\n",
      "Method:                           MLE   Df Model:                           69\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2628\n",
      "Time:                        16:34:58   Log-Likelihood:                -282.04\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.703e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1113      0.315    -13.069      0.000      -4.728      -3.495\n",
      "2             -0.1442      0.118     -1.218      0.223      -0.376       0.088\n",
      "3             -0.1112      0.136     -0.815      0.415      -0.379       0.156\n",
      "4             -0.0478      0.156     -0.306      0.760      -0.354       0.259\n",
      "15            -0.3568      0.140     -2.543      0.011      -0.632      -0.082\n",
      "16             0.3241      0.257      1.262      0.207      -0.179       0.827\n",
      "22             0.2659      0.107      2.485      0.013       0.056       0.476\n",
      "24            -0.0666      0.124     -0.536      0.592      -0.310       0.177\n",
      "41            -0.0413      0.116     -0.355      0.722      -0.269       0.186\n",
      "52             0.1677      0.138      1.214      0.225      -0.103       0.438\n",
      "56            -0.0734      0.142     -0.516      0.606      -0.352       0.205\n",
      "60             0.9676      0.162      5.966      0.000       0.650       1.285\n",
      "63            -0.3963      0.218     -1.818      0.069      -0.824       0.031\n",
      "65             0.6164      0.132      4.681      0.000       0.358       0.874\n",
      "69            -0.2781      0.154     -1.812      0.070      -0.579       0.023\n",
      "72             0.2349      0.114      2.055      0.040       0.011       0.459\n",
      "73             0.5074      0.318      1.595      0.111      -0.116       1.131\n",
      "74            -0.9520      0.481     -1.979      0.048      -1.895      -0.009\n",
      "89             0.0400      0.125      0.321      0.748      -0.204       0.284\n",
      "91            -0.2138      0.125     -1.706      0.088      -0.459       0.032\n",
      "134            0.2283      0.134      1.707      0.088      -0.034       0.490\n",
      "137           -0.0905      0.159     -0.571      0.568      -0.401       0.220\n",
      "138            0.0995      0.170      0.585      0.558      -0.234       0.433\n",
      "139            0.1172      0.127      0.923      0.356      -0.132       0.366\n",
      "152           -2.0203      0.943     -2.143      0.032      -3.868      -0.173\n",
      "184            0.4170      0.208      2.008      0.045       0.010       0.824\n",
      "188           -0.4812      0.360     -1.338      0.181      -1.186       0.224\n",
      "209           -0.2586      0.119     -2.173      0.030      -0.492      -0.025\n",
      "224           -0.0394      0.095     -0.414      0.679      -0.226       0.147\n",
      "251           -0.0570      0.149     -0.383      0.702      -0.349       0.235\n",
      "253           -7.8107      7.363     -1.061      0.289     -22.242       6.620\n",
      "269            0.1589      0.125      1.267      0.205      -0.087       0.405\n",
      "275            0.0472      0.137      0.345      0.730      -0.221       0.316\n",
      "419           -0.0984      0.124     -0.794      0.427      -0.341       0.144\n",
      "420            0.0726      0.122      0.595      0.552      -0.166       0.312\n",
      "424            0.1296      0.129      1.008      0.313      -0.122       0.382\n",
      "429           -0.9968      0.629     -1.586      0.113      -2.229       0.235\n",
      "433           -0.1165      0.140     -0.831      0.406      -0.391       0.158\n",
      "434            0.2296      0.117      1.967      0.049       0.001       0.458\n",
      "439            0.2270      0.135      1.677      0.094      -0.038       0.492\n",
      "440           -0.2104      0.182     -1.154      0.248      -0.568       0.147\n",
      "461            0.3487      0.128      2.721      0.007       0.097       0.600\n",
      "468            0.1013      0.090      1.125      0.260      -0.075       0.278\n",
      "469           -0.0965      0.149     -0.649      0.516      -0.388       0.195\n",
      "472            0.2083      0.192      1.087      0.277      -0.167       0.584\n",
      "473            0.2396      0.158      1.512      0.130      -0.071       0.550\n",
      "474            0.1319      0.145      0.908      0.364      -0.153       0.417\n",
      "484            0.0521      0.128      0.407      0.684      -0.199       0.303\n",
      "485           -0.2010      0.127     -1.578      0.115      -0.451       0.049\n",
      "486           -0.1414      0.131     -1.081      0.280      -0.398       0.115\n",
      "487            0.2783      0.127      2.200      0.028       0.030       0.526\n",
      "488            0.1163      0.126      0.921      0.357      -0.131       0.364\n",
      "489           -0.2647      0.123     -2.154      0.031      -0.506      -0.024\n",
      "490            0.1648      0.119      1.383      0.167      -0.069       0.398\n",
      "491           -0.1242      0.129     -0.961      0.336      -0.377       0.129\n",
      "495            0.0630      0.080      0.785      0.432      -0.094       0.220\n",
      "497           -0.0840      0.132     -0.636      0.525      -0.343       0.175\n",
      "500           -0.1096      0.122     -0.900      0.368      -0.348       0.129\n",
      "501            0.1420      0.116      1.228      0.219      -0.085       0.369\n",
      "511            0.1407      0.096      1.459      0.144      -0.048       0.330\n",
      "512            0.2879      0.114      2.534      0.011       0.065       0.511\n",
      "520            0.0713      0.098      0.729      0.466      -0.120       0.263\n",
      "542            0.2522      0.116      2.169      0.030       0.024       0.480\n",
      "548            0.1850      0.130      1.425      0.154      -0.069       0.440\n",
      "562            0.0312      0.112      0.279      0.780      -0.188       0.251\n",
      "563           -0.1830      0.124     -1.480      0.139      -0.425       0.059\n",
      "570            0.2090      0.111      1.888      0.059      -0.008       0.426\n",
      "573            0.3932      0.256      1.534      0.125      -0.109       0.896\n",
      "578           -0.6724      0.298     -2.256      0.024      -1.257      -0.088\n",
      "582           -0.1611      0.140     -1.154      0.249      -0.435       0.113\n",
      "==============================================================================\n",
      "Removing feature 562 with p-value 0.780416\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180012\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1498\n",
      "Method:                           MLE   Df Model:                           68\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2627\n",
      "Time:                        16:34:58   Log-Likelihood:                -282.08\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.549e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1131      0.315    -13.076      0.000      -4.730      -3.497\n",
      "2             -0.1462      0.118     -1.238      0.216      -0.378       0.085\n",
      "3             -0.1107      0.136     -0.812      0.417      -0.378       0.157\n",
      "4             -0.0484      0.156     -0.309      0.757      -0.355       0.258\n",
      "15            -0.3545      0.140     -2.533      0.011      -0.629      -0.080\n",
      "16             0.3214      0.257      1.252      0.210      -0.182       0.824\n",
      "22             0.2641      0.107      2.476      0.013       0.055       0.473\n",
      "24            -0.0668      0.124     -0.539      0.590      -0.310       0.176\n",
      "41            -0.0404      0.116     -0.348      0.728      -0.268       0.187\n",
      "52             0.1665      0.138      1.206      0.228      -0.104       0.437\n",
      "56            -0.0722      0.142     -0.509      0.611      -0.350       0.206\n",
      "60             0.9682      0.162      5.967      0.000       0.650       1.286\n",
      "63            -0.3971      0.218     -1.821      0.069      -0.825       0.030\n",
      "65             0.6162      0.132      4.684      0.000       0.358       0.874\n",
      "69            -0.2780      0.154     -1.806      0.071      -0.580       0.024\n",
      "72             0.2358      0.114      2.063      0.039       0.012       0.460\n",
      "73             0.5086      0.318      1.600      0.110      -0.114       1.132\n",
      "74            -0.9520      0.481     -1.980      0.048      -1.895      -0.010\n",
      "89             0.0425      0.124      0.342      0.732      -0.201       0.286\n",
      "91            -0.2156      0.125     -1.722      0.085      -0.461       0.030\n",
      "134            0.2294      0.134      1.714      0.086      -0.033       0.492\n",
      "137           -0.0903      0.159     -0.569      0.569      -0.401       0.220\n",
      "138            0.0984      0.170      0.579      0.563      -0.235       0.431\n",
      "139            0.1172      0.127      0.923      0.356      -0.132       0.366\n",
      "152           -2.0059      0.940     -2.135      0.033      -3.848      -0.164\n",
      "184            0.4184      0.207      2.016      0.044       0.012       0.825\n",
      "188           -0.4835      0.359     -1.347      0.178      -1.187       0.220\n",
      "209           -0.2584      0.119     -2.171      0.030      -0.492      -0.025\n",
      "224           -0.0398      0.095     -0.418      0.676      -0.226       0.147\n",
      "251           -0.0559      0.149     -0.376      0.707      -0.347       0.236\n",
      "253           -7.9078      7.358     -1.075      0.283     -22.330       6.514\n",
      "269            0.1589      0.125      1.267      0.205      -0.087       0.405\n",
      "275            0.0482      0.137      0.352      0.725      -0.220       0.317\n",
      "419           -0.0967      0.124     -0.782      0.434      -0.339       0.146\n",
      "420            0.0723      0.122      0.593      0.553      -0.167       0.311\n",
      "424            0.1325      0.128      1.035      0.301      -0.118       0.383\n",
      "429           -1.0029      0.629     -1.595      0.111      -2.236       0.230\n",
      "433           -0.1174      0.140     -0.839      0.401      -0.392       0.157\n",
      "434            0.2314      0.117      1.985      0.047       0.003       0.460\n",
      "439            0.2283      0.136      1.677      0.094      -0.039       0.495\n",
      "440           -0.2113      0.182     -1.159      0.246      -0.568       0.146\n",
      "461            0.3508      0.128      2.744      0.006       0.100       0.601\n",
      "468            0.1020      0.090      1.137      0.256      -0.074       0.278\n",
      "469           -0.0959      0.149     -0.645      0.519      -0.387       0.196\n",
      "472            0.2097      0.191      1.095      0.273      -0.166       0.585\n",
      "473            0.2389      0.158      1.509      0.131      -0.071       0.549\n",
      "474            0.1309      0.145      0.901      0.368      -0.154       0.416\n",
      "484            0.0526      0.128      0.410      0.682      -0.199       0.304\n",
      "485           -0.2005      0.127     -1.574      0.115      -0.450       0.049\n",
      "486           -0.1400      0.131     -1.072      0.284      -0.396       0.116\n",
      "487            0.2793      0.126      2.208      0.027       0.031       0.527\n",
      "488            0.1159      0.126      0.918      0.358      -0.131       0.363\n",
      "489           -0.2619      0.122     -2.139      0.032      -0.502      -0.022\n",
      "490            0.1648      0.119      1.384      0.166      -0.069       0.398\n",
      "491           -0.1252      0.129     -0.969      0.332      -0.378       0.128\n",
      "495            0.0623      0.080      0.777      0.437      -0.095       0.219\n",
      "497           -0.0811      0.131     -0.617      0.537      -0.339       0.176\n",
      "500           -0.1102      0.122     -0.905      0.366      -0.349       0.128\n",
      "501            0.1436      0.116      1.243      0.214      -0.083       0.370\n",
      "511            0.1418      0.096      1.471      0.141      -0.047       0.331\n",
      "512            0.2869      0.114      2.526      0.012       0.064       0.509\n",
      "520            0.0690      0.097      0.708      0.479      -0.122       0.260\n",
      "542            0.2518      0.116      2.166      0.030       0.024       0.480\n",
      "548            0.1875      0.130      1.447      0.148      -0.066       0.442\n",
      "563           -0.1838      0.124     -1.487      0.137      -0.426       0.058\n",
      "570            0.2107      0.110      1.907      0.057      -0.006       0.427\n",
      "573            0.3963      0.256      1.547      0.122      -0.106       0.898\n",
      "578           -0.6784      0.298     -2.279      0.023      -1.262      -0.095\n",
      "582           -0.1608      0.140     -1.152      0.249      -0.435       0.113\n",
      "==============================================================================\n",
      "Removing feature 4 with p-value 0.756989\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180043\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1499\n",
      "Method:                           MLE   Df Model:                           67\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2626\n",
      "Time:                        16:34:58   Log-Likelihood:                -282.13\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.685e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1099      0.314    -13.094      0.000      -4.725      -3.495\n",
      "2             -0.1451      0.118     -1.230      0.219      -0.376       0.086\n",
      "3             -0.1286      0.123     -1.042      0.297      -0.371       0.113\n",
      "15            -0.3515      0.140     -2.516      0.012      -0.625      -0.078\n",
      "16             0.3210      0.257      1.251      0.211      -0.182       0.824\n",
      "22             0.2649      0.107      2.483      0.013       0.056       0.474\n",
      "24            -0.0647      0.124     -0.522      0.602      -0.308       0.178\n",
      "41            -0.0389      0.116     -0.335      0.737      -0.266       0.188\n",
      "52             0.1683      0.138      1.220      0.223      -0.102       0.439\n",
      "56            -0.0720      0.142     -0.507      0.612      -0.350       0.206\n",
      "60             0.9696      0.162      5.979      0.000       0.652       1.288\n",
      "63            -0.3945      0.218     -1.810      0.070      -0.822       0.033\n",
      "65             0.6170      0.132      4.689      0.000       0.359       0.875\n",
      "69            -0.2769      0.154     -1.800      0.072      -0.578       0.025\n",
      "72             0.2343      0.114      2.050      0.040       0.010       0.458\n",
      "73             0.5065      0.318      1.593      0.111      -0.117       1.130\n",
      "74            -0.9640      0.479     -2.013      0.044      -1.902      -0.026\n",
      "89             0.0413      0.124      0.333      0.739      -0.202       0.284\n",
      "91            -0.2138      0.125     -1.709      0.087      -0.459       0.031\n",
      "134            0.2273      0.134      1.702      0.089      -0.034       0.489\n",
      "137           -0.0923      0.158     -0.583      0.560      -0.402       0.218\n",
      "138            0.1015      0.169      0.599      0.549      -0.230       0.434\n",
      "139            0.1239      0.125      0.994      0.320      -0.120       0.368\n",
      "152           -2.0034      0.940     -2.131      0.033      -3.846      -0.161\n",
      "184            0.4177      0.208      2.011      0.044       0.011       0.825\n",
      "188           -0.4854      0.360     -1.350      0.177      -1.190       0.219\n",
      "209           -0.2589      0.119     -2.179      0.029      -0.492      -0.026\n",
      "224           -0.0384      0.095     -0.404      0.686      -0.225       0.148\n",
      "251           -0.0535      0.148     -0.361      0.718      -0.344       0.237\n",
      "253           -7.8163      7.341     -1.065      0.287     -22.204       6.571\n",
      "269            0.1637      0.125      1.314      0.189      -0.080       0.408\n",
      "275            0.0417      0.135      0.309      0.757      -0.223       0.306\n",
      "419           -0.0970      0.124     -0.785      0.432      -0.339       0.145\n",
      "420            0.0708      0.122      0.581      0.561      -0.168       0.310\n",
      "424            0.1332      0.128      1.040      0.298      -0.118       0.384\n",
      "429           -1.0016      0.626     -1.599      0.110      -2.229       0.226\n",
      "433           -0.1184      0.140     -0.845      0.398      -0.393       0.156\n",
      "434            0.2316      0.117      1.987      0.047       0.003       0.460\n",
      "439            0.2292      0.136      1.684      0.092      -0.037       0.496\n",
      "440           -0.2116      0.182     -1.160      0.246      -0.569       0.146\n",
      "461            0.3518      0.128      2.748      0.006       0.101       0.603\n",
      "468            0.1020      0.090      1.139      0.255      -0.074       0.278\n",
      "469           -0.0970      0.149     -0.652      0.515      -0.389       0.195\n",
      "472            0.2101      0.190      1.105      0.269      -0.163       0.583\n",
      "473            0.2395      0.158      1.511      0.131      -0.071       0.550\n",
      "474            0.1324      0.145      0.914      0.361      -0.151       0.416\n",
      "484            0.0540      0.128      0.422      0.673      -0.197       0.305\n",
      "485           -0.1992      0.127     -1.565      0.117      -0.449       0.050\n",
      "486           -0.1398      0.131     -1.069      0.285      -0.396       0.116\n",
      "487            0.2784      0.127      2.200      0.028       0.030       0.526\n",
      "488            0.1134      0.126      0.900      0.368      -0.134       0.360\n",
      "489           -0.2607      0.122     -2.131      0.033      -0.500      -0.021\n",
      "490            0.1629      0.119      1.369      0.171      -0.070       0.396\n",
      "491           -0.1220      0.129     -0.948      0.343      -0.374       0.130\n",
      "495            0.0612      0.080      0.764      0.445      -0.096       0.218\n",
      "497           -0.0795      0.131     -0.605      0.545      -0.337       0.178\n",
      "500           -0.1115      0.122     -0.916      0.359      -0.350       0.127\n",
      "501            0.1431      0.115      1.239      0.215      -0.083       0.369\n",
      "511            0.1428      0.096      1.483      0.138      -0.046       0.331\n",
      "512            0.2893      0.113      2.555      0.011       0.067       0.511\n",
      "520            0.0699      0.097      0.717      0.473      -0.121       0.261\n",
      "542            0.2531      0.116      2.177      0.029       0.025       0.481\n",
      "548            0.1877      0.130      1.448      0.147      -0.066       0.442\n",
      "563           -0.1842      0.124     -1.490      0.136      -0.427       0.058\n",
      "570            0.2086      0.110      1.893      0.058      -0.007       0.424\n",
      "573            0.3991      0.256      1.559      0.119      -0.103       0.901\n",
      "578           -0.6823      0.297     -2.294      0.022      -1.265      -0.099\n",
      "582           -0.1587      0.139     -1.138      0.255      -0.432       0.115\n",
      "==============================================================================\n",
      "Removing feature 275 with p-value 0.756952\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180073\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1500\n",
      "Method:                           MLE   Df Model:                           66\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2624\n",
      "Time:                        16:34:59   Log-Likelihood:                -282.17\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.572e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1134      0.314    -13.101      0.000      -4.729      -3.498\n",
      "2             -0.1453      0.118     -1.233      0.218      -0.376       0.086\n",
      "3             -0.1274      0.123     -1.033      0.301      -0.369       0.114\n",
      "15            -0.3544      0.139     -2.540      0.011      -0.628      -0.081\n",
      "16             0.3187      0.256      1.243      0.214      -0.184       0.821\n",
      "22             0.2657      0.107      2.492      0.013       0.057       0.475\n",
      "24            -0.0651      0.124     -0.526      0.599      -0.308       0.178\n",
      "41            -0.0427      0.115     -0.370      0.711      -0.269       0.183\n",
      "52             0.1693      0.138      1.227      0.220      -0.101       0.440\n",
      "56            -0.0707      0.142     -0.499      0.618      -0.349       0.207\n",
      "60             0.9701      0.162      5.979      0.000       0.652       1.288\n",
      "63            -0.3944      0.218     -1.812      0.070      -0.821       0.032\n",
      "65             0.6180      0.132      4.698      0.000       0.360       0.876\n",
      "69            -0.2746      0.153     -1.791      0.073      -0.575       0.026\n",
      "72             0.2381      0.113      2.099      0.036       0.016       0.461\n",
      "73             0.5054      0.318      1.592      0.111      -0.117       1.128\n",
      "74            -0.9595      0.479     -2.005      0.045      -1.898      -0.021\n",
      "89             0.0402      0.124      0.325      0.746      -0.203       0.283\n",
      "91            -0.2102      0.125     -1.687      0.092      -0.454       0.034\n",
      "134            0.2240      0.133      1.681      0.093      -0.037       0.485\n",
      "137           -0.0911      0.158     -0.576      0.565      -0.401       0.219\n",
      "138            0.1027      0.169      0.606      0.544      -0.229       0.435\n",
      "139            0.1293      0.124      1.044      0.296      -0.113       0.372\n",
      "152           -2.0088      0.939     -2.138      0.032      -3.850      -0.167\n",
      "184            0.4167      0.208      2.006      0.045       0.010       0.824\n",
      "188           -0.4864      0.360     -1.353      0.176      -1.191       0.218\n",
      "209           -0.2630      0.118     -2.226      0.026      -0.495      -0.031\n",
      "224           -0.0385      0.095     -0.405      0.685      -0.225       0.148\n",
      "251           -0.0529      0.148     -0.357      0.721      -0.343       0.238\n",
      "253           -7.9173      7.331     -1.080      0.280     -22.286       6.451\n",
      "269            0.1637      0.125      1.314      0.189      -0.080       0.408\n",
      "419           -0.0990      0.123     -0.802      0.423      -0.341       0.143\n",
      "420            0.0698      0.122      0.574      0.566      -0.169       0.308\n",
      "424            0.1359      0.128      1.063      0.288      -0.115       0.386\n",
      "429           -1.0083      0.626     -1.611      0.107      -2.235       0.218\n",
      "433           -0.1205      0.140     -0.862      0.389      -0.394       0.153\n",
      "434            0.2326      0.117      1.995      0.046       0.004       0.461\n",
      "439            0.2272      0.135      1.680      0.093      -0.038       0.492\n",
      "440           -0.2100      0.182     -1.154      0.249      -0.567       0.147\n",
      "461            0.3498      0.128      2.731      0.006       0.099       0.601\n",
      "468            0.1021      0.090      1.139      0.255      -0.074       0.278\n",
      "469           -0.0950      0.149     -0.639      0.523      -0.387       0.197\n",
      "472            0.2108      0.191      1.106      0.269      -0.163       0.584\n",
      "473            0.2428      0.158      1.539      0.124      -0.066       0.552\n",
      "474            0.1360      0.144      0.942      0.346      -0.147       0.419\n",
      "484            0.0548      0.128      0.428      0.668      -0.196       0.305\n",
      "485           -0.1980      0.127     -1.558      0.119      -0.447       0.051\n",
      "486           -0.1378      0.130     -1.056      0.291      -0.393       0.118\n",
      "487            0.2794      0.126      2.210      0.027       0.032       0.527\n",
      "488            0.1148      0.126      0.912      0.362      -0.132       0.362\n",
      "489           -0.2634      0.122     -2.158      0.031      -0.503      -0.024\n",
      "490            0.1635      0.119      1.376      0.169      -0.069       0.396\n",
      "491           -0.1185      0.128     -0.925      0.355      -0.370       0.133\n",
      "495            0.0603      0.080      0.754      0.451      -0.097       0.217\n",
      "497           -0.0767      0.131     -0.585      0.558      -0.334       0.180\n",
      "500           -0.1112      0.122     -0.914      0.361      -0.350       0.127\n",
      "501            0.1462      0.115      1.270      0.204      -0.079       0.372\n",
      "511            0.1404      0.096      1.462      0.144      -0.048       0.328\n",
      "512            0.2884      0.113      2.547      0.011       0.066       0.510\n",
      "520            0.0679      0.097      0.698      0.485      -0.123       0.259\n",
      "542            0.2554      0.116      2.199      0.028       0.028       0.483\n",
      "548            0.1860      0.130      1.436      0.151      -0.068       0.440\n",
      "563           -0.1824      0.124     -1.475      0.140      -0.425       0.060\n",
      "570            0.2081      0.110      1.889      0.059      -0.008       0.424\n",
      "573            0.3974      0.255      1.556      0.120      -0.103       0.898\n",
      "578           -0.6811      0.297     -2.293      0.022      -1.263      -0.099\n",
      "582           -0.1596      0.139     -1.147      0.251      -0.432       0.113\n",
      "==============================================================================\n",
      "Removing feature 89 with p-value 0.745521\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180107\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1501\n",
      "Method:                           MLE   Df Model:                           65\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2623\n",
      "Time:                        16:34:59   Log-Likelihood:                -282.23\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.177e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1118      0.314    -13.103      0.000      -4.727      -3.497\n",
      "2             -0.1444      0.118     -1.226      0.220      -0.375       0.086\n",
      "3             -0.1279      0.123     -1.037      0.300      -0.369       0.114\n",
      "15            -0.3527      0.139     -2.533      0.011      -0.626      -0.080\n",
      "16             0.3214      0.256      1.256      0.209      -0.180       0.823\n",
      "22             0.2654      0.107      2.490      0.013       0.056       0.474\n",
      "24            -0.0643      0.124     -0.520      0.603      -0.307       0.178\n",
      "41            -0.0416      0.115     -0.361      0.718      -0.268       0.184\n",
      "52             0.1685      0.138      1.221      0.222      -0.102       0.439\n",
      "56            -0.0722      0.142     -0.509      0.611      -0.350       0.206\n",
      "60             0.9749      0.162      6.030      0.000       0.658       1.292\n",
      "63            -0.3956      0.218     -1.818      0.069      -0.822       0.031\n",
      "65             0.6194      0.132      4.701      0.000       0.361       0.878\n",
      "69            -0.2733      0.153     -1.783      0.075      -0.574       0.027\n",
      "72             0.2380      0.113      2.100      0.036       0.016       0.460\n",
      "73             0.5072      0.318      1.597      0.110      -0.115       1.130\n",
      "74            -0.9663      0.478     -2.021      0.043      -1.904      -0.029\n",
      "91            -0.2084      0.124     -1.677      0.094      -0.452       0.035\n",
      "134            0.2242      0.133      1.683      0.092      -0.037       0.485\n",
      "137           -0.0936      0.158     -0.592      0.554      -0.403       0.216\n",
      "138            0.1051      0.169      0.622      0.534      -0.226       0.436\n",
      "139            0.1287      0.124      1.040      0.298      -0.114       0.371\n",
      "152           -2.0080      0.938     -2.142      0.032      -3.846      -0.170\n",
      "184            0.4185      0.208      2.014      0.044       0.011       0.826\n",
      "188           -0.4929      0.359     -1.371      0.170      -1.197       0.212\n",
      "209           -0.2652      0.118     -2.248      0.025      -0.496      -0.034\n",
      "224           -0.0416      0.094     -0.444      0.657      -0.226       0.142\n",
      "251           -0.0463      0.146     -0.316      0.752      -0.333       0.241\n",
      "253           -7.8475      7.325     -1.071      0.284     -22.205       6.509\n",
      "269            0.1646      0.125      1.321      0.186      -0.080       0.409\n",
      "419           -0.1021      0.123     -0.829      0.407      -0.343       0.139\n",
      "420            0.0716      0.122      0.588      0.556      -0.167       0.310\n",
      "424            0.1398      0.127      1.099      0.272      -0.109       0.389\n",
      "429           -1.0092      0.625     -1.614      0.107      -2.235       0.217\n",
      "433           -0.1205      0.140     -0.861      0.389      -0.395       0.154\n",
      "434            0.2331      0.117      2.000      0.045       0.005       0.462\n",
      "439            0.2283      0.136      1.684      0.092      -0.037       0.494\n",
      "440           -0.2098      0.183     -1.148      0.251      -0.568       0.149\n",
      "461            0.3514      0.128      2.744      0.006       0.100       0.602\n",
      "468            0.1000      0.090      1.115      0.265      -0.076       0.276\n",
      "469           -0.0952      0.149     -0.640      0.522      -0.387       0.196\n",
      "472            0.2117      0.191      1.110      0.267      -0.162       0.585\n",
      "473            0.2440      0.158      1.542      0.123      -0.066       0.554\n",
      "474            0.1347      0.144      0.934      0.350      -0.148       0.417\n",
      "484            0.0554      0.128      0.433      0.665      -0.195       0.306\n",
      "485           -0.1986      0.127     -1.563      0.118      -0.448       0.050\n",
      "486           -0.1384      0.131     -1.060      0.289      -0.394       0.118\n",
      "487            0.2799      0.127      2.213      0.027       0.032       0.528\n",
      "488            0.1133      0.126      0.900      0.368      -0.133       0.360\n",
      "489           -0.2643      0.122     -2.167      0.030      -0.503      -0.025\n",
      "490            0.1622      0.119      1.364      0.173      -0.071       0.395\n",
      "491           -0.1180      0.128     -0.921      0.357      -0.369       0.133\n",
      "495            0.0601      0.080      0.751      0.453      -0.097       0.217\n",
      "497           -0.0782      0.131     -0.596      0.551      -0.335       0.179\n",
      "500           -0.1115      0.122     -0.917      0.359      -0.350       0.127\n",
      "501            0.1452      0.115      1.263      0.207      -0.080       0.371\n",
      "511            0.1411      0.096      1.471      0.141      -0.047       0.329\n",
      "512            0.2882      0.113      2.546      0.011       0.066       0.510\n",
      "520            0.0647      0.097      0.669      0.504      -0.125       0.255\n",
      "542            0.2554      0.116      2.199      0.028       0.028       0.483\n",
      "548            0.1836      0.129      1.422      0.155      -0.069       0.437\n",
      "563           -0.1847      0.124     -1.495      0.135      -0.427       0.057\n",
      "570            0.2047      0.110      1.869      0.062      -0.010       0.420\n",
      "573            0.3985      0.255      1.562      0.118      -0.101       0.898\n",
      "578           -0.6824      0.297     -2.298      0.022      -1.264      -0.100\n",
      "582           -0.1588      0.139     -1.142      0.253      -0.431       0.114\n",
      "==============================================================================\n",
      "Removing feature 251 with p-value 0.752146\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180140\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1502\n",
      "Method:                           MLE   Df Model:                           64\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2622\n",
      "Time:                        16:34:59   Log-Likelihood:                -282.28\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.312e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1127      0.314    -13.088      0.000      -4.729      -3.497\n",
      "2             -0.1452      0.118     -1.233      0.218      -0.376       0.086\n",
      "3             -0.1285      0.123     -1.043      0.297      -0.370       0.113\n",
      "15            -0.3501      0.139     -2.519      0.012      -0.622      -0.078\n",
      "16             0.3201      0.256      1.251      0.211      -0.182       0.822\n",
      "22             0.2645      0.107      2.481      0.013       0.056       0.474\n",
      "24            -0.0628      0.124     -0.508      0.612      -0.305       0.180\n",
      "41            -0.0407      0.115     -0.353      0.724      -0.267       0.185\n",
      "52             0.1684      0.138      1.220      0.222      -0.102       0.439\n",
      "56            -0.0713      0.142     -0.503      0.615      -0.349       0.206\n",
      "60             0.9707      0.161      6.033      0.000       0.655       1.286\n",
      "63            -0.3981      0.217     -1.831      0.067      -0.824       0.028\n",
      "65             0.6165      0.131      4.694      0.000       0.359       0.874\n",
      "69            -0.2722      0.153     -1.776      0.076      -0.573       0.028\n",
      "72             0.2361      0.113      2.086      0.037       0.014       0.458\n",
      "73             0.5064      0.318      1.594      0.111      -0.116       1.129\n",
      "74            -0.9468      0.475     -1.995      0.046      -1.877      -0.017\n",
      "91            -0.2070      0.124     -1.667      0.095      -0.450       0.036\n",
      "134            0.2251      0.133      1.689      0.091      -0.036       0.486\n",
      "137           -0.0932      0.158     -0.591      0.555      -0.403       0.216\n",
      "138            0.1048      0.169      0.620      0.535      -0.226       0.436\n",
      "139            0.1299      0.124      1.050      0.294      -0.113       0.372\n",
      "152           -2.0041      0.937     -2.138      0.033      -3.841      -0.167\n",
      "184            0.4211      0.208      2.027      0.043       0.014       0.828\n",
      "188           -0.4980      0.359     -1.386      0.166      -1.202       0.206\n",
      "209           -0.2647      0.118     -2.244      0.025      -0.496      -0.034\n",
      "224           -0.0442      0.093     -0.473      0.636      -0.227       0.139\n",
      "253           -7.9787      7.325     -1.089      0.276     -22.335       6.377\n",
      "269            0.1645      0.125      1.320      0.187      -0.080       0.409\n",
      "419           -0.1023      0.123     -0.831      0.406      -0.344       0.139\n",
      "420            0.0725      0.122      0.596      0.551      -0.166       0.311\n",
      "424            0.1408      0.127      1.107      0.268      -0.109       0.390\n",
      "429           -1.0072      0.626     -1.609      0.108      -2.234       0.220\n",
      "433           -0.1205      0.140     -0.860      0.390      -0.395       0.154\n",
      "434            0.2346      0.117      2.013      0.044       0.006       0.463\n",
      "439            0.2288      0.135      1.691      0.091      -0.036       0.494\n",
      "440           -0.2126      0.182     -1.168      0.243      -0.570       0.144\n",
      "461            0.3535      0.128      2.764      0.006       0.103       0.604\n",
      "468            0.1001      0.090      1.118      0.264      -0.075       0.276\n",
      "469           -0.0945      0.149     -0.636      0.525      -0.386       0.197\n",
      "472            0.2118      0.191      1.110      0.267      -0.162       0.586\n",
      "473            0.2435      0.158      1.540      0.123      -0.066       0.553\n",
      "474            0.1339      0.144      0.928      0.353      -0.149       0.417\n",
      "484            0.0538      0.128      0.422      0.673      -0.196       0.304\n",
      "485           -0.1970      0.127     -1.554      0.120      -0.445       0.051\n",
      "486           -0.1382      0.131     -1.057      0.291      -0.394       0.118\n",
      "487            0.2787      0.126      2.206      0.027       0.031       0.526\n",
      "488            0.1142      0.126      0.907      0.364      -0.133       0.361\n",
      "489           -0.2629      0.122     -2.161      0.031      -0.501      -0.024\n",
      "490            0.1592      0.119      1.342      0.180      -0.073       0.392\n",
      "491           -0.1168      0.128     -0.912      0.362      -0.368       0.134\n",
      "495            0.0612      0.080      0.764      0.445      -0.096       0.218\n",
      "497           -0.0788      0.131     -0.602      0.547      -0.335       0.178\n",
      "500           -0.1103      0.121     -0.908      0.364      -0.348       0.128\n",
      "501            0.1451      0.115      1.261      0.207      -0.080       0.371\n",
      "511            0.1406      0.096      1.466      0.143      -0.047       0.328\n",
      "512            0.2880      0.113      2.545      0.011       0.066       0.510\n",
      "520            0.0677      0.096      0.703      0.482      -0.121       0.257\n",
      "542            0.2574      0.116      2.216      0.027       0.030       0.485\n",
      "548            0.1849      0.129      1.433      0.152      -0.068       0.438\n",
      "563           -0.1854      0.123     -1.503      0.133      -0.427       0.056\n",
      "570            0.2032      0.110      1.856      0.063      -0.011       0.418\n",
      "573            0.3980      0.255      1.564      0.118      -0.101       0.897\n",
      "578           -0.6849      0.297     -2.307      0.021      -1.267      -0.103\n",
      "582           -0.1599      0.139     -1.152      0.249      -0.432       0.112\n",
      "==============================================================================\n",
      "Removing feature 41 with p-value 0.724255\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180179\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1503\n",
      "Method:                           MLE   Df Model:                           63\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2620\n",
      "Time:                        16:34:59   Log-Likelihood:                -282.34\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.074e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1098      0.314    -13.076      0.000      -4.726      -3.494\n",
      "2             -0.1465      0.118     -1.243      0.214      -0.378       0.084\n",
      "3             -0.1301      0.123     -1.056      0.291      -0.372       0.111\n",
      "15            -0.3452      0.138     -2.499      0.012      -0.616      -0.074\n",
      "16             0.3180      0.256      1.244      0.213      -0.183       0.819\n",
      "22             0.2711      0.105      2.584      0.010       0.066       0.477\n",
      "24            -0.0643      0.124     -0.520      0.603      -0.306       0.178\n",
      "52             0.1701      0.138      1.234      0.217      -0.100       0.440\n",
      "56            -0.0690      0.141     -0.488      0.626      -0.346       0.208\n",
      "60             0.9704      0.161      6.038      0.000       0.655       1.285\n",
      "63            -0.3963      0.217     -1.827      0.068      -0.821       0.029\n",
      "65             0.6179      0.131      4.705      0.000       0.360       0.875\n",
      "69            -0.2702      0.153     -1.769      0.077      -0.570       0.029\n",
      "72             0.2389      0.113      2.122      0.034       0.018       0.460\n",
      "73             0.5106      0.317      1.609      0.108      -0.111       1.133\n",
      "74            -0.9419      0.474     -1.986      0.047      -1.872      -0.012\n",
      "91            -0.2100      0.124     -1.694      0.090      -0.453       0.033\n",
      "134            0.2278      0.133      1.715      0.086      -0.032       0.488\n",
      "137           -0.0911      0.158     -0.578      0.564      -0.400       0.218\n",
      "138            0.1032      0.169      0.611      0.541      -0.228       0.434\n",
      "139            0.1299      0.124      1.050      0.294      -0.113       0.372\n",
      "152           -1.9716      0.930     -2.119      0.034      -3.795      -0.148\n",
      "184            0.4243      0.208      2.043      0.041       0.017       0.831\n",
      "188           -0.5049      0.359     -1.407      0.159      -1.208       0.198\n",
      "209           -0.2650      0.118     -2.249      0.025      -0.496      -0.034\n",
      "224           -0.0455      0.094     -0.485      0.628      -0.229       0.138\n",
      "253           -7.9121      7.324     -1.080      0.280     -22.267       6.443\n",
      "269            0.1658      0.125      1.330      0.183      -0.078       0.410\n",
      "419           -0.1041      0.123     -0.847      0.397      -0.345       0.137\n",
      "420            0.0735      0.122      0.605      0.545      -0.165       0.312\n",
      "424            0.1409      0.127      1.108      0.268      -0.108       0.390\n",
      "429           -1.0250      0.627     -1.635      0.102      -2.254       0.204\n",
      "433           -0.1220      0.140     -0.872      0.383      -0.396       0.152\n",
      "434            0.2275      0.115      1.982      0.047       0.003       0.452\n",
      "439            0.2273      0.135      1.682      0.093      -0.038       0.492\n",
      "440           -0.2104      0.181     -1.161      0.246      -0.566       0.145\n",
      "461            0.3557      0.128      2.784      0.005       0.105       0.606\n",
      "468            0.1001      0.089      1.122      0.262      -0.075       0.275\n",
      "469           -0.0948      0.149     -0.638      0.523      -0.386       0.196\n",
      "472            0.2126      0.191      1.114      0.265      -0.162       0.587\n",
      "473            0.2430      0.158      1.540      0.124      -0.066       0.552\n",
      "474            0.1343      0.144      0.932      0.352      -0.148       0.417\n",
      "484            0.0502      0.127      0.394      0.693      -0.199       0.299\n",
      "485           -0.1960      0.127     -1.548      0.122      -0.444       0.052\n",
      "486           -0.1352      0.130     -1.039      0.299      -0.390       0.120\n",
      "487            0.2799      0.126      2.216      0.027       0.032       0.527\n",
      "488            0.1120      0.126      0.892      0.372      -0.134       0.358\n",
      "489           -0.2637      0.122     -2.165      0.030      -0.502      -0.025\n",
      "490            0.1592      0.119      1.343      0.179      -0.073       0.392\n",
      "491           -0.1155      0.128     -0.902      0.367      -0.366       0.135\n",
      "495            0.0608      0.080      0.760      0.447      -0.096       0.218\n",
      "497           -0.0808      0.131     -0.618      0.537      -0.337       0.176\n",
      "500           -0.1120      0.121     -0.924      0.356      -0.350       0.126\n",
      "501            0.1468      0.115      1.277      0.202      -0.079       0.372\n",
      "511            0.1457      0.094      1.542      0.123      -0.039       0.331\n",
      "512            0.2906      0.113      2.574      0.010       0.069       0.512\n",
      "520            0.0675      0.096      0.700      0.484      -0.121       0.256\n",
      "542            0.2572      0.116      2.214      0.027       0.030       0.485\n",
      "548            0.1888      0.129      1.469      0.142      -0.063       0.441\n",
      "563           -0.1858      0.123     -1.507      0.132      -0.427       0.056\n",
      "570            0.2065      0.109      1.892      0.058      -0.007       0.420\n",
      "573            0.4029      0.254      1.588      0.112      -0.094       0.900\n",
      "578           -0.6900      0.297     -2.325      0.020      -1.272      -0.108\n",
      "582           -0.1583      0.138     -1.143      0.253      -0.430       0.113\n",
      "==============================================================================\n",
      "Removing feature 484 with p-value 0.693229\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180228\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1504\n",
      "Method:                           MLE   Df Model:                           62\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2618\n",
      "Time:                        16:35:00   Log-Likelihood:                -282.42\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.785e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1095      0.314    -13.076      0.000      -4.726      -3.494\n",
      "2             -0.1492      0.118     -1.266      0.205      -0.380       0.082\n",
      "3             -0.1315      0.123     -1.069      0.285      -0.372       0.110\n",
      "15            -0.3480      0.138     -2.525      0.012      -0.618      -0.078\n",
      "16             0.3173      0.256      1.241      0.215      -0.184       0.818\n",
      "22             0.2711      0.105      2.582      0.010       0.065       0.477\n",
      "24            -0.0677      0.123     -0.549      0.583      -0.309       0.174\n",
      "52             0.1745      0.137      1.270      0.204      -0.095       0.444\n",
      "56            -0.0719      0.141     -0.510      0.610      -0.348       0.204\n",
      "60             0.9645      0.160      6.035      0.000       0.651       1.278\n",
      "63            -0.3995      0.217     -1.842      0.066      -0.825       0.026\n",
      "65             0.6140      0.131      4.686      0.000       0.357       0.871\n",
      "69            -0.2717      0.153     -1.775      0.076      -0.572       0.028\n",
      "72             0.2404      0.113      2.136      0.033       0.020       0.461\n",
      "73             0.5153      0.317      1.624      0.104      -0.106       1.137\n",
      "74            -0.9345      0.474     -1.971      0.049      -1.864      -0.005\n",
      "91            -0.2129      0.124     -1.721      0.085      -0.455       0.030\n",
      "134            0.2283      0.133      1.719      0.086      -0.032       0.489\n",
      "137           -0.0909      0.158     -0.576      0.564      -0.400       0.218\n",
      "138            0.1003      0.169      0.594      0.552      -0.231       0.431\n",
      "139            0.1289      0.124      1.043      0.297      -0.113       0.371\n",
      "152           -1.9898      0.929     -2.142      0.032      -3.811      -0.169\n",
      "184            0.4202      0.207      2.030      0.042       0.014       0.826\n",
      "188           -0.4972      0.358     -1.391      0.164      -1.198       0.204\n",
      "209           -0.2653      0.118     -2.254      0.024      -0.496      -0.035\n",
      "224           -0.0448      0.093     -0.480      0.631      -0.228       0.138\n",
      "253           -7.8095      7.313     -1.068      0.286     -22.142       6.523\n",
      "269            0.1623      0.124      1.306      0.191      -0.081       0.406\n",
      "419           -0.1056      0.123     -0.859      0.391      -0.347       0.135\n",
      "420            0.0738      0.122      0.606      0.544      -0.165       0.312\n",
      "424            0.1434      0.127      1.129      0.259      -0.105       0.392\n",
      "429           -1.0369      0.629     -1.649      0.099      -2.270       0.196\n",
      "433           -0.1205      0.140     -0.863      0.388      -0.394       0.153\n",
      "434            0.2267      0.115      1.978      0.048       0.002       0.451\n",
      "439            0.2236      0.135      1.656      0.098      -0.041       0.488\n",
      "440           -0.2065      0.182     -1.137      0.256      -0.563       0.150\n",
      "461            0.3561      0.128      2.789      0.005       0.106       0.606\n",
      "468            0.1011      0.089      1.135      0.256      -0.074       0.276\n",
      "469           -0.0940      0.148     -0.634      0.526      -0.385       0.197\n",
      "472            0.2116      0.191      1.110      0.267      -0.162       0.585\n",
      "473            0.2409      0.158      1.523      0.128      -0.069       0.551\n",
      "474            0.1313      0.144      0.913      0.361      -0.151       0.413\n",
      "485           -0.1946      0.126     -1.540      0.123      -0.442       0.053\n",
      "486           -0.1352      0.130     -1.040      0.298      -0.390       0.120\n",
      "487            0.2836      0.126      2.250      0.024       0.037       0.531\n",
      "488            0.1106      0.126      0.881      0.378      -0.136       0.357\n",
      "489           -0.2633      0.122     -2.164      0.030      -0.502      -0.025\n",
      "490            0.1567      0.118      1.323      0.186      -0.075       0.389\n",
      "491           -0.1171      0.128     -0.916      0.360      -0.368       0.133\n",
      "495            0.0609      0.080      0.758      0.448      -0.096       0.218\n",
      "497           -0.0803      0.131     -0.614      0.539      -0.336       0.176\n",
      "500           -0.1150      0.121     -0.949      0.342      -0.352       0.122\n",
      "501            0.1446      0.115      1.259      0.208      -0.080       0.370\n",
      "511            0.1437      0.094      1.525      0.127      -0.041       0.328\n",
      "512            0.2914      0.113      2.582      0.010       0.070       0.513\n",
      "520            0.0669      0.096      0.695      0.487      -0.122       0.256\n",
      "542            0.2559      0.116      2.205      0.027       0.028       0.483\n",
      "548            0.1884      0.128      1.468      0.142      -0.063       0.440\n",
      "563           -0.1869      0.123     -1.516      0.129      -0.428       0.055\n",
      "570            0.2068      0.109      1.892      0.058      -0.007       0.421\n",
      "573            0.4054      0.253      1.601      0.109      -0.091       0.902\n",
      "578           -0.6879      0.296     -2.323      0.020      -1.268      -0.108\n",
      "582           -0.1561      0.138     -1.127      0.260      -0.427       0.115\n",
      "==============================================================================\n",
      "Removing feature 224 with p-value 0.631248\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180310\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1505\n",
      "Method:                           MLE   Df Model:                           61\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2615\n",
      "Time:                        16:35:00   Log-Likelihood:                -282.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.067e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1077      0.314    -13.062      0.000      -4.724      -3.491\n",
      "2             -0.1460      0.118     -1.240      0.215      -0.377       0.085\n",
      "3             -0.1319      0.123     -1.073      0.283      -0.373       0.109\n",
      "15            -0.3484      0.138     -2.527      0.012      -0.619      -0.078\n",
      "16             0.3184      0.256      1.245      0.213      -0.183       0.820\n",
      "22             0.2736      0.105      2.611      0.009       0.068       0.479\n",
      "24            -0.0657      0.123     -0.533      0.594      -0.307       0.176\n",
      "52             0.1730      0.137      1.258      0.208      -0.097       0.442\n",
      "56            -0.0754      0.141     -0.535      0.592      -0.351       0.201\n",
      "60             0.9643      0.160      6.025      0.000       0.651       1.278\n",
      "63            -0.4022      0.217     -1.856      0.063      -0.827       0.022\n",
      "65             0.6138      0.131      4.688      0.000       0.357       0.870\n",
      "69            -0.2731      0.153     -1.784      0.074      -0.573       0.027\n",
      "72             0.2356      0.112      2.098      0.036       0.015       0.456\n",
      "73             0.5176      0.317      1.632      0.103      -0.104       1.139\n",
      "74            -0.9327      0.474     -1.967      0.049      -1.862      -0.003\n",
      "91            -0.2132      0.124     -1.726      0.084      -0.455       0.029\n",
      "134            0.2301      0.133      1.734      0.083      -0.030       0.490\n",
      "137           -0.0872      0.158     -0.553      0.580      -0.397       0.222\n",
      "138            0.0949      0.169      0.562      0.574      -0.236       0.426\n",
      "139            0.1308      0.124      1.058      0.290      -0.111       0.373\n",
      "152           -2.0066      0.931     -2.156      0.031      -3.830      -0.183\n",
      "184            0.4211      0.207      2.031      0.042       0.015       0.828\n",
      "188           -0.5012      0.358     -1.398      0.162      -1.204       0.201\n",
      "209           -0.2640      0.118     -2.243      0.025      -0.495      -0.033\n",
      "253           -7.7541      7.309     -1.061      0.289     -22.080       6.572\n",
      "269            0.1644      0.124      1.324      0.186      -0.079       0.408\n",
      "419           -0.1046      0.123     -0.851      0.395      -0.346       0.136\n",
      "420            0.0776      0.122      0.638      0.523      -0.161       0.316\n",
      "424            0.1443      0.127      1.137      0.255      -0.104       0.393\n",
      "429           -1.0333      0.629     -1.642      0.101      -2.266       0.200\n",
      "433           -0.1197      0.140     -0.858      0.391      -0.393       0.154\n",
      "434            0.2280      0.114      1.991      0.046       0.004       0.452\n",
      "439            0.2209      0.134      1.648      0.099      -0.042       0.484\n",
      "440           -0.2006      0.180     -1.114      0.265      -0.553       0.152\n",
      "461            0.3506      0.127      2.752      0.006       0.101       0.600\n",
      "468            0.1022      0.089      1.151      0.250      -0.072       0.276\n",
      "469           -0.0937      0.148     -0.632      0.527      -0.384       0.197\n",
      "472            0.2119      0.191      1.112      0.266      -0.162       0.586\n",
      "473            0.2399      0.158      1.517      0.129      -0.070       0.550\n",
      "474            0.1325      0.144      0.921      0.357      -0.149       0.415\n",
      "485           -0.1895      0.126     -1.507      0.132      -0.436       0.057\n",
      "486           -0.1326      0.129     -1.024      0.306      -0.386       0.121\n",
      "487            0.2838      0.126      2.252      0.024       0.037       0.531\n",
      "488            0.1103      0.125      0.879      0.379      -0.136       0.356\n",
      "489           -0.2636      0.122     -2.167      0.030      -0.502      -0.025\n",
      "490            0.1567      0.118      1.323      0.186      -0.075       0.389\n",
      "491           -0.1106      0.127     -0.872      0.383      -0.359       0.138\n",
      "495            0.0396      0.070      0.566      0.571      -0.098       0.177\n",
      "497           -0.0807      0.131     -0.616      0.538      -0.338       0.176\n",
      "500           -0.1146      0.121     -0.946      0.344      -0.352       0.123\n",
      "501            0.1403      0.115      1.224      0.221      -0.084       0.365\n",
      "511            0.1438      0.094      1.526      0.127      -0.041       0.329\n",
      "512            0.2877      0.113      2.555      0.011       0.067       0.508\n",
      "520            0.0672      0.096      0.699      0.485      -0.121       0.256\n",
      "542            0.2553      0.116      2.199      0.028       0.028       0.483\n",
      "548            0.1844      0.128      1.442      0.149      -0.066       0.435\n",
      "563           -0.1856      0.123     -1.508      0.132      -0.427       0.056\n",
      "570            0.2103      0.109      1.934      0.053      -0.003       0.423\n",
      "573            0.4061      0.253      1.606      0.108      -0.089       0.902\n",
      "578           -0.6945      0.296     -2.342      0.019      -1.276      -0.113\n",
      "582           -0.1578      0.139     -1.139      0.255      -0.429       0.114\n",
      "==============================================================================\n",
      "Removing feature 24 with p-value 0.594255\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180400\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1506\n",
      "Method:                           MLE   Df Model:                           60\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2611\n",
      "Time:                        16:35:00   Log-Likelihood:                -282.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.397e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1075      0.315    -13.051      0.000      -4.724      -3.491\n",
      "2             -0.1479      0.118     -1.254      0.210      -0.379       0.083\n",
      "3             -0.1294      0.123     -1.055      0.292      -0.370       0.111\n",
      "15            -0.3494      0.138     -2.534      0.011      -0.620      -0.079\n",
      "16             0.3211      0.255      1.258      0.209      -0.179       0.822\n",
      "22             0.2606      0.101      2.570      0.010       0.062       0.459\n",
      "52             0.1673      0.137      1.221      0.222      -0.101       0.436\n",
      "56            -0.0716      0.141     -0.509      0.611      -0.347       0.204\n",
      "60             0.9581      0.160      6.004      0.000       0.645       1.271\n",
      "63            -0.4021      0.216     -1.858      0.063      -0.826       0.022\n",
      "65             0.6096      0.131      4.670      0.000       0.354       0.865\n",
      "69            -0.2756      0.153     -1.801      0.072      -0.576       0.024\n",
      "72             0.2343      0.112      2.092      0.036       0.015       0.454\n",
      "73             0.5090      0.316      1.609      0.108      -0.111       1.129\n",
      "74            -0.9252      0.474     -1.953      0.051      -1.854       0.003\n",
      "91            -0.2114      0.124     -1.710      0.087      -0.454       0.031\n",
      "134            0.2313      0.133      1.744      0.081      -0.029       0.491\n",
      "137           -0.0886      0.158     -0.562      0.574      -0.398       0.220\n",
      "138            0.0963      0.169      0.571      0.568      -0.234       0.427\n",
      "139            0.1344      0.123      1.090      0.276      -0.107       0.376\n",
      "152           -2.0029      0.930     -2.154      0.031      -3.825      -0.180\n",
      "184            0.4283      0.207      2.072      0.038       0.023       0.834\n",
      "188           -0.5163      0.357     -1.447      0.148      -1.216       0.183\n",
      "209           -0.2630      0.117     -2.239      0.025      -0.493      -0.033\n",
      "253           -7.9556      7.312     -1.088      0.277     -22.286       6.375\n",
      "269            0.1633      0.124      1.314      0.189      -0.080       0.407\n",
      "419           -0.1045      0.123     -0.850      0.395      -0.345       0.136\n",
      "420            0.0744      0.121      0.613      0.540      -0.164       0.312\n",
      "424            0.1423      0.126      1.126      0.260      -0.105       0.390\n",
      "429           -1.0204      0.623     -1.638      0.101      -2.241       0.201\n",
      "433           -0.1282      0.137     -0.939      0.348      -0.396       0.139\n",
      "434            0.2293      0.114      2.004      0.045       0.005       0.454\n",
      "439            0.2217      0.133      1.663      0.096      -0.040       0.483\n",
      "440           -0.1965      0.179     -1.099      0.272      -0.547       0.154\n",
      "461            0.3564      0.127      2.808      0.005       0.108       0.605\n",
      "468            0.1024      0.089      1.154      0.248      -0.072       0.276\n",
      "469           -0.0956      0.148     -0.644      0.519      -0.386       0.195\n",
      "472            0.2131      0.190      1.120      0.263      -0.160       0.586\n",
      "473            0.2384      0.158      1.506      0.132      -0.072       0.549\n",
      "474            0.1359      0.144      0.945      0.345      -0.146       0.418\n",
      "485           -0.1919      0.126     -1.528      0.126      -0.438       0.054\n",
      "486           -0.1302      0.130     -1.005      0.315      -0.384       0.124\n",
      "487            0.2791      0.125      2.225      0.026       0.033       0.525\n",
      "488            0.1068      0.125      0.852      0.394      -0.139       0.352\n",
      "489           -0.2609      0.122     -2.145      0.032      -0.499      -0.023\n",
      "490            0.1540      0.118      1.303      0.193      -0.078       0.386\n",
      "491           -0.1095      0.127     -0.863      0.388      -0.358       0.139\n",
      "495            0.0362      0.070      0.519      0.603      -0.100       0.173\n",
      "497           -0.0793      0.131     -0.605      0.545      -0.337       0.178\n",
      "500           -0.1162      0.121     -0.959      0.338      -0.354       0.121\n",
      "501            0.1374      0.115      1.199      0.231      -0.087       0.362\n",
      "511            0.1466      0.094      1.559      0.119      -0.038       0.331\n",
      "512            0.2874      0.112      2.555      0.011       0.067       0.508\n",
      "520            0.0664      0.096      0.689      0.491      -0.122       0.255\n",
      "542            0.2549      0.116      2.197      0.028       0.028       0.482\n",
      "548            0.1821      0.128      1.422      0.155      -0.069       0.433\n",
      "563           -0.1849      0.123     -1.503      0.133      -0.426       0.056\n",
      "570            0.2079      0.109      1.909      0.056      -0.006       0.421\n",
      "573            0.3914      0.251      1.560      0.119      -0.100       0.883\n",
      "578           -0.6892      0.297     -2.322      0.020      -1.271      -0.107\n",
      "582           -0.1525      0.138     -1.104      0.270      -0.423       0.118\n",
      "==============================================================================\n",
      "Removing feature 56 with p-value 0.610657\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180483\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1507\n",
      "Method:                           MLE   Df Model:                           59\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2607\n",
      "Time:                        16:35:00   Log-Likelihood:                -282.82\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.773e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1010      0.314    -13.067      0.000      -4.716      -3.486\n",
      "2             -0.1473      0.118     -1.252      0.210      -0.378       0.083\n",
      "3             -0.1289      0.123     -1.050      0.294      -0.370       0.112\n",
      "15            -0.3438      0.137     -2.506      0.012      -0.613      -0.075\n",
      "16             0.3147      0.254      1.237      0.216      -0.184       0.813\n",
      "22             0.2563      0.101      2.538      0.011       0.058       0.454\n",
      "52             0.1660      0.137      1.213      0.225      -0.102       0.434\n",
      "60             0.9476      0.158      5.995      0.000       0.638       1.257\n",
      "63            -0.3928      0.216     -1.819      0.069      -0.816       0.030\n",
      "65             0.6089      0.131      4.655      0.000       0.353       0.865\n",
      "69            -0.2734      0.153     -1.789      0.074      -0.573       0.026\n",
      "72             0.2343      0.112      2.098      0.036       0.015       0.453\n",
      "73             0.4726      0.309      1.528      0.126      -0.134       1.079\n",
      "74            -0.9386      0.474     -1.979      0.048      -1.868      -0.009\n",
      "91            -0.2081      0.123     -1.686      0.092      -0.450       0.034\n",
      "134            0.2350      0.132      1.776      0.076      -0.024       0.494\n",
      "137           -0.0842      0.158     -0.534      0.593      -0.393       0.225\n",
      "138            0.0948      0.169      0.562      0.574      -0.236       0.425\n",
      "139            0.1336      0.123      1.085      0.278      -0.108       0.375\n",
      "152           -1.9693      0.925     -2.129      0.033      -3.783      -0.156\n",
      "184            0.4553      0.200      2.278      0.023       0.063       0.847\n",
      "188           -0.5139      0.357     -1.441      0.150      -1.213       0.185\n",
      "209           -0.2659      0.117     -2.266      0.023      -0.496      -0.036\n",
      "253           -8.0352      7.311     -1.099      0.272     -22.365       6.294\n",
      "269            0.1613      0.124      1.298      0.194      -0.082       0.405\n",
      "419           -0.1077      0.123     -0.877      0.380      -0.348       0.133\n",
      "420            0.0685      0.121      0.568      0.570      -0.168       0.305\n",
      "424            0.1407      0.127      1.112      0.266      -0.107       0.389\n",
      "429           -1.0103      0.611     -1.653      0.098      -2.208       0.188\n",
      "433           -0.1288      0.137     -0.941      0.347      -0.397       0.139\n",
      "434            0.2316      0.114      2.027      0.043       0.008       0.456\n",
      "439            0.2229      0.132      1.685      0.092      -0.036       0.482\n",
      "440           -0.1959      0.178     -1.103      0.270      -0.544       0.152\n",
      "461            0.3643      0.126      2.897      0.004       0.118       0.611\n",
      "468            0.0992      0.088      1.121      0.262      -0.074       0.272\n",
      "469           -0.0934      0.148     -0.630      0.529      -0.384       0.197\n",
      "472            0.2165      0.192      1.130      0.258      -0.159       0.592\n",
      "473            0.2466      0.158      1.563      0.118      -0.063       0.556\n",
      "474            0.1329      0.144      0.924      0.356      -0.149       0.415\n",
      "485           -0.1920      0.125     -1.533      0.125      -0.438       0.054\n",
      "486           -0.1318      0.130     -1.016      0.310      -0.386       0.122\n",
      "487            0.2802      0.125      2.235      0.025       0.034       0.526\n",
      "488            0.1144      0.124      0.919      0.358      -0.130       0.358\n",
      "489           -0.2634      0.122     -2.166      0.030      -0.502      -0.025\n",
      "490            0.1527      0.118      1.292      0.196      -0.079       0.384\n",
      "491           -0.1087      0.127     -0.856      0.392      -0.358       0.140\n",
      "495            0.0367      0.070      0.526      0.599      -0.100       0.173\n",
      "497           -0.0805      0.131     -0.612      0.540      -0.338       0.177\n",
      "500           -0.1132      0.121     -0.935      0.350      -0.350       0.124\n",
      "501            0.1365      0.114      1.192      0.233      -0.088       0.361\n",
      "511            0.1432      0.094      1.527      0.127      -0.041       0.327\n",
      "512            0.2840      0.112      2.529      0.011       0.064       0.504\n",
      "520            0.0654      0.096      0.678      0.498      -0.124       0.254\n",
      "542            0.2586      0.116      2.234      0.025       0.032       0.485\n",
      "548            0.1822      0.128      1.425      0.154      -0.068       0.433\n",
      "563           -0.1816      0.123     -1.480      0.139      -0.422       0.059\n",
      "570            0.2111      0.109      1.944      0.052      -0.002       0.424\n",
      "573            0.3980      0.251      1.587      0.112      -0.093       0.889\n",
      "578           -0.6977      0.297     -2.348      0.019      -1.280      -0.115\n",
      "582           -0.1523      0.138     -1.103      0.270      -0.423       0.118\n",
      "==============================================================================\n",
      "Removing feature 495 with p-value 0.598745\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180566\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1508\n",
      "Method:                           MLE   Df Model:                           58\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2604\n",
      "Time:                        16:35:01   Log-Likelihood:                -282.95\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.210e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1074      0.314    -13.088      0.000      -4.722      -3.492\n",
      "2             -0.1449      0.117     -1.234      0.217      -0.375       0.085\n",
      "3             -0.1304      0.123     -1.064      0.287      -0.371       0.110\n",
      "15            -0.3504      0.137     -2.565      0.010      -0.618      -0.083\n",
      "16             0.3215      0.254      1.266      0.205      -0.176       0.819\n",
      "22             0.2576      0.101      2.552      0.011       0.060       0.455\n",
      "52             0.1699      0.137      1.244      0.214      -0.098       0.438\n",
      "60             0.9550      0.157      6.068      0.000       0.647       1.263\n",
      "63            -0.3876      0.216     -1.797      0.072      -0.810       0.035\n",
      "65             0.6122      0.131      4.690      0.000       0.356       0.868\n",
      "69            -0.2778      0.153     -1.820      0.069      -0.577       0.021\n",
      "72             0.2330      0.112      2.085      0.037       0.014       0.452\n",
      "73             0.4562      0.308      1.483      0.138      -0.147       1.059\n",
      "74            -0.9676      0.471     -2.053      0.040      -1.891      -0.044\n",
      "91            -0.2026      0.123     -1.650      0.099      -0.443       0.038\n",
      "134            0.2336      0.132      1.767      0.077      -0.025       0.493\n",
      "137           -0.0872      0.157     -0.554      0.579      -0.396       0.221\n",
      "138            0.0918      0.169      0.544      0.586      -0.239       0.422\n",
      "139            0.1328      0.123      1.079      0.280      -0.108       0.374\n",
      "152           -1.9891      0.925     -2.151      0.031      -3.802      -0.177\n",
      "184            0.4558      0.200      2.284      0.022       0.065       0.847\n",
      "188           -0.5117      0.356     -1.438      0.150      -1.209       0.186\n",
      "209           -0.2655      0.117     -2.262      0.024      -0.496      -0.035\n",
      "253           -8.2753      7.296     -1.134      0.257     -22.575       6.024\n",
      "269            0.1568      0.124      1.265      0.206      -0.086       0.400\n",
      "419           -0.1069      0.123     -0.872      0.383      -0.347       0.133\n",
      "420            0.0669      0.121      0.555      0.579      -0.169       0.303\n",
      "424            0.1340      0.126      1.064      0.287      -0.113       0.381\n",
      "429           -1.0098      0.609     -1.659      0.097      -2.203       0.183\n",
      "433           -0.1321      0.137     -0.964      0.335      -0.401       0.137\n",
      "434            0.2292      0.114      2.008      0.045       0.005       0.453\n",
      "439            0.2212      0.132      1.677      0.094      -0.037       0.480\n",
      "440           -0.1931      0.177     -1.090      0.276      -0.540       0.154\n",
      "461            0.3672      0.125      2.927      0.003       0.121       0.613\n",
      "468            0.0983      0.088      1.112      0.266      -0.075       0.272\n",
      "469           -0.0949      0.148     -0.641      0.522      -0.385       0.195\n",
      "472            0.2177      0.192      1.134      0.257      -0.159       0.594\n",
      "473            0.2471      0.158      1.567      0.117      -0.062       0.556\n",
      "474            0.1302      0.144      0.905      0.366      -0.152       0.412\n",
      "485           -0.1907      0.125     -1.522      0.128      -0.436       0.055\n",
      "486           -0.1269      0.129     -0.981      0.326      -0.380       0.127\n",
      "487            0.2763      0.125      2.210      0.027       0.031       0.521\n",
      "488            0.1139      0.124      0.916      0.360      -0.130       0.358\n",
      "489           -0.2604      0.121     -2.147      0.032      -0.498      -0.023\n",
      "490            0.1520      0.118      1.286      0.198      -0.080       0.384\n",
      "491           -0.1123      0.127     -0.886      0.376      -0.361       0.136\n",
      "497           -0.0806      0.131     -0.614      0.539      -0.338       0.177\n",
      "500           -0.1174      0.121     -0.972      0.331      -0.354       0.119\n",
      "501            0.1394      0.114      1.220      0.223      -0.085       0.363\n",
      "511            0.1394      0.094      1.487      0.137      -0.044       0.323\n",
      "512            0.2827      0.112      2.519      0.012       0.063       0.503\n",
      "520            0.0626      0.096      0.648      0.517      -0.127       0.252\n",
      "542            0.2566      0.116      2.220      0.026       0.030       0.483\n",
      "548            0.1833      0.128      1.435      0.151      -0.067       0.434\n",
      "563           -0.1787      0.122     -1.459      0.145      -0.419       0.061\n",
      "570            0.2183      0.107      2.031      0.042       0.008       0.429\n",
      "573            0.3999      0.250      1.597      0.110      -0.091       0.891\n",
      "578           -0.6998      0.297     -2.358      0.018      -1.282      -0.118\n",
      "582           -0.1479      0.137     -1.080      0.280      -0.416       0.120\n",
      "==============================================================================\n",
      "Removing feature 138 with p-value 0.586102\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180660\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1509\n",
      "Method:                           MLE   Df Model:                           57\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2600\n",
      "Time:                        16:35:01   Log-Likelihood:                -283.09\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.301e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1079      0.314    -13.082      0.000      -4.723      -3.492\n",
      "2             -0.1582      0.115     -1.372      0.170      -0.384       0.068\n",
      "3             -0.1338      0.122     -1.095      0.274      -0.373       0.106\n",
      "15            -0.3518      0.137     -2.577      0.010      -0.619      -0.084\n",
      "16             0.3288      0.253      1.298      0.194      -0.168       0.825\n",
      "22             0.2578      0.101      2.558      0.011       0.060       0.455\n",
      "52             0.1700      0.137      1.245      0.213      -0.098       0.438\n",
      "60             0.9588      0.157      6.105      0.000       0.651       1.267\n",
      "63            -0.3887      0.216     -1.804      0.071      -0.811       0.034\n",
      "65             0.6170      0.130      4.743      0.000       0.362       0.872\n",
      "69            -0.2761      0.152     -1.811      0.070      -0.575       0.023\n",
      "72             0.2301      0.112      2.063      0.039       0.011       0.449\n",
      "73             0.4556      0.307      1.483      0.138      -0.147       1.058\n",
      "74            -0.9629      0.471     -2.044      0.041      -1.886      -0.040\n",
      "91            -0.2023      0.123     -1.650      0.099      -0.443       0.038\n",
      "134            0.2381      0.132      1.803      0.071      -0.021       0.497\n",
      "137           -0.0297      0.116     -0.255      0.799      -0.258       0.199\n",
      "139            0.1398      0.122      1.144      0.253      -0.100       0.379\n",
      "152           -2.0039      0.925     -2.166      0.030      -3.817      -0.191\n",
      "184            0.4564      0.200      2.284      0.022       0.065       0.848\n",
      "188           -0.5138      0.357     -1.439      0.150      -1.214       0.186\n",
      "209           -0.2640      0.117     -2.252      0.024      -0.494      -0.034\n",
      "253           -8.3605      7.289     -1.147      0.251     -22.646       5.925\n",
      "269            0.1588      0.124      1.281      0.200      -0.084       0.402\n",
      "419           -0.1064      0.123     -0.868      0.386      -0.347       0.134\n",
      "420            0.0682      0.121      0.566      0.571      -0.168       0.304\n",
      "424            0.1337      0.126      1.060      0.289      -0.114       0.381\n",
      "429           -1.0046      0.609     -1.649      0.099      -2.198       0.189\n",
      "433           -0.1312      0.137     -0.959      0.338      -0.400       0.137\n",
      "434            0.2270      0.114      1.992      0.046       0.004       0.450\n",
      "439            0.2279      0.132      1.725      0.085      -0.031       0.487\n",
      "440           -0.1969      0.178     -1.108      0.268      -0.545       0.152\n",
      "461            0.3645      0.125      2.905      0.004       0.119       0.610\n",
      "468            0.0990      0.088      1.120      0.263      -0.074       0.272\n",
      "469           -0.0971      0.148     -0.657      0.511      -0.387       0.192\n",
      "472            0.2090      0.192      1.087      0.277      -0.168       0.586\n",
      "473            0.2463      0.158      1.562      0.118      -0.063       0.555\n",
      "474            0.1345      0.144      0.935      0.350      -0.147       0.416\n",
      "485           -0.1912      0.125     -1.527      0.127      -0.437       0.054\n",
      "486           -0.1244      0.129     -0.962      0.336      -0.378       0.129\n",
      "487            0.2742      0.125      2.197      0.028       0.030       0.519\n",
      "488            0.1154      0.124      0.927      0.354      -0.129       0.359\n",
      "489           -0.2607      0.121     -2.149      0.032      -0.499      -0.023\n",
      "490            0.1488      0.118      1.261      0.207      -0.083       0.380\n",
      "491           -0.1153      0.127     -0.910      0.363      -0.364       0.133\n",
      "497           -0.0787      0.131     -0.599      0.549      -0.336       0.179\n",
      "500           -0.1194      0.121     -0.990      0.322      -0.356       0.117\n",
      "501            0.1396      0.114      1.223      0.221      -0.084       0.363\n",
      "511            0.1405      0.093      1.506      0.132      -0.042       0.323\n",
      "512            0.2852      0.112      2.544      0.011       0.066       0.505\n",
      "520            0.0654      0.096      0.682      0.495      -0.123       0.254\n",
      "542            0.2569      0.116      2.220      0.026       0.030       0.484\n",
      "548            0.1800      0.128      1.411      0.158      -0.070       0.430\n",
      "563           -0.1779      0.123     -1.451      0.147      -0.418       0.062\n",
      "570            0.2172      0.107      2.026      0.043       0.007       0.427\n",
      "573            0.3896      0.249      1.562      0.118      -0.099       0.879\n",
      "578           -0.6908      0.296     -2.336      0.019      -1.270      -0.111\n",
      "582           -0.1481      0.137     -1.082      0.279      -0.416       0.120\n",
      "==============================================================================\n",
      "Removing feature 137 with p-value 0.798783\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180681\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1510\n",
      "Method:                           MLE   Df Model:                           56\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2599\n",
      "Time:                        16:35:01   Log-Likelihood:                -283.13\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.991e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1082      0.314    -13.087      0.000      -4.723      -3.493\n",
      "2             -0.1562      0.115     -1.361      0.173      -0.381       0.069\n",
      "3             -0.1348      0.122     -1.104      0.270      -0.374       0.105\n",
      "15            -0.3502      0.136     -2.567      0.010      -0.617      -0.083\n",
      "16             0.3273      0.253      1.295      0.195      -0.168       0.823\n",
      "22             0.2565      0.101      2.550      0.011       0.059       0.454\n",
      "52             0.1703      0.137      1.247      0.212      -0.097       0.438\n",
      "60             0.9581      0.157      6.105      0.000       0.651       1.266\n",
      "63            -0.3884      0.216     -1.802      0.072      -0.811       0.034\n",
      "65             0.6160      0.130      4.741      0.000       0.361       0.871\n",
      "69            -0.2764      0.153     -1.812      0.070      -0.575       0.023\n",
      "72             0.2298      0.112      2.058      0.040       0.011       0.449\n",
      "73             0.4577      0.307      1.490      0.136      -0.144       1.060\n",
      "74            -0.9637      0.471     -2.044      0.041      -1.888      -0.040\n",
      "91            -0.2006      0.122     -1.641      0.101      -0.440       0.039\n",
      "134            0.2383      0.132      1.803      0.071      -0.021       0.497\n",
      "139            0.1407      0.122      1.152      0.249      -0.099       0.380\n",
      "152           -1.9879      0.921     -2.158      0.031      -3.794      -0.182\n",
      "184            0.4569      0.200      2.287      0.022       0.065       0.848\n",
      "188           -0.5125      0.357     -1.435      0.151      -1.213       0.188\n",
      "209           -0.2637      0.117     -2.251      0.024      -0.493      -0.034\n",
      "253           -8.3934      7.290     -1.151      0.250     -22.681       5.894\n",
      "269            0.1586      0.124      1.281      0.200      -0.084       0.401\n",
      "419           -0.1063      0.123     -0.867      0.386      -0.347       0.134\n",
      "420            0.0678      0.120      0.563      0.574      -0.168       0.304\n",
      "424            0.1336      0.126      1.059      0.290      -0.114       0.381\n",
      "429           -1.0092      0.610     -1.654      0.098      -2.205       0.186\n",
      "433           -0.1332      0.137     -0.972      0.331      -0.402       0.135\n",
      "434            0.2283      0.114      2.007      0.045       0.005       0.451\n",
      "439            0.2306      0.131      1.755      0.079      -0.027       0.488\n",
      "440           -0.2001      0.177     -1.130      0.258      -0.547       0.147\n",
      "461            0.3645      0.125      2.909      0.004       0.119       0.610\n",
      "468            0.0977      0.088      1.107      0.268      -0.075       0.271\n",
      "469           -0.0985      0.148     -0.666      0.505      -0.388       0.191\n",
      "472            0.2117      0.192      1.101      0.271      -0.165       0.589\n",
      "473            0.2442      0.157      1.551      0.121      -0.064       0.553\n",
      "474            0.1337      0.144      0.928      0.353      -0.149       0.416\n",
      "485           -0.1900      0.125     -1.520      0.129      -0.435       0.055\n",
      "486           -0.1243      0.129     -0.962      0.336      -0.378       0.129\n",
      "487            0.2759      0.125      2.215      0.027       0.032       0.520\n",
      "488            0.1149      0.125      0.922      0.356      -0.129       0.359\n",
      "489           -0.2597      0.121     -2.143      0.032      -0.497      -0.022\n",
      "490            0.1498      0.118      1.271      0.204      -0.081       0.381\n",
      "491           -0.1144      0.127     -0.902      0.367      -0.363       0.134\n",
      "497           -0.0782      0.132     -0.595      0.552      -0.336       0.180\n",
      "500           -0.1183      0.120     -0.982      0.326      -0.354       0.118\n",
      "501            0.1393      0.114      1.220      0.222      -0.084       0.363\n",
      "511            0.1400      0.093      1.504      0.133      -0.042       0.322\n",
      "512            0.2834      0.112      2.533      0.011       0.064       0.503\n",
      "520            0.0656      0.096      0.683      0.494      -0.123       0.254\n",
      "542            0.2577      0.116      2.229      0.026       0.031       0.484\n",
      "548            0.1799      0.128      1.409      0.159      -0.070       0.430\n",
      "563           -0.1782      0.123     -1.454      0.146      -0.418       0.062\n",
      "570            0.2154      0.107      2.012      0.044       0.006       0.425\n",
      "573            0.3945      0.249      1.586      0.113      -0.093       0.882\n",
      "578           -0.6944      0.295     -2.352      0.019      -1.273      -0.116\n",
      "582           -0.1518      0.136     -1.116      0.264      -0.418       0.115\n",
      "==============================================================================\n",
      "Removing feature 420 with p-value 0.573744\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180781\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1511\n",
      "Method:                           MLE   Df Model:                           55\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2595\n",
      "Time:                        16:35:01   Log-Likelihood:                -283.28\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.082e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1059      0.313    -13.100      0.000      -4.720      -3.492\n",
      "2             -0.1589      0.115     -1.383      0.167      -0.384       0.066\n",
      "3             -0.1329      0.122     -1.087      0.277      -0.373       0.107\n",
      "15            -0.3440      0.136     -2.531      0.011      -0.610      -0.078\n",
      "16             0.3254      0.253      1.289      0.198      -0.170       0.820\n",
      "22             0.2598      0.100      2.585      0.010       0.063       0.457\n",
      "52             0.1729      0.136      1.269      0.204      -0.094       0.440\n",
      "60             0.9536      0.157      6.093      0.000       0.647       1.260\n",
      "63            -0.3929      0.215     -1.826      0.068      -0.814       0.029\n",
      "65             0.6167      0.130      4.753      0.000       0.362       0.871\n",
      "69            -0.2824      0.152     -1.858      0.063      -0.580       0.016\n",
      "72             0.2298      0.112      2.055      0.040       0.011       0.449\n",
      "73             0.4509      0.307      1.470      0.142      -0.150       1.052\n",
      "74            -0.9653      0.470     -2.052      0.040      -1.887      -0.043\n",
      "91            -0.1986      0.122     -1.623      0.104      -0.438       0.041\n",
      "134            0.2372      0.132      1.795      0.073      -0.022       0.496\n",
      "139            0.1368      0.122      1.121      0.262      -0.102       0.376\n",
      "152           -1.9506      0.915     -2.131      0.033      -3.745      -0.156\n",
      "184            0.4540      0.199      2.279      0.023       0.063       0.845\n",
      "188           -0.5167      0.356     -1.453      0.146      -1.214       0.180\n",
      "209           -0.2602      0.117     -2.222      0.026      -0.490      -0.031\n",
      "253           -8.5225      7.281     -1.171      0.242     -22.792       5.747\n",
      "269            0.1534      0.124      1.242      0.214      -0.089       0.396\n",
      "419           -0.1061      0.123     -0.866      0.386      -0.346       0.134\n",
      "424            0.1322      0.126      1.048      0.295      -0.115       0.380\n",
      "429           -1.0043      0.611     -1.643      0.100      -2.202       0.194\n",
      "433           -0.1320      0.137     -0.961      0.336      -0.401       0.137\n",
      "434            0.2287      0.114      2.010      0.044       0.006       0.452\n",
      "439            0.2265      0.131      1.727      0.084      -0.031       0.484\n",
      "440           -0.1966      0.176     -1.119      0.263      -0.541       0.148\n",
      "461            0.3692      0.125      2.962      0.003       0.125       0.613\n",
      "468            0.0976      0.089      1.098      0.272      -0.077       0.272\n",
      "469           -0.0986      0.148     -0.667      0.505      -0.388       0.191\n",
      "472            0.2169      0.192      1.127      0.260      -0.160       0.594\n",
      "473            0.2539      0.156      1.625      0.104      -0.052       0.560\n",
      "474            0.1317      0.144      0.914      0.361      -0.151       0.414\n",
      "485           -0.1937      0.125     -1.550      0.121      -0.439       0.051\n",
      "486           -0.1220      0.129     -0.943      0.346      -0.375       0.131\n",
      "487            0.2795      0.124      2.248      0.025       0.036       0.523\n",
      "488            0.1108      0.124      0.891      0.373      -0.133       0.355\n",
      "489           -0.2543      0.121     -2.106      0.035      -0.491      -0.018\n",
      "490            0.1504      0.118      1.277      0.202      -0.080       0.381\n",
      "491           -0.1170      0.126     -0.925      0.355      -0.365       0.131\n",
      "497           -0.0740      0.132     -0.562      0.574      -0.332       0.184\n",
      "500           -0.1215      0.120     -1.009      0.313      -0.358       0.114\n",
      "501            0.1400      0.114      1.228      0.219      -0.083       0.364\n",
      "511            0.1384      0.093      1.488      0.137      -0.044       0.321\n",
      "512            0.2854      0.112      2.553      0.011       0.066       0.505\n",
      "520            0.0702      0.096      0.735      0.462      -0.117       0.257\n",
      "542            0.2552      0.116      2.206      0.027       0.029       0.482\n",
      "548            0.1827      0.128      1.433      0.152      -0.067       0.433\n",
      "563           -0.1796      0.122     -1.467      0.142      -0.420       0.060\n",
      "570            0.2152      0.107      2.012      0.044       0.006       0.425\n",
      "573            0.3998      0.249      1.603      0.109      -0.089       0.889\n",
      "578           -0.6959      0.295     -2.359      0.018      -1.274      -0.118\n",
      "582           -0.1546      0.136     -1.133      0.257      -0.422       0.113\n",
      "==============================================================================\n",
      "Removing feature 497 with p-value 0.574318\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.180887\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1512\n",
      "Method:                           MLE   Df Model:                           54\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2591\n",
      "Time:                        16:35:01   Log-Likelihood:                -283.45\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.380e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1069      0.314    -13.094      0.000      -4.722      -3.492\n",
      "2             -0.1571      0.115     -1.368      0.171      -0.382       0.068\n",
      "3             -0.1358      0.122     -1.110      0.267      -0.375       0.104\n",
      "15            -0.3491      0.136     -2.575      0.010      -0.615      -0.083\n",
      "16             0.3165      0.253      1.251      0.211      -0.180       0.813\n",
      "22             0.2552      0.100      2.552      0.011       0.059       0.451\n",
      "52             0.1704      0.136      1.250      0.211      -0.097       0.438\n",
      "60             0.9506      0.156      6.075      0.000       0.644       1.257\n",
      "63            -0.3885      0.215     -1.807      0.071      -0.810       0.033\n",
      "65             0.6160      0.130      4.744      0.000       0.361       0.870\n",
      "69            -0.2814      0.152     -1.851      0.064      -0.579       0.017\n",
      "72             0.2264      0.112      2.023      0.043       0.007       0.446\n",
      "73             0.4365      0.306      1.425      0.154      -0.164       1.037\n",
      "74            -0.9733      0.470     -2.069      0.039      -1.895      -0.051\n",
      "91            -0.2017      0.122     -1.649      0.099      -0.441       0.038\n",
      "134            0.2351      0.132      1.783      0.075      -0.023       0.494\n",
      "139            0.1303      0.121      1.074      0.283      -0.107       0.368\n",
      "152           -1.9942      0.919     -2.171      0.030      -3.795      -0.194\n",
      "184            0.4551      0.199      2.284      0.022       0.065       0.846\n",
      "188           -0.5215      0.356     -1.466      0.143      -1.219       0.176\n",
      "209           -0.2577      0.117     -2.200      0.028      -0.487      -0.028\n",
      "253           -8.4960      7.276     -1.168      0.243     -22.757       5.765\n",
      "269            0.1491      0.123      1.209      0.227      -0.093       0.391\n",
      "419           -0.1098      0.122     -0.898      0.369      -0.350       0.130\n",
      "424            0.1297      0.126      1.028      0.304      -0.118       0.377\n",
      "429           -1.0140      0.613     -1.655      0.098      -2.215       0.187\n",
      "433           -0.1331      0.137     -0.971      0.332      -0.402       0.136\n",
      "434            0.2268      0.114      1.993      0.046       0.004       0.450\n",
      "439            0.2303      0.130      1.765      0.078      -0.025       0.486\n",
      "440           -0.2015      0.177     -1.139      0.255      -0.548       0.145\n",
      "461            0.3689      0.125      2.956      0.003       0.124       0.614\n",
      "468            0.0980      0.089      1.101      0.271      -0.076       0.272\n",
      "469           -0.1005      0.148     -0.678      0.498      -0.391       0.190\n",
      "472            0.2133      0.194      1.101      0.271      -0.166       0.593\n",
      "473            0.2538      0.156      1.626      0.104      -0.052       0.560\n",
      "474            0.1317      0.144      0.912      0.362      -0.151       0.415\n",
      "485           -0.1953      0.125     -1.562      0.118      -0.440       0.050\n",
      "486           -0.1256      0.129     -0.972      0.331      -0.379       0.128\n",
      "487            0.2817      0.124      2.263      0.024       0.038       0.526\n",
      "488            0.1086      0.124      0.873      0.383      -0.135       0.352\n",
      "489           -0.2539      0.121     -2.106      0.035      -0.490      -0.018\n",
      "490            0.1550      0.117      1.320      0.187      -0.075       0.385\n",
      "491           -0.1116      0.126     -0.886      0.375      -0.358       0.135\n",
      "500           -0.1260      0.120     -1.048      0.295      -0.362       0.110\n",
      "501            0.1410      0.114      1.237      0.216      -0.082       0.364\n",
      "511            0.1396      0.093      1.500      0.134      -0.043       0.322\n",
      "512            0.2849      0.112      2.550      0.011       0.066       0.504\n",
      "520            0.0743      0.095      0.780      0.435      -0.112       0.261\n",
      "542            0.2568      0.116      2.217      0.027       0.030       0.484\n",
      "548            0.1787      0.127      1.404      0.160      -0.071       0.428\n",
      "563           -0.1796      0.122     -1.469      0.142      -0.419       0.060\n",
      "570            0.2147      0.107      2.007      0.045       0.005       0.424\n",
      "573            0.3906      0.248      1.576      0.115      -0.095       0.876\n",
      "578           -0.6917      0.294     -2.353      0.019      -1.268      -0.116\n",
      "582           -0.1519      0.136     -1.117      0.264      -0.418       0.115\n",
      "==============================================================================\n",
      "Removing feature 469 with p-value 0.498014\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181037\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1513\n",
      "Method:                           MLE   Df Model:                           53\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2585\n",
      "Time:                        16:35:01   Log-Likelihood:                -283.69\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.450e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1016      0.314    -13.083      0.000      -4.716      -3.487\n",
      "2             -0.1573      0.115     -1.369      0.171      -0.382       0.068\n",
      "3             -0.1287      0.122     -1.058      0.290      -0.367       0.110\n",
      "15            -0.3462      0.135     -2.558      0.011      -0.611      -0.081\n",
      "16             0.3181      0.253      1.259      0.208      -0.177       0.813\n",
      "22             0.2548      0.100      2.542      0.011       0.058       0.451\n",
      "52             0.1717      0.136      1.259      0.208      -0.096       0.439\n",
      "60             0.9823      0.150      6.531      0.000       0.688       1.277\n",
      "63            -0.3902      0.216     -1.810      0.070      -0.813       0.032\n",
      "65             0.6153      0.130      4.735      0.000       0.361       0.870\n",
      "69            -0.2801      0.153     -1.834      0.067      -0.579       0.019\n",
      "72             0.2221      0.112      1.983      0.047       0.003       0.441\n",
      "73             0.4388      0.307      1.431      0.152      -0.162       1.040\n",
      "74            -1.0022      0.471     -2.129      0.033      -1.925      -0.080\n",
      "91            -0.2042      0.122     -1.675      0.094      -0.443       0.035\n",
      "134            0.2407      0.131      1.835      0.067      -0.016       0.498\n",
      "139            0.1285      0.121      1.061      0.289      -0.109       0.366\n",
      "152           -1.9889      0.917     -2.168      0.030      -3.787      -0.191\n",
      "184            0.4435      0.199      2.234      0.026       0.054       0.833\n",
      "188           -0.5084      0.356     -1.427      0.154      -1.207       0.190\n",
      "209           -0.2570      0.117     -2.195      0.028      -0.487      -0.027\n",
      "253           -8.4602      7.280     -1.162      0.245     -22.729       5.809\n",
      "269            0.1474      0.123      1.195      0.232      -0.094       0.389\n",
      "419           -0.1127      0.122     -0.923      0.356      -0.352       0.127\n",
      "424            0.1347      0.126      1.068      0.285      -0.112       0.382\n",
      "429           -1.0070      0.609     -1.653      0.098      -2.201       0.187\n",
      "433           -0.1333      0.137     -0.973      0.331      -0.402       0.135\n",
      "434            0.2292      0.114      2.019      0.044       0.007       0.452\n",
      "439            0.2173      0.129      1.683      0.092      -0.036       0.470\n",
      "440           -0.1911      0.176     -1.089      0.276      -0.535       0.153\n",
      "461            0.3709      0.124      2.983      0.003       0.127       0.615\n",
      "468            0.1026      0.089      1.154      0.248      -0.072       0.277\n",
      "472            0.2000      0.198      1.012      0.311      -0.187       0.587\n",
      "473            0.2481      0.156      1.595      0.111      -0.057       0.553\n",
      "474            0.1259      0.146      0.865      0.387      -0.159       0.411\n",
      "485           -0.1916      0.125     -1.535      0.125      -0.436       0.053\n",
      "486           -0.1283      0.129     -0.993      0.321      -0.382       0.125\n",
      "487            0.2822      0.124      2.267      0.023       0.038       0.526\n",
      "488            0.1130      0.124      0.908      0.364      -0.131       0.357\n",
      "489           -0.2526      0.120     -2.099      0.036      -0.489      -0.017\n",
      "490            0.1537      0.117      1.311      0.190      -0.076       0.383\n",
      "491           -0.1042      0.125     -0.831      0.406      -0.350       0.142\n",
      "500           -0.1338      0.120     -1.118      0.264      -0.368       0.101\n",
      "501            0.1425      0.114      1.251      0.211      -0.081       0.366\n",
      "511            0.1421      0.093      1.529      0.126      -0.040       0.324\n",
      "512            0.2822      0.112      2.527      0.012       0.063       0.501\n",
      "520            0.0716      0.095      0.755      0.451      -0.114       0.257\n",
      "542            0.2522      0.115      2.183      0.029       0.026       0.479\n",
      "548            0.1796      0.127      1.411      0.158      -0.070       0.429\n",
      "563           -0.1814      0.122     -1.485      0.137      -0.421       0.058\n",
      "570            0.2128      0.107      1.985      0.047       0.003       0.423\n",
      "573            0.3826      0.247      1.549      0.121      -0.102       0.867\n",
      "578           -0.6874      0.294     -2.335      0.020      -1.264      -0.110\n",
      "582           -0.1537      0.137     -1.126      0.260      -0.421       0.114\n",
      "==============================================================================\n",
      "Removing feature 520 with p-value 0.450513\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181211\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1514\n",
      "Method:                           MLE   Df Model:                           52\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2578\n",
      "Time:                        16:35:02   Log-Likelihood:                -283.96\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.017e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.1013      0.314    -13.077      0.000      -4.716      -3.487\n",
      "2             -0.1578      0.115     -1.377      0.169      -0.382       0.067\n",
      "3             -0.1214      0.121     -1.005      0.315      -0.358       0.115\n",
      "15            -0.3467      0.135     -2.569      0.010      -0.611      -0.082\n",
      "16             0.3166      0.253      1.254      0.210      -0.178       0.812\n",
      "22             0.2574      0.100      2.565      0.010       0.061       0.454\n",
      "52             0.1621      0.136      1.194      0.232      -0.104       0.428\n",
      "60             1.0007      0.148      6.747      0.000       0.710       1.291\n",
      "63            -0.3968      0.216     -1.840      0.066      -0.820       0.026\n",
      "65             0.6261      0.129      4.839      0.000       0.373       0.880\n",
      "69            -0.2824      0.153     -1.850      0.064      -0.581       0.017\n",
      "72             0.2168      0.112      1.940      0.052      -0.002       0.436\n",
      "73             0.4407      0.307      1.437      0.151      -0.160       1.042\n",
      "74            -1.0282      0.469     -2.194      0.028      -1.947      -0.110\n",
      "91            -0.2023      0.122     -1.658      0.097      -0.442       0.037\n",
      "134            0.2358      0.131      1.803      0.071      -0.021       0.492\n",
      "139            0.1280      0.121      1.060      0.289      -0.109       0.365\n",
      "152           -2.0250      0.917     -2.209      0.027      -3.822      -0.228\n",
      "184            0.4419      0.199      2.216      0.027       0.051       0.833\n",
      "188           -0.5100      0.359     -1.422      0.155      -1.213       0.193\n",
      "209           -0.2518      0.117     -2.159      0.031      -0.480      -0.023\n",
      "253           -8.2928      7.279     -1.139      0.255     -22.559       5.974\n",
      "269            0.1460      0.123      1.185      0.236      -0.096       0.387\n",
      "419           -0.1116      0.122     -0.915      0.360      -0.351       0.127\n",
      "424            0.1389      0.125      1.108      0.268      -0.107       0.385\n",
      "429           -0.9988      0.608     -1.644      0.100      -2.190       0.192\n",
      "433           -0.1318      0.137     -0.963      0.335      -0.400       0.136\n",
      "434            0.2335      0.113      2.064      0.039       0.012       0.455\n",
      "439            0.2128      0.129      1.650      0.099      -0.040       0.466\n",
      "440           -0.1829      0.175     -1.046      0.295      -0.525       0.160\n",
      "461            0.3660      0.124      2.948      0.003       0.123       0.609\n",
      "468            0.1023      0.089      1.148      0.251      -0.072       0.277\n",
      "472            0.1970      0.197      1.000      0.317      -0.189       0.583\n",
      "473            0.2407      0.156      1.546      0.122      -0.065       0.546\n",
      "474            0.1339      0.145      0.924      0.356      -0.150       0.418\n",
      "485           -0.1971      0.125     -1.582      0.114      -0.441       0.047\n",
      "486           -0.1306      0.129     -1.011      0.312      -0.384       0.123\n",
      "487            0.2864      0.124      2.303      0.021       0.043       0.530\n",
      "488            0.1147      0.124      0.922      0.357      -0.129       0.359\n",
      "489           -0.2524      0.120     -2.097      0.036      -0.488      -0.016\n",
      "490            0.1553      0.117      1.328      0.184      -0.074       0.384\n",
      "491           -0.1029      0.125     -0.823      0.410      -0.348       0.142\n",
      "500           -0.1378      0.120     -1.153      0.249      -0.372       0.096\n",
      "501            0.1460      0.114      1.284      0.199      -0.077       0.369\n",
      "511            0.1458      0.093      1.575      0.115      -0.036       0.327\n",
      "512            0.2832      0.111      2.541      0.011       0.065       0.502\n",
      "542            0.2468      0.115      2.145      0.032       0.021       0.472\n",
      "548            0.1821      0.127      1.434      0.152      -0.067       0.431\n",
      "563           -0.1861      0.122     -1.525      0.127      -0.425       0.053\n",
      "570            0.2170      0.107      2.029      0.042       0.007       0.427\n",
      "573            0.3785      0.248      1.529      0.126      -0.107       0.864\n",
      "578           -0.6887      0.295     -2.338      0.019      -1.266      -0.111\n",
      "582           -0.1589      0.137     -1.162      0.245      -0.427       0.109\n",
      "==============================================================================\n",
      "Removing feature 491 with p-value 0.410298\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181431\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1515\n",
      "Method:                           MLE   Df Model:                           51\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2569\n",
      "Time:                        16:35:02   Log-Likelihood:                -284.30\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.881e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0927      0.313    -13.068      0.000      -4.707      -3.479\n",
      "2             -0.1550      0.114     -1.355      0.175      -0.379       0.069\n",
      "3             -0.1158      0.121     -0.960      0.337      -0.352       0.121\n",
      "15            -0.3488      0.135     -2.586      0.010      -0.613      -0.084\n",
      "16             0.3189      0.252      1.267      0.205      -0.175       0.812\n",
      "22             0.2620      0.100      2.616      0.009       0.066       0.458\n",
      "52             0.1645      0.136      1.213      0.225      -0.101       0.430\n",
      "60             0.9974      0.148      6.737      0.000       0.707       1.288\n",
      "63            -0.4104      0.215     -1.908      0.056      -0.832       0.011\n",
      "65             0.6231      0.130      4.801      0.000       0.369       0.877\n",
      "69            -0.2810      0.153     -1.834      0.067      -0.581       0.019\n",
      "72             0.2106      0.111      1.889      0.059      -0.008       0.429\n",
      "73             0.4537      0.306      1.483      0.138      -0.146       1.053\n",
      "74            -1.0011      0.467     -2.143      0.032      -1.916      -0.086\n",
      "91            -0.2071      0.123     -1.688      0.091      -0.447       0.033\n",
      "134            0.2376      0.131      1.817      0.069      -0.019       0.494\n",
      "139            0.1217      0.120      1.012      0.312      -0.114       0.358\n",
      "152           -2.0708      0.917     -2.257      0.024      -3.869      -0.273\n",
      "184            0.4354      0.199      2.188      0.029       0.045       0.826\n",
      "188           -0.5161      0.358     -1.443      0.149      -1.217       0.185\n",
      "209           -0.2490      0.116     -2.138      0.032      -0.477      -0.021\n",
      "253           -7.8834      7.250     -1.087      0.277     -22.094       6.327\n",
      "269            0.1528      0.123      1.242      0.214      -0.088       0.394\n",
      "419           -0.1164      0.122     -0.955      0.340      -0.355       0.122\n",
      "424            0.1448      0.125      1.157      0.247      -0.100       0.390\n",
      "429           -1.0226      0.609     -1.678      0.093      -2.217       0.172\n",
      "433           -0.1318      0.137     -0.963      0.335      -0.400       0.136\n",
      "434            0.2306      0.113      2.041      0.041       0.009       0.452\n",
      "439            0.2109      0.130      1.628      0.104      -0.043       0.465\n",
      "440           -0.1746      0.175     -0.999      0.318      -0.517       0.168\n",
      "461            0.3661      0.124      2.944      0.003       0.122       0.610\n",
      "468            0.1039      0.089      1.164      0.245      -0.071       0.279\n",
      "472            0.1995      0.196      1.019      0.308      -0.184       0.583\n",
      "473            0.2479      0.157      1.583      0.113      -0.059       0.555\n",
      "474            0.1321      0.145      0.910      0.363      -0.152       0.417\n",
      "485           -0.1919      0.124     -1.549      0.121      -0.435       0.051\n",
      "486           -0.1285      0.130     -0.991      0.322      -0.382       0.126\n",
      "487            0.2929      0.124      2.358      0.018       0.049       0.536\n",
      "488            0.1148      0.124      0.924      0.356      -0.129       0.358\n",
      "489           -0.2515      0.121     -2.086      0.037      -0.488      -0.015\n",
      "490            0.1546      0.117      1.322      0.186      -0.075       0.384\n",
      "500           -0.1292      0.119     -1.088      0.277      -0.362       0.104\n",
      "501            0.1470      0.114      1.290      0.197      -0.076       0.370\n",
      "511            0.1389      0.093      1.500      0.134      -0.043       0.320\n",
      "512            0.2799      0.111      2.512      0.012       0.062       0.498\n",
      "542            0.2484      0.115      2.158      0.031       0.023       0.474\n",
      "548            0.1861      0.127      1.470      0.142      -0.062       0.434\n",
      "563           -0.1773      0.121     -1.462      0.144      -0.415       0.060\n",
      "570            0.2149      0.106      2.019      0.043       0.006       0.423\n",
      "573            0.3875      0.248      1.565      0.118      -0.098       0.873\n",
      "578           -0.7047      0.295     -2.388      0.017      -1.283      -0.126\n",
      "582           -0.1619      0.138     -1.174      0.241      -0.432       0.108\n",
      "==============================================================================\n",
      "Removing feature 474 with p-value 0.362777\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181685\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1516\n",
      "Method:                           MLE   Df Model:                           50\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2558\n",
      "Time:                        16:35:02   Log-Likelihood:                -284.70\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.964e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0916      0.313    -13.067      0.000      -4.705      -3.478\n",
      "2             -0.1520      0.114     -1.330      0.184      -0.376       0.072\n",
      "3             -0.1218      0.120     -1.014      0.311      -0.357       0.114\n",
      "15            -0.3511      0.135     -2.610      0.009      -0.615      -0.087\n",
      "16             0.3396      0.250      1.359      0.174      -0.150       0.830\n",
      "22             0.2605      0.100      2.604      0.009       0.064       0.457\n",
      "52             0.1778      0.135      1.316      0.188      -0.087       0.443\n",
      "60             0.9877      0.148      6.694      0.000       0.699       1.277\n",
      "63            -0.4156      0.217     -1.919      0.055      -0.840       0.009\n",
      "65             0.5955      0.126      4.716      0.000       0.348       0.843\n",
      "69            -0.2772      0.155     -1.786      0.074      -0.581       0.027\n",
      "72             0.2121      0.112      1.897      0.058      -0.007       0.431\n",
      "73             0.4727      0.306      1.543      0.123      -0.128       1.073\n",
      "74            -1.0710      0.467     -2.293      0.022      -1.987      -0.155\n",
      "91            -0.2094      0.123     -1.706      0.088      -0.450       0.031\n",
      "134            0.2338      0.131      1.791      0.073      -0.022       0.490\n",
      "139            0.1223      0.120      1.017      0.309      -0.113       0.358\n",
      "152           -2.0768      0.911     -2.280      0.023      -3.862      -0.291\n",
      "184            0.4062      0.198      2.053      0.040       0.018       0.794\n",
      "188           -0.4887      0.359     -1.361      0.174      -1.192       0.215\n",
      "209           -0.2534      0.117     -2.172      0.030      -0.482      -0.025\n",
      "253           -8.0568      7.252     -1.111      0.267     -22.271       6.158\n",
      "269            0.1525      0.123      1.244      0.214      -0.088       0.393\n",
      "419           -0.1183      0.122     -0.970      0.332      -0.357       0.121\n",
      "424            0.1494      0.125      1.196      0.232      -0.095       0.394\n",
      "429           -0.9925      0.603     -1.647      0.100      -2.173       0.188\n",
      "433           -0.1332      0.137     -0.970      0.332      -0.402       0.136\n",
      "434            0.2285      0.113      2.017      0.044       0.006       0.451\n",
      "439            0.2133      0.128      1.664      0.096      -0.038       0.464\n",
      "440           -0.1727      0.174     -0.991      0.322      -0.514       0.169\n",
      "461            0.3752      0.124      3.023      0.003       0.132       0.618\n",
      "468            0.0971      0.090      1.083      0.279      -0.079       0.273\n",
      "472            0.3002      0.183      1.642      0.101      -0.058       0.659\n",
      "473            0.2726      0.156      1.752      0.080      -0.032       0.578\n",
      "485           -0.1871      0.124     -1.513      0.130      -0.429       0.055\n",
      "486           -0.1248      0.130     -0.962      0.336      -0.379       0.129\n",
      "487            0.2853      0.124      2.304      0.021       0.043       0.528\n",
      "488            0.1222      0.124      0.987      0.324      -0.121       0.365\n",
      "489           -0.2508      0.121     -2.079      0.038      -0.487      -0.014\n",
      "490            0.1537      0.117      1.314      0.189      -0.076       0.383\n",
      "500           -0.1351      0.119     -1.136      0.256      -0.368       0.098\n",
      "501            0.1475      0.114      1.296      0.195      -0.076       0.371\n",
      "511            0.1413      0.092      1.535      0.125      -0.039       0.322\n",
      "512            0.2812      0.111      2.525      0.012       0.063       0.500\n",
      "542            0.2414      0.115      2.097      0.036       0.016       0.467\n",
      "548            0.1906      0.126      1.514      0.130      -0.056       0.438\n",
      "563           -0.1734      0.121     -1.427      0.154      -0.411       0.065\n",
      "570            0.2106      0.106      1.979      0.048       0.002       0.419\n",
      "573            0.3850      0.248      1.552      0.121      -0.101       0.871\n",
      "578           -0.7003      0.295     -2.373      0.018      -1.279      -0.122\n",
      "582           -0.1645      0.139     -1.187      0.235      -0.436       0.107\n",
      "==============================================================================\n",
      "Removing feature 486 with p-value 0.335830\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.181993\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1517\n",
      "Method:                           MLE   Df Model:                           49\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2546\n",
      "Time:                        16:35:02   Log-Likelihood:                -285.18\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.832e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0807      0.312    -13.075      0.000      -4.692      -3.469\n",
      "2             -0.1532      0.114     -1.341      0.180      -0.377       0.071\n",
      "3             -0.1198      0.120     -0.998      0.318      -0.355       0.115\n",
      "15            -0.3579      0.134     -2.662      0.008      -0.621      -0.094\n",
      "16             0.3433      0.251      1.369      0.171      -0.148       0.835\n",
      "22             0.2594      0.100      2.593      0.010       0.063       0.455\n",
      "52             0.1795      0.135      1.328      0.184      -0.085       0.444\n",
      "60             0.9903      0.147      6.724      0.000       0.702       1.279\n",
      "63            -0.4255      0.216     -1.972      0.049      -0.848      -0.003\n",
      "65             0.5982      0.126      4.754      0.000       0.352       0.845\n",
      "69            -0.2817      0.156     -1.810      0.070      -0.587       0.023\n",
      "72             0.2164      0.112      1.941      0.052      -0.002       0.435\n",
      "73             0.4755      0.305      1.557      0.119      -0.123       1.074\n",
      "74            -1.0410      0.462     -2.253      0.024      -1.946      -0.136\n",
      "91            -0.2105      0.122     -1.719      0.086      -0.450       0.029\n",
      "134            0.2318      0.130      1.787      0.074      -0.022       0.486\n",
      "139            0.1270      0.120      1.056      0.291      -0.109       0.363\n",
      "152           -2.1024      0.912     -2.304      0.021      -3.891      -0.314\n",
      "184            0.4122      0.198      2.078      0.038       0.023       0.801\n",
      "188           -0.4848      0.360     -1.347      0.178      -1.190       0.221\n",
      "209           -0.2501      0.116     -2.148      0.032      -0.478      -0.022\n",
      "253           -7.9510      7.246     -1.097      0.273     -22.153       6.251\n",
      "269            0.1532      0.122      1.251      0.211      -0.087       0.393\n",
      "419           -0.1193      0.122     -0.979      0.328      -0.358       0.120\n",
      "424            0.1462      0.124      1.176      0.240      -0.097       0.390\n",
      "429           -1.0017      0.603     -1.662      0.096      -2.183       0.179\n",
      "433           -0.1334      0.138     -0.970      0.332      -0.403       0.136\n",
      "434            0.2344      0.113      2.077      0.038       0.013       0.455\n",
      "439            0.2215      0.128      1.735      0.083      -0.029       0.472\n",
      "440           -0.1768      0.175     -1.011      0.312      -0.519       0.166\n",
      "461            0.3708      0.125      2.977      0.003       0.127       0.615\n",
      "468            0.0907      0.088      1.027      0.304      -0.082       0.264\n",
      "472            0.2838      0.180      1.578      0.115      -0.069       0.636\n",
      "473            0.2659      0.155      1.720      0.085      -0.037       0.569\n",
      "485           -0.1854      0.124     -1.500      0.134      -0.428       0.057\n",
      "487            0.2855      0.124      2.306      0.021       0.043       0.528\n",
      "488            0.1138      0.123      0.922      0.357      -0.128       0.356\n",
      "489           -0.2449      0.120     -2.039      0.041      -0.480      -0.010\n",
      "490            0.1467      0.117      1.258      0.208      -0.082       0.375\n",
      "500           -0.1415      0.119     -1.191      0.234      -0.374       0.091\n",
      "501            0.1484      0.114      1.305      0.192      -0.075       0.371\n",
      "511            0.1376      0.092      1.496      0.135      -0.043       0.318\n",
      "512            0.2828      0.111      2.542      0.011       0.065       0.501\n",
      "542            0.2372      0.114      2.072      0.038       0.013       0.462\n",
      "548            0.1863      0.126      1.479      0.139      -0.061       0.433\n",
      "563           -0.1710      0.122     -1.406      0.160      -0.409       0.067\n",
      "570            0.2069      0.106      1.951      0.051      -0.001       0.415\n",
      "573            0.3851      0.249      1.546      0.122      -0.103       0.873\n",
      "578           -0.6977      0.295     -2.366      0.018      -1.276      -0.120\n",
      "582           -0.1615      0.136     -1.184      0.237      -0.429       0.106\n",
      "==============================================================================\n",
      "Removing feature 488 with p-value 0.356509\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182258\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1518\n",
      "Method:                           MLE   Df Model:                           48\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2535\n",
      "Time:                        16:35:02   Log-Likelihood:                -285.60\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.909e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0715      0.312    -13.051      0.000      -4.683      -3.460\n",
      "2             -0.1566      0.114     -1.372      0.170      -0.380       0.067\n",
      "3             -0.1159      0.120     -0.969      0.332      -0.350       0.118\n",
      "15            -0.3533      0.134     -2.637      0.008      -0.616      -0.091\n",
      "16             0.3414      0.250      1.366      0.172      -0.148       0.831\n",
      "22             0.2644      0.100      2.650      0.008       0.069       0.460\n",
      "52             0.1757      0.135      1.304      0.192      -0.088       0.440\n",
      "60             0.9647      0.144      6.692      0.000       0.682       1.247\n",
      "63            -0.4116      0.215     -1.914      0.056      -0.833       0.010\n",
      "65             0.5876      0.125      4.691      0.000       0.342       0.833\n",
      "69            -0.2775      0.155     -1.792      0.073      -0.581       0.026\n",
      "72             0.2228      0.111      2.005      0.045       0.005       0.441\n",
      "73             0.4705      0.305      1.542      0.123      -0.127       1.068\n",
      "74            -1.0353      0.462     -2.240      0.025      -1.941      -0.130\n",
      "91            -0.2136      0.122     -1.748      0.080      -0.453       0.026\n",
      "134            0.2327      0.130      1.787      0.074      -0.022       0.488\n",
      "139            0.1197      0.120      0.999      0.318      -0.115       0.354\n",
      "152           -2.0908      0.909     -2.301      0.021      -3.872      -0.310\n",
      "184            0.4064      0.198      2.055      0.040       0.019       0.794\n",
      "188           -0.4789      0.358     -1.339      0.181      -1.180       0.222\n",
      "209           -0.2523      0.116     -2.167      0.030      -0.480      -0.024\n",
      "253           -7.9212      7.272     -1.089      0.276     -22.175       6.332\n",
      "269            0.1399      0.121      1.153      0.249      -0.098       0.378\n",
      "419           -0.1152      0.122     -0.946      0.344      -0.354       0.123\n",
      "424            0.1481      0.124      1.193      0.233      -0.095       0.391\n",
      "429           -1.0145      0.607     -1.671      0.095      -2.204       0.176\n",
      "433           -0.1315      0.138     -0.954      0.340      -0.402       0.139\n",
      "434            0.2322      0.113      2.059      0.040       0.011       0.453\n",
      "439            0.2251      0.129      1.745      0.081      -0.028       0.478\n",
      "440           -0.1822      0.173     -1.052      0.293      -0.521       0.157\n",
      "461            0.3724      0.124      2.995      0.003       0.129       0.616\n",
      "468            0.0918      0.089      1.036      0.300      -0.082       0.266\n",
      "472            0.2941      0.177      1.660      0.097      -0.053       0.641\n",
      "473            0.2672      0.155      1.729      0.084      -0.036       0.570\n",
      "485           -0.1910      0.124     -1.546      0.122      -0.433       0.051\n",
      "487            0.2908      0.124      2.354      0.019       0.049       0.533\n",
      "489           -0.2399      0.120     -2.001      0.045      -0.475      -0.005\n",
      "490            0.1467      0.117      1.258      0.208      -0.082       0.375\n",
      "500           -0.1436      0.119     -1.211      0.226      -0.376       0.089\n",
      "501            0.1482      0.114      1.304      0.192      -0.075       0.371\n",
      "511            0.1382      0.092      1.501      0.133      -0.042       0.319\n",
      "512            0.2830      0.111      2.544      0.011       0.065       0.501\n",
      "542            0.2405      0.115      2.098      0.036       0.016       0.465\n",
      "548            0.1849      0.126      1.468      0.142      -0.062       0.432\n",
      "563           -0.1649      0.121     -1.358      0.175      -0.403       0.073\n",
      "570            0.2034      0.106      1.920      0.055      -0.004       0.411\n",
      "573            0.3813      0.249      1.532      0.126      -0.107       0.869\n",
      "578           -0.6990      0.295     -2.371      0.018      -1.277      -0.121\n",
      "582           -0.1629      0.137     -1.191      0.234      -0.431       0.105\n",
      "==============================================================================\n",
      "Removing feature 419 with p-value 0.344001\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182548\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1519\n",
      "Method:                           MLE   Df Model:                           47\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2523\n",
      "Time:                        16:35:02   Log-Likelihood:                -286.05\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.318e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0660      0.312    -13.024      0.000      -4.678      -3.454\n",
      "2             -0.1584      0.114     -1.388      0.165      -0.382       0.065\n",
      "3             -0.1169      0.119     -0.980      0.327      -0.351       0.117\n",
      "15            -0.3559      0.134     -2.650      0.008      -0.619      -0.093\n",
      "16             0.3322      0.251      1.326      0.185      -0.159       0.823\n",
      "22             0.2648      0.100      2.660      0.008       0.070       0.460\n",
      "52             0.1767      0.135      1.311      0.190      -0.087       0.441\n",
      "60             0.9633      0.144      6.684      0.000       0.681       1.246\n",
      "63            -0.4084      0.215     -1.902      0.057      -0.829       0.012\n",
      "65             0.5878      0.126      4.681      0.000       0.342       0.834\n",
      "69            -0.2862      0.155     -1.848      0.065      -0.590       0.017\n",
      "72             0.2257      0.111      2.040      0.041       0.009       0.443\n",
      "73             0.4431      0.303      1.462      0.144      -0.151       1.037\n",
      "74            -1.0367      0.461     -2.250      0.024      -1.940      -0.134\n",
      "91            -0.2189      0.122     -1.792      0.073      -0.458       0.020\n",
      "134            0.2381      0.130      1.831      0.067      -0.017       0.493\n",
      "139            0.1211      0.120      1.012      0.312      -0.113       0.356\n",
      "152           -2.1001      0.914     -2.299      0.022      -3.891      -0.309\n",
      "184            0.3999      0.196      2.037      0.042       0.015       0.785\n",
      "188           -0.4634      0.354     -1.310      0.190      -1.157       0.230\n",
      "209           -0.2533      0.117     -2.168      0.030      -0.482      -0.024\n",
      "253           -8.1521      7.292     -1.118      0.264     -22.444       6.140\n",
      "269            0.1413      0.121      1.163      0.245      -0.097       0.379\n",
      "424            0.1496      0.125      1.201      0.230      -0.095       0.394\n",
      "429           -1.0092      0.606     -1.666      0.096      -2.196       0.178\n",
      "433           -0.1286      0.138     -0.935      0.350      -0.398       0.141\n",
      "434            0.2289      0.112      2.038      0.042       0.009       0.449\n",
      "439            0.2202      0.128      1.719      0.086      -0.031       0.471\n",
      "440           -0.1807      0.173     -1.045      0.296      -0.520       0.158\n",
      "461            0.3697      0.124      2.980      0.003       0.127       0.613\n",
      "468            0.0900      0.089      1.016      0.310      -0.084       0.264\n",
      "472            0.2944      0.176      1.678      0.093      -0.050       0.638\n",
      "473            0.2647      0.156      1.700      0.089      -0.040       0.570\n",
      "485           -0.1954      0.124     -1.581      0.114      -0.438       0.047\n",
      "487            0.2898      0.123      2.347      0.019       0.048       0.532\n",
      "489           -0.2359      0.120     -1.968      0.049      -0.471      -0.001\n",
      "490            0.1448      0.117      1.242      0.214      -0.084       0.373\n",
      "500           -0.1530      0.118     -1.293      0.196      -0.385       0.079\n",
      "501            0.1430      0.114      1.260      0.208      -0.080       0.366\n",
      "511            0.1385      0.092      1.497      0.134      -0.043       0.320\n",
      "512            0.2728      0.111      2.468      0.014       0.056       0.489\n",
      "542            0.2375      0.115      2.067      0.039       0.012       0.463\n",
      "548            0.1853      0.126      1.474      0.141      -0.061       0.432\n",
      "563           -0.1557      0.121     -1.287      0.198      -0.393       0.081\n",
      "570            0.2033      0.106      1.921      0.055      -0.004       0.411\n",
      "573            0.3758      0.249      1.507      0.132      -0.113       0.865\n",
      "578           -0.6958      0.295     -2.359      0.018      -1.274      -0.118\n",
      "582           -0.1607      0.136     -1.179      0.238      -0.428       0.106\n",
      "==============================================================================\n",
      "Removing feature 433 with p-value 0.349782\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.182855\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1520\n",
      "Method:                           MLE   Df Model:                           46\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2510\n",
      "Time:                        16:35:03   Log-Likelihood:                -286.53\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.232e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0614      0.313    -12.993      0.000      -4.674      -3.449\n",
      "2             -0.1663      0.114     -1.463      0.143      -0.389       0.057\n",
      "3             -0.1176      0.119     -0.987      0.324      -0.351       0.116\n",
      "15            -0.3552      0.134     -2.643      0.008      -0.619      -0.092\n",
      "16             0.3182      0.250      1.271      0.204      -0.172       0.809\n",
      "22             0.2348      0.094      2.493      0.013       0.050       0.419\n",
      "52             0.1752      0.134      1.303      0.193      -0.088       0.439\n",
      "60             0.9695      0.144      6.722      0.000       0.687       1.252\n",
      "63            -0.4056      0.214     -1.892      0.059      -0.826       0.015\n",
      "65             0.5909      0.126      4.694      0.000       0.344       0.838\n",
      "69            -0.2742      0.154     -1.776      0.076      -0.577       0.028\n",
      "72             0.2211      0.111      1.992      0.046       0.004       0.439\n",
      "73             0.4500      0.303      1.485      0.138      -0.144       1.044\n",
      "74            -1.0381      0.460     -2.258      0.024      -1.939      -0.137\n",
      "91            -0.2106      0.122     -1.730      0.084      -0.449       0.028\n",
      "134            0.2368      0.130      1.818      0.069      -0.018       0.492\n",
      "139            0.1136      0.120      0.950      0.342      -0.121       0.348\n",
      "152           -2.1080      0.916     -2.302      0.021      -3.903      -0.314\n",
      "184            0.3993      0.197      2.030      0.042       0.014       0.785\n",
      "188           -0.4637      0.355     -1.307      0.191      -1.159       0.231\n",
      "209           -0.2490      0.116     -2.139      0.032      -0.477      -0.021\n",
      "253           -8.1119      7.305     -1.110      0.267     -22.430       6.206\n",
      "269            0.1438      0.121      1.187      0.235      -0.094       0.381\n",
      "424            0.1517      0.125      1.217      0.224      -0.093       0.396\n",
      "429           -1.0327      0.612     -1.688      0.091      -2.231       0.166\n",
      "434            0.2362      0.112      2.110      0.035       0.017       0.456\n",
      "439            0.2205      0.128      1.726      0.084      -0.030       0.471\n",
      "440           -0.1800      0.173     -1.038      0.299      -0.520       0.160\n",
      "461            0.3728      0.124      3.017      0.003       0.131       0.615\n",
      "468            0.0872      0.087      1.001      0.317      -0.084       0.258\n",
      "472            0.2892      0.176      1.643      0.100      -0.056       0.634\n",
      "473            0.2581      0.155      1.661      0.097      -0.046       0.563\n",
      "485           -0.1976      0.124     -1.599      0.110      -0.440       0.045\n",
      "487            0.2859      0.123      2.324      0.020       0.045       0.527\n",
      "489           -0.2342      0.120     -1.953      0.051      -0.469       0.001\n",
      "490            0.1427      0.116      1.225      0.220      -0.086       0.371\n",
      "500           -0.1500      0.118     -1.267      0.205      -0.382       0.082\n",
      "501            0.1417      0.114      1.248      0.212      -0.081       0.364\n",
      "511            0.1399      0.093      1.509      0.131      -0.042       0.321\n",
      "512            0.2726      0.110      2.469      0.014       0.056       0.489\n",
      "542            0.2329      0.114      2.036      0.042       0.009       0.457\n",
      "548            0.1824      0.126      1.448      0.148      -0.064       0.429\n",
      "563           -0.1531      0.120     -1.272      0.204      -0.389       0.083\n",
      "570            0.2071      0.105      1.967      0.049       0.001       0.413\n",
      "573            0.3829      0.249      1.537      0.124      -0.105       0.871\n",
      "578           -0.6935      0.295     -2.353      0.019      -1.271      -0.116\n",
      "582           -0.1602      0.136     -1.179      0.239      -0.427       0.106\n",
      "==============================================================================\n",
      "Removing feature 139 with p-value 0.341893\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183137\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1521\n",
      "Method:                           MLE   Df Model:                           45\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2499\n",
      "Time:                        16:35:03   Log-Likelihood:                -286.98\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.228e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0411      0.310    -13.042      0.000      -4.648      -3.434\n",
      "2             -0.1744      0.114     -1.533      0.125      -0.397       0.049\n",
      "3             -0.0815      0.113     -0.721      0.471      -0.303       0.140\n",
      "15            -0.3559      0.135     -2.641      0.008      -0.620      -0.092\n",
      "16             0.3225      0.249      1.293      0.196      -0.166       0.811\n",
      "22             0.2383      0.094      2.524      0.012       0.053       0.423\n",
      "52             0.1734      0.134      1.290      0.197      -0.090       0.437\n",
      "60             0.9811      0.144      6.832      0.000       0.700       1.263\n",
      "63            -0.3950      0.213     -1.852      0.064      -0.813       0.023\n",
      "65             0.5991      0.125      4.777      0.000       0.353       0.845\n",
      "69            -0.2621      0.153     -1.708      0.088      -0.563       0.039\n",
      "72             0.2225      0.111      2.010      0.044       0.006       0.439\n",
      "73             0.4453      0.302      1.475      0.140      -0.146       1.037\n",
      "74            -1.0666      0.458     -2.327      0.020      -1.965      -0.168\n",
      "91            -0.2113      0.122     -1.730      0.084      -0.451       0.028\n",
      "134            0.2349      0.130      1.809      0.071      -0.020       0.489\n",
      "152           -2.0685      0.908     -2.277      0.023      -3.849      -0.288\n",
      "184            0.4026      0.197      2.039      0.041       0.016       0.790\n",
      "188           -0.4748      0.356     -1.333      0.182      -1.173       0.223\n",
      "209           -0.2528      0.116     -2.174      0.030      -0.481      -0.025\n",
      "253           -7.9426      7.302     -1.088      0.277     -22.254       6.369\n",
      "269            0.1459      0.121      1.203      0.229      -0.092       0.384\n",
      "424            0.1581      0.124      1.273      0.203      -0.085       0.401\n",
      "429           -1.0390      0.607     -1.710      0.087      -2.230       0.152\n",
      "434            0.2341      0.112      2.092      0.036       0.015       0.453\n",
      "439            0.2204      0.128      1.726      0.084      -0.030       0.471\n",
      "440           -0.1772      0.171     -1.037      0.300      -0.512       0.158\n",
      "461            0.3697      0.123      2.997      0.003       0.128       0.611\n",
      "468            0.0882      0.087      1.009      0.313      -0.083       0.259\n",
      "472            0.2820      0.176      1.602      0.109      -0.063       0.627\n",
      "473            0.2552      0.155      1.641      0.101      -0.050       0.560\n",
      "485           -0.1854      0.123     -1.507      0.132      -0.427       0.056\n",
      "487            0.2812      0.123      2.295      0.022       0.041       0.521\n",
      "489           -0.2327      0.120     -1.945      0.052      -0.467       0.002\n",
      "490            0.1359      0.116      1.173      0.241      -0.091       0.363\n",
      "500           -0.1460      0.118     -1.236      0.217      -0.378       0.086\n",
      "501            0.1410      0.114      1.242      0.214      -0.082       0.364\n",
      "511            0.1370      0.093      1.481      0.139      -0.044       0.318\n",
      "512            0.2706      0.110      2.451      0.014       0.054       0.487\n",
      "542            0.2336      0.114      2.044      0.041       0.010       0.457\n",
      "548            0.1824      0.126      1.449      0.147      -0.064       0.429\n",
      "563           -0.1551      0.120     -1.292      0.196      -0.390       0.080\n",
      "570            0.2047      0.105      1.945      0.052      -0.002       0.411\n",
      "573            0.3890      0.247      1.572      0.116      -0.096       0.874\n",
      "578           -0.6975      0.294     -2.375      0.018      -1.273      -0.122\n",
      "582           -0.1564      0.136     -1.149      0.250      -0.423       0.110\n",
      "==============================================================================\n",
      "Removing feature 3 with p-value 0.470660\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183303\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1522\n",
      "Method:                           MLE   Df Model:                           44\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2492\n",
      "Time:                        16:35:03   Log-Likelihood:                -287.24\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.616e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0264      0.307    -13.094      0.000      -4.629      -3.424\n",
      "2             -0.1735      0.114     -1.524      0.128      -0.397       0.050\n",
      "15            -0.3565      0.135     -2.643      0.008      -0.621      -0.092\n",
      "16             0.3220      0.250      1.289      0.197      -0.168       0.812\n",
      "22             0.2351      0.094      2.494      0.013       0.050       0.420\n",
      "52             0.1766      0.134      1.316      0.188      -0.087       0.440\n",
      "60             0.9704      0.143      6.809      0.000       0.691       1.250\n",
      "63            -0.3935      0.213     -1.844      0.065      -0.812       0.025\n",
      "65             0.5965      0.125      4.758      0.000       0.351       0.842\n",
      "69            -0.2564      0.153     -1.672      0.094      -0.557       0.044\n",
      "72             0.2148      0.110      1.951      0.051      -0.001       0.430\n",
      "73             0.4388      0.301      1.458      0.145      -0.151       1.029\n",
      "74            -1.0471      0.457     -2.291      0.022      -1.943      -0.151\n",
      "91            -0.2156      0.122     -1.761      0.078      -0.455       0.024\n",
      "134            0.2399      0.130      1.849      0.065      -0.014       0.494\n",
      "152           -2.0812      0.909     -2.289      0.022      -3.864      -0.299\n",
      "184            0.4038      0.197      2.048      0.041       0.017       0.790\n",
      "188           -0.4760      0.356     -1.335      0.182      -1.175       0.223\n",
      "209           -0.2530      0.116     -2.173      0.030      -0.481      -0.025\n",
      "253           -7.7047      7.258     -1.062      0.288     -21.930       6.521\n",
      "269            0.1544      0.121      1.279      0.201      -0.082       0.391\n",
      "424            0.1557      0.124      1.254      0.210      -0.088       0.399\n",
      "429           -1.0062      0.603     -1.670      0.095      -2.187       0.175\n",
      "434            0.2369      0.112      2.122      0.034       0.018       0.456\n",
      "439            0.2266      0.127      1.782      0.075      -0.023       0.476\n",
      "440           -0.1736      0.169     -1.026      0.305      -0.505       0.158\n",
      "461            0.3693      0.123      2.998      0.003       0.128       0.611\n",
      "468            0.0916      0.087      1.053      0.292      -0.079       0.262\n",
      "472            0.2843      0.177      1.609      0.108      -0.062       0.631\n",
      "473            0.2529      0.156      1.625      0.104      -0.052       0.558\n",
      "485           -0.1881      0.123     -1.527      0.127      -0.429       0.053\n",
      "487            0.2778      0.123      2.263      0.024       0.037       0.518\n",
      "489           -0.2335      0.120     -1.950      0.051      -0.468       0.001\n",
      "490            0.1340      0.116      1.158      0.247      -0.093       0.361\n",
      "500           -0.1453      0.118     -1.230      0.219      -0.377       0.086\n",
      "501            0.1402      0.113      1.236      0.216      -0.082       0.362\n",
      "511            0.1378      0.093      1.487      0.137      -0.044       0.319\n",
      "512            0.2630      0.110      2.393      0.017       0.048       0.478\n",
      "542            0.2341      0.114      2.053      0.040       0.011       0.458\n",
      "548            0.1807      0.126      1.438      0.150      -0.066       0.427\n",
      "563           -0.1580      0.120     -1.318      0.188      -0.393       0.077\n",
      "570            0.2001      0.105      1.902      0.057      -0.006       0.406\n",
      "573            0.3838      0.250      1.538      0.124      -0.105       0.873\n",
      "578           -0.6888      0.294     -2.345      0.019      -1.265      -0.113\n",
      "582           -0.1553      0.135     -1.148      0.251      -0.421       0.110\n",
      "==============================================================================\n",
      "Removing feature 440 with p-value 0.304978\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.183687\n",
      "         Iterations 11\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1523\n",
      "Method:                           MLE   Df Model:                           43\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2476\n",
      "Time:                        16:35:03   Log-Likelihood:                -287.84\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.724e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -4.0186      0.307    -13.089      0.000      -4.620      -3.417\n",
      "2             -0.1741      0.113     -1.535      0.125      -0.396       0.048\n",
      "15            -0.3491      0.135     -2.592      0.010      -0.613      -0.085\n",
      "16             0.3202      0.249      1.285      0.199      -0.168       0.809\n",
      "22             0.2334      0.094      2.476      0.013       0.049       0.418\n",
      "52             0.1714      0.134      1.280      0.200      -0.091       0.434\n",
      "60             0.9814      0.142      6.917      0.000       0.703       1.259\n",
      "63            -0.3853      0.212     -1.814      0.070      -0.801       0.031\n",
      "65             0.6031      0.125      4.814      0.000       0.358       0.849\n",
      "69            -0.2618      0.153     -1.716      0.086      -0.561       0.037\n",
      "72             0.2050      0.110      1.869      0.062      -0.010       0.420\n",
      "73             0.4316      0.300      1.439      0.150      -0.156       1.019\n",
      "74            -1.0932      0.454     -2.405      0.016      -1.984      -0.202\n",
      "91            -0.2244      0.122     -1.843      0.065      -0.463       0.014\n",
      "134            0.2394      0.129      1.849      0.064      -0.014       0.493\n",
      "152           -2.0901      0.908     -2.301      0.021      -3.870      -0.310\n",
      "184            0.3933      0.196      2.005      0.045       0.009       0.778\n",
      "188           -0.4662      0.355     -1.312      0.189      -1.163       0.230\n",
      "209           -0.2572      0.116     -2.215      0.027      -0.485      -0.030\n",
      "253           -7.4829      7.258     -1.031      0.303     -21.709       6.743\n",
      "269            0.1577      0.120      1.309      0.190      -0.078       0.394\n",
      "424            0.1475      0.124      1.189      0.234      -0.096       0.391\n",
      "429           -0.9983      0.603     -1.655      0.098      -2.180       0.184\n",
      "434            0.2380      0.111      2.140      0.032       0.020       0.456\n",
      "439            0.1240      0.083      1.495      0.135      -0.039       0.287\n",
      "461            0.3605      0.123      2.922      0.003       0.119       0.602\n",
      "468            0.0921      0.088      1.052      0.293      -0.080       0.264\n",
      "472            0.2842      0.176      1.611      0.107      -0.062       0.630\n",
      "473            0.2586      0.155      1.669      0.095      -0.045       0.562\n",
      "485           -0.1777      0.123     -1.448      0.148      -0.418       0.063\n",
      "487            0.2724      0.122      2.225      0.026       0.032       0.512\n",
      "489           -0.2346      0.120     -1.961      0.050      -0.469   -8.88e-05\n",
      "490            0.1348      0.116      1.166      0.244      -0.092       0.362\n",
      "500           -0.1476      0.118     -1.250      0.211      -0.379       0.084\n",
      "501            0.1411      0.113      1.245      0.213      -0.081       0.363\n",
      "511            0.1426      0.092      1.543      0.123      -0.038       0.324\n",
      "512            0.2596      0.110      2.366      0.018       0.045       0.475\n",
      "542            0.2283      0.114      2.011      0.044       0.006       0.451\n",
      "548            0.1903      0.125      1.523      0.128      -0.055       0.435\n",
      "563           -0.1580      0.119     -1.323      0.186      -0.392       0.076\n",
      "570            0.1964      0.105      1.865      0.062      -0.010       0.403\n",
      "573            0.3970      0.248      1.603      0.109      -0.088       0.882\n",
      "578           -0.7017      0.293     -2.396      0.017      -1.276      -0.128\n",
      "582           -0.1542      0.136     -1.137      0.255      -0.420       0.111\n",
      "==============================================================================\n",
      "Removing feature 253 with p-value 0.302551\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184057\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1524\n",
      "Method:                           MLE   Df Model:                           42\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2461\n",
      "Time:                        16:35:03   Log-Likelihood:                -288.42\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.005e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.8179      0.229    -16.686      0.000      -4.266      -3.369\n",
      "2             -0.1674      0.113     -1.477      0.140      -0.390       0.055\n",
      "15            -0.3448      0.135     -2.556      0.011      -0.609      -0.080\n",
      "16             0.2995      0.249      1.203      0.229      -0.189       0.788\n",
      "22             0.2266      0.094      2.407      0.016       0.042       0.411\n",
      "52             0.1775      0.133      1.331      0.183      -0.084       0.439\n",
      "60             0.9538      0.139      6.870      0.000       0.682       1.226\n",
      "63            -0.3970      0.213     -1.867      0.062      -0.814       0.020\n",
      "65             0.5961      0.125      4.765      0.000       0.351       0.841\n",
      "69            -0.2613      0.153     -1.712      0.087      -0.560       0.038\n",
      "72             0.2099      0.110      1.911      0.056      -0.005       0.425\n",
      "73             0.4373      0.299      1.463      0.144      -0.149       1.023\n",
      "74            -1.1343      0.451     -2.514      0.012      -2.019      -0.250\n",
      "91            -0.2330      0.122     -1.917      0.055      -0.471       0.005\n",
      "134            0.2422      0.129      1.874      0.061      -0.011       0.496\n",
      "152           -2.0618      0.909     -2.269      0.023      -3.843      -0.281\n",
      "184            0.3996      0.196      2.044      0.041       0.016       0.783\n",
      "188           -0.4810      0.352     -1.366      0.172      -1.171       0.209\n",
      "209           -0.2560      0.116     -2.213      0.027      -0.483      -0.029\n",
      "269            0.1566      0.121      1.297      0.195      -0.080       0.393\n",
      "424            0.1459      0.124      1.176      0.240      -0.097       0.389\n",
      "429           -0.9588      0.596     -1.608      0.108      -2.128       0.210\n",
      "434            0.2359      0.111      2.126      0.034       0.018       0.453\n",
      "439            0.1299      0.083      1.559      0.119      -0.033       0.293\n",
      "461            0.3561      0.123      2.893      0.004       0.115       0.597\n",
      "468            0.0924      0.087      1.063      0.288      -0.078       0.263\n",
      "472            0.2775      0.174      1.595      0.111      -0.064       0.619\n",
      "473            0.2684      0.155      1.731      0.083      -0.035       0.572\n",
      "485           -0.1822      0.123     -1.486      0.137      -0.423       0.058\n",
      "487            0.2713      0.123      2.212      0.027       0.031       0.512\n",
      "489           -0.2432      0.120     -2.031      0.042      -0.478      -0.009\n",
      "490            0.1326      0.116      1.144      0.253      -0.095       0.360\n",
      "500           -0.1503      0.118     -1.272      0.203      -0.382       0.081\n",
      "501            0.1357      0.113      1.199      0.231      -0.086       0.357\n",
      "511            0.1379      0.092      1.494      0.135      -0.043       0.319\n",
      "512            0.2514      0.109      2.299      0.022       0.037       0.466\n",
      "542            0.2189      0.113      1.944      0.052      -0.002       0.440\n",
      "548            0.2008      0.125      1.611      0.107      -0.043       0.445\n",
      "563           -0.1608      0.120     -1.344      0.179      -0.395       0.074\n",
      "570            0.1972      0.105      1.878      0.060      -0.009       0.403\n",
      "573            0.4059      0.247      1.645      0.100      -0.078       0.890\n",
      "578           -0.6957      0.290     -2.398      0.016      -1.264      -0.127\n",
      "582           -0.1480      0.135     -1.099      0.272      -0.412       0.116\n",
      "==============================================================================\n",
      "Removing feature 468 with p-value 0.287930\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184384\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1525\n",
      "Method:                           MLE   Df Model:                           41\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2448\n",
      "Time:                        16:35:04   Log-Likelihood:                -288.93\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.387e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.8048      0.227    -16.749      0.000      -4.250      -3.360\n",
      "2             -0.1615      0.113     -1.427      0.154      -0.383       0.060\n",
      "15            -0.3278      0.134     -2.443      0.015      -0.591      -0.065\n",
      "16             0.2976      0.250      1.193      0.233      -0.192       0.787\n",
      "22             0.2318      0.094      2.468      0.014       0.048       0.416\n",
      "52             0.1823      0.133      1.371      0.170      -0.078       0.443\n",
      "60             0.9586      0.139      6.918      0.000       0.687       1.230\n",
      "63            -0.4051      0.212     -1.912      0.056      -0.820       0.010\n",
      "65             0.5967      0.125      4.776      0.000       0.352       0.842\n",
      "69            -0.2761      0.152     -1.822      0.068      -0.573       0.021\n",
      "72             0.2163      0.110      1.974      0.048       0.002       0.431\n",
      "73             0.4003      0.296      1.351      0.177      -0.180       0.981\n",
      "74            -1.1662      0.451     -2.587      0.010      -2.050      -0.283\n",
      "91            -0.2254      0.122     -1.847      0.065      -0.465       0.014\n",
      "134            0.2343      0.129      1.822      0.069      -0.018       0.486\n",
      "152           -2.0719      0.909     -2.279      0.023      -3.854      -0.290\n",
      "184            0.3902      0.195      2.004      0.045       0.009       0.772\n",
      "188           -0.4769      0.351     -1.359      0.174      -1.165       0.211\n",
      "209           -0.2510      0.116     -2.167      0.030      -0.478      -0.024\n",
      "269            0.1596      0.120      1.325      0.185      -0.077       0.396\n",
      "424            0.1468      0.124      1.186      0.236      -0.096       0.389\n",
      "429           -0.9394      0.586     -1.603      0.109      -2.088       0.209\n",
      "434            0.2332      0.111      2.103      0.035       0.016       0.451\n",
      "439            0.1282      0.083      1.549      0.121      -0.034       0.291\n",
      "461            0.3616      0.123      2.952      0.003       0.121       0.602\n",
      "472            0.2883      0.174      1.654      0.098      -0.053       0.630\n",
      "473            0.2743      0.154      1.781      0.075      -0.028       0.576\n",
      "485           -0.1832      0.122     -1.496      0.135      -0.423       0.057\n",
      "487            0.2746      0.123      2.237      0.025       0.034       0.515\n",
      "489           -0.2525      0.119     -2.116      0.034      -0.486      -0.019\n",
      "490            0.1415      0.115      1.230      0.219      -0.084       0.367\n",
      "500           -0.1445      0.118     -1.228      0.219      -0.375       0.086\n",
      "501            0.1359      0.113      1.202      0.229      -0.086       0.357\n",
      "511            0.1407      0.092      1.523      0.128      -0.040       0.322\n",
      "512            0.2515      0.109      2.303      0.021       0.037       0.466\n",
      "542            0.2284      0.112      2.039      0.041       0.009       0.448\n",
      "548            0.2065      0.124      1.660      0.097      -0.037       0.450\n",
      "563           -0.1576      0.119     -1.320      0.187      -0.392       0.076\n",
      "570            0.1889      0.105      1.799      0.072      -0.017       0.395\n",
      "573            0.4184      0.243      1.721      0.085      -0.058       0.895\n",
      "578           -0.6683      0.285     -2.347      0.019      -1.226      -0.110\n",
      "582           -0.1500      0.134     -1.117      0.264      -0.413       0.113\n",
      "==============================================================================\n",
      "Removing feature 582 with p-value 0.263923\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.184829\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1526\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2429\n",
      "Time:                        16:35:04   Log-Likelihood:                -289.63\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.105e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7955      0.227    -16.746      0.000      -4.240      -3.351\n",
      "2             -0.1764      0.112     -1.573      0.116      -0.396       0.043\n",
      "15            -0.3330      0.134     -2.486      0.013      -0.596      -0.070\n",
      "16             0.2952      0.249      1.185      0.236      -0.193       0.784\n",
      "22             0.2288      0.093      2.456      0.014       0.046       0.411\n",
      "52             0.1818      0.133      1.369      0.171      -0.079       0.442\n",
      "60             0.9521      0.138      6.908      0.000       0.682       1.222\n",
      "63            -0.4135      0.212     -1.950      0.051      -0.829       0.002\n",
      "65             0.5885      0.124      4.736      0.000       0.345       0.832\n",
      "69            -0.2849      0.151     -1.882      0.060      -0.581       0.012\n",
      "72             0.2104      0.109      1.928      0.054      -0.004       0.424\n",
      "73             0.3988      0.296      1.347      0.178      -0.181       0.979\n",
      "74            -1.1516      0.449     -2.565      0.010      -2.032      -0.272\n",
      "91            -0.2244      0.122     -1.843      0.065      -0.463       0.014\n",
      "134            0.2398      0.128      1.877      0.060      -0.011       0.490\n",
      "152           -2.1390      0.910     -2.350      0.019      -3.923      -0.355\n",
      "184            0.3805      0.193      1.969      0.049       0.002       0.759\n",
      "188           -0.4559      0.348     -1.309      0.191      -1.139       0.227\n",
      "209           -0.2482      0.116     -2.143      0.032      -0.475      -0.021\n",
      "269            0.1557      0.120      1.297      0.195      -0.080       0.391\n",
      "424            0.1530      0.123      1.240      0.215      -0.089       0.395\n",
      "429           -0.9164      0.581     -1.576      0.115      -2.056       0.223\n",
      "434            0.2252      0.111      2.028      0.043       0.008       0.443\n",
      "439            0.1289      0.082      1.568      0.117      -0.032       0.290\n",
      "461            0.3608      0.123      2.944      0.003       0.121       0.601\n",
      "472            0.2944      0.174      1.690      0.091      -0.047       0.636\n",
      "473            0.2821      0.153      1.839      0.066      -0.019       0.583\n",
      "485           -0.1690      0.122     -1.389      0.165      -0.407       0.069\n",
      "487            0.2652      0.122      2.169      0.030       0.026       0.505\n",
      "489           -0.2548      0.119     -2.137      0.033      -0.488      -0.021\n",
      "490            0.1327      0.115      1.153      0.249      -0.093       0.358\n",
      "500           -0.1478      0.118     -1.256      0.209      -0.379       0.083\n",
      "501            0.1264      0.113      1.123      0.261      -0.094       0.347\n",
      "511            0.1333      0.092      1.448      0.148      -0.047       0.314\n",
      "512            0.2497      0.109      2.284      0.022       0.035       0.464\n",
      "542            0.2288      0.112      2.040      0.041       0.009       0.449\n",
      "548            0.2062      0.124      1.661      0.097      -0.037       0.450\n",
      "563           -0.1443      0.118     -1.220      0.223      -0.376       0.088\n",
      "570            0.1951      0.104      1.874      0.061      -0.009       0.399\n",
      "573            0.4193      0.242      1.731      0.083      -0.055       0.894\n",
      "578           -0.6856      0.286     -2.400      0.016      -1.246      -0.126\n",
      "==============================================================================\n",
      "Removing feature 501 with p-value 0.261317\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.185224\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1527\n",
      "Method:                           MLE   Df Model:                           39\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2413\n",
      "Time:                        16:35:04   Log-Likelihood:                -290.25\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.211e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7818      0.225    -16.775      0.000      -4.224      -3.340\n",
      "2             -0.1759      0.112     -1.566      0.117      -0.396       0.044\n",
      "15            -0.3233      0.133     -2.427      0.015      -0.584      -0.062\n",
      "16             0.3009      0.248      1.213      0.225      -0.185       0.787\n",
      "22             0.2303      0.093      2.469      0.014       0.048       0.413\n",
      "52             0.1722      0.132      1.300      0.194      -0.087       0.432\n",
      "60             0.9508      0.138      6.902      0.000       0.681       1.221\n",
      "63            -0.4159      0.211     -1.969      0.049      -0.830      -0.002\n",
      "65             0.5797      0.124      4.675      0.000       0.337       0.823\n",
      "69            -0.2826      0.151     -1.877      0.061      -0.578       0.013\n",
      "72             0.2092      0.109      1.917      0.055      -0.005       0.423\n",
      "73             0.3991      0.295      1.353      0.176      -0.179       0.977\n",
      "74            -1.1557      0.449     -2.574      0.010      -2.036      -0.276\n",
      "91            -0.2273      0.122     -1.868      0.062      -0.466       0.011\n",
      "134            0.2420      0.128      1.894      0.058      -0.008       0.492\n",
      "152           -2.0761      0.903     -2.300      0.021      -3.845      -0.307\n",
      "184            0.3640      0.192      1.893      0.058      -0.013       0.741\n",
      "188           -0.4293      0.347     -1.236      0.216      -1.110       0.251\n",
      "209           -0.2430      0.115     -2.106      0.035      -0.469      -0.017\n",
      "269            0.1532      0.120      1.280      0.201      -0.081       0.388\n",
      "424            0.1537      0.123      1.250      0.211      -0.087       0.395\n",
      "429           -0.8982      0.581     -1.546      0.122      -2.037       0.240\n",
      "434            0.2275      0.111      2.041      0.041       0.009       0.446\n",
      "439            0.1231      0.083      1.486      0.137      -0.039       0.285\n",
      "461            0.3585      0.123      2.919      0.004       0.118       0.599\n",
      "472            0.2999      0.177      1.699      0.089      -0.046       0.646\n",
      "473            0.2741      0.153      1.793      0.073      -0.025       0.574\n",
      "485           -0.1753      0.122     -1.439      0.150      -0.414       0.063\n",
      "487            0.2658      0.122      2.173      0.030       0.026       0.506\n",
      "489           -0.2546      0.119     -2.134      0.033      -0.488      -0.021\n",
      "490            0.1402      0.115      1.225      0.221      -0.084       0.365\n",
      "500           -0.1494      0.118     -1.270      0.204      -0.380       0.081\n",
      "511            0.1248      0.092      1.363      0.173      -0.055       0.304\n",
      "512            0.2462      0.109      2.258      0.024       0.033       0.460\n",
      "542            0.2406      0.112      2.144      0.032       0.021       0.460\n",
      "548            0.2008      0.124      1.622      0.105      -0.042       0.443\n",
      "563           -0.1476      0.118     -1.249      0.212      -0.379       0.084\n",
      "570            0.1927      0.104      1.854      0.064      -0.011       0.396\n",
      "573            0.4211      0.240      1.754      0.079      -0.049       0.892\n",
      "578           -0.7083      0.287     -2.469      0.014      -1.271      -0.146\n",
      "==============================================================================\n",
      "Removing feature 16 with p-value 0.225226\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.185690\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1528\n",
      "Method:                           MLE   Df Model:                           38\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2394\n",
      "Time:                        16:35:04   Log-Likelihood:                -290.98\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.624e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7890      0.225    -16.825      0.000      -4.230      -3.348\n",
      "2             -0.1716      0.112     -1.527      0.127      -0.392       0.049\n",
      "15            -0.2674      0.125     -2.144      0.032      -0.512      -0.023\n",
      "22             0.2315      0.092      2.508      0.012       0.051       0.412\n",
      "52             0.1700      0.133      1.283      0.199      -0.090       0.430\n",
      "60             0.9572      0.138      6.937      0.000       0.687       1.228\n",
      "63            -0.4169      0.211     -1.976      0.048      -0.830      -0.003\n",
      "65             0.5670      0.123      4.615      0.000       0.326       0.808\n",
      "69            -0.2836      0.151     -1.877      0.061      -0.580       0.013\n",
      "72             0.2120      0.109      1.948      0.051      -0.001       0.425\n",
      "73             0.3876      0.295      1.312      0.189      -0.191       0.966\n",
      "74            -1.1356      0.445     -2.551      0.011      -2.008      -0.263\n",
      "91            -0.2199      0.121     -1.823      0.068      -0.456       0.016\n",
      "134            0.2481      0.127      1.950      0.051      -0.001       0.497\n",
      "152           -2.0538      0.904     -2.271      0.023      -3.826      -0.281\n",
      "184            0.3521      0.192      1.831      0.067      -0.025       0.729\n",
      "188           -0.4151      0.348     -1.193      0.233      -1.097       0.267\n",
      "209           -0.2455      0.115     -2.136      0.033      -0.471      -0.020\n",
      "269            0.1590      0.119      1.331      0.183      -0.075       0.393\n",
      "424            0.1679      0.122      1.376      0.169      -0.071       0.407\n",
      "429           -0.9239      0.576     -1.605      0.108      -2.052       0.204\n",
      "434            0.2343      0.112      2.101      0.036       0.016       0.453\n",
      "439            0.1253      0.082      1.529      0.126      -0.035       0.286\n",
      "461            0.3669      0.122      3.002      0.003       0.127       0.606\n",
      "472            0.2818      0.176      1.604      0.109      -0.063       0.626\n",
      "473            0.2822      0.152      1.856      0.063      -0.016       0.580\n",
      "485           -0.1801      0.121     -1.484      0.138      -0.418       0.058\n",
      "487            0.2596      0.122      2.125      0.034       0.020       0.499\n",
      "489           -0.2496      0.119     -2.094      0.036      -0.483      -0.016\n",
      "490            0.1364      0.114      1.192      0.233      -0.088       0.361\n",
      "500           -0.1490      0.117     -1.270      0.204      -0.379       0.081\n",
      "511            0.1303      0.091      1.431      0.153      -0.048       0.309\n",
      "512            0.2450      0.109      2.248      0.025       0.031       0.459\n",
      "542            0.2453      0.113      2.177      0.030       0.024       0.466\n",
      "548            0.2033      0.124      1.643      0.100      -0.039       0.446\n",
      "563           -0.1558      0.118     -1.325      0.185      -0.386       0.075\n",
      "570            0.1932      0.103      1.868      0.062      -0.010       0.396\n",
      "573            0.4314      0.239      1.807      0.071      -0.036       0.899\n",
      "578           -0.7221      0.286     -2.522      0.012      -1.283      -0.161\n",
      "==============================================================================\n",
      "Removing feature 490 with p-value 0.233423\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.186132\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1529\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2376\n",
      "Time:                        16:35:04   Log-Likelihood:                -291.67\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.154e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7699      0.223    -16.892      0.000      -4.207      -3.333\n",
      "2             -0.1669      0.112     -1.490      0.136      -0.386       0.053\n",
      "15            -0.2598      0.124     -2.096      0.036      -0.503      -0.017\n",
      "22             0.2211      0.092      2.408      0.016       0.041       0.401\n",
      "52             0.1798      0.132      1.363      0.173      -0.079       0.438\n",
      "60             0.9424      0.137      6.893      0.000       0.674       1.210\n",
      "63            -0.4231      0.211     -2.004      0.045      -0.837      -0.009\n",
      "65             0.5653      0.122      4.635      0.000       0.326       0.804\n",
      "69            -0.2869      0.152     -1.886      0.059      -0.585       0.011\n",
      "72             0.2121      0.109      1.947      0.052      -0.001       0.426\n",
      "73             0.3993      0.295      1.356      0.175      -0.178       0.977\n",
      "74            -1.0868      0.441     -2.462      0.014      -1.952      -0.222\n",
      "91            -0.2147      0.121     -1.782      0.075      -0.451       0.021\n",
      "134            0.2531      0.127      1.992      0.046       0.004       0.502\n",
      "152           -1.9390      0.887     -2.185      0.029      -3.678      -0.200\n",
      "184            0.3561      0.192      1.859      0.063      -0.019       0.732\n",
      "188           -0.4183      0.347     -1.206      0.228      -1.098       0.261\n",
      "209           -0.2408      0.115     -2.102      0.036      -0.465      -0.016\n",
      "269            0.1631      0.119      1.366      0.172      -0.071       0.397\n",
      "424            0.1811      0.121      1.492      0.136      -0.057       0.419\n",
      "429           -0.9234      0.580     -1.592      0.111      -2.061       0.214\n",
      "434            0.2396      0.111      2.154      0.031       0.022       0.458\n",
      "439            0.1294      0.082      1.584      0.113      -0.031       0.290\n",
      "461            0.3650      0.121      3.008      0.003       0.127       0.603\n",
      "472            0.2785      0.174      1.605      0.108      -0.062       0.619\n",
      "473            0.2889      0.151      1.911      0.056      -0.007       0.585\n",
      "485           -0.1801      0.121     -1.486      0.137      -0.418       0.058\n",
      "487            0.2538      0.122      2.081      0.037       0.015       0.493\n",
      "489           -0.2505      0.119     -2.107      0.035      -0.484      -0.017\n",
      "500           -0.1483      0.117     -1.264      0.206      -0.378       0.082\n",
      "511            0.1391      0.090      1.547      0.122      -0.037       0.315\n",
      "512            0.2500      0.109      2.302      0.021       0.037       0.463\n",
      "542            0.2479      0.112      2.204      0.028       0.027       0.468\n",
      "548            0.1906      0.123      1.544      0.123      -0.051       0.433\n",
      "563           -0.1503      0.117     -1.284      0.199      -0.380       0.079\n",
      "570            0.1998      0.103      1.935      0.053      -0.003       0.402\n",
      "573            0.4232      0.238      1.780      0.075      -0.043       0.889\n",
      "578           -0.7239      0.287     -2.526      0.012      -1.286      -0.162\n",
      "==============================================================================\n",
      "Removing feature 188 with p-value 0.227757\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187190\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1530\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2333\n",
      "Time:                        16:35:04   Log-Likelihood:                -293.33\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.718e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7222      0.217    -17.189      0.000      -4.147      -3.298\n",
      "2             -0.1755      0.112     -1.562      0.118      -0.396       0.045\n",
      "15            -0.2482      0.124     -2.006      0.045      -0.491      -0.006\n",
      "22             0.2090      0.092      2.283      0.022       0.030       0.389\n",
      "52             0.1926      0.130      1.478      0.139      -0.063       0.448\n",
      "60             0.9401      0.135      6.950      0.000       0.675       1.205\n",
      "63            -0.4177      0.210     -1.992      0.046      -0.829      -0.007\n",
      "65             0.5704      0.122      4.660      0.000       0.331       0.810\n",
      "69            -0.2576      0.150     -1.714      0.086      -0.552       0.037\n",
      "72             0.1946      0.108      1.794      0.073      -0.018       0.407\n",
      "73             0.3929      0.295      1.332      0.183      -0.185       0.971\n",
      "74            -1.0804      0.438     -2.466      0.014      -1.939      -0.222\n",
      "91            -0.2266      0.119     -1.899      0.058      -0.461       0.007\n",
      "134            0.2527      0.127      1.992      0.046       0.004       0.501\n",
      "152           -1.7956      0.877     -2.048      0.041      -3.514      -0.077\n",
      "184            0.1347      0.123      1.096      0.273      -0.106       0.376\n",
      "209           -0.2387      0.115     -2.079      0.038      -0.464      -0.014\n",
      "269            0.1648      0.119      1.380      0.167      -0.069       0.399\n",
      "424            0.1800      0.122      1.478      0.139      -0.059       0.419\n",
      "429           -0.8445      0.549     -1.538      0.124      -1.921       0.232\n",
      "434            0.2269      0.111      2.040      0.041       0.009       0.445\n",
      "439            0.1185      0.083      1.427      0.153      -0.044       0.281\n",
      "461            0.3147      0.114      2.761      0.006       0.091       0.538\n",
      "472            0.2913      0.170      1.714      0.087      -0.042       0.625\n",
      "473            0.2908      0.148      1.963      0.050       0.001       0.581\n",
      "485           -0.2067      0.121     -1.711      0.087      -0.444       0.030\n",
      "487            0.2540      0.121      2.093      0.036       0.016       0.492\n",
      "489           -0.2403      0.118     -2.031      0.042      -0.472      -0.008\n",
      "500           -0.1414      0.117     -1.208      0.227      -0.371       0.088\n",
      "511            0.1483      0.089      1.663      0.096      -0.026       0.323\n",
      "512            0.2611      0.108      2.412      0.016       0.049       0.473\n",
      "542            0.2562      0.112      2.293      0.022       0.037       0.475\n",
      "548            0.2000      0.123      1.623      0.105      -0.042       0.442\n",
      "563           -0.1566      0.116     -1.349      0.177      -0.384       0.071\n",
      "570            0.2037      0.102      1.995      0.046       0.004       0.404\n",
      "573            0.4537      0.236      1.920      0.055      -0.009       0.917\n",
      "578           -0.7493      0.287     -2.615      0.009      -1.311      -0.188\n",
      "==============================================================================\n",
      "Removing feature 184 with p-value 0.273086\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187574\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1531\n",
      "Method:                           MLE   Df Model:                           35\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2317\n",
      "Time:                        16:35:04   Log-Likelihood:                -293.93\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.220e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7158      0.216    -17.216      0.000      -4.139      -3.293\n",
      "2             -0.1774      0.112     -1.577      0.115      -0.398       0.043\n",
      "15            -0.2516      0.123     -2.043      0.041      -0.493      -0.010\n",
      "22             0.2019      0.091      2.217      0.027       0.023       0.380\n",
      "52             0.2090      0.129      1.617      0.106      -0.044       0.462\n",
      "60             0.9421      0.135      6.985      0.000       0.678       1.206\n",
      "63            -0.3778      0.207     -1.827      0.068      -0.783       0.028\n",
      "65             0.5964      0.120      4.973      0.000       0.361       0.831\n",
      "69            -0.2433      0.149     -1.631      0.103      -0.536       0.049\n",
      "72             0.1930      0.109      1.776      0.076      -0.020       0.406\n",
      "73             0.3153      0.287      1.099      0.272      -0.247       0.877\n",
      "74            -1.1709      0.431     -2.714      0.007      -2.016      -0.325\n",
      "91            -0.2315      0.119     -1.942      0.052      -0.465       0.002\n",
      "134            0.2654      0.126      2.104      0.035       0.018       0.513\n",
      "152           -1.7419      0.875     -1.990      0.047      -3.457      -0.027\n",
      "209           -0.2438      0.114     -2.131      0.033      -0.468      -0.020\n",
      "269            0.1715      0.120      1.431      0.152      -0.063       0.406\n",
      "424            0.1660      0.121      1.370      0.171      -0.072       0.404\n",
      "429           -0.8563      0.540     -1.586      0.113      -1.914       0.202\n",
      "434            0.2209      0.111      1.989      0.047       0.003       0.439\n",
      "439            0.1213      0.082      1.480      0.139      -0.039       0.282\n",
      "461            0.3388      0.112      3.014      0.003       0.119       0.559\n",
      "472            0.3231      0.166      1.947      0.052      -0.002       0.648\n",
      "473            0.3292      0.142      2.325      0.020       0.052       0.607\n",
      "485           -0.1938      0.120     -1.621      0.105      -0.428       0.040\n",
      "487            0.2578      0.121      2.127      0.033       0.020       0.495\n",
      "489           -0.2358      0.118     -2.001      0.045      -0.467      -0.005\n",
      "500           -0.1455      0.117     -1.244      0.213      -0.375       0.084\n",
      "511            0.1521      0.089      1.711      0.087      -0.022       0.326\n",
      "512            0.2598      0.108      2.402      0.016       0.048       0.472\n",
      "542            0.2544      0.112      2.275      0.023       0.035       0.474\n",
      "548            0.2151      0.123      1.751      0.080      -0.026       0.456\n",
      "563           -0.1516      0.116     -1.312      0.189      -0.378       0.075\n",
      "570            0.1981      0.102      1.950      0.051      -0.001       0.397\n",
      "573            0.4741      0.237      2.000      0.046       0.009       0.939\n",
      "578           -0.7766      0.286     -2.716      0.007      -1.337      -0.216\n",
      "==============================================================================\n",
      "Removing feature 73 with p-value 0.271685\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.187958\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1532\n",
      "Method:                           MLE   Df Model:                           34\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2301\n",
      "Time:                        16:35:04   Log-Likelihood:                -294.53\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.406e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.7045      0.215    -17.260      0.000      -4.125      -3.284\n",
      "2             -0.1796      0.112     -1.603      0.109      -0.399       0.040\n",
      "15            -0.2524      0.123     -2.049      0.040      -0.494      -0.011\n",
      "22             0.2000      0.091      2.204      0.028       0.022       0.378\n",
      "52             0.1935      0.129      1.502      0.133      -0.059       0.446\n",
      "60             0.9318      0.134      6.968      0.000       0.670       1.194\n",
      "63            -0.2233      0.151     -1.479      0.139      -0.519       0.073\n",
      "65             0.5827      0.120      4.868      0.000       0.348       0.817\n",
      "69            -0.2175      0.147     -1.476      0.140      -0.506       0.071\n",
      "72             0.1975      0.108      1.822      0.068      -0.015       0.410\n",
      "74            -1.4288      0.359     -3.984      0.000      -2.132      -0.726\n",
      "91            -0.2313      0.119     -1.943      0.052      -0.465       0.002\n",
      "134            0.2663      0.126      2.110      0.035       0.019       0.514\n",
      "152           -1.7526      0.878     -1.997      0.046      -3.473      -0.032\n",
      "209           -0.2497      0.114     -2.184      0.029      -0.474      -0.026\n",
      "269            0.1793      0.120      1.496      0.135      -0.056       0.414\n",
      "424            0.1602      0.121      1.324      0.185      -0.077       0.397\n",
      "429           -0.8615      0.542     -1.591      0.112      -1.923       0.200\n",
      "434            0.2172      0.111      1.958      0.050      -0.000       0.435\n",
      "439            0.1165      0.081      1.445      0.148      -0.042       0.274\n",
      "461            0.3341      0.111      2.998      0.003       0.116       0.553\n",
      "472            0.2828      0.157      1.800      0.072      -0.025       0.591\n",
      "473            0.3147      0.142      2.218      0.027       0.037       0.593\n",
      "485           -0.1977      0.119     -1.655      0.098      -0.432       0.036\n",
      "487            0.2536      0.121      2.101      0.036       0.017       0.490\n",
      "489           -0.2377      0.118     -2.017      0.044      -0.469      -0.007\n",
      "500           -0.1406      0.117     -1.204      0.228      -0.369       0.088\n",
      "511            0.1595      0.088      1.806      0.071      -0.014       0.333\n",
      "512            0.2567      0.108      2.376      0.018       0.045       0.468\n",
      "542            0.2573      0.112      2.297      0.022       0.038       0.477\n",
      "548            0.2125      0.123      1.733      0.083      -0.028       0.453\n",
      "563           -0.1530      0.116     -1.324      0.186      -0.380       0.074\n",
      "570            0.2024      0.102      1.993      0.046       0.003       0.401\n",
      "573            0.4603      0.236      1.950      0.051      -0.002       0.923\n",
      "578           -0.7600      0.284     -2.678      0.007      -1.316      -0.204\n",
      "==============================================================================\n",
      "Removing feature 500 with p-value 0.228435\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.188434\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1533\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2282\n",
      "Time:                        16:35:04   Log-Likelihood:                -295.28\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.480e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6927      0.214    -17.258      0.000      -4.112      -3.273\n",
      "2             -0.1706      0.112     -1.528      0.126      -0.389       0.048\n",
      "15            -0.2534      0.123     -2.054      0.040      -0.495      -0.012\n",
      "22             0.2039      0.091      2.245      0.025       0.026       0.382\n",
      "52             0.1896      0.129      1.475      0.140      -0.062       0.442\n",
      "60             0.9375      0.133      7.031      0.000       0.676       1.199\n",
      "63            -0.2154      0.151     -1.431      0.152      -0.510       0.080\n",
      "65             0.5857      0.119      4.913      0.000       0.352       0.819\n",
      "69            -0.2081      0.147     -1.417      0.156      -0.496       0.080\n",
      "72             0.1894      0.109      1.743      0.081      -0.024       0.402\n",
      "74            -1.4253      0.360     -3.957      0.000      -2.131      -0.719\n",
      "91            -0.2280      0.118     -1.926      0.054      -0.460       0.004\n",
      "134            0.2666      0.126      2.121      0.034       0.020       0.513\n",
      "152           -1.7397      0.875     -1.987      0.047      -3.456      -0.024\n",
      "209           -0.2478      0.114     -2.165      0.030      -0.472      -0.023\n",
      "269            0.1746      0.119      1.463      0.144      -0.059       0.409\n",
      "424            0.1641      0.120      1.364      0.173      -0.072       0.400\n",
      "429           -0.8373      0.544     -1.540      0.124      -1.903       0.229\n",
      "434            0.2231      0.111      2.017      0.044       0.006       0.440\n",
      "439            0.1090      0.080      1.363      0.173      -0.048       0.266\n",
      "461            0.3356      0.111      3.013      0.003       0.117       0.554\n",
      "472            0.2766      0.160      1.733      0.083      -0.036       0.589\n",
      "473            0.3153      0.141      2.234      0.025       0.039       0.592\n",
      "485           -0.1978      0.119     -1.656      0.098      -0.432       0.036\n",
      "487            0.2562      0.121      2.120      0.034       0.019       0.493\n",
      "489           -0.2327      0.117     -1.984      0.047      -0.463      -0.003\n",
      "511            0.1487      0.088      1.693      0.091      -0.023       0.321\n",
      "512            0.2562      0.108      2.373      0.018       0.045       0.468\n",
      "542            0.2617      0.112      2.337      0.019       0.042       0.481\n",
      "548            0.2138      0.123      1.743      0.081      -0.027       0.454\n",
      "563           -0.1431      0.115     -1.245      0.213      -0.368       0.082\n",
      "570            0.2044      0.102      2.013      0.044       0.005       0.403\n",
      "573            0.4567      0.235      1.946      0.052      -0.003       0.917\n",
      "578           -0.7679      0.284     -2.704      0.007      -1.324      -0.211\n",
      "==============================================================================\n",
      "Removing feature 563 with p-value 0.213274\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.188929\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1534\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2262\n",
      "Time:                        16:35:04   Log-Likelihood:                -296.05\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.794e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6921      0.215    -17.187      0.000      -4.113      -3.271\n",
      "2             -0.1771      0.111     -1.589      0.112      -0.396       0.041\n",
      "15            -0.2494      0.123     -2.024      0.043      -0.491      -0.008\n",
      "22             0.2032      0.090      2.251      0.024       0.026       0.380\n",
      "52             0.1850      0.128      1.440      0.150      -0.067       0.437\n",
      "60             0.9453      0.133      7.108      0.000       0.685       1.206\n",
      "63            -0.2178      0.150     -1.448      0.148      -0.513       0.077\n",
      "65             0.5857      0.120      4.886      0.000       0.351       0.821\n",
      "69            -0.2086      0.147     -1.420      0.155      -0.496       0.079\n",
      "72             0.1837      0.108      1.696      0.090      -0.029       0.396\n",
      "74            -1.4276      0.358     -3.983      0.000      -2.130      -0.725\n",
      "91            -0.2334      0.118     -1.982      0.048      -0.464      -0.003\n",
      "134            0.2694      0.126      2.146      0.032       0.023       0.515\n",
      "152           -1.6798      0.868     -1.934      0.053      -3.382       0.022\n",
      "209           -0.2526      0.114     -2.212      0.027      -0.476      -0.029\n",
      "269            0.1702      0.119      1.426      0.154      -0.064       0.404\n",
      "424            0.1705      0.120      1.424      0.154      -0.064       0.405\n",
      "429           -0.9151      0.571     -1.603      0.109      -2.034       0.204\n",
      "434            0.2148      0.110      1.951      0.051      -0.001       0.430\n",
      "439            0.1093      0.079      1.388      0.165      -0.045       0.264\n",
      "461            0.3358      0.111      3.035      0.002       0.119       0.553\n",
      "472            0.2718      0.159      1.709      0.087      -0.040       0.583\n",
      "473            0.3105      0.142      2.181      0.029       0.031       0.590\n",
      "485           -0.2004      0.119     -1.681      0.093      -0.434       0.033\n",
      "487            0.2596      0.120      2.159      0.031       0.024       0.495\n",
      "489           -0.2327      0.117     -1.993      0.046      -0.462      -0.004\n",
      "511            0.1477      0.087      1.689      0.091      -0.024       0.319\n",
      "512            0.2551      0.108      2.365      0.018       0.044       0.467\n",
      "542            0.2720      0.111      2.439      0.015       0.053       0.491\n",
      "548            0.2223      0.122      1.823      0.068      -0.017       0.461\n",
      "570            0.2114      0.100      2.105      0.035       0.015       0.408\n",
      "573            0.4538      0.234      1.940      0.052      -0.005       0.912\n",
      "578           -0.7750      0.286     -2.712      0.007      -1.335      -0.215\n",
      "==============================================================================\n",
      "Removing feature 439 with p-value 0.165016\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.189420\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1535\n",
      "Method:                           MLE   Df Model:                           31\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2241\n",
      "Time:                        16:35:04   Log-Likelihood:                -296.82\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.212e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6834      0.214    -17.217      0.000      -4.103      -3.264\n",
      "2             -0.1752      0.111     -1.579      0.114      -0.393       0.042\n",
      "15            -0.2508      0.123     -2.038      0.042      -0.492      -0.010\n",
      "22             0.2032      0.090      2.261      0.024       0.027       0.379\n",
      "52             0.1829      0.128      1.428      0.153      -0.068       0.434\n",
      "60             0.9204      0.131      7.024      0.000       0.664       1.177\n",
      "63            -0.2219      0.150     -1.475      0.140      -0.517       0.073\n",
      "65             0.5775      0.119      4.836      0.000       0.343       0.812\n",
      "69            -0.2079      0.147     -1.417      0.156      -0.495       0.080\n",
      "72             0.1887      0.108      1.743      0.081      -0.023       0.401\n",
      "74            -1.3687      0.353     -3.877      0.000      -2.061      -0.677\n",
      "91            -0.2343      0.118     -1.993      0.046      -0.465      -0.004\n",
      "134            0.2687      0.126      2.135      0.033       0.022       0.515\n",
      "152           -1.6793      0.866     -1.939      0.053      -3.377       0.018\n",
      "209           -0.2614      0.114     -2.292      0.022      -0.485      -0.038\n",
      "269            0.1750      0.119      1.471      0.141      -0.058       0.408\n",
      "424            0.1855      0.119      1.561      0.119      -0.047       0.418\n",
      "429           -0.9221      0.564     -1.634      0.102      -2.028       0.184\n",
      "434            0.2231      0.109      2.038      0.042       0.009       0.438\n",
      "461            0.3358      0.110      3.045      0.002       0.120       0.552\n",
      "472            0.2691      0.158      1.704      0.088      -0.040       0.579\n",
      "473            0.3148      0.142      2.216      0.027       0.036       0.593\n",
      "485           -0.1999      0.119     -1.683      0.092      -0.433       0.033\n",
      "487            0.2655      0.120      2.216      0.027       0.031       0.500\n",
      "489           -0.2361      0.116     -2.027      0.043      -0.464      -0.008\n",
      "511            0.1478      0.087      1.689      0.091      -0.024       0.319\n",
      "512            0.2585      0.108      2.397      0.017       0.047       0.470\n",
      "542            0.2774      0.111      2.501      0.012       0.060       0.495\n",
      "548            0.2195      0.122      1.804      0.071      -0.019       0.458\n",
      "570            0.2152      0.100      2.147      0.032       0.019       0.412\n",
      "573            0.4532      0.234      1.938      0.053      -0.005       0.912\n",
      "578           -0.7777      0.285     -2.731      0.006      -1.336      -0.219\n",
      "==============================================================================\n",
      "Removing feature 69 with p-value 0.156398\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190069\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1536\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2215\n",
      "Time:                        16:35:04   Log-Likelihood:                -297.84\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.139e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6713      0.213    -17.249      0.000      -4.088      -3.254\n",
      "2             -0.1791      0.111     -1.620      0.105      -0.396       0.038\n",
      "15            -0.2418      0.123     -1.973      0.049      -0.482      -0.002\n",
      "22             0.1992      0.090      2.218      0.027       0.023       0.375\n",
      "52             0.1734      0.128      1.354      0.176      -0.078       0.424\n",
      "60             0.9280      0.131      7.101      0.000       0.672       1.184\n",
      "63            -0.1135      0.130     -0.873      0.383      -0.368       0.141\n",
      "65             0.5664      0.120      4.739      0.000       0.332       0.801\n",
      "72             0.1638      0.105      1.557      0.120      -0.042       0.370\n",
      "74            -1.3911      0.354     -3.930      0.000      -2.085      -0.697\n",
      "91            -0.2253      0.118     -1.917      0.055      -0.456       0.005\n",
      "134            0.2760      0.125      2.205      0.027       0.031       0.521\n",
      "152           -1.6509      0.862     -1.915      0.056      -3.341       0.039\n",
      "209           -0.2682      0.113     -2.373      0.018      -0.490      -0.047\n",
      "269            0.1824      0.119      1.535      0.125      -0.051       0.415\n",
      "424            0.1923      0.119      1.618      0.106      -0.041       0.425\n",
      "429           -0.9213      0.564     -1.633      0.102      -2.027       0.184\n",
      "434            0.2233      0.109      2.042      0.041       0.009       0.438\n",
      "461            0.3395      0.110      3.082      0.002       0.124       0.555\n",
      "472            0.2366      0.152      1.561      0.119      -0.061       0.534\n",
      "473            0.2657      0.139      1.905      0.057      -0.008       0.539\n",
      "485           -0.1988      0.119     -1.674      0.094      -0.432       0.034\n",
      "487            0.2488      0.119      2.084      0.037       0.015       0.483\n",
      "489           -0.2451      0.116     -2.107      0.035      -0.473      -0.017\n",
      "511            0.1530      0.087      1.758      0.079      -0.018       0.324\n",
      "512            0.2627      0.108      2.437      0.015       0.051       0.474\n",
      "542            0.2815      0.111      2.543      0.011       0.065       0.498\n",
      "548            0.2211      0.121      1.823      0.068      -0.017       0.459\n",
      "570            0.2112      0.100      2.107      0.035       0.015       0.408\n",
      "573            0.4415      0.235      1.882      0.060      -0.018       0.901\n",
      "578           -0.7739      0.286     -2.704      0.007      -1.335      -0.213\n",
      "==============================================================================\n",
      "Removing feature 63 with p-value 0.382706\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190313\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1537\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2205\n",
      "Time:                        16:35:04   Log-Likelihood:                -298.22\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.201e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6758      0.212    -17.307      0.000      -4.092      -3.260\n",
      "2             -0.1756      0.110     -1.591      0.112      -0.392       0.041\n",
      "15            -0.2429      0.122     -1.985      0.047      -0.483      -0.003\n",
      "22             0.2046      0.090      2.270      0.023       0.028       0.381\n",
      "52             0.1636      0.127      1.283      0.199      -0.086       0.413\n",
      "60             0.9153      0.130      7.055      0.000       0.661       1.170\n",
      "65             0.5636      0.119      4.720      0.000       0.330       0.798\n",
      "72             0.1556      0.105      1.482      0.138      -0.050       0.362\n",
      "74            -1.5457      0.305     -5.062      0.000      -2.144      -0.947\n",
      "91            -0.2284      0.118     -1.938      0.053      -0.459       0.003\n",
      "134            0.2800      0.125      2.242      0.025       0.035       0.525\n",
      "152           -1.6205      0.856     -1.894      0.058      -3.297       0.056\n",
      "209           -0.2638      0.113     -2.338      0.019      -0.485      -0.043\n",
      "269            0.1733      0.118      1.464      0.143      -0.059       0.405\n",
      "424            0.1987      0.119      1.674      0.094      -0.034       0.431\n",
      "429           -0.9262      0.561     -1.650      0.099      -2.026       0.174\n",
      "434            0.2184      0.109      2.000      0.045       0.004       0.432\n",
      "461            0.3395      0.110      3.082      0.002       0.124       0.555\n",
      "472            0.2168      0.151      1.438      0.151      -0.079       0.512\n",
      "473            0.2263      0.132      1.710      0.087      -0.033       0.486\n",
      "485           -0.2031      0.119     -1.707      0.088      -0.436       0.030\n",
      "487            0.2521      0.119      2.114      0.035       0.018       0.486\n",
      "489           -0.2443      0.116     -2.106      0.035      -0.472      -0.017\n",
      "511            0.1599      0.086      1.851      0.064      -0.009       0.329\n",
      "512            0.2658      0.108      2.465      0.014       0.054       0.477\n",
      "542            0.2781      0.110      2.523      0.012       0.062       0.494\n",
      "548            0.2248      0.121      1.857      0.063      -0.012       0.462\n",
      "570            0.2156      0.100      2.153      0.031       0.019       0.412\n",
      "573            0.4439      0.234      1.900      0.057      -0.014       0.902\n",
      "578           -0.7762      0.287     -2.708      0.007      -1.338      -0.214\n",
      "==============================================================================\n",
      "Removing feature 52 with p-value 0.199332\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.190846\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1538\n",
      "Method:                           MLE   Df Model:                           28\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2183\n",
      "Time:                        16:35:04   Log-Likelihood:                -299.06\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.783e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6619      0.211    -17.341      0.000      -4.076      -3.248\n",
      "2             -0.1657      0.109     -1.514      0.130      -0.380       0.049\n",
      "15            -0.2381      0.123     -1.939      0.053      -0.479       0.003\n",
      "22             0.2072      0.090      2.300      0.021       0.031       0.384\n",
      "60             0.9146      0.130      7.057      0.000       0.661       1.169\n",
      "65             0.5503      0.118      4.645      0.000       0.318       0.782\n",
      "72             0.1615      0.105      1.536      0.125      -0.045       0.368\n",
      "74            -1.5345      0.304     -5.042      0.000      -2.131      -0.938\n",
      "91            -0.2245      0.118     -1.908      0.056      -0.455       0.006\n",
      "134            0.2871      0.125      2.288      0.022       0.041       0.533\n",
      "152           -1.6024      0.856     -1.872      0.061      -3.280       0.076\n",
      "209           -0.2598      0.113     -2.301      0.021      -0.481      -0.039\n",
      "269            0.1757      0.118      1.488      0.137      -0.056       0.407\n",
      "424            0.2033      0.118      1.718      0.086      -0.029       0.435\n",
      "429           -0.9264      0.561     -1.652      0.099      -2.026       0.173\n",
      "434            0.2099      0.109      1.930      0.054      -0.003       0.423\n",
      "461            0.2811      0.101      2.787      0.005       0.083       0.479\n",
      "472            0.2098      0.152      1.379      0.168      -0.088       0.508\n",
      "473            0.1834      0.128      1.433      0.152      -0.067       0.434\n",
      "485           -0.2097      0.119     -1.759      0.079      -0.443       0.024\n",
      "487            0.2413      0.119      2.030      0.042       0.008       0.474\n",
      "489           -0.2483      0.116     -2.145      0.032      -0.475      -0.021\n",
      "511            0.1539      0.086      1.785      0.074      -0.015       0.323\n",
      "512            0.2706      0.108      2.509      0.012       0.059       0.482\n",
      "542            0.2842      0.111      2.568      0.010       0.067       0.501\n",
      "548            0.2267      0.121      1.871      0.061      -0.011       0.464\n",
      "570            0.2172      0.101      2.160      0.031       0.020       0.414\n",
      "573            0.4059      0.233      1.744      0.081      -0.050       0.862\n",
      "578           -0.7532      0.286     -2.634      0.008      -1.314      -0.193\n",
      "==============================================================================\n",
      "Removing feature 472 with p-value 0.167846\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.191527\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1539\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2155\n",
      "Time:                        16:35:04   Log-Likelihood:                -300.12\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.596e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6553      0.211    -17.326      0.000      -4.069      -3.242\n",
      "2             -0.1682      0.109     -1.541      0.123      -0.382       0.046\n",
      "15            -0.2375      0.122     -1.942      0.052      -0.477       0.002\n",
      "22             0.2033      0.090      2.267      0.023       0.028       0.379\n",
      "60             0.9323      0.128      7.284      0.000       0.681       1.183\n",
      "65             0.5654      0.117      4.812      0.000       0.335       0.796\n",
      "72             0.1579      0.105      1.511      0.131      -0.047       0.363\n",
      "74            -1.3649      0.278     -4.905      0.000      -1.910      -0.820\n",
      "91            -0.2235      0.117     -1.905      0.057      -0.454       0.006\n",
      "134            0.2730      0.125      2.185      0.029       0.028       0.518\n",
      "152           -1.6331      0.858     -1.904      0.057      -3.314       0.048\n",
      "209           -0.2686      0.113     -2.385      0.017      -0.489      -0.048\n",
      "269            0.1905      0.118      1.620      0.105      -0.040       0.421\n",
      "424            0.1942      0.118      1.649      0.099      -0.037       0.425\n",
      "429           -0.9877      0.577     -1.711      0.087      -2.119       0.143\n",
      "434            0.2137      0.108      1.985      0.047       0.003       0.425\n",
      "461            0.2704      0.100      2.691      0.007       0.073       0.467\n",
      "473            0.1858      0.127      1.458      0.145      -0.064       0.436\n",
      "485           -0.2161      0.119     -1.822      0.068      -0.449       0.016\n",
      "487            0.2535      0.118      2.151      0.032       0.022       0.485\n",
      "489           -0.2471      0.116     -2.131      0.033      -0.474      -0.020\n",
      "511            0.1418      0.086      1.647      0.100      -0.027       0.310\n",
      "512            0.2788      0.107      2.594      0.009       0.068       0.489\n",
      "542            0.2972      0.110      2.690      0.007       0.081       0.514\n",
      "548            0.2230      0.121      1.844      0.065      -0.014       0.460\n",
      "570            0.2170      0.101      2.148      0.032       0.019       0.415\n",
      "573            0.3848      0.232      1.657      0.098      -0.070       0.840\n",
      "578           -0.7257      0.285     -2.549      0.011      -1.284      -0.168\n",
      "==============================================================================\n",
      "Removing feature 473 with p-value 0.144896\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192199\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1540\n",
      "Method:                           MLE   Df Model:                           26\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2128\n",
      "Time:                        16:35:04   Log-Likelihood:                -301.18\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.230e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6571      0.212    -17.282      0.000      -4.072      -3.242\n",
      "2             -0.1717      0.109     -1.570      0.116      -0.386       0.043\n",
      "15            -0.2497      0.122     -2.043      0.041      -0.489      -0.010\n",
      "22             0.2033      0.089      2.294      0.022       0.030       0.377\n",
      "60             0.9202      0.127      7.257      0.000       0.672       1.169\n",
      "65             0.4897      0.103      4.748      0.000       0.288       0.692\n",
      "72             0.1650      0.105      1.573      0.116      -0.041       0.371\n",
      "74            -1.3646      0.274     -4.974      0.000      -1.902      -0.827\n",
      "91            -0.2350      0.117     -2.013      0.044      -0.464      -0.006\n",
      "134            0.2846      0.125      2.283      0.022       0.040       0.529\n",
      "152           -1.6413      0.864     -1.900      0.057      -3.334       0.051\n",
      "209           -0.2545      0.112     -2.273      0.023      -0.474      -0.035\n",
      "269            0.2015      0.117      1.723      0.085      -0.028       0.431\n",
      "424            0.1706      0.116      1.470      0.142      -0.057       0.398\n",
      "429           -1.0058      0.578     -1.739      0.082      -2.140       0.128\n",
      "434            0.2194      0.107      2.041      0.041       0.009       0.430\n",
      "461            0.2734      0.100      2.725      0.006       0.077       0.470\n",
      "485           -0.2158      0.119     -1.820      0.069      -0.448       0.017\n",
      "487            0.2545      0.117      2.171      0.030       0.025       0.484\n",
      "489           -0.2610      0.116     -2.259      0.024      -0.488      -0.035\n",
      "511            0.1444      0.086      1.683      0.092      -0.024       0.313\n",
      "512            0.2776      0.107      2.592      0.010       0.068       0.488\n",
      "542            0.2988      0.111      2.703      0.007       0.082       0.515\n",
      "548            0.2179      0.120      1.810      0.070      -0.018       0.454\n",
      "570            0.2249      0.101      2.236      0.025       0.028       0.422\n",
      "573            0.3622      0.235      1.543      0.123      -0.098       0.822\n",
      "578           -0.7034      0.284     -2.479      0.013      -1.259      -0.147\n",
      "==============================================================================\n",
      "Removing feature 424 with p-value 0.141649\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192866\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1541\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2100\n",
      "Time:                        16:35:04   Log-Likelihood:                -302.22\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.726e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6444      0.211    -17.288      0.000      -4.058      -3.231\n",
      "2             -0.1624      0.109     -1.487      0.137      -0.377       0.052\n",
      "15            -0.2930      0.120     -2.433      0.015      -0.529      -0.057\n",
      "22             0.2030      0.088      2.300      0.021       0.030       0.376\n",
      "60             0.9336      0.127      7.373      0.000       0.685       1.182\n",
      "65             0.4901      0.103      4.751      0.000       0.288       0.692\n",
      "72             0.1784      0.104      1.719      0.086      -0.025       0.382\n",
      "74            -1.3946      0.274     -5.081      0.000      -1.933      -0.857\n",
      "91            -0.2224      0.116     -1.915      0.056      -0.450       0.005\n",
      "134            0.2756      0.124      2.219      0.027       0.032       0.519\n",
      "152           -1.6719      0.864     -1.934      0.053      -3.366       0.022\n",
      "209           -0.2513      0.111     -2.255      0.024      -0.470      -0.033\n",
      "269            0.1961      0.117      1.683      0.092      -0.032       0.424\n",
      "429           -0.9852      0.587     -1.677      0.093      -2.136       0.166\n",
      "434            0.2103      0.107      1.969      0.049       0.001       0.420\n",
      "461            0.2514      0.099      2.535      0.011       0.057       0.446\n",
      "485           -0.2166      0.118     -1.828      0.068      -0.449       0.016\n",
      "487            0.2447      0.117      2.097      0.036       0.016       0.473\n",
      "489           -0.2607      0.116     -2.249      0.024      -0.488      -0.034\n",
      "511            0.1497      0.085      1.755      0.079      -0.017       0.317\n",
      "512            0.2657      0.107      2.493      0.013       0.057       0.475\n",
      "542            0.2845      0.110      2.580      0.010       0.068       0.501\n",
      "548            0.2186      0.120      1.821      0.069      -0.017       0.454\n",
      "570            0.2235      0.100      2.228      0.026       0.027       0.420\n",
      "573            0.3639      0.235      1.548      0.122      -0.097       0.825\n",
      "578           -0.6966      0.284     -2.449      0.014      -1.254      -0.139\n",
      "==============================================================================\n",
      "Removing feature 2 with p-value 0.137140\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193572\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1542\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2071\n",
      "Time:                        16:35:04   Log-Likelihood:                -303.33\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.625e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6295      0.210    -17.296      0.000      -4.041      -3.218\n",
      "15            -0.2871      0.120     -2.395      0.017      -0.522      -0.052\n",
      "22             0.1959      0.088      2.234      0.025       0.024       0.368\n",
      "60             0.9236      0.126      7.311      0.000       0.676       1.171\n",
      "65             0.4808      0.102      4.693      0.000       0.280       0.682\n",
      "72             0.1815      0.104      1.749      0.080      -0.022       0.385\n",
      "74            -1.3843      0.277     -4.991      0.000      -1.928      -0.841\n",
      "91            -0.2216      0.116     -1.914      0.056      -0.448       0.005\n",
      "134            0.2821      0.124      2.278      0.023       0.039       0.525\n",
      "152           -1.6564      0.865     -1.916      0.055      -3.351       0.038\n",
      "209           -0.2434      0.111     -2.190      0.028      -0.461      -0.026\n",
      "269            0.1851      0.116      1.594      0.111      -0.042       0.413\n",
      "429           -0.9733      0.588     -1.654      0.098      -2.127       0.180\n",
      "434            0.2113      0.107      1.979      0.048       0.002       0.420\n",
      "461            0.2410      0.098      2.449      0.014       0.048       0.434\n",
      "485           -0.2090      0.118     -1.767      0.077      -0.441       0.023\n",
      "487            0.2487      0.116      2.136      0.033       0.021       0.477\n",
      "489           -0.2583      0.115     -2.240      0.025      -0.484      -0.032\n",
      "511            0.1494      0.085      1.760      0.078      -0.017       0.316\n",
      "512            0.2513      0.106      2.372      0.018       0.044       0.459\n",
      "542            0.2842      0.110      2.584      0.010       0.069       0.500\n",
      "548            0.2054      0.119      1.724      0.085      -0.028       0.439\n",
      "570            0.2214      0.100      2.217      0.027       0.026       0.417\n",
      "573            0.3585      0.234      1.534      0.125      -0.100       0.817\n",
      "578           -0.6851      0.283     -2.422      0.015      -1.239      -0.131\n",
      "==============================================================================\n",
      "Removing feature 573 with p-value 0.125028\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194230\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1543\n",
      "Method:                           MLE   Df Model:                           23\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2044\n",
      "Time:                        16:35:04   Log-Likelihood:                -304.36\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.902e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6125      0.208    -17.341      0.000      -4.021      -3.204\n",
      "15            -0.2899      0.120     -2.422      0.015      -0.525      -0.055\n",
      "22             0.1919      0.087      2.197      0.028       0.021       0.363\n",
      "60             0.9141      0.126      7.275      0.000       0.668       1.160\n",
      "65             0.4655      0.102      4.579      0.000       0.266       0.665\n",
      "72             0.1805      0.103      1.748      0.081      -0.022       0.383\n",
      "74            -1.3641      0.275     -4.955      0.000      -1.904      -0.824\n",
      "91            -0.2184      0.116     -1.889      0.059      -0.445       0.008\n",
      "134            0.2766      0.123      2.240      0.025       0.035       0.519\n",
      "152           -1.6136      0.861     -1.873      0.061      -3.302       0.075\n",
      "209           -0.2391      0.111     -2.158      0.031      -0.456      -0.022\n",
      "269            0.1817      0.116      1.569      0.117      -0.045       0.409\n",
      "429           -0.9243      0.565     -1.636      0.102      -2.032       0.183\n",
      "434            0.2101      0.106      1.977      0.048       0.002       0.418\n",
      "461            0.2393      0.099      2.421      0.015       0.046       0.433\n",
      "485           -0.2053      0.118     -1.738      0.082      -0.437       0.026\n",
      "487            0.2471      0.116      2.127      0.033       0.019       0.475\n",
      "489           -0.2632      0.115     -2.288      0.022      -0.489      -0.038\n",
      "511            0.1586      0.084      1.883      0.060      -0.006       0.324\n",
      "512            0.2515      0.106      2.375      0.018       0.044       0.459\n",
      "542            0.2784      0.110      2.538      0.011       0.063       0.493\n",
      "548            0.1989      0.119      1.675      0.094      -0.034       0.432\n",
      "570            0.2168      0.099      2.180      0.029       0.022       0.412\n",
      "578           -0.4255      0.235     -1.811      0.070      -0.886       0.035\n",
      "==============================================================================\n",
      "Removing feature 269 with p-value 0.116632\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.195007\n",
      "         Iterations 10\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1544\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2013\n",
      "Time:                        16:35:04   Log-Likelihood:                -305.58\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.423e-22\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.6133      0.209    -17.285      0.000      -4.023      -3.204\n",
      "15            -0.2912      0.120     -2.436      0.015      -0.526      -0.057\n",
      "22             0.2100      0.086      2.438      0.015       0.041       0.379\n",
      "60             0.8872      0.123      7.203      0.000       0.646       1.129\n",
      "65             0.4355      0.099      4.408      0.000       0.242       0.629\n",
      "72             0.1778      0.103      1.719      0.086      -0.025       0.381\n",
      "74            -1.3031      0.276     -4.729      0.000      -1.843      -0.763\n",
      "91            -0.2282      0.115     -1.977      0.048      -0.455      -0.002\n",
      "134            0.2675      0.122      2.187      0.029       0.028       0.507\n",
      "152           -1.7330      0.865     -2.004      0.045      -3.428      -0.038\n",
      "209           -0.2293      0.111     -2.071      0.038      -0.446      -0.012\n",
      "429           -0.9326      0.561     -1.664      0.096      -2.031       0.166\n",
      "434            0.2126      0.106      2.008      0.045       0.005       0.420\n",
      "461            0.2256      0.098      2.295      0.022       0.033       0.418\n",
      "485           -0.2125      0.118     -1.803      0.071      -0.443       0.018\n",
      "487            0.2426      0.116      2.090      0.037       0.015       0.470\n",
      "489           -0.2596      0.115     -2.265      0.024      -0.484      -0.035\n",
      "511            0.1606      0.085      1.893      0.058      -0.006       0.327\n",
      "512            0.2589      0.105      2.462      0.014       0.053       0.465\n",
      "542            0.2683      0.109      2.467      0.014       0.055       0.482\n",
      "548            0.2044      0.118      1.728      0.084      -0.027       0.436\n",
      "570            0.2151      0.099      2.179      0.029       0.022       0.409\n",
      "578           -0.4235      0.236     -1.795      0.073      -0.886       0.039\n",
      "==============================================================================\n",
      "Removing feature 429 with p-value 0.096144\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196864\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1545\n",
      "Method:                           MLE   Df Model:                           21\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1937\n",
      "Time:                        16:35:04   Log-Likelihood:                -308.49\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.901e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.5037      0.191    -18.318      0.000      -3.879      -3.129\n",
      "15            -0.2969      0.119     -2.502      0.012      -0.530      -0.064\n",
      "22             0.2180      0.086      2.544      0.011       0.050       0.386\n",
      "60             0.8527      0.120      7.083      0.000       0.617       1.089\n",
      "65             0.4162      0.097      4.276      0.000       0.225       0.607\n",
      "72             0.1753      0.103      1.703      0.089      -0.026       0.377\n",
      "74            -1.2334      0.273     -4.510      0.000      -1.769      -0.697\n",
      "91            -0.2189      0.115     -1.909      0.056      -0.444       0.006\n",
      "134            0.2860      0.122      2.346      0.019       0.047       0.525\n",
      "152           -1.6345      0.851     -1.922      0.055      -3.302       0.033\n",
      "209           -0.2275      0.110     -2.062      0.039      -0.444      -0.011\n",
      "434            0.2187      0.106      2.064      0.039       0.011       0.426\n",
      "461            0.2143      0.099      2.166      0.030       0.020       0.408\n",
      "485           -0.2197      0.116     -1.889      0.059      -0.448       0.008\n",
      "487            0.2484      0.116      2.136      0.033       0.020       0.476\n",
      "489           -0.2455      0.114     -2.145      0.032      -0.470      -0.021\n",
      "511            0.1514      0.085      1.792      0.073      -0.014       0.317\n",
      "512            0.2462      0.105      2.348      0.019       0.041       0.452\n",
      "542            0.2765      0.109      2.545      0.011       0.064       0.489\n",
      "548            0.2050      0.118      1.738      0.082      -0.026       0.436\n",
      "570            0.1701      0.098      1.742      0.081      -0.021       0.361\n",
      "578           -0.4235      0.236     -1.797      0.072      -0.886       0.038\n",
      "==============================================================================\n",
      "Removing feature 72 with p-value 0.088634\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197744\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1546\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1900\n",
      "Time:                        16:35:04   Log-Likelihood:                -309.86\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.724e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4726      0.188    -18.452      0.000      -3.841      -3.104\n",
      "15            -0.2927      0.118     -2.474      0.013      -0.525      -0.061\n",
      "22             0.2259      0.085      2.647      0.008       0.059       0.393\n",
      "60             0.8103      0.116      6.985      0.000       0.583       1.038\n",
      "65             0.4043      0.096      4.200      0.000       0.216       0.593\n",
      "74            -1.1789      0.269     -4.386      0.000      -1.706      -0.652\n",
      "91            -0.2161      0.114     -1.888      0.059      -0.440       0.008\n",
      "134            0.2821      0.122      2.309      0.021       0.043       0.522\n",
      "152           -1.5517      0.843     -1.842      0.066      -3.203       0.100\n",
      "209           -0.2136      0.109     -1.959      0.050      -0.427       0.000\n",
      "434            0.2120      0.106      1.999      0.046       0.004       0.420\n",
      "461            0.2104      0.099      2.128      0.033       0.017       0.404\n",
      "485           -0.2094      0.116     -1.801      0.072      -0.437       0.018\n",
      "487            0.2301      0.116      1.987      0.047       0.003       0.457\n",
      "489           -0.2380      0.115     -2.078      0.038      -0.462      -0.014\n",
      "511            0.1430      0.084      1.697      0.090      -0.022       0.308\n",
      "512            0.2463      0.105      2.350      0.019       0.041       0.452\n",
      "542            0.2748      0.109      2.512      0.012       0.060       0.489\n",
      "548            0.1976      0.118      1.670      0.095      -0.034       0.430\n",
      "570            0.1708      0.097      1.753      0.080      -0.020       0.362\n",
      "578           -0.4454      0.238     -1.869      0.062      -0.912       0.022\n",
      "==============================================================================\n",
      "Removing feature 548 with p-value 0.094941\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198648\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1547\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1863\n",
      "Time:                        16:35:05   Log-Likelihood:                -311.28\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.847e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4497      0.185    -18.610      0.000      -3.813      -3.086\n",
      "15            -0.2921      0.119     -2.464      0.014      -0.524      -0.060\n",
      "22             0.2304      0.086      2.676      0.007       0.062       0.399\n",
      "60             0.8234      0.116      7.109      0.000       0.596       1.050\n",
      "65             0.4090      0.097      4.237      0.000       0.220       0.598\n",
      "74            -1.1949      0.263     -4.537      0.000      -1.711      -0.679\n",
      "91            -0.2244      0.115     -1.949      0.051      -0.450       0.001\n",
      "134            0.2681      0.121      2.218      0.027       0.031       0.505\n",
      "152           -1.5148      0.830     -1.824      0.068      -3.142       0.113\n",
      "209           -0.1973      0.108     -1.820      0.069      -0.410       0.015\n",
      "434            0.2023      0.106      1.908      0.056      -0.005       0.410\n",
      "461            0.2121      0.099      2.138      0.033       0.018       0.407\n",
      "485           -0.1990      0.115     -1.725      0.084      -0.425       0.027\n",
      "487            0.2328      0.115      2.022      0.043       0.007       0.458\n",
      "489           -0.2366      0.114     -2.072      0.038      -0.460      -0.013\n",
      "511            0.1434      0.084      1.698      0.090      -0.022       0.309\n",
      "512            0.2424      0.104      2.324      0.020       0.038       0.447\n",
      "542            0.2786      0.109      2.562      0.010       0.065       0.492\n",
      "570            0.1687      0.097      1.738      0.082      -0.022       0.359\n",
      "578           -0.4436      0.237     -1.872      0.061      -0.908       0.021\n",
      "==============================================================================\n",
      "Removing feature 511 with p-value 0.089506\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199492\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1548\n",
      "Method:                           MLE   Df Model:                           18\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1829\n",
      "Time:                        16:35:05   Log-Likelihood:                -312.60\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.582e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4440      0.185    -18.637      0.000      -3.806      -3.082\n",
      "15            -0.2973      0.119     -2.493      0.013      -0.531      -0.064\n",
      "22             0.2477      0.085      2.903      0.004       0.080       0.415\n",
      "60             0.8755      0.111      7.866      0.000       0.657       1.094\n",
      "65             0.4366      0.095      4.582      0.000       0.250       0.623\n",
      "74            -1.2618      0.255     -4.949      0.000      -1.762      -0.762\n",
      "91            -0.2137      0.115     -1.857      0.063      -0.439       0.012\n",
      "134            0.2682      0.121      2.223      0.026       0.032       0.505\n",
      "152           -1.4925      0.827     -1.804      0.071      -3.114       0.129\n",
      "209           -0.2034      0.109     -1.874      0.061      -0.416       0.009\n",
      "434            0.1950      0.106      1.841      0.066      -0.013       0.403\n",
      "461            0.2120      0.099      2.143      0.032       0.018       0.406\n",
      "485           -0.1884      0.115     -1.639      0.101      -0.414       0.037\n",
      "487            0.2366      0.115      2.062      0.039       0.012       0.462\n",
      "489           -0.2305      0.114     -2.025      0.043      -0.454      -0.007\n",
      "512            0.2381      0.104      2.294      0.022       0.035       0.442\n",
      "542            0.2829      0.108      2.613      0.009       0.071       0.495\n",
      "570            0.1704      0.097      1.757      0.079      -0.020       0.361\n",
      "578           -0.4524      0.237     -1.910      0.056      -0.917       0.012\n",
      "==============================================================================\n",
      "Removing feature 485 with p-value 0.101236\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.200413\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1549\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1791\n",
      "Time:                        16:35:05   Log-Likelihood:                -314.05\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.144e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4264      0.183    -18.721      0.000      -3.785      -3.068\n",
      "15            -0.3017      0.118     -2.547      0.011      -0.534      -0.070\n",
      "22             0.2580      0.084      3.058      0.002       0.093       0.423\n",
      "60             0.8641      0.111      7.785      0.000       0.647       1.082\n",
      "65             0.4327      0.095      4.563      0.000       0.247       0.619\n",
      "74            -1.2630      0.259     -4.869      0.000      -1.771      -0.755\n",
      "91            -0.2220      0.115     -1.933      0.053      -0.447       0.003\n",
      "134            0.2690      0.121      2.226      0.026       0.032       0.506\n",
      "152           -1.5026      0.826     -1.819      0.069      -3.121       0.116\n",
      "209           -0.2064      0.109     -1.895      0.058      -0.420       0.007\n",
      "434            0.1962      0.106      1.852      0.064      -0.011       0.404\n",
      "461            0.2001      0.099      2.030      0.042       0.007       0.393\n",
      "487            0.2348      0.115      2.051      0.040       0.010       0.459\n",
      "489           -0.2262      0.114     -1.990      0.047      -0.449      -0.003\n",
      "512            0.2382      0.104      2.296      0.022       0.035       0.442\n",
      "542            0.2792      0.109      2.563      0.010       0.066       0.493\n",
      "570            0.1799      0.096      1.868      0.062      -0.009       0.369\n",
      "578           -0.4489      0.233     -1.923      0.054      -0.906       0.009\n",
      "==============================================================================\n",
      "Removing feature 152 with p-value 0.068837\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202123\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1550\n",
      "Method:                           MLE   Df Model:                           16\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1721\n",
      "Time:                        16:35:05   Log-Likelihood:                -316.73\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.012e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2904      0.159    -20.725      0.000      -3.602      -2.979\n",
      "15            -0.2078      0.108     -1.925      0.054      -0.419       0.004\n",
      "22             0.2621      0.084      3.124      0.002       0.098       0.427\n",
      "60             0.8575      0.110      7.794      0.000       0.642       1.073\n",
      "65             0.4169      0.094      4.457      0.000       0.234       0.600\n",
      "74            -1.2367      0.261     -4.739      0.000      -1.748      -0.725\n",
      "91            -0.2035      0.114     -1.790      0.073      -0.426       0.019\n",
      "134            0.2800      0.120      2.332      0.020       0.045       0.515\n",
      "209           -0.2078      0.109     -1.901      0.057      -0.422       0.006\n",
      "434            0.1953      0.106      1.839      0.066      -0.013       0.403\n",
      "461            0.1912      0.098      1.948      0.051      -0.001       0.384\n",
      "487            0.2397      0.114      2.101      0.036       0.016       0.463\n",
      "489           -0.2211      0.114     -1.937      0.053      -0.445       0.003\n",
      "512            0.2325      0.104      2.243      0.025       0.029       0.436\n",
      "542            0.2609      0.109      2.392      0.017       0.047       0.475\n",
      "570            0.1757      0.096      1.839      0.066      -0.012       0.363\n",
      "578           -0.4493      0.233     -1.925      0.054      -0.907       0.008\n",
      "==============================================================================\n",
      "Removing feature 91 with p-value 0.073468\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.203167\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1551\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1678\n",
      "Time:                        16:35:05   Log-Likelihood:                -318.36\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.330e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2743      0.158    -20.769      0.000      -3.583      -2.965\n",
      "15            -0.2046      0.107     -1.918      0.055      -0.414       0.004\n",
      "22             0.2635      0.084      3.154      0.002       0.100       0.427\n",
      "60             0.8703      0.110      7.893      0.000       0.654       1.086\n",
      "65             0.4078      0.094      4.359      0.000       0.224       0.591\n",
      "74            -1.2538      0.261     -4.808      0.000      -1.765      -0.743\n",
      "134            0.2785      0.120      2.326      0.020       0.044       0.513\n",
      "209           -0.1886      0.108     -1.745      0.081      -0.401       0.023\n",
      "434            0.2105      0.106      1.989      0.047       0.003       0.418\n",
      "461            0.1849      0.098      1.886      0.059      -0.007       0.377\n",
      "487            0.2299      0.113      2.027      0.043       0.008       0.452\n",
      "489           -0.2253      0.114     -1.969      0.049      -0.450      -0.001\n",
      "512            0.2335      0.103      2.260      0.024       0.031       0.436\n",
      "542            0.2632      0.109      2.421      0.015       0.050       0.476\n",
      "570            0.1815      0.095      1.920      0.055      -0.004       0.367\n",
      "578           -0.4576      0.235     -1.951      0.051      -0.917       0.002\n",
      "==============================================================================\n",
      "Removing feature 209 with p-value 0.081041\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.204147\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1552\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1638\n",
      "Time:                        16:35:05   Log-Likelihood:                -319.90\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.607e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2513      0.155    -20.940      0.000      -3.556      -2.947\n",
      "15            -0.2200      0.107     -2.066      0.039      -0.429      -0.011\n",
      "22             0.2674      0.084      3.194      0.001       0.103       0.431\n",
      "60             0.8439      0.109      7.761      0.000       0.631       1.057\n",
      "65             0.3888      0.093      4.191      0.000       0.207       0.571\n",
      "74            -1.2520      0.256     -4.891      0.000      -1.754      -0.750\n",
      "134            0.2707      0.119      2.267      0.023       0.037       0.505\n",
      "434            0.2129      0.106      2.013      0.044       0.006       0.420\n",
      "461            0.1924      0.098      1.972      0.049       0.001       0.384\n",
      "487            0.2286      0.113      2.018      0.044       0.007       0.451\n",
      "489           -0.2216      0.114     -1.936      0.053      -0.446       0.003\n",
      "512            0.2226      0.103      2.167      0.030       0.021       0.424\n",
      "542            0.2588      0.109      2.384      0.017       0.046       0.472\n",
      "570            0.1790      0.095      1.890      0.059      -0.007       0.365\n",
      "578           -0.4425      0.231     -1.914      0.056      -0.895       0.011\n",
      "==============================================================================\n",
      "Removing feature 570 with p-value 0.058787\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.205198\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1553\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1595\n",
      "Time:                        16:35:05   Log-Likelihood:                -321.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.904e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2352      0.154    -21.005      0.000      -3.537      -2.933\n",
      "15            -0.2209      0.106     -2.092      0.036      -0.428      -0.014\n",
      "22             0.2625      0.084      3.139      0.002       0.099       0.426\n",
      "60             0.8476      0.109      7.783      0.000       0.634       1.061\n",
      "65             0.3963      0.093      4.270      0.000       0.214       0.578\n",
      "74            -1.2659      0.256     -4.942      0.000      -1.768      -0.764\n",
      "134            0.2767      0.119      2.322      0.020       0.043       0.510\n",
      "434            0.2171      0.105      2.060      0.039       0.011       0.424\n",
      "461            0.1980      0.098      2.027      0.043       0.007       0.390\n",
      "487            0.2325      0.113      2.058      0.040       0.011       0.454\n",
      "489           -0.2209      0.114     -1.934      0.053      -0.445       0.003\n",
      "512            0.2147      0.103      2.092      0.036       0.014       0.416\n",
      "542            0.2674      0.108      2.476      0.013       0.056       0.479\n",
      "578           -0.4404      0.229     -1.927      0.054      -0.888       0.008\n",
      "==============================================================================\n",
      "Removing feature 578 with p-value 0.054012\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.207057\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1554\n",
      "Method:                           MLE   Df Model:                           12\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1519\n",
      "Time:                        16:35:05   Log-Likelihood:                -324.46\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.481e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.1740      0.145    -21.904      0.000      -3.458      -2.890\n",
      "15            -0.2283      0.105     -2.172      0.030      -0.434      -0.022\n",
      "22             0.2553      0.084      3.056      0.002       0.092       0.419\n",
      "60             0.8512      0.109      7.843      0.000       0.639       1.064\n",
      "65             0.3849      0.093      4.124      0.000       0.202       0.568\n",
      "74            -1.2638      0.253     -5.002      0.000      -1.759      -0.769\n",
      "134            0.2688      0.119      2.264      0.024       0.036       0.502\n",
      "434            0.2115      0.105      2.014      0.044       0.006       0.417\n",
      "461            0.1914      0.097      1.978      0.048       0.002       0.381\n",
      "487            0.2182      0.112      1.948      0.051      -0.001       0.438\n",
      "489           -0.2259      0.115     -1.970      0.049      -0.451      -0.001\n",
      "512            0.2217      0.103      2.161      0.031       0.021       0.423\n",
      "542            0.2423      0.107      2.270      0.023       0.033       0.451\n",
      "==============================================================================\n",
      "Removing feature 487 with p-value 0.051468\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.208244\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1555\n",
      "Method:                           MLE   Df Model:                           11\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1470\n",
      "Time:                        16:35:05   Log-Likelihood:                -326.32\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.776e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.1459      0.142    -22.138      0.000      -3.424      -2.867\n",
      "15            -0.2227      0.104     -2.136      0.033      -0.427      -0.018\n",
      "22             0.2551      0.083      3.064      0.002       0.092       0.418\n",
      "60             0.8234      0.107      7.721      0.000       0.614       1.032\n",
      "65             0.3804      0.093      4.084      0.000       0.198       0.563\n",
      "74            -1.1940      0.250     -4.776      0.000      -1.684      -0.704\n",
      "134            0.2614      0.118      2.211      0.027       0.030       0.493\n",
      "434            0.2105      0.105      2.005      0.045       0.005       0.416\n",
      "461            0.1792      0.097      1.846      0.065      -0.011       0.370\n",
      "489           -0.2276      0.114     -1.995      0.046      -0.451      -0.004\n",
      "512            0.2291      0.102      2.238      0.025       0.028       0.430\n",
      "542            0.2450      0.106      2.304      0.021       0.037       0.454\n",
      "==============================================================================\n",
      "Removing feature 461 with p-value 0.064877\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.209274\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1556\n",
      "Method:                           MLE   Df Model:                           10\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1428\n",
      "Time:                        16:35:05   Log-Likelihood:                -327.93\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.452e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.1263      0.140    -22.275      0.000      -3.401      -2.851\n",
      "15            -0.2222      0.103     -2.149      0.032      -0.425      -0.019\n",
      "22             0.2388      0.082      2.896      0.004       0.077       0.400\n",
      "60             0.8372      0.107      7.856      0.000       0.628       1.046\n",
      "65             0.3899      0.093      4.188      0.000       0.207       0.572\n",
      "74            -1.2123      0.247     -4.917      0.000      -1.696      -0.729\n",
      "134            0.2725      0.118      2.310      0.021       0.041       0.504\n",
      "434            0.2076      0.105      1.974      0.048       0.002       0.414\n",
      "489           -0.2199      0.113     -1.946      0.052      -0.441       0.002\n",
      "512            0.2212      0.102      2.161      0.031       0.021       0.422\n",
      "542            0.2485      0.106      2.341      0.019       0.040       0.457\n",
      "==============================================================================\n",
      "Removing feature 489 with p-value 0.051694\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.210518\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1557\n",
      "Method:                           MLE   Df Model:                            9\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1377\n",
      "Time:                        16:35:05   Log-Likelihood:                -329.88\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.277e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.1056      0.138    -22.424      0.000      -3.377      -2.834\n",
      "15            -0.2253      0.103     -2.183      0.029      -0.428      -0.023\n",
      "22             0.2336      0.082      2.865      0.004       0.074       0.393\n",
      "60             0.8288      0.106      7.826      0.000       0.621       1.036\n",
      "65             0.4034      0.092      4.376      0.000       0.223       0.584\n",
      "74            -1.2193      0.247     -4.936      0.000      -1.703      -0.735\n",
      "134            0.2643      0.118      2.244      0.025       0.033       0.495\n",
      "434            0.2060      0.105      1.963      0.050       0.000       0.412\n",
      "512            0.2192      0.102      2.146      0.032       0.019       0.420\n",
      "542            0.2333      0.106      2.207      0.027       0.026       0.440\n",
      "==============================================================================\n",
      "All remaining features are significant (p < 0.05). Stopping.\n",
      "Final significant features (Lasso-based, VIF-filtered, iteratively reduced): ['15', '22', '60', '65', '74', '134', '434', '512', '542']\n"
     ]
    }
   ],
   "source": [
    "# Use X_lasso_scaled_reduced directly (already scaled, 79 features, with intercept)\n",
    "X_current = X_lasso_scaled_reduced.copy()\n",
    "features_to_keep = list(X_lasso_scaled_reduced.columns)\n",
    "features_to_keep.remove('const')  # Exclude the intercept from the feature list\n",
    "\n",
    "while True:\n",
    "    # Fit the model\n",
    "    logit_model = sm.Logit(Y, X_current)\n",
    "    result = logit_model.fit(method='newton', maxiter=1000)\n",
    "    print(result.summary())\n",
    "\n",
    "    # Get p-values\n",
    "    p_values = result.pvalues\n",
    "    p_values = p_values.drop('const')\n",
    "\n",
    "    # Check if all remaining features are significant (p < 0.05)\n",
    "    if (p_values >= 0.05).sum() == 0:\n",
    "        print(\"All remaining features are significant (p < 0.05). Stopping.\")\n",
    "        break\n",
    "\n",
    "    # Find feature with highest p-value\n",
    "    max_p_feature = p_values.idxmax()\n",
    "    max_p_value = p_values[max_p_feature]\n",
    "    print(f\"Removing feature {max_p_feature} with p-value {max_p_value:.6f}\")\n",
    "\n",
    "    # Remove the feature\n",
    "    X_current = X_current.drop(columns=max_p_feature)\n",
    "    features_to_keep.remove(max_p_feature)\n",
    "\n",
    "print(f\"Final significant features (Lasso-based, VIF-filtered, iteratively reduced): {features_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "949f66f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with significant features (Lasso-based, VIF-filtered, iteratively reduced): 0.223325\n",
      "\n",
      "Logistic Regression Coefficients (significant features):\n",
      "         15       22        60        65        74       134       434  \\\n",
      "0 -0.081939  0.00047  0.099661  0.084214 -0.095425  0.044236  0.001296   \n",
      "\n",
      "       512       542  \n",
      "0  0.00068  0.068105  \n"
     ]
    }
   ],
   "source": [
    "# Final significant features\n",
    "significant_features = ['15', '22', '60', '65', '74', '134', '434', '512', '542']\n",
    "X_final_significant = X_indicators[significant_features]\n",
    "\n",
    "# Fit logistic regression with class weights\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_final_significant, Y)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "f1_score = cross_val_score(model, X_final_significant, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with significant features (Lasso-based, VIF-filtered, iteratively reduced): {f1_score:.6f}\")\n",
    "\n",
    "# Inspect coefficients\n",
    "coeffs = pd.DataFrame(model.coef_, columns=X_final_significant.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (significant features):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c669d134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.167068\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1472\n",
      "Method:                           MLE   Df Model:                           94\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.3157\n",
      "Time:                        16:36:58   Log-Likelihood:                -261.80\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.063e-15\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.9176      0.220    -17.769      0.000      -4.350      -3.485\n",
      "15                          -0.2178      0.132     -1.656      0.098      -0.476       0.040\n",
      "22                           0.7346      0.395      1.858      0.063      -0.040       1.510\n",
      "23                          -0.3251      0.389     -0.836      0.403      -1.087       0.437\n",
      "27                          -0.5709      0.488     -1.169      0.242      -1.528       0.386\n",
      "29                           0.0537      0.396      0.136      0.892      -0.722       0.829\n",
      "32                           0.4660      0.344      1.354      0.176      -0.209       1.141\n",
      "33                           0.0666      0.200      0.334      0.739      -0.324       0.458\n",
      "34                           0.0470      0.174      0.270      0.787      -0.294       0.388\n",
      "39                          -0.1084      0.135     -0.806      0.420      -0.372       0.155\n",
      "41                          -0.1584      0.130     -1.217      0.224      -0.414       0.097\n",
      "60                           1.0175      0.217      4.700      0.000       0.593       1.442\n",
      "64                           0.7794        nan        nan        nan         nan         nan\n",
      "65                           1.1555      0.401      2.879      0.004       0.369       1.942\n",
      "66                          -0.9450      0.489     -1.932      0.053      -1.904       0.014\n",
      "69                          -0.1348      0.192     -0.702      0.483      -0.511       0.242\n",
      "71                           0.0665      0.480      0.139      0.890      -0.874       1.007\n",
      "91                          -0.1505      0.182     -0.825      0.409      -0.508       0.207\n",
      "112                         -0.2948      0.123     -2.391      0.017      -0.536      -0.053\n",
      "116                         -0.0609      0.171     -0.357      0.721      -0.396       0.274\n",
      "123                          0.3377      0.683      0.494      0.621      -1.001       1.677\n",
      "124                         -0.1800      0.317     -0.568      0.570      -0.801       0.441\n",
      "125                          0.4152      0.318      1.306      0.192      -0.208       1.038\n",
      "126                         -0.0036      0.224     -0.016      0.987      -0.444       0.436\n",
      "127                          0.1685      0.143      1.177      0.239      -0.112       0.449\n",
      "128                         -0.4077      0.936     -0.435      0.663      -2.242       1.427\n",
      "130                          0.4666      0.193      2.422      0.015       0.089       0.844\n",
      "134                          0.0997      0.197      0.507      0.612      -0.286       0.485\n",
      "139                         -0.7675      2.209     -0.347      0.728      -5.097       3.562\n",
      "160                          1.3172      1.909      0.690      0.490      -2.424       5.058\n",
      "161                         -2.0021      2.939     -0.681      0.496      -7.762       3.758\n",
      "164                          1.5184      2.033      0.747      0.455      -2.466       5.503\n",
      "165                         -3.5852      4.965     -0.722      0.470     -13.317       6.147\n",
      "166                          2.6194      2.647      0.990      0.322      -2.569       7.808\n",
      "167                         -1.2136      2.737     -0.443      0.657      -6.578       4.151\n",
      "176                          0.2477      0.152      1.632      0.103      -0.050       0.545\n",
      "181                         -5.6468      1.663     -3.395      0.001      -8.907      -2.386\n",
      "182                         -0.0976      0.175     -0.559      0.576      -0.440       0.245\n",
      "184                         -5.6500      9.630     -0.587      0.557     -24.524      13.224\n",
      "189                         -0.0429      0.178     -0.240      0.810      -0.392       0.307\n",
      "197                         -1.5782      0.859     -1.837      0.066      -3.262       0.106\n",
      "198                         20.5550        nan        nan        nan         nan         nan\n",
      "200                          1.8388      2.336      0.787      0.431      -2.740       6.418\n",
      "201                          0.1657      0.212      0.783      0.433      -0.249       0.580\n",
      "204                          7.1055      5.851      1.214      0.225      -4.363      18.574\n",
      "206                          5.8577      1.773      3.303      0.001       2.382       9.333\n",
      "295                         -1.7385      1.955     -0.889      0.374      -5.570       2.093\n",
      "296                          0.6751      2.288      0.295      0.768      -3.809       5.159\n",
      "300                          2.5049      4.657      0.538      0.591      -6.624      11.633\n",
      "301                         -2.1483      0.767     -2.802      0.005      -3.651      -0.645\n",
      "302                          0.2488      0.495      0.503      0.615      -0.722       1.219\n",
      "317                         -0.2026      0.513     -0.395      0.693      -1.207       0.802\n",
      "320                         -0.0034      0.935     -0.004      0.997      -1.836       1.829\n",
      "334                          1.7291      0.864      2.002      0.045       0.036       3.422\n",
      "338                          0.4852      0.342      1.418      0.156      -0.186       1.156\n",
      "342                         -0.8425      1.872     -0.450      0.653      -4.511       2.826\n",
      "412                          0.9450      2.018      0.468      0.640      -3.010       4.900\n",
      "417                         -0.3259      0.163     -1.999      0.046      -0.645      -0.006\n",
      "418                         -0.0089      0.153     -0.058      0.954      -0.309       0.291\n",
      "424                          0.1288      0.115      1.116      0.264      -0.097       0.355\n",
      "431                          0.9977      3.004      0.332      0.740      -4.889       6.885\n",
      "432                          3.4194      2.806      1.219      0.223      -2.080       8.919\n",
      "434                          0.1820      0.134      1.358      0.174      -0.081       0.444\n",
      "435                         -3.5984      6.902     -0.521      0.602     -17.126       9.929\n",
      "436                          2.9287      5.726      0.511      0.609      -8.294      14.152\n",
      "437                         -4.1609      4.871     -0.854      0.393     -13.709       5.387\n",
      "438                          1.2901      2.473      0.522      0.602      -3.557       6.137\n",
      "453                          5.4174      2.528      2.143      0.032       0.463      10.372\n",
      "456                          5.7480      7.066      0.813      0.416      -8.101      19.597\n",
      "461                          0.3273      0.153      2.146      0.032       0.028       0.626\n",
      "469                          0.1068      0.155      0.688      0.491      -0.197       0.411\n",
      "470                        -22.1539        nan        nan        nan         nan         nan\n",
      "472                         -0.7697      1.783     -0.432      0.666      -4.264       2.725\n",
      "476                         -6.8930      6.007     -1.147      0.251     -18.667       4.881\n",
      "478                         -6.1175      2.168     -2.822      0.005     -10.367      -1.868\n",
      "485                         -0.1572      0.134     -1.173      0.241      -0.420       0.106\n",
      "489                         -0.2167      0.135     -1.609      0.108      -0.481       0.047\n",
      "511                          0.1071      0.104      1.028      0.304      -0.097       0.311\n",
      "512                          0.2309      0.119      1.934      0.053      -0.003       0.465\n",
      "520                          0.0433      0.104      0.418      0.676      -0.160       0.247\n",
      "546                         -0.1748      0.159     -1.097      0.273      -0.487       0.137\n",
      "548                         -0.0013      0.263     -0.005      0.996      -0.517       0.515\n",
      "551                          2.3891      4.580      0.522      0.602      -6.588      11.367\n",
      "552                         -6.4342      3.703     -1.738      0.082     -13.691       0.823\n",
      "554                          0.4219      0.642      0.657      0.511      -0.836       1.680\n",
      "555                          1.2484      1.947      0.641      0.521      -2.567       5.064\n",
      "557                         -3.3734      4.835     -0.698      0.485     -12.849       6.103\n",
      "558                          5.7343      2.947      1.946      0.052      -0.041      11.510\n",
      "563                         -0.2245      0.132     -1.702      0.089      -0.483       0.034\n",
      "570                          0.1316      0.116      1.133      0.257      -0.096       0.359\n",
      "574                         -0.7217      0.541     -1.333      0.183      -1.783       0.340\n",
      "578                          0.1786      0.498      0.359      0.720      -0.797       1.154\n",
      "missing_73                   0.0027        nan        nan        nan         nan         nan\n",
      "missing_74                   0.0027        nan        nan        nan         nan         nan\n",
      "missing_113                  0.0717   1.81e+13   3.96e-15      1.000   -3.55e+13    3.55e+13\n",
      "missing_248                  0.0717   2.47e+13    2.9e-15      1.000   -4.85e+13    4.85e+13\n",
      "missing_346                  0.0027   8.53e+13   3.19e-17      1.000   -1.67e+14    1.67e+14\n",
      "missing_347                  0.0027        nan        nan        nan         nan         nan\n",
      "missing_386                  0.0717        nan        nan        nan         nan         nan\n",
      "missing_520                  0.0717   2.68e+06   2.68e-08      1.000   -5.25e+06    5.25e+06\n",
      "missing_late_utilization     0.0361      0.169      0.214      0.831      -0.294       0.367\n",
      "============================================================================================\n",
      "Significant features (p < 0.05): ['60', '65', '112', '130', '181', '206', '301', '334', '417', '453', '461', '478']\n"
     ]
    }
   ],
   "source": [
    "# Subset to Variance + SelectKBest features\n",
    "X_varkbest = X_indicators[selected_features_kbest_var]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_varkbest_scaled = scaler.fit_transform(X_varkbest)\n",
    "X_varkbest_scaled = pd.DataFrame(X_varkbest_scaled, columns=X_varkbest.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_varkbest_scaled = sm.add_constant(X_varkbest_scaled)\n",
    "\n",
    "# Fit logistic regression with 'lbfgs'\n",
    "logit_model = sm.Logit(Y, X_varkbest_scaled)\n",
    "result = logit_model.fit(method='newton', maxiter=500)\n",
    "print(result.summary())\n",
    "\n",
    "# Extract significant features\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6328bbc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 59 features due to high VIF: ['22', '27', '64', '65', '66', '71', '123', '128', '139', '160', '161', '164', '165', '166', '167', '181', '184', '197', '198', '200', '204', '206', '295', '296', '300', '301', '302', '320', '334', '342', '412', '431', '432', '435', '436', '437', '438', '453', '456', '470', '472', '476', '478', '551', '552', '554', '555', '557', '558', '574', '578', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520']\n"
     ]
    }
   ],
   "source": [
    "X_selected = X_indicators[selected_features_kbest_var]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Compute VIF to reduce multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_scaled.columns[1:]  # Exclude 'const'\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) for i in range(1, X_scaled.shape[1])]\n",
    "\n",
    "# Remove features with high VIF (> 10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"]\n",
    "X_scaled_reduced = X_scaled.drop(columns=high_vif_features)\n",
    "print(f\"Removed {len(high_vif_features)} features due to high VIF: {list(high_vif_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "09f8491c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197015\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1525\n",
      "Method:                           MLE   Df Model:                           41\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:14   Log-Likelihood:                -308.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.724e-14\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.3565      0.166    -20.241      0.000      -3.681      -3.031\n",
      "15                          -0.2214      0.114     -1.935      0.053      -0.446       0.003\n",
      "23                          -0.2018      0.098     -2.065      0.039      -0.393      -0.010\n",
      "29                          -0.1996      0.137     -1.455      0.146      -0.468       0.069\n",
      "32                          -0.2208      0.154     -1.438      0.150      -0.522       0.080\n",
      "33                           0.0018      0.183      0.010      0.992      -0.357       0.360\n",
      "34                           0.0807      0.158      0.509      0.611      -0.230       0.391\n",
      "39                          -0.1044      0.102     -1.020      0.308      -0.305       0.096\n",
      "41                          -0.0532      0.110     -0.484      0.629      -0.269       0.162\n",
      "60                           0.2609      0.094      2.789      0.005       0.078       0.444\n",
      "69                           0.0175      0.117      0.150      0.881      -0.211       0.246\n",
      "91                          -0.1320      0.158     -0.837      0.403      -0.441       0.177\n",
      "112                         -0.2352      0.118     -1.995      0.046      -0.466      -0.004\n",
      "116                         -0.0740      0.156     -0.475      0.635      -0.379       0.231\n",
      "124                         -0.0667      0.280     -0.238      0.812      -0.616       0.483\n",
      "125                          0.3114      0.272      1.145      0.252      -0.222       0.845\n",
      "126                         -0.0035      0.149     -0.023      0.981      -0.295       0.288\n",
      "127                          0.1223      0.113      1.079      0.280      -0.100       0.344\n",
      "130                          0.5297      0.171      3.106      0.002       0.195       0.864\n",
      "134                          0.1668      0.159      1.048      0.295      -0.145       0.479\n",
      "176                          0.1867      0.125      1.497      0.134      -0.058       0.431\n",
      "182                         -0.1643      0.143     -1.146      0.252      -0.445       0.117\n",
      "189                          0.0142      0.126      0.113      0.910      -0.233       0.262\n",
      "201                         -0.0623      0.104     -0.600      0.549      -0.266       0.141\n",
      "317                         -0.1901      0.139     -1.366      0.172      -0.463       0.083\n",
      "338                          0.1702      0.111      1.533      0.125      -0.047       0.388\n",
      "417                         -0.3382      0.134     -2.525      0.012      -0.601      -0.076\n",
      "418                         -0.0032      0.138     -0.023      0.981      -0.273       0.267\n",
      "424                          0.0845      0.103      0.818      0.413      -0.118       0.287\n",
      "434                          0.1480      0.114      1.300      0.194      -0.075       0.371\n",
      "461                          0.2678      0.122      2.202      0.028       0.029       0.506\n",
      "469                         -0.0075      0.128     -0.058      0.954      -0.259       0.244\n",
      "485                         -0.1786      0.121     -1.471      0.141      -0.417       0.059\n",
      "489                         -0.2545      0.118     -2.161      0.031      -0.485      -0.024\n",
      "511                          0.1687      0.087      1.946      0.052      -0.001       0.339\n",
      "512                          0.2307      0.107      2.153      0.031       0.021       0.441\n",
      "520                          0.1167      0.087      1.336      0.182      -0.055       0.288\n",
      "546                         -0.1256      0.142     -0.886      0.376      -0.404       0.152\n",
      "548                          0.1196      0.120      0.994      0.320      -0.116       0.355\n",
      "563                         -0.1330      0.114     -1.166      0.244      -0.357       0.091\n",
      "570                          0.1827      0.101      1.809      0.070      -0.015       0.381\n",
      "missing_late_utilization    -0.0158      0.149     -0.106      0.916      -0.308       0.276\n",
      "============================================================================================\n",
      "Removing feature 33 with p-value 0.992267\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197015\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1526\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:14   Log-Likelihood:                -308.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.934e-14\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.3564      0.166    -20.244      0.000      -3.681      -3.031\n",
      "15                          -0.2214      0.114     -1.935      0.053      -0.446       0.003\n",
      "23                          -0.2017      0.098     -2.065      0.039      -0.393      -0.010\n",
      "29                          -0.1995      0.137     -1.455      0.146      -0.468       0.069\n",
      "32                          -0.2208      0.154     -1.438      0.150      -0.522       0.080\n",
      "34                           0.0820      0.076      1.081      0.280      -0.067       0.231\n",
      "39                          -0.1045      0.102     -1.029      0.303      -0.304       0.095\n",
      "41                          -0.0534      0.109     -0.490      0.624      -0.267       0.160\n",
      "60                           0.2608      0.093      2.798      0.005       0.078       0.444\n",
      "69                           0.0175      0.117      0.150      0.881      -0.211       0.246\n",
      "91                          -0.1320      0.158     -0.837      0.403      -0.441       0.177\n",
      "112                         -0.2351      0.118     -1.995      0.046      -0.466      -0.004\n",
      "116                         -0.0740      0.156     -0.475      0.635      -0.379       0.231\n",
      "124                         -0.0667      0.280     -0.238      0.812      -0.616       0.483\n",
      "125                          0.3114      0.272      1.145      0.252      -0.222       0.845\n",
      "126                         -0.0035      0.149     -0.023      0.981      -0.295       0.288\n",
      "127                          0.1223      0.113      1.079      0.280      -0.100       0.344\n",
      "130                          0.5297      0.171      3.106      0.002       0.195       0.864\n",
      "134                          0.1668      0.159      1.048      0.295      -0.145       0.479\n",
      "176                          0.1864      0.121      1.538      0.124      -0.051       0.424\n",
      "182                         -0.1644      0.143     -1.147      0.252      -0.445       0.117\n",
      "189                          0.0142      0.126      0.113      0.910      -0.233       0.261\n",
      "201                         -0.0624      0.104     -0.600      0.548      -0.266       0.141\n",
      "317                         -0.1901      0.139     -1.366      0.172      -0.463       0.083\n",
      "338                          0.1703      0.111      1.538      0.124      -0.047       0.387\n",
      "417                         -0.3382      0.134     -2.526      0.012      -0.601      -0.076\n",
      "418                         -0.0033      0.138     -0.024      0.981      -0.273       0.267\n",
      "424                          0.0845      0.103      0.818      0.413      -0.118       0.287\n",
      "434                          0.1481      0.114      1.300      0.194      -0.075       0.371\n",
      "461                          0.2678      0.121      2.210      0.027       0.030       0.505\n",
      "469                         -0.0075      0.128     -0.058      0.953      -0.259       0.244\n",
      "485                         -0.1786      0.121     -1.471      0.141      -0.417       0.059\n",
      "489                         -0.2544      0.117     -2.167      0.030      -0.485      -0.024\n",
      "511                          0.1688      0.087      1.947      0.052      -0.001       0.339\n",
      "512                          0.2307      0.107      2.153      0.031       0.021       0.441\n",
      "520                          0.1168      0.087      1.338      0.181      -0.054       0.288\n",
      "546                         -0.1257      0.142     -0.887      0.375      -0.403       0.152\n",
      "548                          0.1195      0.120      0.995      0.320      -0.116       0.355\n",
      "563                         -0.1330      0.114     -1.166      0.244      -0.357       0.091\n",
      "570                          0.1826      0.101      1.809      0.070      -0.015       0.381\n",
      "missing_late_utilization    -0.0156      0.148     -0.105      0.916      -0.306       0.275\n",
      "============================================================================================\n",
      "Removing feature 126 with p-value 0.981498\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197015\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1527\n",
      "Method:                           MLE   Df Model:                           39\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:14   Log-Likelihood:                -308.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.485e-14\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.3565      0.166    -20.244      0.000      -3.681      -3.031\n",
      "15                          -0.2214      0.114     -1.935      0.053      -0.446       0.003\n",
      "23                          -0.2019      0.097     -2.072      0.038      -0.393      -0.011\n",
      "29                          -0.2000      0.135     -1.477      0.140      -0.465       0.065\n",
      "32                          -0.2209      0.153     -1.440      0.150      -0.522       0.080\n",
      "34                           0.0820      0.076      1.081      0.280      -0.067       0.231\n",
      "39                          -0.1044      0.101     -1.029      0.303      -0.303       0.094\n",
      "41                          -0.0534      0.109     -0.490      0.624      -0.267       0.160\n",
      "60                           0.2609      0.093      2.804      0.005       0.079       0.443\n",
      "69                           0.0175      0.117      0.150      0.880      -0.211       0.246\n",
      "91                          -0.1319      0.158     -0.837      0.403      -0.441       0.177\n",
      "112                         -0.2352      0.118     -1.995      0.046      -0.466      -0.004\n",
      "116                         -0.0742      0.156     -0.477      0.634      -0.379       0.231\n",
      "124                         -0.0673      0.279     -0.241      0.809      -0.614       0.480\n",
      "125                          0.3117      0.271      1.148      0.251      -0.220       0.844\n",
      "127                          0.1229      0.111      1.110      0.267      -0.094       0.340\n",
      "130                          0.5310      0.160      3.313      0.001       0.217       0.845\n",
      "134                          0.1669      0.159      1.049      0.294      -0.145       0.479\n",
      "176                          0.1867      0.121      1.545      0.122      -0.050       0.424\n",
      "182                         -0.1645      0.143     -1.148      0.251      -0.445       0.116\n",
      "189                          0.0143      0.126      0.113      0.910      -0.233       0.261\n",
      "201                         -0.0626      0.104     -0.604      0.546      -0.265       0.140\n",
      "317                         -0.1901      0.139     -1.366      0.172      -0.463       0.083\n",
      "338                          0.1703      0.111      1.539      0.124      -0.047       0.387\n",
      "417                         -0.3380      0.134     -2.529      0.011      -0.600      -0.076\n",
      "418                         -0.0033      0.138     -0.024      0.981      -0.273       0.267\n",
      "424                          0.0848      0.102      0.828      0.408      -0.116       0.285\n",
      "434                          0.1480      0.114      1.300      0.194      -0.075       0.371\n",
      "461                          0.2677      0.121      2.211      0.027       0.030       0.505\n",
      "469                         -0.0074      0.128     -0.057      0.954      -0.259       0.244\n",
      "485                         -0.1786      0.121     -1.471      0.141      -0.417       0.059\n",
      "489                         -0.2546      0.117     -2.170      0.030      -0.484      -0.025\n",
      "511                          0.1690      0.086      1.957      0.050      -0.000       0.338\n",
      "512                          0.2306      0.107      2.153      0.031       0.021       0.441\n",
      "520                          0.1168      0.087      1.339      0.181      -0.054       0.288\n",
      "546                         -0.1255      0.141     -0.887      0.375      -0.403       0.152\n",
      "548                          0.1197      0.120      0.999      0.318      -0.115       0.355\n",
      "563                         -0.1330      0.114     -1.166      0.244      -0.357       0.091\n",
      "570                          0.1828      0.101      1.812      0.070      -0.015       0.380\n",
      "missing_late_utilization    -0.0157      0.148     -0.106      0.915      -0.306       0.274\n",
      "============================================================================================\n",
      "Removing feature 418 with p-value 0.981049\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197015\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1528\n",
      "Method:                           MLE   Df Model:                           38\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:14   Log-Likelihood:                -308.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.421e-15\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.3564      0.166    -20.248      0.000      -3.681      -3.031\n",
      "15                          -0.2215      0.114     -1.940      0.052      -0.445       0.002\n",
      "23                          -0.2017      0.097     -2.075      0.038      -0.392      -0.011\n",
      "29                          -0.2003      0.135     -1.484      0.138      -0.465       0.064\n",
      "32                          -0.2211      0.153     -1.444      0.149      -0.521       0.079\n",
      "34                           0.0819      0.076      1.081      0.280      -0.067       0.231\n",
      "39                          -0.1042      0.101     -1.031      0.303      -0.302       0.094\n",
      "41                          -0.0533      0.109     -0.489      0.625      -0.267       0.160\n",
      "60                           0.2611      0.093      2.809      0.005       0.079       0.443\n",
      "69                           0.0177      0.116      0.153      0.879      -0.210       0.246\n",
      "91                          -0.1318      0.158     -0.836      0.403      -0.441       0.177\n",
      "112                         -0.2351      0.118     -1.995      0.046      -0.466      -0.004\n",
      "116                         -0.0744      0.155     -0.479      0.632      -0.379       0.230\n",
      "124                         -0.0675      0.279     -0.242      0.809      -0.614       0.479\n",
      "125                          0.3121      0.271      1.151      0.250      -0.219       0.843\n",
      "127                          0.1231      0.110      1.115      0.265      -0.093       0.339\n",
      "130                          0.5312      0.160      3.319      0.001       0.218       0.845\n",
      "134                          0.1667      0.159      1.049      0.294      -0.145       0.478\n",
      "176                          0.1869      0.121      1.550      0.121      -0.049       0.423\n",
      "182                         -0.1644      0.143     -1.148      0.251      -0.445       0.116\n",
      "189                          0.0145      0.126      0.115      0.909      -0.232       0.261\n",
      "201                         -0.0624      0.103     -0.604      0.546      -0.265       0.140\n",
      "317                         -0.1902      0.139     -1.369      0.171      -0.463       0.082\n",
      "338                          0.1701      0.110      1.545      0.122      -0.046       0.386\n",
      "417                         -0.3380      0.134     -2.529      0.011      -0.600      -0.076\n",
      "424                          0.0846      0.102      0.828      0.407      -0.116       0.285\n",
      "434                          0.1482      0.114      1.304      0.192      -0.075       0.371\n",
      "461                          0.2677      0.121      2.211      0.027       0.030       0.505\n",
      "469                         -0.0075      0.128     -0.058      0.954      -0.259       0.244\n",
      "485                         -0.1787      0.121     -1.473      0.141      -0.417       0.059\n",
      "489                         -0.2547      0.117     -2.173      0.030      -0.484      -0.025\n",
      "511                          0.1689      0.086      1.957      0.050      -0.000       0.338\n",
      "512                          0.2306      0.107      2.153      0.031       0.021       0.441\n",
      "520                          0.1167      0.087      1.339      0.181      -0.054       0.288\n",
      "546                         -0.1257      0.141     -0.889      0.374      -0.403       0.151\n",
      "548                          0.1198      0.120      1.001      0.317      -0.115       0.354\n",
      "563                         -0.1329      0.114     -1.166      0.244      -0.357       0.091\n",
      "570                          0.1826      0.101      1.814      0.070      -0.015       0.380\n",
      "missing_late_utilization    -0.0156      0.148     -0.106      0.916      -0.305       0.274\n",
      "============================================================================================\n",
      "Removing feature 469 with p-value 0.953598\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197017\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1529\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.72\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.662e-15\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -3.3565      0.166    -20.248      0.000      -3.681      -3.032\n",
      "15                          -0.2214      0.114     -1.940      0.052      -0.445       0.002\n",
      "23                          -0.2016      0.097     -2.074      0.038      -0.392      -0.011\n",
      "29                          -0.2011      0.134     -1.497      0.134      -0.464       0.062\n",
      "32                          -0.2209      0.153     -1.443      0.149      -0.521       0.079\n",
      "34                           0.0819      0.076      1.081      0.280      -0.067       0.231\n",
      "39                          -0.1039      0.101     -1.029      0.303      -0.302       0.094\n",
      "41                          -0.0530      0.109     -0.487      0.626      -0.266       0.160\n",
      "60                           0.2622      0.091      2.880      0.004       0.084       0.441\n",
      "69                           0.0182      0.116      0.157      0.875      -0.209       0.246\n",
      "91                          -0.1317      0.158     -0.836      0.403      -0.441       0.177\n",
      "112                         -0.2350      0.118     -1.996      0.046      -0.466      -0.004\n",
      "116                         -0.0747      0.155     -0.482      0.630      -0.379       0.229\n",
      "124                         -0.0671      0.279     -0.241      0.810      -0.614       0.480\n",
      "125                          0.3114      0.271      1.149      0.250      -0.220       0.842\n",
      "127                          0.1234      0.110      1.119      0.263      -0.093       0.340\n",
      "130                          0.5316      0.160      3.324      0.001       0.218       0.845\n",
      "134                          0.1673      0.159      1.055      0.292      -0.144       0.478\n",
      "176                          0.1871      0.120      1.553      0.121      -0.049       0.423\n",
      "182                         -0.1643      0.143     -1.147      0.251      -0.445       0.116\n",
      "189                          0.0146      0.126      0.116      0.908      -0.232       0.261\n",
      "201                         -0.0624      0.103     -0.604      0.546      -0.265       0.140\n",
      "317                         -0.1908      0.139     -1.376      0.169      -0.463       0.081\n",
      "338                          0.1699      0.110      1.543      0.123      -0.046       0.386\n",
      "417                         -0.3388      0.133     -2.546      0.011      -0.600      -0.078\n",
      "424                          0.0850      0.102      0.833      0.405      -0.115       0.285\n",
      "434                          0.1480      0.114      1.303      0.192      -0.075       0.371\n",
      "461                          0.2679      0.121      2.214      0.027       0.031       0.505\n",
      "485                         -0.1787      0.121     -1.473      0.141      -0.416       0.059\n",
      "489                         -0.2546      0.117     -2.173      0.030      -0.484      -0.025\n",
      "511                          0.1692      0.086      1.961      0.050    5.11e-05       0.338\n",
      "512                          0.2304      0.107      2.152      0.031       0.021       0.440\n",
      "520                          0.1169      0.087      1.342      0.180      -0.054       0.288\n",
      "546                         -0.1258      0.141     -0.891      0.373      -0.403       0.151\n",
      "548                          0.1200      0.120      1.003      0.316      -0.115       0.354\n",
      "563                         -0.1329      0.114     -1.165      0.244      -0.356       0.091\n",
      "570                          0.1828      0.101      1.816      0.069      -0.014       0.380\n",
      "missing_late_utilization    -0.0153      0.148     -0.103      0.918      -0.305       0.274\n",
      "============================================================================================\n",
      "Removing feature missing_late_utilization with p-value 0.917626\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197020\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1530\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.73\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.787e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3565      0.166    -20.248      0.000      -3.681      -3.032\n",
      "15            -0.2211      0.114     -1.938      0.053      -0.445       0.002\n",
      "23            -0.2024      0.097     -2.087      0.037      -0.392      -0.012\n",
      "29            -0.1996      0.134     -1.494      0.135      -0.462       0.062\n",
      "32            -0.2212      0.153     -1.444      0.149      -0.521       0.079\n",
      "34             0.0819      0.076      1.081      0.280      -0.067       0.230\n",
      "39            -0.1047      0.101     -1.040      0.298      -0.302       0.093\n",
      "41            -0.0531      0.109     -0.488      0.626      -0.266       0.160\n",
      "60             0.2619      0.091      2.878      0.004       0.084       0.440\n",
      "69             0.0181      0.116      0.156      0.876      -0.209       0.246\n",
      "91            -0.1313      0.158     -0.833      0.405      -0.440       0.178\n",
      "112           -0.2359      0.118     -2.001      0.045      -0.467      -0.005\n",
      "116           -0.0733      0.154     -0.474      0.635      -0.376       0.229\n",
      "124           -0.0661      0.279     -0.237      0.813      -0.613       0.480\n",
      "125            0.3094      0.270      1.145      0.252      -0.220       0.839\n",
      "127            0.1234      0.110      1.120      0.263      -0.093       0.339\n",
      "130            0.5301      0.159      3.327      0.001       0.218       0.842\n",
      "134            0.1679      0.159      1.060      0.289      -0.143       0.479\n",
      "176            0.1869      0.120      1.551      0.121      -0.049       0.423\n",
      "182           -0.1643      0.143     -1.148      0.251      -0.445       0.116\n",
      "189            0.0146      0.126      0.116      0.907      -0.232       0.261\n",
      "201           -0.0628      0.103     -0.608      0.543      -0.265       0.140\n",
      "317           -0.1892      0.138     -1.373      0.170      -0.459       0.081\n",
      "338            0.1702      0.110      1.547      0.122      -0.045       0.386\n",
      "417           -0.3396      0.133     -2.556      0.011      -0.600      -0.079\n",
      "424            0.0850      0.102      0.834      0.404      -0.115       0.285\n",
      "434            0.1484      0.114      1.306      0.191      -0.074       0.371\n",
      "461            0.2680      0.121      2.215      0.027       0.031       0.505\n",
      "485           -0.1788      0.121     -1.474      0.140      -0.416       0.059\n",
      "489           -0.2536      0.117     -2.171      0.030      -0.483      -0.025\n",
      "511            0.1686      0.086      1.958      0.050      -0.000       0.337\n",
      "512            0.2301      0.107      2.150      0.032       0.020       0.440\n",
      "520            0.1166      0.087      1.338      0.181      -0.054       0.287\n",
      "546           -0.1245      0.141     -0.886      0.376      -0.400       0.151\n",
      "548            0.1195      0.120      1.000      0.318      -0.115       0.354\n",
      "563           -0.1325      0.114     -1.162      0.245      -0.356       0.091\n",
      "570            0.1835      0.100      1.826      0.068      -0.013       0.380\n",
      "==============================================================================\n",
      "Removing feature 189 with p-value 0.907439\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197024\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1531\n",
      "Method:                           MLE   Df Model:                           35\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.74\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.611e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3565      0.166    -20.246      0.000      -3.681      -3.032\n",
      "15            -0.2207      0.114     -1.935      0.053      -0.444       0.003\n",
      "23            -0.2021      0.097     -2.084      0.037      -0.392      -0.012\n",
      "29            -0.2011      0.133     -1.511      0.131      -0.462       0.060\n",
      "32            -0.2214      0.153     -1.446      0.148      -0.522       0.079\n",
      "34             0.0815      0.076      1.077      0.282      -0.067       0.230\n",
      "39            -0.1046      0.101     -1.039      0.299      -0.302       0.093\n",
      "41            -0.0527      0.109     -0.485      0.628      -0.266       0.161\n",
      "60             0.2620      0.091      2.881      0.004       0.084       0.440\n",
      "69             0.0180      0.116      0.155      0.877      -0.209       0.245\n",
      "91            -0.1314      0.158     -0.834      0.404      -0.440       0.177\n",
      "112           -0.2368      0.118     -2.014      0.044      -0.467      -0.006\n",
      "116           -0.0733      0.155     -0.474      0.635      -0.376       0.230\n",
      "124           -0.0652      0.279     -0.234      0.815      -0.612       0.481\n",
      "125            0.3090      0.270      1.143      0.253      -0.221       0.839\n",
      "127            0.1230      0.110      1.117      0.264      -0.093       0.339\n",
      "130            0.5307      0.159      3.331      0.001       0.218       0.843\n",
      "134            0.1670      0.158      1.054      0.292      -0.143       0.477\n",
      "176            0.1876      0.120      1.559      0.119      -0.048       0.424\n",
      "182           -0.1687      0.138     -1.222      0.222      -0.439       0.102\n",
      "201           -0.0622      0.103     -0.604      0.546      -0.264       0.140\n",
      "317           -0.1908      0.137     -1.390      0.165      -0.460       0.078\n",
      "338            0.1705      0.110      1.551      0.121      -0.045       0.386\n",
      "417           -0.3381      0.132     -2.557      0.011      -0.597      -0.079\n",
      "424            0.0854      0.102      0.839      0.402      -0.114       0.285\n",
      "434            0.1480      0.114      1.303      0.192      -0.075       0.371\n",
      "461            0.2745      0.107      2.566      0.010       0.065       0.484\n",
      "485           -0.1797      0.121     -1.484      0.138      -0.417       0.058\n",
      "489           -0.2533      0.117     -2.169      0.030      -0.482      -0.024\n",
      "511            0.1678      0.086      1.956      0.050      -0.000       0.336\n",
      "512            0.2301      0.107      2.150      0.032       0.020       0.440\n",
      "520            0.1161      0.087      1.334      0.182      -0.054       0.287\n",
      "546           -0.1232      0.140     -0.880      0.379      -0.398       0.151\n",
      "548            0.1194      0.120      0.998      0.318      -0.115       0.354\n",
      "563           -0.1316      0.114     -1.157      0.247      -0.355       0.091\n",
      "570            0.1837      0.100      1.829      0.067      -0.013       0.381\n",
      "==============================================================================\n",
      "Removing feature 69 with p-value 0.876750\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197032\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1532\n",
      "Method:                           MLE   Df Model:                           34\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1930\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.75\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.106e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3556      0.166    -20.262      0.000      -3.680      -3.031\n",
      "15            -0.2216      0.114     -1.944      0.052      -0.445       0.002\n",
      "23            -0.2023      0.097     -2.085      0.037      -0.393      -0.012\n",
      "29            -0.2012      0.133     -1.513      0.130      -0.462       0.060\n",
      "32            -0.2219      0.153     -1.448      0.147      -0.522       0.078\n",
      "34             0.0817      0.076      1.075      0.282      -0.067       0.231\n",
      "39            -0.1025      0.100     -1.027      0.304      -0.298       0.093\n",
      "41            -0.0536      0.109     -0.493      0.622      -0.267       0.159\n",
      "60             0.2566      0.084      3.054      0.002       0.092       0.421\n",
      "91            -0.1324      0.157     -0.841      0.400      -0.441       0.176\n",
      "112           -0.2349      0.117     -2.009      0.045      -0.464      -0.006\n",
      "116           -0.0740      0.154     -0.479      0.632      -0.377       0.229\n",
      "124           -0.0621      0.279     -0.223      0.824      -0.609       0.484\n",
      "125            0.3047      0.269      1.131      0.258      -0.223       0.833\n",
      "127            0.1213      0.110      1.107      0.268      -0.093       0.336\n",
      "130            0.5314      0.159      3.338      0.001       0.219       0.843\n",
      "134            0.1677      0.158      1.059      0.290      -0.143       0.478\n",
      "176            0.1899      0.119      1.589      0.112      -0.044       0.424\n",
      "182           -0.1721      0.136     -1.261      0.207      -0.439       0.095\n",
      "201           -0.0652      0.102     -0.641      0.522      -0.264       0.134\n",
      "317           -0.1884      0.136     -1.382      0.167      -0.456       0.079\n",
      "338            0.1690      0.111      1.523      0.128      -0.049       0.387\n",
      "417           -0.3385      0.132     -2.560      0.010      -0.598      -0.079\n",
      "424            0.0854      0.102      0.839      0.401      -0.114       0.285\n",
      "434            0.1476      0.113      1.300      0.194      -0.075       0.370\n",
      "461            0.2746      0.107      2.566      0.010       0.065       0.484\n",
      "485           -0.1806      0.121     -1.492      0.136      -0.418       0.057\n",
      "489           -0.2524      0.117     -2.165      0.030      -0.481      -0.024\n",
      "511            0.1675      0.086      1.951      0.051      -0.001       0.336\n",
      "512            0.2303      0.107      2.152      0.031       0.021       0.440\n",
      "520            0.1169      0.087      1.346      0.178      -0.053       0.287\n",
      "546           -0.1235      0.140     -0.882      0.378      -0.398       0.151\n",
      "548            0.1191      0.120      0.996      0.319      -0.115       0.354\n",
      "563           -0.1306      0.114     -1.151      0.250      -0.353       0.092\n",
      "570            0.1838      0.100      1.831      0.067      -0.013       0.381\n",
      "==============================================================================\n",
      "Removing feature 124 with p-value 0.823794\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197047\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1533\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1929\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.77\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.947e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3569      0.166    -20.260      0.000      -3.682      -3.032\n",
      "15            -0.2210      0.114     -1.941      0.052      -0.444       0.002\n",
      "23            -0.2027      0.097     -2.088      0.037      -0.393      -0.012\n",
      "29            -0.2011      0.133     -1.511      0.131      -0.462       0.060\n",
      "32            -0.2193      0.153     -1.436      0.151      -0.519       0.080\n",
      "34             0.0812      0.076      1.070      0.285      -0.068       0.230\n",
      "39            -0.1017      0.100     -1.021      0.307      -0.297       0.094\n",
      "41            -0.0524      0.108     -0.483      0.629      -0.265       0.160\n",
      "60             0.2570      0.084      3.060      0.002       0.092       0.422\n",
      "91            -0.1340      0.157     -0.852      0.394      -0.442       0.174\n",
      "112           -0.2338      0.116     -2.009      0.045      -0.462      -0.006\n",
      "116           -0.0738      0.155     -0.477      0.633      -0.377       0.229\n",
      "125            0.2547      0.148      1.718      0.086      -0.036       0.545\n",
      "127            0.1209      0.110      1.102      0.270      -0.094       0.336\n",
      "130            0.5379      0.156      3.442      0.001       0.232       0.844\n",
      "134            0.1616      0.156      1.035      0.301      -0.144       0.468\n",
      "176            0.1889      0.119      1.583      0.113      -0.045       0.423\n",
      "182           -0.1715      0.136     -1.257      0.209      -0.439       0.096\n",
      "201           -0.0645      0.102     -0.635      0.525      -0.263       0.134\n",
      "317           -0.1887      0.136     -1.385      0.166      -0.456       0.078\n",
      "338            0.1698      0.111      1.530      0.126      -0.048       0.387\n",
      "417           -0.3370      0.132     -2.552      0.011      -0.596      -0.078\n",
      "424            0.0860      0.102      0.844      0.399      -0.114       0.286\n",
      "434            0.1467      0.113      1.293      0.196      -0.076       0.369\n",
      "461            0.2765      0.107      2.593      0.010       0.068       0.486\n",
      "485           -0.1805      0.121     -1.492      0.136      -0.418       0.057\n",
      "489           -0.2512      0.116     -2.158      0.031      -0.479      -0.023\n",
      "511            0.1686      0.086      1.968      0.049       0.001       0.336\n",
      "512            0.2315      0.107      2.166      0.030       0.022       0.441\n",
      "520            0.1158      0.087      1.335      0.182      -0.054       0.286\n",
      "546           -0.1251      0.140     -0.894      0.371      -0.399       0.149\n",
      "548            0.1188      0.120      0.993      0.321      -0.116       0.353\n",
      "563           -0.1324      0.113     -1.169      0.242      -0.354       0.090\n",
      "570            0.1844      0.100      1.838      0.066      -0.012       0.381\n",
      "==============================================================================\n",
      "Removing feature 116 with p-value 0.633058\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197119\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1534\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1926\n",
      "Time:                        16:37:15   Log-Likelihood:                -308.89\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.757e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3548      0.165    -20.280      0.000      -3.679      -3.031\n",
      "15            -0.2220      0.114     -1.950      0.051      -0.445       0.001\n",
      "23            -0.2025      0.097     -2.089      0.037      -0.393      -0.012\n",
      "29            -0.1973      0.133     -1.485      0.137      -0.458       0.063\n",
      "32            -0.2165      0.153     -1.418      0.156      -0.516       0.083\n",
      "34             0.0780      0.076      1.028      0.304      -0.071       0.227\n",
      "39            -0.1008      0.099     -1.013      0.311      -0.296       0.094\n",
      "41            -0.0575      0.108     -0.534      0.594      -0.269       0.154\n",
      "60             0.2559      0.084      3.047      0.002       0.091       0.420\n",
      "91            -0.1827      0.119     -1.530      0.126      -0.417       0.051\n",
      "112           -0.2324      0.116     -2.003      0.045      -0.460      -0.005\n",
      "125            0.2546      0.148      1.717      0.086      -0.036       0.545\n",
      "127            0.1192      0.109      1.089      0.276      -0.095       0.334\n",
      "130            0.5397      0.156      3.451      0.001       0.233       0.846\n",
      "134            0.1642      0.156      1.052      0.293      -0.142       0.470\n",
      "176            0.1919      0.119      1.612      0.107      -0.041       0.425\n",
      "182           -0.1732      0.137     -1.269      0.205      -0.441       0.094\n",
      "201           -0.0647      0.102     -0.637      0.524      -0.264       0.134\n",
      "317           -0.1842      0.136     -1.353      0.176      -0.451       0.083\n",
      "338            0.1698      0.111      1.532      0.126      -0.047       0.387\n",
      "417           -0.3386      0.132     -2.566      0.010      -0.597      -0.080\n",
      "424            0.0823      0.102      0.810      0.418      -0.117       0.282\n",
      "434            0.1480      0.113      1.306      0.192      -0.074       0.370\n",
      "461            0.2757      0.107      2.583      0.010       0.066       0.485\n",
      "485           -0.1796      0.121     -1.485      0.138      -0.417       0.057\n",
      "489           -0.2507      0.116     -2.155      0.031      -0.479      -0.023\n",
      "511            0.1712      0.085      2.007      0.045       0.004       0.338\n",
      "512            0.2286      0.107      2.144      0.032       0.020       0.438\n",
      "520            0.1166      0.087      1.345      0.179      -0.053       0.287\n",
      "546           -0.1221      0.139     -0.876      0.381      -0.395       0.151\n",
      "548            0.1193      0.120      0.996      0.319      -0.115       0.354\n",
      "563           -0.1337      0.113     -1.180      0.238      -0.356       0.088\n",
      "570            0.1864      0.100      1.864      0.062      -0.010       0.382\n",
      "==============================================================================\n",
      "Removing feature 41 with p-value 0.593658\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197208\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1535\n",
      "Method:                           MLE   Df Model:                           31\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1922\n",
      "Time:                        16:37:15   Log-Likelihood:                -309.02\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.927e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3532      0.165    -20.298      0.000      -3.677      -3.029\n",
      "15            -0.2190      0.114     -1.927      0.054      -0.442       0.004\n",
      "23            -0.2062      0.097     -2.124      0.034      -0.396      -0.016\n",
      "29            -0.2022      0.132     -1.528      0.127      -0.462       0.057\n",
      "32            -0.2179      0.153     -1.429      0.153      -0.517       0.081\n",
      "34             0.0764      0.076      1.009      0.313      -0.072       0.225\n",
      "39            -0.0995      0.099     -1.001      0.317      -0.294       0.095\n",
      "60             0.2599      0.084      3.111      0.002       0.096       0.424\n",
      "91            -0.1852      0.119     -1.550      0.121      -0.419       0.049\n",
      "112           -0.2343      0.117     -2.010      0.044      -0.463      -0.006\n",
      "125            0.2561      0.148      1.729      0.084      -0.034       0.546\n",
      "127            0.1217      0.109      1.113      0.266      -0.093       0.336\n",
      "130            0.5345      0.156      3.422      0.001       0.228       0.841\n",
      "134            0.1650      0.156      1.060      0.289      -0.140       0.470\n",
      "176            0.1870      0.119      1.574      0.115      -0.046       0.420\n",
      "182           -0.1706      0.136     -1.251      0.211      -0.438       0.097\n",
      "201           -0.0661      0.101     -0.652      0.514      -0.265       0.133\n",
      "317           -0.1902      0.136     -1.400      0.161      -0.456       0.076\n",
      "338            0.1694      0.111      1.526      0.127      -0.048       0.387\n",
      "417           -0.3362      0.132     -2.550      0.011      -0.595      -0.078\n",
      "424            0.0826      0.102      0.813      0.416      -0.117       0.282\n",
      "434            0.1403      0.112      1.248      0.212      -0.080       0.361\n",
      "461            0.2812      0.106      2.644      0.008       0.073       0.490\n",
      "485           -0.1773      0.121     -1.469      0.142      -0.414       0.059\n",
      "489           -0.2489      0.116     -2.139      0.032      -0.477      -0.021\n",
      "511            0.1762      0.085      2.082      0.037       0.010       0.342\n",
      "512            0.2327      0.106      2.187      0.029       0.024       0.441\n",
      "520            0.1146      0.087      1.323      0.186      -0.055       0.284\n",
      "546           -0.1188      0.140     -0.848      0.396      -0.393       0.156\n",
      "548            0.1239      0.119      1.037      0.300      -0.110       0.358\n",
      "563           -0.1330      0.113     -1.173      0.241      -0.355       0.089\n",
      "570            0.1899      0.100      1.904      0.057      -0.006       0.385\n",
      "==============================================================================\n",
      "Removing feature 201 with p-value 0.514465\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197345\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1536\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1917\n",
      "Time:                        16:37:15   Log-Likelihood:                -309.24\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.608e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3476      0.165    -20.341      0.000      -3.670      -3.025\n",
      "15            -0.2136      0.113     -1.885      0.059      -0.436       0.009\n",
      "23            -0.2059      0.097     -2.122      0.034      -0.396      -0.016\n",
      "29            -0.2008      0.132     -1.517      0.129      -0.460       0.059\n",
      "32            -0.2178      0.152     -1.429      0.153      -0.517       0.081\n",
      "34             0.0782      0.075      1.036      0.300      -0.070       0.226\n",
      "39            -0.0958      0.099     -0.964      0.335      -0.291       0.099\n",
      "60             0.2453      0.081      3.040      0.002       0.087       0.404\n",
      "91            -0.1874      0.119     -1.570      0.116      -0.421       0.047\n",
      "112           -0.2334      0.117     -1.998      0.046      -0.462      -0.004\n",
      "125            0.2489      0.148      1.683      0.092      -0.041       0.539\n",
      "127            0.1230      0.109      1.129      0.259      -0.091       0.337\n",
      "130            0.5377      0.156      3.451      0.001       0.232       0.843\n",
      "134            0.1659      0.156      1.065      0.287      -0.139       0.471\n",
      "176            0.1868      0.119      1.571      0.116      -0.046       0.420\n",
      "182           -0.1601      0.135     -1.183      0.237      -0.426       0.105\n",
      "317           -0.1894      0.136     -1.395      0.163      -0.456       0.077\n",
      "338            0.1169      0.076      1.532      0.125      -0.033       0.266\n",
      "417           -0.3431      0.132     -2.605      0.009      -0.601      -0.085\n",
      "424            0.0855      0.101      0.844      0.399      -0.113       0.284\n",
      "434            0.1367      0.112      1.216      0.224      -0.084       0.357\n",
      "461            0.2806      0.106      2.646      0.008       0.073       0.489\n",
      "485           -0.1746      0.121     -1.446      0.148      -0.411       0.062\n",
      "489           -0.2468      0.116     -2.119      0.034      -0.475      -0.019\n",
      "511            0.1777      0.084      2.106      0.035       0.012       0.343\n",
      "512            0.2281      0.106      2.148      0.032       0.020       0.436\n",
      "520            0.1167      0.086      1.356      0.175      -0.052       0.285\n",
      "546           -0.1196      0.140     -0.854      0.393      -0.394       0.155\n",
      "548            0.1189      0.119      0.999      0.318      -0.114       0.352\n",
      "563           -0.1320      0.113     -1.166      0.243      -0.354       0.090\n",
      "570            0.1893      0.100      1.897      0.058      -0.006       0.385\n",
      "==============================================================================\n",
      "Removing feature 424 with p-value 0.398547\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197565\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1537\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1908\n",
      "Time:                        16:37:15   Log-Likelihood:                -309.59\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.512e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3419      0.164    -20.377      0.000      -3.663      -3.020\n",
      "15            -0.2411      0.110     -2.194      0.028      -0.456      -0.026\n",
      "23            -0.2048      0.097     -2.115      0.034      -0.395      -0.015\n",
      "29            -0.2007      0.132     -1.518      0.129      -0.460       0.058\n",
      "32            -0.2202      0.152     -1.446      0.148      -0.519       0.078\n",
      "34             0.0795      0.076      1.052      0.293      -0.069       0.228\n",
      "39            -0.0918      0.100     -0.921      0.357      -0.287       0.103\n",
      "60             0.2452      0.081      3.039      0.002       0.087       0.403\n",
      "91            -0.1787      0.119     -1.505      0.132      -0.411       0.054\n",
      "112           -0.2318      0.118     -1.970      0.049      -0.462      -0.001\n",
      "125            0.2365      0.147      1.608      0.108      -0.052       0.525\n",
      "127            0.1234      0.109      1.135      0.256      -0.090       0.336\n",
      "130            0.5416      0.155      3.488      0.000       0.237       0.846\n",
      "134            0.1678      0.155      1.080      0.280      -0.137       0.472\n",
      "176            0.1891      0.119      1.590      0.112      -0.044       0.422\n",
      "182           -0.1653      0.135     -1.222      0.222      -0.431       0.100\n",
      "317           -0.1974      0.136     -1.456      0.145      -0.463       0.068\n",
      "338            0.1149      0.077      1.499      0.134      -0.035       0.265\n",
      "417           -0.3403      0.132     -2.581      0.010      -0.599      -0.082\n",
      "434            0.1374      0.112      1.223      0.221      -0.083       0.358\n",
      "461            0.2744      0.106      2.592      0.010       0.067       0.482\n",
      "485           -0.1753      0.121     -1.453      0.146      -0.412       0.061\n",
      "489           -0.2461      0.117     -2.110      0.035      -0.475      -0.017\n",
      "511            0.1790      0.084      2.128      0.033       0.014       0.344\n",
      "512            0.2215      0.106      2.095      0.036       0.014       0.429\n",
      "520            0.1180      0.086      1.374      0.169      -0.050       0.286\n",
      "546           -0.1213      0.141     -0.861      0.389      -0.397       0.155\n",
      "548            0.1188      0.119      0.997      0.319      -0.115       0.352\n",
      "563           -0.1385      0.113     -1.227      0.220      -0.360       0.083\n",
      "570            0.1874      0.100      1.882      0.060      -0.008       0.383\n",
      "==============================================================================\n",
      "Removing feature 546 with p-value 0.389005\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197819\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1538\n",
      "Method:                           MLE   Df Model:                           28\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1897\n",
      "Time:                        16:37:15   Log-Likelihood:                -309.98\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.034e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3416      0.164    -20.350      0.000      -3.663      -3.020\n",
      "15            -0.2388      0.110     -2.169      0.030      -0.455      -0.023\n",
      "23            -0.2007      0.097     -2.071      0.038      -0.391      -0.011\n",
      "29            -0.2139      0.131     -1.631      0.103      -0.471       0.043\n",
      "32            -0.2093      0.152     -1.381      0.167      -0.506       0.088\n",
      "34             0.0786      0.076      1.040      0.298      -0.069       0.227\n",
      "39            -0.0834      0.099     -0.843      0.399      -0.277       0.110\n",
      "60             0.2501      0.081      3.104      0.002       0.092       0.408\n",
      "91            -0.1763      0.119     -1.487      0.137      -0.409       0.056\n",
      "112           -0.2199      0.114     -1.927      0.054      -0.443       0.004\n",
      "125            0.2407      0.147      1.640      0.101      -0.047       0.528\n",
      "127            0.1264      0.109      1.162      0.245      -0.087       0.340\n",
      "130            0.5497      0.155      3.558      0.000       0.247       0.852\n",
      "134            0.1731      0.155      1.117      0.264      -0.131       0.477\n",
      "176            0.1990      0.118      1.682      0.093      -0.033       0.431\n",
      "182           -0.1669      0.136     -1.229      0.219      -0.433       0.099\n",
      "317           -0.1941      0.135     -1.438      0.150      -0.459       0.070\n",
      "338            0.1057      0.075      1.417      0.156      -0.040       0.252\n",
      "417           -0.3375      0.132     -2.558      0.011      -0.596      -0.079\n",
      "434            0.1404      0.112      1.253      0.210      -0.079       0.360\n",
      "461            0.2771      0.106      2.611      0.009       0.069       0.485\n",
      "485           -0.1728      0.121     -1.433      0.152      -0.409       0.064\n",
      "489           -0.2433      0.116     -2.094      0.036      -0.471      -0.016\n",
      "511            0.1811      0.084      2.157      0.031       0.017       0.346\n",
      "512            0.2275      0.105      2.159      0.031       0.021       0.434\n",
      "520            0.1216      0.086      1.420      0.156      -0.046       0.290\n",
      "548            0.1237      0.119      1.041      0.298      -0.109       0.357\n",
      "563           -0.1463      0.112     -1.306      0.192      -0.366       0.073\n",
      "570            0.1898      0.099      1.909      0.056      -0.005       0.385\n",
      "==============================================================================\n",
      "Removing feature 39 with p-value 0.399140\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198047\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1539\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1888\n",
      "Time:                        16:37:15   Log-Likelihood:                -310.34\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.151e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3377      0.164    -20.376      0.000      -3.659      -3.017\n",
      "15            -0.2304      0.109     -2.113      0.035      -0.444      -0.017\n",
      "23            -0.1804      0.094     -1.926      0.054      -0.364       0.003\n",
      "29            -0.1924      0.128     -1.499      0.134      -0.444       0.059\n",
      "32            -0.2100      0.151     -1.390      0.165      -0.506       0.086\n",
      "34             0.0725      0.075      0.963      0.336      -0.075       0.220\n",
      "60             0.2440      0.080      3.035      0.002       0.086       0.402\n",
      "91            -0.1772      0.119     -1.495      0.135      -0.410       0.055\n",
      "112           -0.2219      0.116     -1.908      0.056      -0.450       0.006\n",
      "125            0.2634      0.144      1.828      0.067      -0.019       0.546\n",
      "127            0.1357      0.108      1.262      0.207      -0.075       0.347\n",
      "130            0.5425      0.154      3.514      0.000       0.240       0.845\n",
      "134            0.1507      0.152      0.989      0.323      -0.148       0.449\n",
      "176            0.2136      0.116      1.834      0.067      -0.015       0.442\n",
      "182           -0.1706      0.136     -1.256      0.209      -0.437       0.096\n",
      "317           -0.1850      0.134     -1.377      0.169      -0.449       0.078\n",
      "338            0.1028      0.073      1.400      0.162      -0.041       0.247\n",
      "417           -0.3438      0.132     -2.612      0.009      -0.602      -0.086\n",
      "434            0.1397      0.112      1.243      0.214      -0.081       0.360\n",
      "461            0.2761      0.106      2.610      0.009       0.069       0.484\n",
      "485           -0.1747      0.120     -1.455      0.146      -0.410       0.061\n",
      "489           -0.2451      0.116     -2.113      0.035      -0.472      -0.018\n",
      "511            0.1734      0.083      2.079      0.038       0.010       0.337\n",
      "512            0.2261      0.105      2.151      0.031       0.020       0.432\n",
      "520            0.1179      0.086      1.372      0.170      -0.050       0.286\n",
      "548            0.1208      0.119      1.017      0.309      -0.112       0.354\n",
      "563           -0.1334      0.111     -1.204      0.229      -0.351       0.084\n",
      "570            0.1951      0.099      1.974      0.048       0.001       0.389\n",
      "==============================================================================\n",
      "Removing feature 34 with p-value 0.335507\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198316\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1540\n",
      "Method:                           MLE   Df Model:                           26\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1877\n",
      "Time:                        16:37:15   Log-Likelihood:                -310.76\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.053e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3380      0.164    -20.360      0.000      -3.659      -3.017\n",
      "15            -0.2312      0.109     -2.122      0.034      -0.445      -0.018\n",
      "23            -0.1769      0.093     -1.894      0.058      -0.360       0.006\n",
      "29            -0.1968      0.128     -1.538      0.124      -0.448       0.054\n",
      "32            -0.2083      0.151     -1.382      0.167      -0.504       0.087\n",
      "60             0.2510      0.080      3.135      0.002       0.094       0.408\n",
      "91            -0.1772      0.119     -1.494      0.135      -0.410       0.055\n",
      "112           -0.2235      0.116     -1.924      0.054      -0.451       0.004\n",
      "125            0.2820      0.142      1.982      0.047       0.003       0.561\n",
      "127            0.1505      0.105      1.431      0.152      -0.056       0.357\n",
      "130            0.5427      0.154      3.517      0.000       0.240       0.845\n",
      "134            0.1429      0.152      0.940      0.347      -0.155       0.441\n",
      "176            0.2285      0.115      1.986      0.047       0.003       0.454\n",
      "182           -0.1677      0.136     -1.233      0.218      -0.434       0.099\n",
      "317           -0.1896      0.135     -1.409      0.159      -0.453       0.074\n",
      "338            0.1133      0.069      1.653      0.098      -0.021       0.248\n",
      "417           -0.3377      0.131     -2.579      0.010      -0.594      -0.081\n",
      "434            0.1356      0.112      1.208      0.227      -0.084       0.356\n",
      "461            0.2697      0.106      2.551      0.011       0.063       0.477\n",
      "485           -0.1787      0.120     -1.492      0.136      -0.414       0.056\n",
      "489           -0.2502      0.116     -2.164      0.030      -0.477      -0.024\n",
      "511            0.1718      0.083      2.065      0.039       0.009       0.335\n",
      "512            0.2260      0.105      2.153      0.031       0.020       0.432\n",
      "520            0.1202      0.086      1.402      0.161      -0.048       0.288\n",
      "548            0.1160      0.118      0.981      0.327      -0.116       0.348\n",
      "563           -0.1359      0.111     -1.229      0.219      -0.353       0.081\n",
      "570            0.1941      0.099      1.965      0.049       0.001       0.388\n",
      "==============================================================================\n",
      "Removing feature 134 with p-value 0.347029\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198597\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1541\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1866\n",
      "Time:                        16:37:15   Log-Likelihood:                -311.20\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.813e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3269      0.163    -20.468      0.000      -3.645      -3.008\n",
      "15            -0.2362      0.109     -2.165      0.030      -0.450      -0.022\n",
      "23            -0.1709      0.093     -1.834      0.067      -0.354       0.012\n",
      "29            -0.2009      0.128     -1.570      0.116      -0.452       0.050\n",
      "32            -0.2157      0.151     -1.433      0.152      -0.511       0.079\n",
      "60             0.2488      0.080      3.110      0.002       0.092       0.406\n",
      "91            -0.1785      0.118     -1.508      0.131      -0.410       0.053\n",
      "112           -0.2199      0.116     -1.903      0.057      -0.446       0.007\n",
      "125            0.3597      0.116      3.110      0.002       0.133       0.586\n",
      "127            0.1649      0.103      1.600      0.110      -0.037       0.367\n",
      "130            0.5220      0.152      3.427      0.001       0.223       0.820\n",
      "176            0.2329      0.115      2.028      0.043       0.008       0.458\n",
      "182           -0.1752      0.136     -1.291      0.197      -0.441       0.091\n",
      "317           -0.1869      0.134     -1.391      0.164      -0.450       0.076\n",
      "338            0.1081      0.068      1.589      0.112      -0.025       0.242\n",
      "417           -0.3361      0.131     -2.558      0.011      -0.594      -0.079\n",
      "434            0.1389      0.112      1.240      0.215      -0.081       0.358\n",
      "461            0.2740      0.106      2.595      0.009       0.067       0.481\n",
      "485           -0.1761      0.120     -1.473      0.141      -0.411       0.058\n",
      "489           -0.2474      0.115     -2.142      0.032      -0.474      -0.021\n",
      "511            0.1685      0.083      2.032      0.042       0.006       0.331\n",
      "512            0.2205      0.105      2.106      0.035       0.015       0.426\n",
      "520            0.1257      0.085      1.479      0.139      -0.041       0.292\n",
      "548            0.1062      0.117      0.908      0.364      -0.123       0.336\n",
      "563           -0.1399      0.111     -1.266      0.206      -0.357       0.077\n",
      "570            0.1946      0.099      1.958      0.050      -0.000       0.389\n",
      "==============================================================================\n",
      "Removing feature 548 with p-value 0.364140\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198864\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1542\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1855\n",
      "Time:                        16:37:15   Log-Likelihood:                -311.62\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.040e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3210      0.162    -20.497      0.000      -3.639      -3.003\n",
      "15            -0.2333      0.109     -2.134      0.033      -0.448      -0.019\n",
      "23            -0.1731      0.094     -1.848      0.065      -0.357       0.011\n",
      "29            -0.2087      0.128     -1.635      0.102      -0.459       0.041\n",
      "32            -0.2104      0.151     -1.397      0.162      -0.506       0.085\n",
      "60             0.2529      0.080      3.166      0.002       0.096       0.409\n",
      "91            -0.1812      0.119     -1.527      0.127      -0.414       0.051\n",
      "112           -0.2200      0.115     -1.909      0.056      -0.446       0.006\n",
      "125            0.3597      0.115      3.125      0.002       0.134       0.585\n",
      "127            0.1656      0.103      1.602      0.109      -0.037       0.368\n",
      "130            0.5237      0.152      3.455      0.001       0.227       0.821\n",
      "176            0.2336      0.115      2.033      0.042       0.008       0.459\n",
      "182           -0.1899      0.135     -1.407      0.159      -0.455       0.075\n",
      "317           -0.1870      0.134     -1.394      0.163      -0.450       0.076\n",
      "338            0.1071      0.068      1.580      0.114      -0.026       0.240\n",
      "417           -0.3391      0.131     -2.588      0.010      -0.596      -0.082\n",
      "434            0.1356      0.112      1.212      0.225      -0.084       0.355\n",
      "461            0.2781      0.106      2.630      0.009       0.071       0.485\n",
      "485           -0.1724      0.119     -1.445      0.148      -0.406       0.061\n",
      "489           -0.2440      0.115     -2.115      0.034      -0.470      -0.018\n",
      "511            0.1670      0.083      2.011      0.044       0.004       0.330\n",
      "512            0.2223      0.105      2.126      0.033       0.017       0.427\n",
      "520            0.1327      0.085      1.559      0.119      -0.034       0.300\n",
      "563           -0.1450      0.110     -1.313      0.189      -0.361       0.071\n",
      "570            0.1936      0.099      1.957      0.050      -0.000       0.387\n",
      "==============================================================================\n",
      "Removing feature 434 with p-value 0.225450\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199319\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1543\n",
      "Method:                           MLE   Df Model:                           23\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1836\n",
      "Time:                        16:37:15   Log-Likelihood:                -312.33\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.554e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3179      0.162    -20.469      0.000      -3.636      -3.000\n",
      "15            -0.2341      0.109     -2.146      0.032      -0.448      -0.020\n",
      "23            -0.1819      0.094     -1.942      0.052      -0.365       0.002\n",
      "29            -0.2027      0.128     -1.588      0.112      -0.453       0.047\n",
      "32            -0.2623      0.143     -1.835      0.067      -0.542       0.018\n",
      "60             0.2481      0.080      3.120      0.002       0.092       0.404\n",
      "91            -0.1931      0.118     -1.633      0.102      -0.425       0.039\n",
      "112           -0.2334      0.114     -2.043      0.041      -0.457      -0.009\n",
      "125            0.3611      0.115      3.137      0.002       0.135       0.587\n",
      "127            0.1593      0.103      1.552      0.121      -0.042       0.361\n",
      "130            0.5265      0.152      3.466      0.001       0.229       0.824\n",
      "176            0.2341      0.115      2.041      0.041       0.009       0.459\n",
      "182           -0.1863      0.135     -1.384      0.166      -0.450       0.078\n",
      "317           -0.1896      0.134     -1.417      0.156      -0.452       0.073\n",
      "338            0.1055      0.067      1.574      0.116      -0.026       0.237\n",
      "417           -0.3342      0.130     -2.565      0.010      -0.590      -0.079\n",
      "461            0.2773      0.106      2.627      0.009       0.070       0.484\n",
      "485           -0.1739      0.119     -1.464      0.143      -0.407       0.059\n",
      "489           -0.2407      0.115     -2.088      0.037      -0.467      -0.015\n",
      "511            0.1655      0.083      1.992      0.046       0.003       0.328\n",
      "512            0.2182      0.104      2.092      0.036       0.014       0.423\n",
      "520            0.1369      0.085      1.619      0.106      -0.029       0.303\n",
      "563           -0.1428      0.110     -1.293      0.196      -0.359       0.074\n",
      "570            0.1987      0.098      2.020      0.043       0.006       0.391\n",
      "==============================================================================\n",
      "Removing feature 563 with p-value 0.196052\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199851\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1544\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1814\n",
      "Time:                        16:37:15   Log-Likelihood:                -313.17\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.998e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3057      0.161    -20.588      0.000      -3.620      -2.991\n",
      "15            -0.2378      0.109     -2.180      0.029      -0.452      -0.024\n",
      "23            -0.1801      0.093     -1.943      0.052      -0.362       0.002\n",
      "29            -0.2126      0.127     -1.670      0.095      -0.462       0.037\n",
      "32            -0.2611      0.143     -1.829      0.067      -0.541       0.019\n",
      "60             0.2495      0.080      3.138      0.002       0.094       0.405\n",
      "91            -0.1930      0.118     -1.635      0.102      -0.424       0.038\n",
      "112           -0.2385      0.112     -2.139      0.032      -0.457      -0.020\n",
      "125            0.3567      0.115      3.112      0.002       0.132       0.581\n",
      "127            0.1653      0.102      1.619      0.106      -0.035       0.365\n",
      "130            0.5202      0.150      3.466      0.001       0.226       0.814\n",
      "176            0.2342      0.115      2.038      0.042       0.009       0.459\n",
      "182           -0.1940      0.135     -1.439      0.150      -0.458       0.070\n",
      "317           -0.1938      0.133     -1.459      0.144      -0.454       0.066\n",
      "338            0.1082      0.065      1.653      0.098      -0.020       0.237\n",
      "417           -0.3305      0.130     -2.541      0.011      -0.585      -0.076\n",
      "461            0.2863      0.105      2.728      0.006       0.081       0.492\n",
      "485           -0.1710      0.118     -1.445      0.149      -0.403       0.061\n",
      "489           -0.2381      0.115     -2.078      0.038      -0.463      -0.014\n",
      "511            0.1649      0.082      2.000      0.046       0.003       0.326\n",
      "512            0.2119      0.104      2.036      0.042       0.008       0.416\n",
      "520            0.1363      0.085      1.609      0.108      -0.030       0.302\n",
      "570            0.2063      0.098      2.113      0.035       0.015       0.398\n",
      "==============================================================================\n",
      "Removing feature 182 with p-value 0.150101\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.200524\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1545\n",
      "Method:                           MLE   Df Model:                           21\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1787\n",
      "Time:                        16:37:15   Log-Likelihood:                -314.22\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.688e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2901      0.159    -20.693      0.000      -3.602      -2.978\n",
      "15            -0.2353      0.109     -2.158      0.031      -0.449      -0.022\n",
      "23            -0.1780      0.092     -1.930      0.054      -0.359       0.003\n",
      "29            -0.2059      0.127     -1.615      0.106      -0.456       0.044\n",
      "32            -0.2595      0.142     -1.822      0.068      -0.538       0.020\n",
      "60             0.2401      0.079      3.032      0.002       0.085       0.395\n",
      "91            -0.2041      0.117     -1.739      0.082      -0.434       0.026\n",
      "112           -0.2395      0.112     -2.142      0.032      -0.459      -0.020\n",
      "125            0.3675      0.114      3.224      0.001       0.144       0.591\n",
      "127            0.1734      0.102      1.706      0.088      -0.026       0.373\n",
      "130            0.5178      0.150      3.449      0.001       0.224       0.812\n",
      "176            0.2400      0.115      2.085      0.037       0.014       0.466\n",
      "317           -0.2456      0.125     -1.969      0.049      -0.490      -0.001\n",
      "338            0.1069      0.066      1.630      0.103      -0.022       0.236\n",
      "417           -0.3219      0.130     -2.473      0.013      -0.577      -0.067\n",
      "461            0.2472      0.101      2.442      0.015       0.049       0.446\n",
      "485           -0.1769      0.118     -1.499      0.134      -0.408       0.054\n",
      "489           -0.2377      0.115     -2.073      0.038      -0.462      -0.013\n",
      "511            0.1667      0.083      2.019      0.044       0.005       0.329\n",
      "512            0.2157      0.104      2.068      0.039       0.011       0.420\n",
      "520            0.1268      0.086      1.479      0.139      -0.041       0.295\n",
      "570            0.2100      0.098      2.140      0.032       0.018       0.402\n",
      "==============================================================================\n",
      "Removing feature 520 with p-value 0.139179\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.201153\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1546\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1761\n",
      "Time:                        16:37:15   Log-Likelihood:                -315.21\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.018e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2784      0.158    -20.772      0.000      -3.588      -2.969\n",
      "15            -0.2365      0.109     -2.171      0.030      -0.450      -0.023\n",
      "23            -0.1886      0.092     -2.046      0.041      -0.369      -0.008\n",
      "29            -0.2174      0.127     -1.712      0.087      -0.466       0.031\n",
      "32            -0.2572      0.142     -1.807      0.071      -0.536       0.022\n",
      "60             0.2477      0.080      3.102      0.002       0.091       0.404\n",
      "91            -0.1959      0.117     -1.674      0.094      -0.425       0.034\n",
      "112           -0.2388      0.111     -2.150      0.032      -0.457      -0.021\n",
      "125            0.3553      0.113      3.134      0.002       0.133       0.577\n",
      "127            0.1659      0.101      1.636      0.102      -0.033       0.365\n",
      "130            0.5364      0.149      3.597      0.000       0.244       0.829\n",
      "176            0.2427      0.115      2.112      0.035       0.017       0.468\n",
      "317           -0.2484      0.125     -1.984      0.047      -0.494      -0.003\n",
      "338            0.1086      0.066      1.652      0.098      -0.020       0.237\n",
      "417           -0.3228      0.130     -2.481      0.013      -0.578      -0.068\n",
      "461            0.2472      0.101      2.452      0.014       0.050       0.445\n",
      "485           -0.1778      0.118     -1.504      0.133      -0.409       0.054\n",
      "489           -0.2383      0.115     -2.079      0.038      -0.463      -0.014\n",
      "511            0.1755      0.082      2.146      0.032       0.015       0.336\n",
      "512            0.2116      0.104      2.031      0.042       0.007       0.416\n",
      "570            0.2187      0.097      2.253      0.024       0.028       0.409\n",
      "==============================================================================\n",
      "Removing feature 485 with p-value 0.132501\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.201924\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1547\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1729\n",
      "Time:                        16:37:15   Log-Likelihood:                -316.41\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.308e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2670      0.157    -20.839      0.000      -3.574      -2.960\n",
      "15            -0.2404      0.109     -2.215      0.027      -0.453      -0.028\n",
      "23            -0.2034      0.092     -2.216      0.027      -0.383      -0.024\n",
      "29            -0.2123      0.127     -1.669      0.095      -0.462       0.037\n",
      "32            -0.2702      0.142     -1.903      0.057      -0.549       0.008\n",
      "60             0.2475      0.080      3.103      0.002       0.091       0.404\n",
      "91            -0.2043      0.117     -1.744      0.081      -0.434       0.025\n",
      "112           -0.2469      0.112     -2.209      0.027      -0.466      -0.028\n",
      "125            0.3535      0.113      3.115      0.002       0.131       0.576\n",
      "127            0.1660      0.101      1.636      0.102      -0.033       0.365\n",
      "130            0.5201      0.148      3.515      0.000       0.230       0.810\n",
      "176            0.2457      0.115      2.144      0.032       0.021       0.470\n",
      "317           -0.2501      0.125     -1.996      0.046      -0.496      -0.005\n",
      "338            0.1064      0.066      1.604      0.109      -0.024       0.236\n",
      "417           -0.3378      0.130     -2.605      0.009      -0.592      -0.084\n",
      "461            0.2521      0.101      2.500      0.012       0.054       0.450\n",
      "489           -0.2315      0.114     -2.028      0.043      -0.455      -0.008\n",
      "511            0.1712      0.081      2.103      0.035       0.012       0.331\n",
      "512            0.2084      0.104      2.004      0.045       0.005       0.412\n",
      "570            0.2238      0.097      2.318      0.020       0.035       0.413\n",
      "==============================================================================\n",
      "Removing feature 338 with p-value 0.108719\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202678\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1548\n",
      "Method:                           MLE   Df Model:                           18\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1698\n",
      "Time:                        16:37:15   Log-Likelihood:                -317.60\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.416e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2590      0.156    -20.884      0.000      -3.565      -2.953\n",
      "15            -0.2344      0.108     -2.172      0.030      -0.446      -0.023\n",
      "23            -0.2037      0.092     -2.216      0.027      -0.384      -0.024\n",
      "29            -0.1983      0.126     -1.567      0.117      -0.446       0.050\n",
      "32            -0.2633      0.141     -1.869      0.062      -0.539       0.013\n",
      "60             0.2655      0.080      3.309      0.001       0.108       0.423\n",
      "91            -0.1925      0.117     -1.652      0.098      -0.421       0.036\n",
      "112           -0.2464      0.113     -2.183      0.029      -0.468      -0.025\n",
      "125            0.3584      0.113      3.160      0.002       0.136       0.581\n",
      "127            0.1774      0.100      1.768      0.077      -0.019       0.374\n",
      "130            0.5185      0.148      3.502      0.000       0.228       0.809\n",
      "176            0.2449      0.114      2.140      0.032       0.021       0.469\n",
      "317           -0.2265      0.124     -1.822      0.068      -0.470       0.017\n",
      "417           -0.3340      0.129     -2.587      0.010      -0.587      -0.081\n",
      "461            0.2458      0.100      2.447      0.014       0.049       0.443\n",
      "489           -0.2490      0.114     -2.186      0.029      -0.472      -0.026\n",
      "511            0.1790      0.081      2.223      0.026       0.021       0.337\n",
      "512            0.2076      0.104      2.002      0.045       0.004       0.411\n",
      "570            0.2205      0.097      2.276      0.023       0.031       0.410\n",
      "==============================================================================\n",
      "Removing feature 29 with p-value 0.117035\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.203464\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1549\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1666\n",
      "Time:                        16:37:15   Log-Likelihood:                -318.83\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.695e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2393      0.154    -21.024      0.000      -3.541      -2.937\n",
      "15            -0.2426      0.108     -2.241      0.025      -0.455      -0.030\n",
      "23            -0.2346      0.091     -2.587      0.010      -0.412      -0.057\n",
      "32            -0.2295      0.139     -1.654      0.098      -0.502       0.043\n",
      "60             0.2867      0.082      3.495      0.000       0.126       0.448\n",
      "91            -0.2038      0.116     -1.754      0.079      -0.432       0.024\n",
      "112           -0.2269      0.111     -2.036      0.042      -0.445      -0.008\n",
      "125            0.3748      0.113      3.330      0.001       0.154       0.595\n",
      "127            0.1514      0.099      1.534      0.125      -0.042       0.345\n",
      "130            0.5546      0.147      3.784      0.000       0.267       0.842\n",
      "176            0.2372      0.114      2.085      0.037       0.014       0.460\n",
      "317           -0.2668      0.122     -2.183      0.029      -0.506      -0.027\n",
      "417           -0.3247      0.129     -2.521      0.012      -0.577      -0.072\n",
      "461            0.2478      0.100      2.480      0.013       0.052       0.444\n",
      "489           -0.2437      0.113     -2.147      0.032      -0.466      -0.021\n",
      "511            0.1999      0.078      2.548      0.011       0.046       0.354\n",
      "512            0.2080      0.104      2.007      0.045       0.005       0.411\n",
      "570            0.2186      0.096      2.280      0.023       0.031       0.407\n",
      "==============================================================================\n",
      "Removing feature 127 with p-value 0.124974\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.204163\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1550\n",
      "Method:                           MLE   Df Model:                           16\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1638\n",
      "Time:                        16:37:15   Log-Likelihood:                -319.92\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.229e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2248      0.153    -21.123      0.000      -3.524      -2.926\n",
      "15            -0.2497      0.108     -2.313      0.021      -0.461      -0.038\n",
      "23            -0.2336      0.090     -2.584      0.010      -0.411      -0.056\n",
      "32            -0.2418      0.138     -1.752      0.080      -0.512       0.029\n",
      "60             0.2795      0.083      3.376      0.001       0.117       0.442\n",
      "91            -0.2082      0.116     -1.795      0.073      -0.436       0.019\n",
      "112           -0.2315      0.113     -2.048      0.041      -0.453      -0.010\n",
      "125            0.3623      0.112      3.221      0.001       0.142       0.583\n",
      "130            0.5867      0.146      4.014      0.000       0.300       0.873\n",
      "176            0.2435      0.113      2.146      0.032       0.021       0.466\n",
      "317           -0.2675      0.121     -2.205      0.027      -0.505      -0.030\n",
      "417           -0.3106      0.128     -2.420      0.016      -0.562      -0.059\n",
      "461            0.2410      0.100      2.408      0.016       0.045       0.437\n",
      "489           -0.2467      0.114     -2.167      0.030      -0.470      -0.024\n",
      "511            0.2007      0.078      2.574      0.010       0.048       0.353\n",
      "512            0.2060      0.104      1.985      0.047       0.003       0.409\n",
      "570            0.2075      0.095      2.176      0.030       0.021       0.394\n",
      "==============================================================================\n",
      "Removing feature 32 with p-value 0.079752\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.205202\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1551\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1595\n",
      "Time:                        16:37:15   Log-Likelihood:                -321.55\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.596e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2031      0.151    -21.235      0.000      -3.499      -2.907\n",
      "15            -0.2540      0.108     -2.352      0.019      -0.466      -0.042\n",
      "23            -0.2988      0.081     -3.692      0.000      -0.457      -0.140\n",
      "60             0.2652      0.079      3.354      0.001       0.110       0.420\n",
      "91            -0.2118      0.117     -1.816      0.069      -0.440       0.017\n",
      "112           -0.2519      0.117     -2.154      0.031      -0.481      -0.023\n",
      "125            0.3507      0.113      3.107      0.002       0.129       0.572\n",
      "130            0.5705      0.146      3.899      0.000       0.284       0.857\n",
      "176            0.2524      0.113      2.237      0.025       0.031       0.474\n",
      "317           -0.2529      0.120     -2.101      0.036      -0.489      -0.017\n",
      "417           -0.3227      0.129     -2.511      0.012      -0.575      -0.071\n",
      "461            0.2310      0.099      2.344      0.019       0.038       0.424\n",
      "489           -0.2568      0.114     -2.253      0.024      -0.480      -0.033\n",
      "511            0.2012      0.078      2.593      0.010       0.049       0.353\n",
      "512            0.2070      0.104      1.993      0.046       0.003       0.411\n",
      "570            0.2136      0.096      2.236      0.025       0.026       0.401\n",
      "==============================================================================\n",
      "Removing feature 91 with p-value 0.069371\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.206269\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1552\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1551\n",
      "Time:                        16:37:15   Log-Likelihood:                -323.22\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.132e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.1832      0.149    -21.343      0.000      -3.475      -2.891\n",
      "15            -0.2498      0.107     -2.337      0.019      -0.459      -0.040\n",
      "23            -0.2995      0.081     -3.713      0.000      -0.458      -0.141\n",
      "60             0.2744      0.080      3.449      0.001       0.118       0.430\n",
      "112           -0.2588      0.117     -2.219      0.026      -0.487      -0.030\n",
      "125            0.3526      0.113      3.120      0.002       0.131       0.574\n",
      "130            0.5865      0.146      4.004      0.000       0.299       0.874\n",
      "176            0.2415      0.113      2.143      0.032       0.021       0.462\n",
      "317           -0.2475      0.120     -2.056      0.040      -0.483      -0.012\n",
      "417           -0.3231      0.129     -2.512      0.012      -0.575      -0.071\n",
      "461            0.2255      0.097      2.322      0.020       0.035       0.416\n",
      "489           -0.2646      0.114     -2.315      0.021      -0.489      -0.041\n",
      "511            0.1926      0.077      2.496      0.013       0.041       0.344\n",
      "512            0.2100      0.104      2.029      0.042       0.007       0.413\n",
      "570            0.2116      0.095      2.224      0.026       0.025       0.398\n",
      "==============================================================================\n",
      "All remaining features are significant (p < 0.05). Stopping.\n",
      "Final significant features (Variance + SelectKBest): ['15', '23', '60', '112', '125', '130', '176', '317', '417', '461', '489', '511', '512', '570']\n"
     ]
    }
   ],
   "source": [
    "# Iterative feature elimination\n",
    "X_current = X_scaled_reduced.copy()\n",
    "features_to_keep = list(X_scaled_reduced.columns)\n",
    "features_to_keep.remove('const')\n",
    "\n",
    "while True:\n",
    "    logit_model = sm.Logit(Y, X_current)\n",
    "    result = logit_model.fit(method='newton', maxiter=1000)\n",
    "    print(result.summary())\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    p_values = p_values.drop('const')\n",
    "\n",
    "    if (p_values >= 0.05).sum() == 0:\n",
    "        print(\"All remaining features are significant (p < 0.05). Stopping.\")\n",
    "        break\n",
    "\n",
    "    max_p_feature = p_values.idxmax()\n",
    "    max_p_value = p_values[max_p_feature]\n",
    "    print(f\"Removing feature {max_p_feature} with p-value {max_p_value:.6f}\")\n",
    "\n",
    "    X_current = X_current.drop(columns=max_p_feature)\n",
    "    features_to_keep.remove(max_p_feature)\n",
    "\n",
    "print(f\"Final significant features (Variance + SelectKBest): {features_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8b3d2542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with significant features (Variance + SelectKBest): 0.272949\n",
      "\n",
      "Logistic Regression Coefficients (significant features):\n",
      "         15        23        60       112     125       130       176  \\\n",
      "0 -0.100408 -0.001111  0.046704 -0.173871  2.7757  0.497225  1.647711   \n",
      "\n",
      "        317       417      461       489       511      512       570  \n",
      "0 -0.292487 -0.290296  0.01842 -0.000788  0.003863  0.00073  0.026281  \n"
     ]
    }
   ],
   "source": [
    "# Final significant features (hypothesized)\n",
    "X_final_significant = X_indicators[features_to_keep]\n",
    "\n",
    "# Fit logistic regression with class weights\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_final_significant, Y)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "f1_score = cross_val_score(model, X_final_significant, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with significant features (Variance + SelectKBest): {f1_score:.6f}\")\n",
    "\n",
    "# Inspect coefficients\n",
    "coeffs = pd.DataFrame(model.coef_, columns=X_final_significant.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (significant features):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "023ae851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1472\n",
      "Method:                           MLE   Df Model:                           94\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.3502\n",
      "Time:                        16:39:11   Log-Likelihood:                -248.60\n",
      "converged:                      False   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.258e-18\n",
      "===============================================================================\n",
      "                  coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------\n",
      "const          -4.2329        nan        nan        nan         nan         nan\n",
      "15             -0.2527        nan        nan        nan         nan         nan\n",
      "22              0.4364        nan        nan        nan         nan         nan\n",
      "23             -0.0024        nan        nan        nan         nan         nan\n",
      "27             -0.5611        nan        nan        nan         nan         nan\n",
      "29              0.0407        nan        nan        nan         nan         nan\n",
      "33              0.0426        nan        nan        nan         nan         nan\n",
      "34              0.0682        nan        nan        nan         nan         nan\n",
      "39             -0.1522        nan        nan        nan         nan         nan\n",
      "41             -0.1866        nan        nan        nan         nan         nan\n",
      "57              0.8022        nan        nan        nan         nan         nan\n",
      "59              0.1527        nan        nan        nan         nan         nan\n",
      "60              1.1843        nan        nan        nan         nan         nan\n",
      "64              0.7071        nan        nan        nan         nan         nan\n",
      "65              1.5473        nan        nan        nan         nan         nan\n",
      "66             -1.4693        nan        nan        nan         nan         nan\n",
      "69             -0.1142        nan        nan        nan         nan         nan\n",
      "71              0.1694        nan        nan        nan         nan         nan\n",
      "77              0.1257        nan        nan        nan         nan         nan\n",
      "80             -0.2036        nan        nan        nan         nan         nan\n",
      "91             -0.1870        nan        nan        nan         nan         nan\n",
      "96             -0.0526        nan        nan        nan         nan         nan\n",
      "101             0.1405        nan        nan        nan         nan         nan\n",
      "104            -0.0177        nan        nan        nan         nan         nan\n",
      "112            -0.6488        nan        nan        nan         nan         nan\n",
      "113            -0.1362        nan        nan        nan         nan         nan\n",
      "122             1.2071        nan        nan        nan         nan         nan\n",
      "123             0.7230        nan        nan        nan         nan         nan\n",
      "124            -0.9297        nan        nan        nan         nan         nan\n",
      "125             0.0757        nan        nan        nan         nan         nan\n",
      "126            -0.1574        nan        nan        nan         nan         nan\n",
      "127             0.1629        nan        nan        nan         nan         nan\n",
      "128            -0.3368        nan        nan        nan         nan         nan\n",
      "130             0.5510        nan        nan        nan         nan         nan\n",
      "131             0.3405        nan        nan        nan         nan         nan\n",
      "134            -0.1227        nan        nan        nan         nan         nan\n",
      "160             3.0064        nan        nan        nan         nan         nan\n",
      "161            -1.7923        nan        nan        nan         nan         nan\n",
      "164             1.0106        nan        nan        nan         nan         nan\n",
      "165            -5.1311        nan        nan        nan         nan         nan\n",
      "166             2.3082        nan        nan        nan         nan         nan\n",
      "167            -0.4587        nan        nan        nan         nan         nan\n",
      "181             0.8941        nan        nan        nan         nan         nan\n",
      "184            -0.5465        nan        nan        nan         nan         nan\n",
      "197            -1.5191        nan        nan        nan         nan         nan\n",
      "198            10.9765        nan        nan        nan         nan         nan\n",
      "200            -0.8957        nan        nan        nan         nan         nan\n",
      "201             0.1581        nan        nan        nan         nan         nan\n",
      "206             3.5158        nan        nan        nan         nan         nan\n",
      "211            -0.7495        nan        nan        nan         nan         nan\n",
      "248             7.7246        nan        nan        nan         nan         nan\n",
      "281            -0.2428        nan        nan        nan         nan         nan\n",
      "295            -3.2714        nan        nan        nan         nan         nan\n",
      "296             0.1604        nan        nan        nan         nan         nan\n",
      "299             1.4662        nan        nan        nan         nan         nan\n",
      "300             3.7780        nan        nan        nan         nan         nan\n",
      "301            -2.3930        nan        nan        nan         nan         nan\n",
      "317            -0.0411        nan        nan        nan         nan         nan\n",
      "320             0.5114        nan        nan        nan         nan         nan\n",
      "338             0.3932        nan        nan        nan         nan         nan\n",
      "349             0.9635        nan        nan        nan         nan         nan\n",
      "366             0.1547        nan        nan        nan         nan         nan\n",
      "431             0.1272        nan        nan        nan         nan         nan\n",
      "432             3.7540        nan        nan        nan         nan         nan\n",
      "435            -5.5782        nan        nan        nan         nan         nan\n",
      "436             4.2830        nan        nan        nan         nan         nan\n",
      "437            -2.5306        nan        nan        nan         nan         nan\n",
      "438             0.7986        nan        nan        nan         nan         nan\n",
      "453            -1.0548        nan        nan        nan         nan         nan\n",
      "456             0.3557        nan        nan        nan         nan         nan\n",
      "461             0.1968        nan        nan        nan         nan         nan\n",
      "469             0.1630        nan        nan        nan         nan         nan\n",
      "470           -10.4057        nan        nan        nan         nan         nan\n",
      "472             1.1993        nan        nan        nan         nan         nan\n",
      "478            -3.6142        nan        nan        nan         nan         nan\n",
      "511             0.0961        nan        nan        nan         nan         nan\n",
      "512             0.2720        nan        nan        nan         nan         nan\n",
      "520            -8.2215        nan        nan        nan         nan         nan\n",
      "543             0.3712        nan        nan        nan         nan         nan\n",
      "544            -0.1437        nan        nan        nan         nan         nan\n",
      "548             0.0155        nan        nan        nan         nan         nan\n",
      "551             4.5354        nan        nan        nan         nan         nan\n",
      "552            -7.2908        nan        nan        nan         nan         nan\n",
      "554             0.2887        nan        nan        nan         nan         nan\n",
      "555             1.8315        nan        nan        nan         nan         nan\n",
      "557            -5.3732        nan        nan        nan         nan         nan\n",
      "558             6.1571        nan        nan        nan         nan         nan\n",
      "563            -0.3199        nan        nan        nan         nan         nan\n",
      "566            -1.2997        nan        nan        nan         nan         nan\n",
      "568             0.6996        nan        nan        nan         nan         nan\n",
      "570             0.6784        nan        nan        nan         nan         nan\n",
      "574            -0.2660        nan        nan        nan         nan         nan\n",
      "576            -0.2125        nan        nan        nan         nan         nan\n",
      "missing_73     -0.0201        nan        nan        nan         nan         nan\n",
      "missing_74     -0.0201        nan        nan        nan         nan         nan\n",
      "missing_113     0.0564        nan        nan        nan         nan         nan\n",
      "missing_248     0.0564        nan        nan        nan         nan         nan\n",
      "missing_346    -0.0201        nan        nan        nan         nan         nan\n",
      "missing_347    -0.0201        nan        nan        nan         nan         nan\n",
      "missing_386     0.0564        nan        nan        nan         nan         nan\n",
      "missing_520     0.0564        nan        nan        nan         nan         nan\n",
      "===============================================================================\n",
      "Significant features (p < 0.05): []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:595: HessianInversionWarning: Inverting hessian failed, no bse or cov_params available\n",
      "  warnings.warn('Inverting hessian failed, no bse or cov_params '\n",
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# Subset to SelectKBest features\n",
    "X_kbest = X_indicators[selected_features_k]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_kbest_scaled = scaler.fit_transform(X_kbest)\n",
    "X_kbest_scaled = pd.DataFrame(X_kbest_scaled, columns=X_kbest.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_kbest_scaled = sm.add_constant(X_kbest_scaled)\n",
    "\n",
    "# Fit logistic regression with 'lbfgs'\n",
    "logit_model = sm.Logit(Y, X_kbest_scaled)\n",
    "result = logit_model.fit(method='lbfgs', maxiter=500)\n",
    "print(result.summary())\n",
    "\n",
    "# Extract significant features\n",
    "p_values = result.pvalues\n",
    "significant_features = p_values[p_values < 0.05].index.tolist()\n",
    "if 'const' in significant_features:\n",
    "    significant_features.remove('const')\n",
    "print(f\"Significant features (p < 0.05): {significant_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "da43972b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rcc_0\\anaconda3\\Lib\\site-packages\\statsmodels\\stats\\outliers_influence.py:197: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  vif = 1. / (1. - r_squared_i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 57 features due to high VIF: ['27', '71', '122', '123', '124', '128', '160', '161', '164', '165', '166', '167', '181', '184', '197', '198', '200', '206', '211', '248', '295', '296', '299', '300', '301', '320', '349', '431', '432', '435', '436', '437', '438', '453', '456', '470', '472', '478', '520', '551', '552', '554', '555', '557', '558', '566', '568', '574', '576', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520']\n"
     ]
    }
   ],
   "source": [
    "X_selected = X_indicators[selected_features_k]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_selected)\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=X_selected.columns)\n",
    "\n",
    "# Add intercept\n",
    "X_scaled = sm.add_constant(X_scaled)\n",
    "\n",
    "# Compute VIF to reduce multicollinearity\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_scaled.columns[1:]  # Exclude 'const'\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_scaled.values, i) for i in range(1, X_scaled.shape[1])]\n",
    "\n",
    "# Remove features with high VIF (> 10)\n",
    "high_vif_features = vif_data[vif_data[\"VIF\"] > 10][\"Feature\"]\n",
    "X_scaled_reduced = X_scaled.drop(columns=high_vif_features)\n",
    "print(f\"Removed {len(high_vif_features)} features due to high VIF: {list(high_vif_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3cc6ea7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192792\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1523\n",
      "Method:                           MLE   Df Model:                           43\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2103\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.10\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.582e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4332      0.175    -19.602      0.000      -3.777      -3.090\n",
      "15            -0.2445      0.113     -2.172      0.030      -0.465      -0.024\n",
      "22             0.2748      0.170      1.614      0.107      -0.059       0.609\n",
      "23            -0.0851      0.170     -0.501      0.616      -0.418       0.248\n",
      "29            -0.0294      0.146     -0.202      0.840      -0.315       0.256\n",
      "33            -0.0377      0.190     -0.199      0.842      -0.410       0.334\n",
      "34             0.1228      0.161      0.762      0.446      -0.193       0.439\n",
      "39            -0.2062      0.101     -2.039      0.041      -0.404      -0.008\n",
      "41            -0.0216      0.111     -0.194      0.846      -0.240       0.197\n",
      "57             0.1978      0.170      1.164      0.244      -0.135       0.531\n",
      "59             0.1710      0.155      1.100      0.271      -0.134       0.476\n",
      "60             0.4973      0.149      3.343      0.001       0.206       0.789\n",
      "64            -0.3579      0.135     -2.646      0.008      -0.623      -0.093\n",
      "65             0.8998      0.320      2.813      0.005       0.273       1.527\n",
      "66            -0.5518      0.374     -1.477      0.140      -1.284       0.181\n",
      "69            -0.0164      0.130     -0.127      0.899      -0.270       0.237\n",
      "77            -0.0426      0.130     -0.327      0.744      -0.298       0.213\n",
      "80            -0.1628      0.126     -1.290      0.197      -0.410       0.085\n",
      "91            -0.2384      0.121     -1.963      0.050      -0.477      -0.000\n",
      "96             0.0318      0.118      0.269      0.788      -0.200       0.263\n",
      "101            0.1474      0.135      1.090      0.276      -0.118       0.412\n",
      "104            0.0982      0.159      0.618      0.537      -0.213       0.410\n",
      "112           -0.3299      0.130     -2.531      0.011      -0.585      -0.074\n",
      "113            0.2193      0.217      1.009      0.313      -0.207       0.645\n",
      "125            0.2767      0.164      1.690      0.091      -0.044       0.598\n",
      "126            0.1201      0.170      0.706      0.480      -0.213       0.453\n",
      "127            0.0851      0.123      0.691      0.489      -0.156       0.326\n",
      "130            0.5806      0.169      3.434      0.001       0.249       0.912\n",
      "131            0.2592      0.172      1.510      0.131      -0.077       0.596\n",
      "134            0.0791      0.176      0.450      0.653      -0.265       0.424\n",
      "201            0.0975      0.157      0.621      0.535      -0.210       0.405\n",
      "281           -0.3126      0.145     -2.155      0.031      -0.597      -0.028\n",
      "317           -0.1961      0.140     -1.401      0.161      -0.470       0.078\n",
      "338            0.2136      0.183      1.167      0.243      -0.145       0.572\n",
      "366            0.1360      0.096      1.410      0.158      -0.053       0.325\n",
      "461            0.1745      0.111      1.566      0.117      -0.044       0.393\n",
      "469           -0.0251      0.131     -0.192      0.848      -0.282       0.232\n",
      "511            0.0597      0.105      0.569      0.570      -0.146       0.266\n",
      "512            0.2655      0.108      2.457      0.014       0.054       0.477\n",
      "543            0.1492      0.163      0.916      0.360      -0.170       0.468\n",
      "544           -0.1034      0.145     -0.712      0.477      -0.388       0.181\n",
      "548            0.1379      0.124      1.115      0.265      -0.105       0.380\n",
      "563           -0.1774      0.123     -1.444      0.149      -0.418       0.063\n",
      "570            0.1957      0.099      1.971      0.049       0.001       0.390\n",
      "==============================================================================\n",
      "Removing feature 69 with p-value 0.899004\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192797\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1524\n",
      "Method:                           MLE   Df Model:                           42\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2103\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.11\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.017e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4343      0.175    -19.615      0.000      -3.777      -3.091\n",
      "15            -0.2441      0.113     -2.169      0.030      -0.465      -0.024\n",
      "22             0.2741      0.170      1.611      0.107      -0.059       0.608\n",
      "23            -0.0852      0.170     -0.502      0.616      -0.418       0.248\n",
      "29            -0.0306      0.145     -0.210      0.833      -0.315       0.254\n",
      "33            -0.0376      0.190     -0.198      0.843      -0.410       0.335\n",
      "34             0.1218      0.161      0.756      0.449      -0.194       0.437\n",
      "39            -0.2067      0.101     -2.047      0.041      -0.405      -0.009\n",
      "41            -0.0204      0.111     -0.184      0.854      -0.238       0.197\n",
      "57             0.1984      0.170      1.168      0.243      -0.135       0.531\n",
      "59             0.1675      0.153      1.095      0.274      -0.132       0.467\n",
      "60             0.4985      0.148      3.359      0.001       0.208       0.789\n",
      "64            -0.3526      0.129     -2.741      0.006      -0.605      -0.101\n",
      "65             0.8927      0.314      2.840      0.005       0.277       1.509\n",
      "66            -0.5473      0.371     -1.474      0.140      -1.275       0.180\n",
      "77            -0.0432      0.130     -0.331      0.740      -0.298       0.212\n",
      "80            -0.1615      0.126     -1.284      0.199      -0.408       0.085\n",
      "91            -0.2376      0.121     -1.958      0.050      -0.476       0.000\n",
      "96             0.0315      0.118      0.267      0.789      -0.200       0.263\n",
      "101            0.1481      0.135      1.097      0.273      -0.116       0.413\n",
      "104            0.0980      0.159      0.617      0.537      -0.213       0.409\n",
      "112           -0.3308      0.130     -2.539      0.011      -0.586      -0.075\n",
      "113            0.2188      0.217      1.007      0.314      -0.207       0.645\n",
      "125            0.2783      0.163      1.704      0.088      -0.042       0.598\n",
      "126            0.1201      0.170      0.706      0.480      -0.213       0.453\n",
      "127            0.0864      0.123      0.705      0.481      -0.154       0.327\n",
      "130            0.5807      0.169      3.435      0.001       0.249       0.912\n",
      "131            0.2591      0.172      1.509      0.131      -0.077       0.596\n",
      "134            0.0787      0.176      0.448      0.654      -0.266       0.423\n",
      "201            0.0975      0.157      0.621      0.534      -0.210       0.405\n",
      "281           -0.3140      0.145     -2.169      0.030      -0.598      -0.030\n",
      "317           -0.1976      0.140     -1.416      0.157      -0.471       0.076\n",
      "338            0.2135      0.183      1.170      0.242      -0.144       0.571\n",
      "366            0.1363      0.096      1.414      0.157      -0.053       0.325\n",
      "461            0.1759      0.111      1.589      0.112      -0.041       0.393\n",
      "469           -0.0248      0.131     -0.189      0.850      -0.282       0.232\n",
      "511            0.0598      0.105      0.570      0.569      -0.146       0.266\n",
      "512            0.2651      0.108      2.455      0.014       0.053       0.477\n",
      "543            0.1496      0.163      0.919      0.358      -0.169       0.469\n",
      "544           -0.1029      0.145     -0.709      0.478      -0.387       0.182\n",
      "548            0.1373      0.124      1.111      0.267      -0.105       0.379\n",
      "563           -0.1781      0.123     -1.451      0.147      -0.419       0.062\n",
      "570            0.1956      0.099      1.970      0.049       0.001       0.390\n",
      "==============================================================================\n",
      "Removing feature 41 with p-value 0.854081\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192808\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1525\n",
      "Method:                           MLE   Df Model:                           41\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2103\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.13\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.042e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4346      0.175    -19.615      0.000      -3.778      -3.091\n",
      "15            -0.2440      0.113     -2.169      0.030      -0.465      -0.024\n",
      "22             0.2761      0.170      1.625      0.104      -0.057       0.609\n",
      "23            -0.0842      0.170     -0.496      0.620      -0.417       0.249\n",
      "29            -0.0345      0.144     -0.240      0.810      -0.316       0.247\n",
      "33            -0.0350      0.189     -0.185      0.853      -0.406       0.336\n",
      "34             0.1192      0.160      0.743      0.457      -0.195       0.434\n",
      "39            -0.2058      0.101     -2.040      0.041      -0.404      -0.008\n",
      "57             0.1968      0.170      1.161      0.246      -0.136       0.529\n",
      "59             0.1673      0.153      1.093      0.274      -0.133       0.467\n",
      "60             0.4992      0.148      3.365      0.001       0.208       0.790\n",
      "64            -0.3513      0.128     -2.737      0.006      -0.603      -0.100\n",
      "65             0.8948      0.314      2.848      0.004       0.279       1.511\n",
      "66            -0.5515      0.371     -1.488      0.137      -1.278       0.175\n",
      "77            -0.0449      0.130     -0.346      0.730      -0.300       0.210\n",
      "80            -0.1627      0.126     -1.296      0.195      -0.409       0.083\n",
      "91            -0.2381      0.121     -1.962      0.050      -0.476      -0.000\n",
      "96             0.0308      0.118      0.261      0.794      -0.200       0.262\n",
      "101            0.1484      0.135      1.100      0.271      -0.116       0.413\n",
      "104            0.0961      0.159      0.606      0.545      -0.215       0.407\n",
      "112           -0.3308      0.130     -2.541      0.011      -0.586      -0.076\n",
      "113            0.2184      0.217      1.006      0.315      -0.207       0.644\n",
      "125            0.2779      0.163      1.702      0.089      -0.042       0.598\n",
      "126            0.1186      0.170      0.698      0.485      -0.214       0.452\n",
      "127            0.0873      0.122      0.713      0.476      -0.153       0.327\n",
      "130            0.5801      0.169      3.432      0.001       0.249       0.911\n",
      "131            0.2569      0.171      1.500      0.134      -0.079       0.593\n",
      "134            0.0803      0.176      0.458      0.647      -0.264       0.424\n",
      "201            0.0978      0.157      0.623      0.533      -0.210       0.406\n",
      "281           -0.3145      0.145     -2.175      0.030      -0.598      -0.031\n",
      "317           -0.2002      0.139     -1.441      0.149      -0.472       0.072\n",
      "338            0.2142      0.183      1.172      0.241      -0.144       0.572\n",
      "366            0.1367      0.096      1.419      0.156      -0.052       0.326\n",
      "461            0.1775      0.110      1.608      0.108      -0.039       0.394\n",
      "469           -0.0250      0.131     -0.191      0.849      -0.282       0.232\n",
      "511            0.0629      0.104      0.607      0.544      -0.140       0.266\n",
      "512            0.2659      0.108      2.464      0.014       0.054       0.477\n",
      "543            0.1508      0.163      0.928      0.354      -0.168       0.469\n",
      "544           -0.1020      0.145     -0.702      0.483      -0.387       0.183\n",
      "548            0.1395      0.123      1.134      0.257      -0.101       0.380\n",
      "563           -0.1780      0.123     -1.450      0.147      -0.419       0.063\n",
      "570            0.1968      0.099      1.985      0.047       0.003       0.391\n",
      "==============================================================================\n",
      "Removing feature 33 with p-value 0.853384\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192819\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1526\n",
      "Method:                           MLE   Df Model:                           40\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2102\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.15\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.013e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4336      0.175    -19.631      0.000      -3.776      -3.091\n",
      "15            -0.2447      0.113     -2.174      0.030      -0.465      -0.024\n",
      "22             0.2761      0.170      1.626      0.104      -0.057       0.609\n",
      "23            -0.0848      0.170     -0.500      0.617      -0.417       0.248\n",
      "29            -0.0331      0.144     -0.231      0.818      -0.314       0.248\n",
      "34             0.0933      0.078      1.195      0.232      -0.060       0.246\n",
      "39            -0.2052      0.101     -2.033      0.042      -0.403      -0.007\n",
      "57             0.1948      0.169      1.152      0.249      -0.137       0.526\n",
      "59             0.1684      0.153      1.101      0.271      -0.131       0.468\n",
      "60             0.5019      0.148      3.401      0.001       0.213       0.791\n",
      "64            -0.3527      0.128     -2.753      0.006      -0.604      -0.102\n",
      "65             0.8947      0.314      2.852      0.004       0.280       1.510\n",
      "66            -0.5484      0.370     -1.484      0.138      -1.273       0.176\n",
      "77            -0.0444      0.130     -0.342      0.732      -0.299       0.210\n",
      "80            -0.1620      0.126     -1.291      0.197      -0.408       0.084\n",
      "91            -0.2385      0.121     -1.964      0.049      -0.477      -0.001\n",
      "96             0.0316      0.118      0.269      0.788      -0.199       0.262\n",
      "101            0.1471      0.135      1.092      0.275      -0.117       0.411\n",
      "104            0.0965      0.158      0.609      0.542      -0.214       0.407\n",
      "112           -0.3310      0.130     -2.542      0.011      -0.586      -0.076\n",
      "113            0.2194      0.217      1.012      0.312      -0.206       0.644\n",
      "125            0.2777      0.163      1.700      0.089      -0.043       0.598\n",
      "126            0.1170      0.170      0.690      0.490      -0.215       0.449\n",
      "127            0.0868      0.122      0.709      0.478      -0.153       0.327\n",
      "130            0.5786      0.169      3.430      0.001       0.248       0.909\n",
      "131            0.2557      0.171      1.495      0.135      -0.079       0.591\n",
      "134            0.0797      0.176      0.454      0.650      -0.265       0.424\n",
      "201            0.0970      0.157      0.619      0.536      -0.210       0.404\n",
      "281           -0.3149      0.145     -2.177      0.029      -0.598      -0.031\n",
      "317           -0.2000      0.139     -1.438      0.151      -0.473       0.073\n",
      "338            0.2141      0.182      1.175      0.240      -0.143       0.571\n",
      "366            0.1382      0.096      1.441      0.150      -0.050       0.326\n",
      "461            0.1758      0.110      1.597      0.110      -0.040       0.392\n",
      "469           -0.0240      0.131     -0.183      0.855      -0.281       0.233\n",
      "511            0.0615      0.103      0.595      0.552      -0.141       0.264\n",
      "512            0.2662      0.108      2.467      0.014       0.055       0.478\n",
      "543            0.1514      0.163      0.931      0.352      -0.167       0.470\n",
      "544           -0.1008      0.145     -0.694      0.488      -0.385       0.184\n",
      "548            0.1406      0.123      1.146      0.252      -0.100       0.381\n",
      "563           -0.1784      0.123     -1.454      0.146      -0.419       0.062\n",
      "570            0.1974      0.099      1.993      0.046       0.003       0.391\n",
      "==============================================================================\n",
      "Removing feature 469 with p-value 0.854865\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192829\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1527\n",
      "Method:                           MLE   Df Model:                           39\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2102\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.16\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 9.901e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4325      0.175    -19.649      0.000      -3.775      -3.090\n",
      "15            -0.2441      0.113     -2.169      0.030      -0.465      -0.024\n",
      "22             0.2767      0.170      1.629      0.103      -0.056       0.610\n",
      "23            -0.0841      0.170     -0.496      0.620      -0.417       0.248\n",
      "29            -0.0347      0.143     -0.242      0.808      -0.315       0.246\n",
      "34             0.0937      0.078      1.200      0.230      -0.059       0.247\n",
      "39            -0.2050      0.101     -2.031      0.042      -0.403      -0.007\n",
      "57             0.1925      0.169      1.141      0.254      -0.138       0.523\n",
      "59             0.1704      0.153      1.116      0.264      -0.129       0.469\n",
      "60             0.5062      0.146      3.473      0.001       0.221       0.792\n",
      "64            -0.3550      0.128     -2.784      0.005      -0.605      -0.105\n",
      "65             0.8919      0.314      2.844      0.004       0.277       1.507\n",
      "66            -0.5428      0.369     -1.472      0.141      -1.265       0.180\n",
      "77            -0.0444      0.130     -0.342      0.733      -0.299       0.210\n",
      "80            -0.1623      0.126     -1.292      0.196      -0.408       0.084\n",
      "91            -0.2382      0.121     -1.963      0.050      -0.476      -0.000\n",
      "96             0.0331      0.117      0.282      0.778      -0.197       0.263\n",
      "101            0.1454      0.134      1.082      0.279      -0.118       0.409\n",
      "104            0.0988      0.158      0.626      0.532      -0.211       0.408\n",
      "112           -0.3303      0.130     -2.539      0.011      -0.585      -0.075\n",
      "113            0.2204      0.217      1.018      0.309      -0.204       0.645\n",
      "125            0.2780      0.163      1.702      0.089      -0.042       0.598\n",
      "126            0.1177      0.170      0.694      0.488      -0.215       0.450\n",
      "127            0.0877      0.122      0.717      0.474      -0.152       0.328\n",
      "130            0.5789      0.169      3.434      0.001       0.248       0.909\n",
      "131            0.2549      0.171      1.491      0.136      -0.080       0.590\n",
      "134            0.0800      0.176      0.456      0.649      -0.264       0.424\n",
      "201            0.0959      0.157      0.612      0.541      -0.211       0.403\n",
      "281           -0.3157      0.145     -2.180      0.029      -0.600      -0.032\n",
      "317           -0.2013      0.139     -1.448      0.148      -0.474       0.071\n",
      "338            0.2096      0.182      1.154      0.249      -0.146       0.566\n",
      "366            0.1374      0.096      1.433      0.152      -0.051       0.325\n",
      "461            0.1769      0.110      1.610      0.107      -0.038       0.392\n",
      "511            0.0610      0.103      0.590      0.555      -0.141       0.263\n",
      "512            0.2661      0.108      2.466      0.014       0.055       0.478\n",
      "543            0.1506      0.163      0.927      0.354      -0.168       0.469\n",
      "544           -0.1002      0.145     -0.690      0.490      -0.385       0.184\n",
      "548            0.1406      0.123      1.146      0.252      -0.100       0.381\n",
      "563           -0.1778      0.123     -1.450      0.147      -0.418       0.063\n",
      "570            0.1975      0.099      1.993      0.046       0.003       0.392\n",
      "==============================================================================\n",
      "Removing feature 29 with p-value 0.808422\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192848\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1528\n",
      "Method:                           MLE   Df Model:                           38\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2101\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.854e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4304      0.174    -19.688      0.000      -3.772      -3.089\n",
      "15            -0.2435      0.112     -2.166      0.030      -0.464      -0.023\n",
      "22             0.2800      0.169      1.654      0.098      -0.052       0.612\n",
      "23            -0.0839      0.170     -0.494      0.621      -0.416       0.249\n",
      "34             0.0939      0.078      1.204      0.229      -0.059       0.247\n",
      "39            -0.2037      0.101     -2.015      0.044      -0.402      -0.006\n",
      "57             0.1929      0.169      1.144      0.253      -0.138       0.523\n",
      "59             0.1721      0.152      1.129      0.259      -0.127       0.471\n",
      "60             0.5122      0.144      3.562      0.000       0.230       0.794\n",
      "64            -0.3592      0.126     -2.841      0.005      -0.607      -0.111\n",
      "65             0.8949      0.313      2.858      0.004       0.281       1.508\n",
      "66            -0.5376      0.367     -1.463      0.144      -1.258       0.183\n",
      "77            -0.0447      0.130     -0.344      0.731      -0.299       0.210\n",
      "80            -0.1608      0.125     -1.281      0.200      -0.407       0.085\n",
      "91            -0.2391      0.121     -1.972      0.049      -0.477      -0.002\n",
      "96             0.0339      0.117      0.289      0.772      -0.196       0.264\n",
      "101            0.1407      0.133      1.057      0.290      -0.120       0.402\n",
      "104            0.1045      0.156      0.669      0.503      -0.202       0.411\n",
      "112           -0.3275      0.129     -2.537      0.011      -0.580      -0.074\n",
      "113            0.2175      0.216      1.006      0.314      -0.206       0.641\n",
      "125            0.2833      0.162      1.750      0.080      -0.034       0.601\n",
      "126            0.1139      0.169      0.674      0.500      -0.217       0.445\n",
      "127            0.0836      0.121      0.689      0.491      -0.154       0.321\n",
      "130            0.5822      0.168      3.461      0.001       0.253       0.912\n",
      "131            0.2525      0.171      1.480      0.139      -0.082       0.587\n",
      "134            0.0780      0.175      0.445      0.657      -0.266       0.422\n",
      "201            0.0953      0.157      0.608      0.543      -0.212       0.402\n",
      "281           -0.3168      0.145     -2.185      0.029      -0.601      -0.033\n",
      "317           -0.2033      0.139     -1.463      0.143      -0.476       0.069\n",
      "338            0.2064      0.181      1.140      0.254      -0.148       0.561\n",
      "366            0.1377      0.096      1.437      0.151      -0.050       0.326\n",
      "461            0.1772      0.110      1.614      0.107      -0.038       0.392\n",
      "511            0.0607      0.103      0.588      0.557      -0.142       0.263\n",
      "512            0.2673      0.108      2.479      0.013       0.056       0.479\n",
      "543            0.1488      0.162      0.919      0.358      -0.169       0.466\n",
      "544           -0.1013      0.145     -0.700      0.484      -0.385       0.182\n",
      "548            0.1417      0.123      1.155      0.248      -0.099       0.382\n",
      "563           -0.1781      0.123     -1.453      0.146      -0.418       0.062\n",
      "570            0.1964      0.099      1.986      0.047       0.003       0.390\n",
      "==============================================================================\n",
      "Removing feature 96 with p-value 0.772223\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192875\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1529\n",
      "Method:                           MLE   Df Model:                           37\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2100\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.24\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.372e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4311      0.174    -19.682      0.000      -3.773      -3.089\n",
      "15            -0.2444      0.113     -2.172      0.030      -0.465      -0.024\n",
      "22             0.2766      0.169      1.640      0.101      -0.054       0.607\n",
      "23            -0.0874      0.169     -0.517      0.605      -0.419       0.244\n",
      "34             0.0931      0.078      1.193      0.233      -0.060       0.246\n",
      "39            -0.2033      0.101     -2.008      0.045      -0.402      -0.005\n",
      "57             0.1953      0.168      1.159      0.246      -0.135       0.525\n",
      "59             0.1705      0.152      1.119      0.263      -0.128       0.469\n",
      "60             0.5159      0.143      3.601      0.000       0.235       0.797\n",
      "64            -0.3603      0.126     -2.852      0.004      -0.608      -0.113\n",
      "65             0.8967      0.313      2.861      0.004       0.282       1.511\n",
      "66            -0.5356      0.368     -1.456      0.145      -1.257       0.186\n",
      "77            -0.0425      0.130     -0.327      0.743      -0.297       0.212\n",
      "80            -0.1596      0.126     -1.272      0.203      -0.406       0.086\n",
      "91            -0.2378      0.121     -1.963      0.050      -0.475      -0.000\n",
      "101            0.1397      0.133      1.050      0.294      -0.121       0.401\n",
      "104            0.1133      0.154      0.737      0.461      -0.188       0.415\n",
      "112           -0.3300      0.129     -2.561      0.010      -0.583      -0.077\n",
      "113            0.2161      0.216      1.000      0.317      -0.207       0.640\n",
      "125            0.2787      0.161      1.729      0.084      -0.037       0.595\n",
      "126            0.1120      0.169      0.663      0.508      -0.219       0.443\n",
      "127            0.0828      0.121      0.682      0.495      -0.155       0.321\n",
      "130            0.5828      0.168      3.466      0.001       0.253       0.912\n",
      "131            0.2553      0.170      1.499      0.134      -0.078       0.589\n",
      "134            0.0813      0.175      0.465      0.642      -0.262       0.424\n",
      "201            0.0918      0.156      0.587      0.557      -0.215       0.398\n",
      "281           -0.3167      0.145     -2.188      0.029      -0.600      -0.033\n",
      "317           -0.2018      0.139     -1.455      0.146      -0.474       0.070\n",
      "338            0.2082      0.181      1.147      0.251      -0.147       0.564\n",
      "366            0.1366      0.095      1.430      0.153      -0.051       0.324\n",
      "461            0.1771      0.110      1.616      0.106      -0.038       0.392\n",
      "511            0.0635      0.103      0.618      0.537      -0.138       0.265\n",
      "512            0.2667      0.108      2.474      0.013       0.055       0.478\n",
      "543            0.1487      0.162      0.917      0.359      -0.169       0.466\n",
      "544           -0.0997      0.144     -0.691      0.489      -0.382       0.183\n",
      "548            0.1435      0.123      1.170      0.242      -0.097       0.384\n",
      "563           -0.1768      0.122     -1.443      0.149      -0.417       0.063\n",
      "570            0.1963      0.099      1.986      0.047       0.003       0.390\n",
      "==============================================================================\n",
      "Removing feature 77 with p-value 0.743418\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192909\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1530\n",
      "Method:                           MLE   Df Model:                           36\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2099\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.29\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.154e-17\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4277      0.174    -19.738      0.000      -3.768      -3.087\n",
      "15            -0.2389      0.111     -2.149      0.032      -0.457      -0.021\n",
      "22             0.2740      0.168      1.626      0.104      -0.056       0.604\n",
      "23            -0.0886      0.169     -0.524      0.600      -0.420       0.243\n",
      "34             0.0945      0.078      1.213      0.225      -0.058       0.247\n",
      "39            -0.2001      0.101     -1.988      0.047      -0.397      -0.003\n",
      "57             0.1922      0.168      1.143      0.253      -0.137       0.522\n",
      "59             0.1667      0.152      1.097      0.273      -0.131       0.464\n",
      "60             0.5259      0.140      3.759      0.000       0.252       0.800\n",
      "64            -0.3682      0.124     -2.969      0.003      -0.611      -0.125\n",
      "65             0.8971      0.313      2.867      0.004       0.284       1.510\n",
      "66            -0.5296      0.367     -1.444      0.149      -1.248       0.189\n",
      "80            -0.1547      0.125     -1.241      0.215      -0.399       0.090\n",
      "91            -0.2390      0.121     -1.971      0.049      -0.477      -0.001\n",
      "101            0.1403      0.133      1.056      0.291      -0.120       0.401\n",
      "104            0.1125      0.154      0.733      0.464      -0.189       0.414\n",
      "112           -0.3286      0.128     -2.560      0.010      -0.580      -0.077\n",
      "113            0.2157      0.216      0.997      0.319      -0.208       0.640\n",
      "125            0.2856      0.160      1.786      0.074      -0.028       0.599\n",
      "126            0.1077      0.168      0.641      0.522      -0.222       0.437\n",
      "127            0.0853      0.121      0.703      0.482      -0.153       0.323\n",
      "130            0.5744      0.166      3.456      0.001       0.249       0.900\n",
      "131            0.2591      0.170      1.527      0.127      -0.074       0.592\n",
      "134            0.0742      0.174      0.428      0.669      -0.266       0.414\n",
      "201            0.0890      0.156      0.570      0.568      -0.217       0.395\n",
      "281           -0.3073      0.141     -2.177      0.029      -0.584      -0.031\n",
      "317           -0.2064      0.138     -1.494      0.135      -0.477       0.064\n",
      "338            0.2097      0.180      1.166      0.244      -0.143       0.562\n",
      "366            0.1373      0.096      1.437      0.151      -0.050       0.325\n",
      "461            0.1768      0.110      1.613      0.107      -0.038       0.392\n",
      "511            0.0613      0.102      0.598      0.550      -0.140       0.262\n",
      "512            0.2658      0.108      2.468      0.014       0.055       0.477\n",
      "543            0.1429      0.161      0.888      0.375      -0.173       0.458\n",
      "544           -0.0994      0.144     -0.690      0.490      -0.382       0.183\n",
      "548            0.1427      0.122      1.165      0.244      -0.097       0.383\n",
      "563           -0.1758      0.122     -1.436      0.151      -0.416       0.064\n",
      "570            0.1984      0.099      2.012      0.044       0.005       0.392\n",
      "==============================================================================\n",
      "Removing feature 134 with p-value 0.668888\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.192967\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1531\n",
      "Method:                           MLE   Df Model:                           35\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2096\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.38\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.710e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4246      0.173    -19.773      0.000      -3.764      -3.085\n",
      "15            -0.2390      0.111     -2.149      0.032      -0.457      -0.021\n",
      "22             0.2721      0.168      1.616      0.106      -0.058       0.602\n",
      "23            -0.0871      0.169     -0.515      0.606      -0.418       0.244\n",
      "34             0.0915      0.078      1.180      0.238      -0.060       0.244\n",
      "39            -0.1958      0.100     -1.955      0.051      -0.392       0.000\n",
      "57             0.1925      0.168      1.146      0.252      -0.137       0.522\n",
      "59             0.1764      0.151      1.172      0.241      -0.119       0.471\n",
      "60             0.5233      0.140      3.746      0.000       0.249       0.797\n",
      "64            -0.3686      0.124     -2.975      0.003      -0.612      -0.126\n",
      "65             0.9024      0.314      2.878      0.004       0.288       1.517\n",
      "66            -0.5317      0.368     -1.445      0.148      -1.253       0.190\n",
      "80            -0.1497      0.124     -1.206      0.228      -0.393       0.094\n",
      "91            -0.2411      0.121     -1.991      0.047      -0.479      -0.004\n",
      "101            0.1370      0.133      1.033      0.302      -0.123       0.397\n",
      "104            0.1153      0.154      0.751      0.453      -0.186       0.416\n",
      "112           -0.3299      0.128     -2.571      0.010      -0.581      -0.078\n",
      "113            0.2200      0.216      1.018      0.309      -0.204       0.644\n",
      "125            0.3293      0.123      2.680      0.007       0.088       0.570\n",
      "126            0.1171      0.166      0.705      0.481      -0.209       0.443\n",
      "127            0.0932      0.120      0.778      0.437      -0.142       0.328\n",
      "130            0.5632      0.164      3.440      0.001       0.242       0.884\n",
      "131            0.2793      0.163      1.711      0.087      -0.041       0.599\n",
      "201            0.0888      0.156      0.568      0.570      -0.218       0.395\n",
      "281           -0.3094      0.142     -2.186      0.029      -0.587      -0.032\n",
      "317           -0.2049      0.138     -1.486      0.137      -0.475       0.065\n",
      "338            0.2048      0.180      1.135      0.256      -0.149       0.558\n",
      "366            0.1360      0.095      1.425      0.154      -0.051       0.323\n",
      "461            0.1773      0.110      1.619      0.105      -0.037       0.392\n",
      "511            0.0582      0.102      0.569      0.569      -0.142       0.259\n",
      "512            0.2656      0.108      2.467      0.014       0.055       0.477\n",
      "543            0.1491      0.161      0.928      0.353      -0.166       0.464\n",
      "544           -0.0997      0.145     -0.690      0.490      -0.383       0.184\n",
      "548            0.1380      0.122      1.132      0.258      -0.101       0.377\n",
      "563           -0.1721      0.122     -1.409      0.159      -0.412       0.067\n",
      "570            0.1983      0.099      2.005      0.045       0.004       0.392\n",
      "==============================================================================\n",
      "Removing feature 23 with p-value 0.606213\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193054\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1532\n",
      "Method:                           MLE   Df Model:                           34\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2093\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.52\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.887e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4272      0.173    -19.772      0.000      -3.767      -3.087\n",
      "15            -0.2390      0.111     -2.150      0.032      -0.457      -0.021\n",
      "22             0.3460      0.088      3.912      0.000       0.173       0.519\n",
      "34             0.0900      0.077      1.162      0.245      -0.062       0.242\n",
      "39            -0.1872      0.099     -1.895      0.058      -0.381       0.006\n",
      "57             0.1897      0.168      1.130      0.258      -0.139       0.519\n",
      "59             0.1757      0.151      1.167      0.243      -0.119       0.471\n",
      "60             0.5251      0.139      3.764      0.000       0.252       0.798\n",
      "64            -0.3704      0.124     -2.993      0.003      -0.613      -0.128\n",
      "65             0.9023      0.313      2.883      0.004       0.289       1.516\n",
      "66            -0.5275      0.367     -1.437      0.151      -1.247       0.192\n",
      "80            -0.1523      0.124     -1.227      0.220      -0.396       0.091\n",
      "91            -0.2412      0.121     -1.993      0.046      -0.478      -0.004\n",
      "101            0.1338      0.132      1.010      0.313      -0.126       0.393\n",
      "104            0.1203      0.153      0.786      0.432      -0.180       0.420\n",
      "112           -0.3300      0.129     -2.566      0.010      -0.582      -0.078\n",
      "113            0.2075      0.214      0.968      0.333      -0.213       0.628\n",
      "125            0.3274      0.123      2.663      0.008       0.086       0.568\n",
      "126            0.1159      0.166      0.698      0.485      -0.210       0.441\n",
      "127            0.0955      0.119      0.800      0.423      -0.138       0.329\n",
      "130            0.5618      0.163      3.438      0.001       0.242       0.882\n",
      "131            0.2796      0.163      1.712      0.087      -0.040       0.600\n",
      "201            0.0912      0.156      0.585      0.559      -0.214       0.397\n",
      "281           -0.3068      0.141     -2.171      0.030      -0.584      -0.030\n",
      "317           -0.2054      0.138     -1.488      0.137      -0.476       0.065\n",
      "338            0.2034      0.179      1.135      0.257      -0.148       0.555\n",
      "366            0.1350      0.095      1.417      0.156      -0.052       0.322\n",
      "461            0.1738      0.110      1.587      0.112      -0.041       0.388\n",
      "511            0.0519      0.102      0.511      0.609      -0.147       0.251\n",
      "512            0.2631      0.108      2.446      0.014       0.052       0.474\n",
      "543            0.1516      0.160      0.945      0.345      -0.163       0.466\n",
      "544           -0.0963      0.144     -0.669      0.504      -0.378       0.186\n",
      "548            0.1348      0.122      1.107      0.268      -0.104       0.373\n",
      "563           -0.1720      0.122     -1.410      0.158      -0.411       0.067\n",
      "570            0.1998      0.099      2.022      0.043       0.006       0.394\n",
      "==============================================================================\n",
      "Removing feature 511 with p-value 0.609370\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193136\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1533\n",
      "Method:                           MLE   Df Model:                           33\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2089\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.64\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.433e-18\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4281      0.173    -19.764      0.000      -3.768      -3.088\n",
      "15            -0.2413      0.111     -2.174      0.030      -0.459      -0.024\n",
      "22             0.3503      0.088      3.977      0.000       0.178       0.523\n",
      "34             0.0888      0.078      1.146      0.252      -0.063       0.241\n",
      "39            -0.1880      0.099     -1.905      0.057      -0.382       0.005\n",
      "57             0.1811      0.167      1.084      0.278      -0.146       0.509\n",
      "59             0.1801      0.150      1.198      0.231      -0.115       0.475\n",
      "60             0.5303      0.139      3.817      0.000       0.258       0.803\n",
      "64            -0.3752      0.123     -3.045      0.002      -0.617      -0.134\n",
      "65             0.9095      0.312      2.911      0.004       0.297       1.522\n",
      "66            -0.5315      0.367     -1.450      0.147      -1.250       0.187\n",
      "80            -0.1526      0.124     -1.227      0.220      -0.396       0.091\n",
      "91            -0.2411      0.121     -1.992      0.046      -0.478      -0.004\n",
      "101            0.1355      0.133      1.020      0.308      -0.125       0.396\n",
      "104            0.1520      0.139      1.090      0.276      -0.121       0.425\n",
      "112           -0.3260      0.128     -2.543      0.011      -0.577      -0.075\n",
      "113            0.2122      0.214      0.991      0.322      -0.208       0.632\n",
      "125            0.3302      0.123      2.688      0.007       0.089       0.571\n",
      "126            0.1148      0.166      0.692      0.489      -0.210       0.440\n",
      "127            0.0960      0.119      0.805      0.421      -0.138       0.330\n",
      "130            0.5576      0.163      3.422      0.001       0.238       0.877\n",
      "131            0.2810      0.163      1.721      0.085      -0.039       0.601\n",
      "201            0.0917      0.156      0.588      0.556      -0.214       0.397\n",
      "281           -0.3090      0.141     -2.186      0.029      -0.586      -0.032\n",
      "317           -0.2079      0.138     -1.508      0.131      -0.478       0.062\n",
      "338            0.2015      0.179      1.124      0.261      -0.150       0.553\n",
      "366            0.1287      0.095      1.357      0.175      -0.057       0.314\n",
      "461            0.1775      0.109      1.628      0.104      -0.036       0.391\n",
      "512            0.2620      0.107      2.438      0.015       0.051       0.473\n",
      "543            0.1471      0.160      0.919      0.358      -0.167       0.461\n",
      "544           -0.0943      0.144     -0.653      0.514      -0.377       0.189\n",
      "548            0.1343      0.122      1.104      0.270      -0.104       0.373\n",
      "563           -0.1699      0.122     -1.393      0.164      -0.409       0.069\n",
      "570            0.2019      0.099      2.050      0.040       0.009       0.395\n",
      "==============================================================================\n",
      "Removing feature 201 with p-value 0.556216\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193248\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1534\n",
      "Method:                           MLE   Df Model:                           32\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2085\n",
      "Time:                        16:39:29   Log-Likelihood:                -302.82\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.281e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4242      0.173    -19.811      0.000      -3.763      -3.085\n",
      "15            -0.2438      0.111     -2.204      0.028      -0.461      -0.027\n",
      "22             0.3513      0.088      3.998      0.000       0.179       0.524\n",
      "34             0.0888      0.077      1.147      0.251      -0.063       0.241\n",
      "39            -0.1916      0.098     -1.950      0.051      -0.384       0.001\n",
      "57             0.1899      0.166      1.141      0.254      -0.136       0.516\n",
      "59             0.1738      0.150      1.158      0.247      -0.120       0.468\n",
      "60             0.5458      0.136      4.002      0.000       0.278       0.813\n",
      "64            -0.3629      0.121     -2.998      0.003      -0.600      -0.126\n",
      "65             0.7970      0.242      3.298      0.001       0.323       1.271\n",
      "66            -0.3802      0.253     -1.503      0.133      -0.876       0.116\n",
      "80            -0.1580      0.124     -1.277      0.202      -0.400       0.084\n",
      "91            -0.2406      0.121     -1.988      0.047      -0.478      -0.003\n",
      "101            0.1423      0.132      1.076      0.282      -0.117       0.402\n",
      "104            0.1429      0.139      1.031      0.303      -0.129       0.415\n",
      "112           -0.3234      0.128     -2.534      0.011      -0.574      -0.073\n",
      "113            0.2185      0.215      1.018      0.309      -0.202       0.639\n",
      "125            0.3376      0.122      2.768      0.006       0.099       0.577\n",
      "126            0.1192      0.165      0.721      0.471      -0.205       0.443\n",
      "127            0.0993      0.119      0.833      0.405      -0.134       0.333\n",
      "130            0.5492      0.163      3.377      0.001       0.230       0.868\n",
      "131            0.2829      0.163      1.734      0.083      -0.037       0.603\n",
      "281           -0.3063      0.141     -2.175      0.030      -0.582      -0.030\n",
      "317           -0.1982      0.137     -1.449      0.147      -0.466       0.070\n",
      "338            0.2295      0.164      1.400      0.162      -0.092       0.551\n",
      "366            0.1282      0.095      1.352      0.176      -0.058       0.314\n",
      "461            0.1800      0.109      1.646      0.100      -0.034       0.394\n",
      "512            0.2632      0.107      2.453      0.014       0.053       0.474\n",
      "543            0.1472      0.160      0.920      0.357      -0.166       0.461\n",
      "544           -0.0914      0.145     -0.632      0.528      -0.375       0.192\n",
      "548            0.1370      0.121      1.128      0.259      -0.101       0.375\n",
      "563           -0.1702      0.122     -1.395      0.163      -0.409       0.069\n",
      "570            0.2018      0.098      2.051      0.040       0.009       0.395\n",
      "==============================================================================\n",
      "Removing feature 544 with p-value 0.527639\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193383\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1535\n",
      "Method:                           MLE   Df Model:                           31\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2079\n",
      "Time:                        16:39:29   Log-Likelihood:                -303.03\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.761e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4216      0.173    -19.832      0.000      -3.760      -3.083\n",
      "15            -0.2425      0.111     -2.191      0.028      -0.459      -0.026\n",
      "22             0.3505      0.088      3.981      0.000       0.178       0.523\n",
      "34             0.0888      0.077      1.146      0.252      -0.063       0.241\n",
      "39            -0.1883      0.098     -1.920      0.055      -0.381       0.004\n",
      "57             0.1864      0.166      1.122      0.262      -0.139       0.512\n",
      "59             0.1713      0.149      1.146      0.252      -0.122       0.464\n",
      "60             0.5493      0.137      4.022      0.000       0.282       0.817\n",
      "64            -0.3646      0.121     -3.007      0.003      -0.602      -0.127\n",
      "65             0.8006      0.242      3.311      0.001       0.327       1.274\n",
      "66            -0.3794      0.253     -1.499      0.134      -0.875       0.117\n",
      "80            -0.1534      0.123     -1.243      0.214      -0.395       0.089\n",
      "91            -0.2396      0.121     -1.980      0.048      -0.477      -0.002\n",
      "101            0.1371      0.132      1.039      0.299      -0.122       0.396\n",
      "104            0.1484      0.138      1.072      0.284      -0.123       0.420\n",
      "112           -0.3154      0.127     -2.485      0.013      -0.564      -0.067\n",
      "113            0.2212      0.215      1.030      0.303      -0.200       0.642\n",
      "125            0.3393      0.122      2.788      0.005       0.101       0.578\n",
      "126            0.1293      0.164      0.788      0.431      -0.192       0.451\n",
      "127            0.0992      0.119      0.834      0.404      -0.134       0.333\n",
      "130            0.5557      0.162      3.429      0.001       0.238       0.873\n",
      "131            0.2872      0.163      1.764      0.078      -0.032       0.606\n",
      "281           -0.3135      0.141     -2.217      0.027      -0.591      -0.036\n",
      "317           -0.1968      0.136     -1.442      0.149      -0.464       0.071\n",
      "338            0.2207      0.162      1.359      0.174      -0.098       0.539\n",
      "366            0.1289      0.095      1.363      0.173      -0.056       0.314\n",
      "461            0.1809      0.110      1.651      0.099      -0.034       0.396\n",
      "512            0.2680      0.107      2.504      0.012       0.058       0.478\n",
      "543            0.1229      0.155      0.791      0.429      -0.182       0.428\n",
      "548            0.1404      0.121      1.160      0.246      -0.097       0.378\n",
      "563           -0.1741      0.121     -1.434      0.152      -0.412       0.064\n",
      "570            0.2035      0.098      2.069      0.039       0.011       0.396\n",
      "==============================================================================\n",
      "Removing feature 126 with p-value 0.430907\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193577\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1536\n",
      "Method:                           MLE   Df Model:                           30\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2071\n",
      "Time:                        16:39:29   Log-Likelihood:                -303.34\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.067e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4179      0.172    -19.836      0.000      -3.756      -3.080\n",
      "15            -0.2416      0.111     -2.185      0.029      -0.458      -0.025\n",
      "22             0.3413      0.087      3.919      0.000       0.171       0.512\n",
      "34             0.0902      0.078      1.158      0.247      -0.062       0.243\n",
      "39            -0.1899      0.098     -1.944      0.052      -0.381       0.002\n",
      "57             0.1863      0.166      1.121      0.262      -0.140       0.512\n",
      "59             0.1764      0.150      1.180      0.238      -0.117       0.470\n",
      "60             0.5478      0.137      4.010      0.000       0.280       0.816\n",
      "64            -0.3652      0.121     -3.013      0.003      -0.603      -0.128\n",
      "65             0.8053      0.242      3.321      0.001       0.330       1.281\n",
      "66            -0.3809      0.254     -1.501      0.133      -0.878       0.116\n",
      "80            -0.1602      0.123     -1.303      0.192      -0.401       0.081\n",
      "91            -0.2368      0.121     -1.961      0.050      -0.473      -0.000\n",
      "101            0.1388      0.132      1.054      0.292      -0.119       0.397\n",
      "104            0.1362      0.137      0.991      0.322      -0.133       0.405\n",
      "112           -0.3179      0.128     -2.489      0.013      -0.568      -0.068\n",
      "113            0.2191      0.215      1.021      0.307      -0.201       0.640\n",
      "125            0.3481      0.121      2.868      0.004       0.110       0.586\n",
      "127            0.0930      0.121      0.769      0.442      -0.144       0.330\n",
      "130            0.5181      0.154      3.364      0.001       0.216       0.820\n",
      "131            0.2278      0.145      1.567      0.117      -0.057       0.513\n",
      "281           -0.3116      0.142     -2.198      0.028      -0.590      -0.034\n",
      "317           -0.1904      0.136     -1.400      0.161      -0.457       0.076\n",
      "338            0.2258      0.163      1.386      0.166      -0.094       0.545\n",
      "366            0.1277      0.095      1.348      0.178      -0.058       0.313\n",
      "461            0.1824      0.109      1.668      0.095      -0.032       0.397\n",
      "512            0.2677      0.107      2.500      0.012       0.058       0.478\n",
      "543            0.1277      0.155      0.821      0.411      -0.177       0.432\n",
      "548            0.1371      0.121      1.134      0.257      -0.100       0.374\n",
      "563           -0.1719      0.121     -1.418      0.156      -0.409       0.066\n",
      "570            0.2006      0.098      2.037      0.042       0.008       0.394\n",
      "==============================================================================\n",
      "Removing feature 127 with p-value 0.441948\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193755\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1537\n",
      "Method:                           MLE   Df Model:                           29\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2064\n",
      "Time:                        16:39:29   Log-Likelihood:                -303.61\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.097e-19\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4113      0.171    -19.910      0.000      -3.747      -3.075\n",
      "15            -0.2473      0.110     -2.242      0.025      -0.464      -0.031\n",
      "22             0.3451      0.087      3.978      0.000       0.175       0.515\n",
      "34             0.0991      0.077      1.289      0.198      -0.052       0.250\n",
      "39            -0.1964      0.097     -2.018      0.044      -0.387      -0.006\n",
      "57             0.1820      0.165      1.100      0.271      -0.142       0.506\n",
      "59             0.1823      0.149      1.222      0.222      -0.110       0.475\n",
      "60             0.5402      0.136      3.984      0.000       0.274       0.806\n",
      "64            -0.3647      0.121     -3.021      0.003      -0.601      -0.128\n",
      "65             0.7971      0.242      3.297      0.001       0.323       1.271\n",
      "66            -0.3812      0.254     -1.501      0.133      -0.879       0.117\n",
      "80            -0.1556      0.123     -1.270      0.204      -0.396       0.085\n",
      "91            -0.2410      0.120     -2.000      0.045      -0.477      -0.005\n",
      "101            0.1426      0.131      1.085      0.278      -0.115       0.400\n",
      "104            0.1329      0.137      0.969      0.332      -0.136       0.402\n",
      "112           -0.3182      0.127     -2.505      0.012      -0.567      -0.069\n",
      "113            0.2200      0.214      1.027      0.304      -0.200       0.640\n",
      "125            0.3337      0.120      2.780      0.005       0.098       0.569\n",
      "130            0.5292      0.154      3.438      0.001       0.228       0.831\n",
      "131            0.2591      0.141      1.839      0.066      -0.017       0.535\n",
      "281           -0.3103      0.141     -2.194      0.028      -0.587      -0.033\n",
      "317           -0.1918      0.135     -1.415      0.157      -0.457       0.074\n",
      "338            0.2250      0.161      1.394      0.163      -0.091       0.541\n",
      "366            0.1283      0.095      1.356      0.175      -0.057       0.314\n",
      "461            0.1807      0.109      1.652      0.099      -0.034       0.395\n",
      "512            0.2660      0.107      2.485      0.013       0.056       0.476\n",
      "543            0.1317      0.155      0.849      0.396      -0.172       0.436\n",
      "548            0.1364      0.121      1.132      0.258      -0.100       0.373\n",
      "563           -0.1725      0.121     -1.426      0.154      -0.410       0.065\n",
      "570            0.1951      0.098      1.988      0.047       0.003       0.387\n",
      "==============================================================================\n",
      "Removing feature 543 with p-value 0.396003\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.193982\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1538\n",
      "Method:                           MLE   Df Model:                           28\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2055\n",
      "Time:                        16:39:29   Log-Likelihood:                -303.97\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 6.124e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4118      0.172    -19.859      0.000      -3.749      -3.075\n",
      "15            -0.2463      0.110     -2.235      0.025      -0.462      -0.030\n",
      "22             0.3471      0.086      4.019      0.000       0.178       0.516\n",
      "34             0.0984      0.077      1.282      0.200      -0.052       0.249\n",
      "39            -0.1970      0.097     -2.024      0.043      -0.388      -0.006\n",
      "57             0.1909      0.166      1.151      0.250      -0.134       0.516\n",
      "59             0.1843      0.149      1.240      0.215      -0.107       0.476\n",
      "60             0.5311      0.135      3.946      0.000       0.267       0.795\n",
      "64            -0.3656      0.120     -3.035      0.002      -0.602      -0.130\n",
      "65             0.8076      0.241      3.353      0.001       0.336       1.280\n",
      "66            -0.3999      0.253     -1.582      0.114      -0.895       0.096\n",
      "80            -0.1658      0.122     -1.359      0.174      -0.405       0.073\n",
      "91            -0.2405      0.120     -2.000      0.045      -0.476      -0.005\n",
      "101            0.1629      0.129      1.260      0.208      -0.091       0.416\n",
      "104            0.1066      0.134      0.798      0.425      -0.155       0.369\n",
      "112           -0.3337      0.126     -2.641      0.008      -0.581      -0.086\n",
      "113            0.2226      0.213      1.045      0.296      -0.195       0.640\n",
      "125            0.3168      0.119      2.673      0.008       0.085       0.549\n",
      "130            0.5038      0.151      3.332      0.001       0.207       0.800\n",
      "131            0.2695      0.141      1.910      0.056      -0.007       0.546\n",
      "281           -0.3090      0.141     -2.197      0.028      -0.585      -0.033\n",
      "317           -0.1919      0.135     -1.418      0.156      -0.457       0.073\n",
      "338            0.2425      0.157      1.543      0.123      -0.066       0.550\n",
      "366            0.1257      0.095      1.328      0.184      -0.060       0.311\n",
      "461            0.1815      0.109      1.662      0.096      -0.033       0.396\n",
      "512            0.2622      0.107      2.452      0.014       0.053       0.472\n",
      "548            0.1244      0.119      1.041      0.298      -0.110       0.359\n",
      "563           -0.1654      0.121     -1.372      0.170      -0.402       0.071\n",
      "570            0.1969      0.098      2.002      0.045       0.004       0.390\n",
      "==============================================================================\n",
      "Removing feature 104 with p-value 0.425069\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194185\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1539\n",
      "Method:                           MLE   Df Model:                           27\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2046\n",
      "Time:                        16:39:29   Log-Likelihood:                -304.29\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.263e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.4063      0.171    -19.903      0.000      -3.742      -3.071\n",
      "15            -0.2475      0.111     -2.234      0.025      -0.465      -0.030\n",
      "22             0.3487      0.087      4.019      0.000       0.179       0.519\n",
      "34             0.0969      0.077      1.261      0.207      -0.054       0.248\n",
      "39            -0.1872      0.097     -1.933      0.053      -0.377       0.003\n",
      "57             0.1948      0.166      1.174      0.240      -0.130       0.520\n",
      "59             0.1904      0.148      1.284      0.199      -0.100       0.481\n",
      "60             0.5534      0.132      4.184      0.000       0.294       0.813\n",
      "64            -0.3824      0.119     -3.215      0.001      -0.615      -0.149\n",
      "65             0.8283      0.239      3.467      0.001       0.360       1.297\n",
      "66            -0.3939      0.251     -1.568      0.117      -0.886       0.099\n",
      "80            -0.1514      0.121     -1.248      0.212      -0.389       0.086\n",
      "91            -0.2351      0.120     -1.962      0.050      -0.470      -0.000\n",
      "101            0.2165      0.110      1.964      0.050       0.000       0.433\n",
      "112           -0.3293      0.126     -2.621      0.009      -0.575      -0.083\n",
      "113            0.2214      0.213      1.040      0.298      -0.196       0.639\n",
      "125            0.3151      0.118      2.665      0.008       0.083       0.547\n",
      "130            0.5263      0.148      3.558      0.000       0.236       0.816\n",
      "131            0.2736      0.141      1.945      0.052      -0.002       0.549\n",
      "281           -0.3057      0.140     -2.183      0.029      -0.580      -0.031\n",
      "317           -0.2032      0.135     -1.505      0.132      -0.468       0.061\n",
      "338            0.2361      0.157      1.501      0.133      -0.072       0.544\n",
      "366            0.1433      0.093      1.540      0.124      -0.039       0.326\n",
      "461            0.1784      0.109      1.636      0.102      -0.035       0.392\n",
      "512            0.2630      0.107      2.462      0.014       0.054       0.472\n",
      "548            0.1270      0.120      1.062      0.288      -0.107       0.361\n",
      "563           -0.1657      0.120     -1.376      0.169      -0.402       0.070\n",
      "570            0.1973      0.098      2.015      0.044       0.005       0.389\n",
      "==============================================================================\n",
      "Removing feature 113 with p-value 0.298263\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194547\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1540\n",
      "Method:                           MLE   Df Model:                           26\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2031\n",
      "Time:                        16:39:29   Log-Likelihood:                -304.86\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.116e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3968      0.170    -19.937      0.000      -3.731      -3.063\n",
      "15            -0.2480      0.111     -2.238      0.025      -0.465      -0.031\n",
      "22             0.3439      0.086      3.984      0.000       0.175       0.513\n",
      "34             0.0845      0.076      1.115      0.265      -0.064       0.233\n",
      "39            -0.1925      0.097     -1.994      0.046      -0.382      -0.003\n",
      "57             0.2128      0.165      1.287      0.198      -0.111       0.537\n",
      "59             0.1909      0.148      1.287      0.198      -0.100       0.482\n",
      "60             0.5983      0.126      4.765      0.000       0.352       0.844\n",
      "64            -0.4141      0.115     -3.592      0.000      -0.640      -0.188\n",
      "65             0.8357      0.238      3.513      0.000       0.369       1.302\n",
      "66            -0.3490      0.247     -1.415      0.157      -0.833       0.134\n",
      "80            -0.1377      0.120     -1.147      0.252      -0.373       0.098\n",
      "91            -0.2338      0.119     -1.958      0.050      -0.468       0.000\n",
      "101            0.2189      0.110      1.988      0.047       0.003       0.435\n",
      "112           -0.3363      0.126     -2.679      0.007      -0.582      -0.090\n",
      "125            0.3152      0.118      2.672      0.008       0.084       0.546\n",
      "130            0.5313      0.148      3.585      0.000       0.241       0.822\n",
      "131            0.2703      0.141      1.923      0.055      -0.005       0.546\n",
      "281           -0.3056      0.141     -2.175      0.030      -0.581      -0.030\n",
      "317           -0.1893      0.134     -1.408      0.159      -0.453       0.074\n",
      "338            0.2157      0.155      1.395      0.163      -0.087       0.519\n",
      "366            0.1435      0.093      1.549      0.121      -0.038       0.325\n",
      "461            0.1825      0.109      1.677      0.093      -0.031       0.396\n",
      "512            0.2652      0.107      2.486      0.013       0.056       0.474\n",
      "548            0.1231      0.119      1.032      0.302      -0.111       0.357\n",
      "563           -0.1658      0.121     -1.376      0.169      -0.402       0.070\n",
      "570            0.1963      0.098      2.012      0.044       0.005       0.388\n",
      "==============================================================================\n",
      "Removing feature 548 with p-value 0.301932\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.194891\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1541\n",
      "Method:                           MLE   Df Model:                           25\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2017\n",
      "Time:                        16:39:30   Log-Likelihood:                -305.39\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.321e-20\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3863      0.169    -20.038      0.000      -3.718      -3.055\n",
      "15            -0.2468      0.111     -2.221      0.026      -0.465      -0.029\n",
      "22             0.3452      0.086      3.997      0.000       0.176       0.514\n",
      "34             0.0821      0.076      1.082      0.279      -0.067       0.231\n",
      "39            -0.1880      0.096     -1.955      0.051      -0.376       0.001\n",
      "57             0.2153      0.165      1.304      0.192      -0.108       0.539\n",
      "59             0.1807      0.148      1.221      0.222      -0.109       0.471\n",
      "60             0.6087      0.126      4.843      0.000       0.362       0.855\n",
      "64            -0.4170      0.116     -3.606      0.000      -0.644      -0.190\n",
      "65             0.8298      0.238      3.484      0.000       0.363       1.297\n",
      "66            -0.3348      0.246     -1.359      0.174      -0.818       0.148\n",
      "80            -0.1404      0.120     -1.167      0.243      -0.376       0.095\n",
      "91            -0.2397      0.120     -2.004      0.045      -0.474      -0.005\n",
      "101            0.2222      0.110      2.018      0.044       0.006       0.438\n",
      "112           -0.3330      0.125     -2.665      0.008      -0.578      -0.088\n",
      "125            0.3114      0.117      2.652      0.008       0.081       0.542\n",
      "130            0.5333      0.148      3.614      0.000       0.244       0.823\n",
      "131            0.2690      0.140      1.921      0.055      -0.005       0.543\n",
      "281           -0.3036      0.140     -2.161      0.031      -0.579      -0.028\n",
      "317           -0.1931      0.134     -1.439      0.150      -0.456       0.070\n",
      "338            0.2111      0.153      1.379      0.168      -0.089       0.511\n",
      "366            0.1419      0.092      1.542      0.123      -0.039       0.322\n",
      "461            0.1871      0.109      1.718      0.086      -0.026       0.401\n",
      "512            0.2627      0.106      2.468      0.014       0.054       0.471\n",
      "563           -0.1710      0.120     -1.421      0.155      -0.407       0.065\n",
      "570            0.1972      0.097      2.029      0.042       0.007       0.388\n",
      "==============================================================================\n",
      "Removing feature 34 with p-value 0.279224\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.195221\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1542\n",
      "Method:                           MLE   Df Model:                           24\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.2004\n",
      "Time:                        16:39:30   Log-Likelihood:                -305.91\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 7.975e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3849      0.169    -20.051      0.000      -3.716      -3.054\n",
      "15            -0.2494      0.111     -2.250      0.024      -0.467      -0.032\n",
      "22             0.3423      0.086      3.970      0.000       0.173       0.511\n",
      "39            -0.1839      0.096     -1.918      0.055      -0.372       0.004\n",
      "57             0.2185      0.165      1.325      0.185      -0.105       0.542\n",
      "59             0.1827      0.147      1.239      0.215      -0.106       0.472\n",
      "60             0.6160      0.125      4.912      0.000       0.370       0.862\n",
      "64            -0.4216      0.115     -3.652      0.000      -0.648      -0.195\n",
      "65             0.8313      0.236      3.521      0.000       0.369       1.294\n",
      "66            -0.3400      0.244     -1.394      0.163      -0.818       0.138\n",
      "80            -0.1375      0.120     -1.145      0.252      -0.373       0.098\n",
      "91            -0.2413      0.120     -2.013      0.044      -0.476      -0.006\n",
      "101            0.2207      0.111      1.997      0.046       0.004       0.437\n",
      "112           -0.3351      0.125     -2.680      0.007      -0.580      -0.090\n",
      "125            0.3242      0.117      2.778      0.005       0.095       0.553\n",
      "130            0.5310      0.148      3.597      0.000       0.242       0.820\n",
      "131            0.2660      0.140      1.905      0.057      -0.008       0.540\n",
      "281           -0.3093      0.141     -2.194      0.028      -0.586      -0.033\n",
      "317           -0.1981      0.134     -1.475      0.140      -0.461       0.065\n",
      "338            0.2190      0.148      1.478      0.139      -0.071       0.509\n",
      "366            0.1437      0.092      1.559      0.119      -0.037       0.324\n",
      "461            0.1801      0.109      1.656      0.098      -0.033       0.393\n",
      "512            0.2657      0.106      2.502      0.012       0.058       0.474\n",
      "563           -0.1695      0.120     -1.413      0.158      -0.405       0.066\n",
      "570            0.1973      0.097      2.036      0.042       0.007       0.387\n",
      "==============================================================================\n",
      "Removing feature 80 with p-value 0.252068\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.195647\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1543\n",
      "Method:                           MLE   Df Model:                           23\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1986\n",
      "Time:                        16:39:30   Log-Likelihood:                -306.58\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 5.399e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3782      0.168    -20.066      0.000      -3.708      -3.048\n",
      "15            -0.2500      0.110     -2.272      0.023      -0.466      -0.034\n",
      "22             0.3383      0.086      3.930      0.000       0.170       0.507\n",
      "39            -0.1990      0.094     -2.107      0.035      -0.384      -0.014\n",
      "57             0.2311      0.164      1.409      0.159      -0.090       0.553\n",
      "59             0.1611      0.146      1.107      0.268      -0.124       0.446\n",
      "60             0.5596      0.115      4.870      0.000       0.334       0.785\n",
      "64            -0.3804      0.109     -3.486      0.000      -0.594      -0.167\n",
      "65             0.7996      0.235      3.401      0.001       0.339       1.260\n",
      "66            -0.3655      0.245     -1.493      0.136      -0.845       0.114\n",
      "91            -0.2330      0.119     -1.951      0.051      -0.467       0.001\n",
      "101            0.2381      0.109      2.185      0.029       0.024       0.452\n",
      "112           -0.3439      0.125     -2.757      0.006      -0.588      -0.099\n",
      "125            0.3166      0.117      2.712      0.007       0.088       0.545\n",
      "130            0.5195      0.148      3.512      0.000       0.230       0.809\n",
      "131            0.2647      0.139      1.899      0.058      -0.009       0.538\n",
      "281           -0.2994      0.139     -2.147      0.032      -0.573      -0.026\n",
      "317           -0.1867      0.132     -1.409      0.159      -0.446       0.073\n",
      "338            0.2308      0.149      1.548      0.122      -0.061       0.523\n",
      "366            0.1391      0.092      1.519      0.129      -0.040       0.319\n",
      "461            0.1793      0.108      1.658      0.097      -0.033       0.391\n",
      "512            0.2622      0.106      2.473      0.013       0.054       0.470\n",
      "563           -0.1692      0.120     -1.408      0.159      -0.405       0.066\n",
      "570            0.1914      0.097      1.972      0.049       0.001       0.382\n",
      "==============================================================================\n",
      "Removing feature 59 with p-value 0.268327\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196038\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1544\n",
      "Method:                           MLE   Df Model:                           22\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1970\n",
      "Time:                        16:39:30   Log-Likelihood:                -307.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.435e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3714      0.168    -20.092      0.000      -3.700      -3.043\n",
      "15            -0.2578      0.110     -2.342      0.019      -0.473      -0.042\n",
      "22             0.3358      0.086      3.888      0.000       0.167       0.505\n",
      "39            -0.1865      0.094     -1.986      0.047      -0.370      -0.002\n",
      "57             0.2872      0.156      1.844      0.065      -0.018       0.592\n",
      "60             0.5645      0.115      4.919      0.000       0.340       0.789\n",
      "64            -0.3734      0.109     -3.424      0.001      -0.587      -0.160\n",
      "65             0.7526      0.229      3.288      0.001       0.304       1.201\n",
      "66            -0.3151      0.237     -1.329      0.184      -0.780       0.149\n",
      "91            -0.2264      0.119     -1.902      0.057      -0.460       0.007\n",
      "101            0.2313      0.108      2.143      0.032       0.020       0.443\n",
      "112           -0.3200      0.122     -2.628      0.009      -0.559      -0.081\n",
      "125            0.3082      0.115      2.668      0.008       0.082       0.535\n",
      "130            0.5087      0.148      3.446      0.001       0.219       0.798\n",
      "131            0.2666      0.140      1.904      0.057      -0.008       0.541\n",
      "281           -0.3119      0.141     -2.214      0.027      -0.588      -0.036\n",
      "317           -0.2060      0.132     -1.561      0.118      -0.464       0.053\n",
      "338            0.1932      0.141      1.367      0.172      -0.084       0.470\n",
      "366            0.1402      0.091      1.546      0.122      -0.038       0.318\n",
      "461            0.2073      0.104      1.984      0.047       0.002       0.412\n",
      "512            0.2568      0.106      2.427      0.015       0.049       0.464\n",
      "563           -0.1802      0.119     -1.512      0.131      -0.414       0.053\n",
      "570            0.1978      0.096      2.054      0.040       0.009       0.387\n",
      "==============================================================================\n",
      "Removing feature 66 with p-value 0.183722\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.196673\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1545\n",
      "Method:                           MLE   Df Model:                           21\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1944\n",
      "Time:                        16:39:30   Log-Likelihood:                -308.19\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.005e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3548      0.166    -20.248      0.000      -3.679      -3.030\n",
      "15            -0.2549      0.110     -2.323      0.020      -0.470      -0.040\n",
      "22             0.3381      0.086      3.936      0.000       0.170       0.506\n",
      "39            -0.1885      0.094     -2.014      0.044      -0.372      -0.005\n",
      "57             0.3139      0.155      2.029      0.043       0.011       0.617\n",
      "60             0.5654      0.113      4.991      0.000       0.343       0.787\n",
      "64            -0.3711      0.109     -3.403      0.001      -0.585      -0.157\n",
      "65             0.5011      0.125      4.002      0.000       0.256       0.747\n",
      "91            -0.2198      0.119     -1.854      0.064      -0.452       0.013\n",
      "101            0.2331      0.108      2.159      0.031       0.021       0.445\n",
      "112           -0.3302      0.121     -2.724      0.006      -0.568      -0.093\n",
      "125            0.3038      0.115      2.636      0.008       0.078       0.530\n",
      "130            0.4968      0.148      3.366      0.001       0.208       0.786\n",
      "131            0.2640      0.140      1.889      0.059      -0.010       0.538\n",
      "281           -0.3090      0.141     -2.199      0.028      -0.585      -0.034\n",
      "317           -0.1713      0.129     -1.325      0.185      -0.425       0.082\n",
      "338            0.0991      0.087      1.140      0.254      -0.071       0.270\n",
      "366            0.1379      0.090      1.525      0.127      -0.039       0.315\n",
      "461            0.2029      0.105      1.939      0.053      -0.002       0.408\n",
      "512            0.2536      0.106      2.401      0.016       0.047       0.461\n",
      "563           -0.1770      0.119     -1.490      0.136      -0.410       0.056\n",
      "570            0.1953      0.096      2.029      0.042       0.007       0.384\n",
      "==============================================================================\n",
      "Removing feature 338 with p-value 0.254318\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197083\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1546\n",
      "Method:                           MLE   Df Model:                           20\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1928\n",
      "Time:                        16:39:30   Log-Likelihood:                -308.83\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.900e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3493      0.165    -20.269      0.000      -3.673      -3.025\n",
      "15            -0.2494      0.109     -2.283      0.022      -0.464      -0.035\n",
      "22             0.3322      0.086      3.883      0.000       0.164       0.500\n",
      "39            -0.1891      0.093     -2.024      0.043      -0.372      -0.006\n",
      "57             0.3130      0.155      2.025      0.043       0.010       0.616\n",
      "60             0.5800      0.111      5.212      0.000       0.362       0.798\n",
      "64            -0.3683      0.107     -3.427      0.001      -0.579      -0.158\n",
      "65             0.5288      0.121      4.356      0.000       0.291       0.767\n",
      "91            -0.2124      0.118     -1.801      0.072      -0.444       0.019\n",
      "101            0.2400      0.108      2.228      0.026       0.029       0.451\n",
      "112           -0.3287      0.122     -2.703      0.007      -0.567      -0.090\n",
      "125            0.3085      0.115      2.674      0.008       0.082       0.535\n",
      "130            0.4944      0.148      3.339      0.001       0.204       0.785\n",
      "131            0.2583      0.139      1.853      0.064      -0.015       0.532\n",
      "281           -0.3030      0.139     -2.181      0.029      -0.575      -0.031\n",
      "317           -0.1487      0.128     -1.163      0.245      -0.399       0.102\n",
      "366            0.1348      0.090      1.490      0.136      -0.043       0.312\n",
      "461            0.1947      0.104      1.865      0.062      -0.010       0.399\n",
      "512            0.2492      0.105      2.364      0.018       0.043       0.456\n",
      "563           -0.1849      0.118     -1.566      0.117      -0.416       0.047\n",
      "570            0.1925      0.097      1.993      0.046       0.003       0.382\n",
      "==============================================================================\n",
      "Removing feature 317 with p-value 0.244948\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.197526\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1547\n",
      "Method:                           MLE   Df Model:                           19\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1909\n",
      "Time:                        16:39:30   Log-Likelihood:                -309.52\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.236e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3408      0.164    -20.332      0.000      -3.663      -3.019\n",
      "15            -0.2557      0.110     -2.333      0.020      -0.471      -0.041\n",
      "22             0.3444      0.086      4.025      0.000       0.177       0.512\n",
      "39            -0.1830      0.094     -1.949      0.051      -0.367       0.001\n",
      "57             0.3548      0.149      2.376      0.018       0.062       0.648\n",
      "60             0.6025      0.110      5.461      0.000       0.386       0.819\n",
      "64            -0.3731      0.108     -3.470      0.001      -0.584      -0.162\n",
      "65             0.5372      0.121      4.426      0.000       0.299       0.775\n",
      "91            -0.2120      0.118     -1.799      0.072      -0.443       0.019\n",
      "101            0.2350      0.107      2.202      0.028       0.026       0.444\n",
      "112           -0.3387      0.121     -2.805      0.005      -0.575      -0.102\n",
      "125            0.3046      0.115      2.650      0.008       0.079       0.530\n",
      "130            0.4978      0.147      3.384      0.001       0.210       0.786\n",
      "131            0.2626      0.139      1.890      0.059      -0.010       0.535\n",
      "281           -0.3057      0.138     -2.211      0.027      -0.577      -0.035\n",
      "366            0.1472      0.089      1.659      0.097      -0.027       0.321\n",
      "461            0.1681      0.102      1.653      0.098      -0.031       0.367\n",
      "512            0.2518      0.105      2.395      0.017       0.046       0.458\n",
      "563           -0.1874      0.117     -1.601      0.109      -0.417       0.042\n",
      "570            0.1968      0.097      2.028      0.043       0.007       0.387\n",
      "==============================================================================\n",
      "Removing feature 563 with p-value 0.109392\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.198339\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1548\n",
      "Method:                           MLE   Df Model:                           18\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1876\n",
      "Time:                        16:39:30   Log-Likelihood:                -310.80\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.321e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3231      0.162    -20.461      0.000      -3.641      -3.005\n",
      "15            -0.2560      0.109     -2.340      0.019      -0.470      -0.042\n",
      "22             0.3336      0.085      3.935      0.000       0.167       0.500\n",
      "39            -0.1654      0.093     -1.779      0.075      -0.348       0.017\n",
      "57             0.3656      0.148      2.465      0.014       0.075       0.656\n",
      "60             0.5994      0.110      5.452      0.000       0.384       0.815\n",
      "64            -0.3685      0.107     -3.435      0.001      -0.579      -0.158\n",
      "65             0.5324      0.121      4.383      0.000       0.294       0.770\n",
      "91            -0.2100      0.118     -1.787      0.074      -0.440       0.020\n",
      "101            0.2237      0.106      2.102      0.036       0.015       0.432\n",
      "112           -0.3492      0.120     -2.912      0.004      -0.584      -0.114\n",
      "125            0.3047      0.114      2.663      0.008       0.080       0.529\n",
      "130            0.4953      0.145      3.406      0.001       0.210       0.780\n",
      "131            0.2808      0.138      2.029      0.042       0.010       0.552\n",
      "281           -0.3106      0.140     -2.221      0.026      -0.585      -0.036\n",
      "366            0.1409      0.089      1.578      0.115      -0.034       0.316\n",
      "461            0.1725      0.101      1.709      0.087      -0.025       0.370\n",
      "512            0.2458      0.105      2.345      0.019       0.040       0.451\n",
      "570            0.1985      0.096      2.061      0.039       0.010       0.387\n",
      "==============================================================================\n",
      "Removing feature 366 with p-value 0.114540\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199042\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1549\n",
      "Method:                           MLE   Df Model:                           17\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1847\n",
      "Time:                        16:39:30   Log-Likelihood:                -311.90\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.194e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.3131      0.162    -20.503      0.000      -3.630      -2.996\n",
      "15            -0.2636      0.110     -2.405      0.016      -0.478      -0.049\n",
      "22             0.3295      0.085      3.888      0.000       0.163       0.496\n",
      "39            -0.1580      0.093     -1.699      0.089      -0.340       0.024\n",
      "57             0.3721      0.148      2.513      0.012       0.082       0.662\n",
      "60             0.5986      0.110      5.432      0.000       0.383       0.815\n",
      "64            -0.3675      0.107     -3.426      0.001      -0.578      -0.157\n",
      "65             0.5330      0.121      4.395      0.000       0.295       0.771\n",
      "91            -0.2197      0.117     -1.880      0.060      -0.449       0.009\n",
      "101            0.2448      0.112      2.177      0.029       0.024       0.465\n",
      "112           -0.3410      0.120     -2.853      0.004      -0.575      -0.107\n",
      "125            0.3065      0.114      2.688      0.007       0.083       0.530\n",
      "130            0.4919      0.144      3.406      0.001       0.209       0.775\n",
      "131            0.2822      0.138      2.043      0.041       0.011       0.553\n",
      "281           -0.3113      0.139     -2.242      0.025      -0.583      -0.039\n",
      "461            0.1760      0.100      1.752      0.080      -0.021       0.373\n",
      "512            0.2439      0.105      2.334      0.020       0.039       0.449\n",
      "570            0.1929      0.096      2.011      0.044       0.005       0.381\n",
      "==============================================================================\n",
      "Removing feature 39 with p-value 0.089373\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.199972\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1550\n",
      "Method:                           MLE   Df Model:                           16\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1809\n",
      "Time:                        16:39:30   Log-Likelihood:                -313.36\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.459e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2992      0.161    -20.540      0.000      -3.614      -2.984\n",
      "15            -0.2528      0.108     -2.335      0.020      -0.465      -0.041\n",
      "22             0.2913      0.082      3.570      0.000       0.131       0.451\n",
      "57             0.3561      0.148      2.407      0.016       0.066       0.646\n",
      "60             0.5441      0.106      5.145      0.000       0.337       0.751\n",
      "64            -0.3301      0.105     -3.135      0.002      -0.536      -0.124\n",
      "65             0.4824      0.118      4.105      0.000       0.252       0.713\n",
      "91            -0.2122      0.116     -1.823      0.068      -0.440       0.016\n",
      "101            0.2659      0.110      2.408      0.016       0.049       0.482\n",
      "112           -0.3400      0.120     -2.843      0.004      -0.574      -0.106\n",
      "125            0.3206      0.114      2.804      0.005       0.096       0.545\n",
      "130            0.4802      0.145      3.319      0.001       0.197       0.764\n",
      "131            0.2720      0.138      1.973      0.049       0.002       0.542\n",
      "281           -0.2926      0.137     -2.141      0.032      -0.561      -0.025\n",
      "461            0.1784      0.100      1.786      0.074      -0.017       0.374\n",
      "512            0.2420      0.104      2.325      0.020       0.038       0.446\n",
      "570            0.2033      0.095      2.133      0.033       0.016       0.390\n",
      "==============================================================================\n",
      "Removing feature 461 with p-value 0.074099\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.200929\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1551\n",
      "Method:                           MLE   Df Model:                           15\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1770\n",
      "Time:                        16:39:30   Log-Likelihood:                -314.86\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.826e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2883      0.160    -20.544      0.000      -3.602      -2.975\n",
      "15            -0.2537      0.107     -2.361      0.018      -0.464      -0.043\n",
      "22             0.2764      0.081      3.422      0.001       0.118       0.435\n",
      "57             0.3920      0.147      2.661      0.008       0.103       0.681\n",
      "60             0.5544      0.106      5.240      0.000       0.347       0.762\n",
      "64            -0.3417      0.105     -3.249      0.001      -0.548      -0.136\n",
      "65             0.4945      0.117      4.213      0.000       0.264       0.725\n",
      "91            -0.2134      0.116     -1.836      0.066      -0.441       0.014\n",
      "101            0.2681      0.110      2.432      0.015       0.052       0.484\n",
      "112           -0.3485      0.120     -2.904      0.004      -0.584      -0.113\n",
      "125            0.3205      0.114      2.823      0.005       0.098       0.543\n",
      "130            0.4830      0.145      3.331      0.001       0.199       0.767\n",
      "131            0.2568      0.138      1.865      0.062      -0.013       0.527\n",
      "281           -0.2961      0.138     -2.146      0.032      -0.566      -0.026\n",
      "512            0.2433      0.104      2.343      0.019       0.040       0.447\n",
      "570            0.2111      0.095      2.228      0.026       0.025       0.397\n",
      "==============================================================================\n",
      "Removing feature 91 with p-value 0.066393\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.202017\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1552\n",
      "Method:                           MLE   Df Model:                           14\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1725\n",
      "Time:                        16:39:30   Log-Likelihood:                -316.56\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 2.705e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2657      0.158    -20.682      0.000      -3.575      -2.956\n",
      "15            -0.2442      0.107     -2.291      0.022      -0.453      -0.035\n",
      "22             0.2798      0.081      3.464      0.001       0.121       0.438\n",
      "57             0.3777      0.146      2.587      0.010       0.092       0.664\n",
      "60             0.5651      0.105      5.360      0.000       0.358       0.772\n",
      "64            -0.3489      0.105     -3.324      0.001      -0.555      -0.143\n",
      "65             0.4934      0.118      4.197      0.000       0.263       0.724\n",
      "101            0.2548      0.109      2.330      0.020       0.040       0.469\n",
      "112           -0.3509      0.119     -2.937      0.003      -0.585      -0.117\n",
      "125            0.3231      0.114      2.835      0.005       0.100       0.547\n",
      "130            0.4958      0.145      3.424      0.001       0.212       0.780\n",
      "131            0.2578      0.138      1.866      0.062      -0.013       0.529\n",
      "281           -0.2988      0.138     -2.162      0.031      -0.570      -0.028\n",
      "512            0.2417      0.103      2.337      0.019       0.039       0.444\n",
      "570            0.2076      0.094      2.207      0.027       0.023       0.392\n",
      "==============================================================================\n",
      "Removing feature 131 with p-value 0.062058\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.203168\n",
      "         Iterations 8\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                 Target   No. Observations:                 1567\n",
      "Model:                          Logit   Df Residuals:                     1553\n",
      "Method:                           MLE   Df Model:                           13\n",
      "Date:                Sun, 04 May 2025   Pseudo R-squ.:                  0.1678\n",
      "Time:                        16:39:30   Log-Likelihood:                -318.36\n",
      "converged:                       True   LL-Null:                       -382.57\n",
      "Covariance Type:            nonrobust   LLR p-value:                 4.318e-21\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -3.2333      0.154    -20.954      0.000      -3.536      -2.931\n",
      "15            -0.2608      0.107     -2.442      0.015      -0.470      -0.051\n",
      "22             0.2878      0.080      3.592      0.000       0.131       0.445\n",
      "57             0.3677      0.145      2.541      0.011       0.084       0.651\n",
      "60             0.5557      0.105      5.292      0.000       0.350       0.762\n",
      "64            -0.3502      0.105     -3.343      0.001      -0.556      -0.145\n",
      "65             0.4944      0.117      4.226      0.000       0.265       0.724\n",
      "101            0.2557      0.110      2.318      0.020       0.039       0.472\n",
      "112           -0.3516      0.119     -2.964      0.003      -0.584      -0.119\n",
      "125            0.3208      0.113      2.831      0.005       0.099       0.543\n",
      "130            0.5499      0.145      3.783      0.000       0.265       0.835\n",
      "281           -0.3070      0.137     -2.242      0.025      -0.575      -0.039\n",
      "512            0.2334      0.103      2.261      0.024       0.031       0.436\n",
      "570            0.2108      0.094      2.248      0.025       0.027       0.395\n",
      "==============================================================================\n",
      "All remaining features are significant (p < 0.05). Stopping.\n",
      "Final significant features (SelectKBest): ['15', '22', '57', '60', '64', '65', '101', '112', '125', '130', '281', '512', '570']\n"
     ]
    }
   ],
   "source": [
    "# Iterative feature elimination\n",
    "X_current = X_scaled_reduced.copy()\n",
    "features_to_keep = list(X_scaled_reduced.columns)\n",
    "features_to_keep.remove('const')\n",
    "\n",
    "while True:\n",
    "    logit_model = sm.Logit(Y, X_current)\n",
    "    result = logit_model.fit(method='newton', maxiter=1000)\n",
    "    print(result.summary())\n",
    "\n",
    "    p_values = result.pvalues\n",
    "    p_values = p_values.drop('const')\n",
    "\n",
    "    if (p_values >= 0.05).sum() == 0:\n",
    "        print(\"All remaining features are significant (p < 0.05). Stopping.\")\n",
    "        break\n",
    "\n",
    "    max_p_feature = p_values.idxmax()\n",
    "    max_p_value = p_values[max_p_feature]\n",
    "    print(f\"Removing feature {max_p_feature} with p-value {max_p_value:.6f}\")\n",
    "\n",
    "    X_current = X_current.drop(columns=max_p_feature)\n",
    "    features_to_keep.remove(max_p_feature)\n",
    "\n",
    "print(f\"Final significant features (SelectKBest): {features_to_keep}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c6bc6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score with significant features (SelectKBest): 0.219725\n",
      "\n",
      "Logistic Regression Coefficients (significant features):\n",
      "         15        22        57        60        64       65       101  \\\n",
      "0 -0.093267  0.000605  0.280442  0.073423  0.003692  0.07158  0.009094   \n",
      "\n",
      "        112       125       130       281       512       570  \n",
      "0 -0.196298  2.960944  0.438956 -0.124185  0.000552  0.020884  \n"
     ]
    }
   ],
   "source": [
    "# Final significant features (hypothesized)\n",
    "X_final_significant = X_indicators[features_to_keep]\n",
    "\n",
    "# Fit logistic regression with class weights\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000, class_weight='balanced')\n",
    "model.fit(X_final_significant, Y)\n",
    "\n",
    "# Evaluate with cross-validation\n",
    "f1_score = cross_val_score(model, X_final_significant, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with significant features (SelectKBest): {f1_score:.6f}\")\n",
    "\n",
    "# Inspect coefficients\n",
    "coeffs = pd.DataFrame(model.coef_, columns=X_final_significant.columns)\n",
    "print(\"\\nLogistic Regression Coefficients (significant features):\")\n",
    "print(coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04edaa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Most Important Features (Mutual Info with Random Forest):\n",
      "   Feature  Importance\n",
      "64     520    0.065658\n",
      "37     248    0.052710\n",
      "50     386    0.044980\n",
      "46     346    0.038994\n",
      "47     347    0.031049\n",
      "18     113    0.027171\n",
      "11      73    0.024113\n",
      "12      74    0.019468\n",
      "9       66    0.018837\n",
      "75     555    0.018163\n",
      "83     566    0.018128\n",
      "80     563    0.017516\n",
      "65     540    0.015816\n",
      "96     580    0.014260\n",
      "59     478    0.013752\n",
      "72     552    0.013747\n",
      "97     581    0.013727\n",
      "87     570    0.013474\n",
      "82     565    0.013002\n",
      "78     558    0.012526\n",
      "F1-score with Random Forest on Mutual Info features: 0.264571\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Mutual Info features (replace with actual list)\n",
    "X_mi = X_indicators[selected_features_mi]\n",
    "\n",
    "# Train Random Forest on Mutual Info features\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_mi, Y)\n",
    "\n",
    "# Extract feature importances\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': selected_features_mi,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Display top 20 features\n",
    "print(\"Top 20 Most Important Features (Mutual Info with Random Forest):\")\n",
    "print(feature_importance_df.head(20))\n",
    "\n",
    "# F1-score (already computed)\n",
    "f1_mi = cross_val_score(rf_model, X_mi, Y, cv=5, scoring='f1').mean()\n",
    "print(f\"F1-score with Random Forest on Mutual Info features: {f1_mi:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "40ed2b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of MI-Selected Features: 100\n",
      "MI-Selected Features: ['7', '20', '26', '34', '36', '38', '41', '42', '57', '66', '71', '73', '74', '111', '112', '113', '122', '125', '127', '128', '129', '131', '132', '133', '134', '135', '138', '139', '140', '143', '154', '173', '246', '247', '248', '270', '274', '275', '276', '278', '289', '306', '310', '311', '332', '346', '347', '383', '385', '386', '406', '407', '408', '412', '414', '417', '429', '444', '478', '511', '518', '519', '520', '541', '542', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '574', '575', '576', '577', '578', '579', '580', '581', '582', '584', '590', 'missing_74']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, MI Features):\n",
      "Threshold: 0.10, Precision: 0.767677, Recall: 0.730769, F1-Score: 0.748768\n",
      "Threshold: 0.15, Precision: 0.818182, Recall: 0.692308, F1-Score: 0.750000\n",
      "Threshold: 0.20, Precision: 0.833333, Recall: 0.673077, F1-Score: 0.744681\n",
      "Threshold: 0.25, Precision: 0.871795, Recall: 0.653846, F1-Score: 0.747253\n",
      "Threshold: 0.30, Precision: 0.888889, Recall: 0.615385, F1-Score: 0.727273\n",
      "Threshold: 0.35, Precision: 0.895522, Recall: 0.576923, F1-Score: 0.701754\n",
      "Threshold: 0.40, Precision: 0.900000, Recall: 0.519231, F1-Score: 0.658537\n",
      "Threshold: 0.45, Precision: 0.898305, Recall: 0.509615, F1-Score: 0.650307\n",
      "Threshold: 0.50, Precision: 0.929825, Recall: 0.509615, F1-Score: 0.658385\n",
      "Threshold: 0.55, Precision: 0.946429, Recall: 0.509615, F1-Score: 0.662500\n",
      "Threshold: 0.60, Precision: 0.961538, Recall: 0.480769, F1-Score: 0.641026\n",
      "Threshold: 0.65, Precision: 1.000000, Recall: 0.451923, F1-Score: 0.622517\n",
      "Threshold: 0.70, Precision: 1.000000, Recall: 0.442308, F1-Score: 0.613333\n",
      "Threshold: 0.75, Precision: 1.000000, Recall: 0.394231, F1-Score: 0.565517\n",
      "Threshold: 0.80, Precision: 1.000000, Recall: 0.384615, F1-Score: 0.555556\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.365385, F1-Score: 0.535211\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.307692, F1-Score: 0.470588\n",
      "\n",
      "XGBoost (MI Features):\n",
      "Best Threshold: 0.15\n",
      "Precision: 0.818182\n",
      "Recall: 0.692308\n",
      "F1-Score: 0.750000\n"
     ]
    }
   ],
   "source": [
    "# Apply Mutual Information to select the top 100 features\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k=100)\n",
    "mi_selector.fit(X_indicators, Y)\n",
    "selected_features_mi = X_indicators.columns[mi_selector.get_support()].tolist()\n",
    "X_mi = X_indicators[selected_features_mi]\n",
    "\n",
    "print(f\"Number of MI-Selected Features: {len(selected_features_mi)}\")\n",
    "print(\"MI-Selected Features:\", selected_features_mi)\n",
    "\n",
    "# Train XGBoost on the MI-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_mi = cross_val_predict(xgb_model, X_mi, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, MI Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_mi >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)  # Already using f1_val\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (MI Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "13bdf29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RFE-Selected Features: 100\n",
      "RFE-Selected Features: ['15', '20', '21', '27', '32', '33', '35', '36', '37', '38', '48', '59', '60', '65', '66', '76', '78', '82', '84', '87', '97', '99', '100', '103', '109', '112', '118', '122', '125', '127', '129', '130', '131', '133', '141', '143', '146', '147', '151', '157', '173', '175', '176', '178', '185', '196', '203', '205', '212', '214', '215', '217', '218', '219', '248', '269', '270', '278', '286', '291', '306', '311', '313', '317', '319', '322', '334', '337', '339', '341', '349', '350', '352', '356', '391', '407', '412', '417', '425', '438', '444', '446', '447', '449', '455', '456', '458', '476', '517', '525', '546', '547', '553', '557', '558', '564', '565', '567', '572', 'missing_113']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, RFE Features):\n",
      "Threshold: 0.10, Precision: 0.632653, Recall: 0.596154, F1-Score: 0.613861\n",
      "Threshold: 0.15, Precision: 0.682927, Recall: 0.538462, F1-Score: 0.602151\n",
      "Threshold: 0.20, Precision: 0.716216, Recall: 0.509615, F1-Score: 0.595506\n",
      "Threshold: 0.25, Precision: 0.772727, Recall: 0.490385, F1-Score: 0.600000\n",
      "Threshold: 0.30, Precision: 0.833333, Recall: 0.480769, F1-Score: 0.609756\n",
      "Threshold: 0.35, Precision: 0.847458, Recall: 0.480769, F1-Score: 0.613497\n",
      "Threshold: 0.40, Precision: 0.877193, Recall: 0.480769, F1-Score: 0.621118\n",
      "Threshold: 0.45, Precision: 0.888889, Recall: 0.461538, F1-Score: 0.607595\n",
      "Threshold: 0.50, Precision: 0.940000, Recall: 0.451923, F1-Score: 0.610390\n",
      "Threshold: 0.55, Precision: 0.940000, Recall: 0.451923, F1-Score: 0.610390\n",
      "Threshold: 0.60, Precision: 0.937500, Recall: 0.432692, F1-Score: 0.592105\n",
      "Threshold: 0.65, Precision: 0.931818, Recall: 0.394231, F1-Score: 0.554054\n",
      "Threshold: 0.70, Precision: 0.926829, Recall: 0.365385, F1-Score: 0.524138\n",
      "Threshold: 0.75, Precision: 0.972222, Recall: 0.336538, F1-Score: 0.500000\n",
      "Threshold: 0.80, Precision: 1.000000, Recall: 0.278846, F1-Score: 0.436090\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.250000, F1-Score: 0.400000\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.230769, F1-Score: 0.375000\n",
      "\n",
      "XGBoost (RFE Features):\n",
      "Best Threshold: 0.40\n",
      "Precision: 0.877193\n",
      "Recall: 0.480769\n",
      "F1-Score: 0.621118\n"
     ]
    }
   ],
   "source": [
    "# Use the 100 RFE-selected features (already defined as selected_features_rfe)\n",
    "X_rfe = X_indicators[selected_features_rfe]\n",
    "print(f\"Number of RFE-Selected Features: {len(selected_features_rfe)}\")\n",
    "print(\"RFE-Selected Features:\", selected_features_rfe)\n",
    "\n",
    "# Train XGBoost on the RFE-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_rfe = cross_val_predict(xgb_model, X_rfe, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, RFE Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_rfe >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (RFE Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9cb07011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SelectKBest (f_classif)-Selected Features: 100\n",
      "SelectKBest-Selected Features: ['15', '22', '23', '27', '29', '33', '34', '39', '41', '57', '59', '60', '64', '65', '66', '69', '71', '77', '80', '91', '96', '101', '104', '112', '113', '122', '123', '124', '125', '126', '127', '128', '130', '131', '134', '160', '161', '164', '165', '166', '167', '181', '184', '197', '198', '200', '201', '206', '211', '248', '281', '295', '296', '299', '300', '301', '317', '320', '338', '349', '366', '431', '432', '435', '436', '437', '438', '453', '456', '461', '469', '470', '472', '478', '511', '512', '520', '543', '544', '548', '551', '552', '554', '555', '557', '558', '563', '566', '568', '570', '574', '576', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, SelectKBest f_classif Features):\n",
      "Threshold: 0.10, Precision: 0.669811, Recall: 0.682692, F1-Score: 0.676190\n",
      "Threshold: 0.15, Precision: 0.741573, Recall: 0.634615, F1-Score: 0.683938\n",
      "Threshold: 0.20, Precision: 0.815789, Recall: 0.596154, F1-Score: 0.688889\n",
      "Threshold: 0.25, Precision: 0.819444, Recall: 0.567308, F1-Score: 0.670455\n",
      "Threshold: 0.30, Precision: 0.880597, Recall: 0.567308, F1-Score: 0.690058\n",
      "Threshold: 0.35, Precision: 0.906250, Recall: 0.557692, F1-Score: 0.690476\n",
      "Threshold: 0.40, Precision: 0.934426, Recall: 0.548077, F1-Score: 0.690909\n",
      "Threshold: 0.45, Precision: 0.948276, Recall: 0.528846, F1-Score: 0.679012\n",
      "Threshold: 0.50, Precision: 0.945455, Recall: 0.500000, F1-Score: 0.654088\n",
      "Threshold: 0.55, Precision: 0.944444, Recall: 0.490385, F1-Score: 0.645570\n",
      "Threshold: 0.60, Precision: 0.941176, Recall: 0.461538, F1-Score: 0.619355\n",
      "Threshold: 0.65, Precision: 0.957447, Recall: 0.432692, F1-Score: 0.596026\n",
      "Threshold: 0.70, Precision: 0.956522, Recall: 0.423077, F1-Score: 0.586667\n",
      "Threshold: 0.75, Precision: 0.955556, Recall: 0.413462, F1-Score: 0.577181\n",
      "Threshold: 0.80, Precision: 0.952381, Recall: 0.384615, F1-Score: 0.547945\n",
      "Threshold: 0.85, Precision: 0.945946, Recall: 0.336538, F1-Score: 0.496454\n",
      "Threshold: 0.90, Precision: 0.965517, Recall: 0.269231, F1-Score: 0.421053\n",
      "\n",
      "XGBoost (SelectKBest f_classif Features):\n",
      "Best Threshold: 0.40\n",
      "Precision: 0.934426\n",
      "Recall: 0.548077\n",
      "F1-Score: 0.690909\n"
     ]
    }
   ],
   "source": [
    "# Select the top 100 features using SelectKBest with f_classif\n",
    "selector = SelectKBest(score_func=f_classif, k=100)\n",
    "selector.fit(X_indicators, Y)\n",
    "selected_features_fclassif = X_indicators.columns[selector.get_support()].tolist()\n",
    "print(f\"Number of SelectKBest (f_classif)-Selected Features: {len(selected_features_fclassif)}\")\n",
    "\n",
    "# Subset the data to the selected features\n",
    "X_fclassif = X_indicators[selected_features_fclassif]\n",
    "print(\"SelectKBest-Selected Features:\", selected_features_fclassif)\n",
    "\n",
    "# Train XGBoost on the SelectKBest (f_classif)-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_fclassif = cross_val_predict(xgb_model, X_fclassif, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, SelectKBest f_classif Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_fclassif >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (SelectKBest f_classif Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5f081b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Boruta-Selected Features: 28\n",
      "Boruta-Selected Features: ['17', '22', '34', '60', '65', '68', '74', '96', '113', '125', '141', '151', '240', '248', '284', '346', '347', '424', '427', '524', '555', '563', '565', '569', '570', '581', 'missing_73', 'missing_113']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, Boruta Features):\n",
      "Threshold: 0.10, Precision: 0.731481, Recall: 0.759615, F1-Score: 0.745283\n",
      "Threshold: 0.15, Precision: 0.791667, Recall: 0.730769, F1-Score: 0.760000\n",
      "Threshold: 0.20, Precision: 0.827586, Recall: 0.692308, F1-Score: 0.753927\n",
      "Threshold: 0.25, Precision: 0.843373, Recall: 0.673077, F1-Score: 0.748663\n",
      "Threshold: 0.30, Precision: 0.846154, Recall: 0.634615, F1-Score: 0.725275\n",
      "Threshold: 0.35, Precision: 0.849315, Recall: 0.596154, F1-Score: 0.700565\n",
      "Threshold: 0.40, Precision: 0.845070, Recall: 0.576923, F1-Score: 0.685714\n",
      "Threshold: 0.45, Precision: 0.876923, Recall: 0.548077, F1-Score: 0.674556\n",
      "Threshold: 0.50, Precision: 0.888889, Recall: 0.538462, F1-Score: 0.670659\n",
      "Threshold: 0.55, Precision: 0.888889, Recall: 0.538462, F1-Score: 0.670659\n",
      "Threshold: 0.60, Precision: 0.933333, Recall: 0.538462, F1-Score: 0.682927\n",
      "Threshold: 0.65, Precision: 0.948276, Recall: 0.528846, F1-Score: 0.679012\n",
      "Threshold: 0.70, Precision: 0.964912, Recall: 0.528846, F1-Score: 0.683230\n",
      "Threshold: 0.75, Precision: 0.962264, Recall: 0.490385, F1-Score: 0.649682\n",
      "Threshold: 0.80, Precision: 0.957447, Recall: 0.432692, F1-Score: 0.596026\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.432692, F1-Score: 0.604027\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.394231, F1-Score: 0.565517\n",
      "\n",
      "XGBoost (Boruta Features):\n",
      "Best Threshold: 0.15\n",
      "Precision: 0.791667\n",
      "Recall: 0.730769\n",
      "F1-Score: 0.760000\n"
     ]
    }
   ],
   "source": [
    "from boruta import BorutaPy\n",
    "\n",
    "# Define the XGBoost model for Boruta\n",
    "xgb_boruta = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Initialize Boruta\n",
    "boruta_selector = BorutaPy(\n",
    "    xgb_boruta,\n",
    "    n_estimators='auto',\n",
    "    random_state=42,\n",
    "    max_iter=100,  # Number of iterations\n",
    "    perc=90  # Percentile threshold for feature acceptance\n",
    ")\n",
    "\n",
    "# Fit Boruta\n",
    "boruta_selector.fit(X_indicators.values, Y.values)\n",
    "\n",
    "# Get selected features\n",
    "selected_features_boruta = X_indicators.columns[boruta_selector.support_].tolist()\n",
    "print(f\"Number of Boruta-Selected Features: {len(selected_features_boruta)}\")\n",
    "print(\"Boruta-Selected Features:\", selected_features_boruta)\n",
    "\n",
    "# Subset the data to the selected features\n",
    "X_boruta = X_indicators[selected_features_boruta]\n",
    "\n",
    "# Train XGBoost on the Boruta-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_boruta = cross_val_predict(xgb_model, X_boruta, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, Boruta Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_boruta >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (Boruta Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b8c5dd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of SHAP-Selected Features: 100\n",
      "SHAP-Selected Features: ['113', '346', '248', '60', '74', '73', '581', '569', '563', 'missing_73', '556', '564', '11', '582', '558', '386', '1', '347', '548', '570', '22', 'missing_113', '470', '567', '164', '478', '299', '91', '520', '512', '308', '461', '572', '160', '547', '249', '576', '492', '103', '313', '568', '555', '134', '554', '46', '301', '163', '487', '566', '206', '182', '432', '549', '352', '173', '580', '426', '185', '288', '128', '552', '15', '87', '272', '284', '114', '17', '557', '172', '297', '485', '104', '96', '292', '41', '117', '427', '80', '565', '447', '141', '153', '551', '588', '589', '489', '161', '123', '147', '438', '151', '125', '34', '146', '19', '333', '5', '181', '420', '240']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, SHAP Features):\n",
      "Threshold: 0.10, Precision: 0.728972, Recall: 0.750000, F1-Score: 0.739336\n",
      "Threshold: 0.15, Precision: 0.784946, Recall: 0.701923, F1-Score: 0.741117\n",
      "Threshold: 0.20, Precision: 0.833333, Recall: 0.673077, F1-Score: 0.744681\n",
      "Threshold: 0.25, Precision: 0.862500, Recall: 0.663462, F1-Score: 0.750000\n",
      "Threshold: 0.30, Precision: 0.871795, Recall: 0.653846, F1-Score: 0.747253\n",
      "Threshold: 0.35, Precision: 0.901408, Recall: 0.615385, F1-Score: 0.731429\n",
      "Threshold: 0.40, Precision: 0.953125, Recall: 0.586538, F1-Score: 0.726190\n",
      "Threshold: 0.45, Precision: 0.967742, Recall: 0.576923, F1-Score: 0.722892\n",
      "Threshold: 0.50, Precision: 0.964286, Recall: 0.519231, F1-Score: 0.675000\n",
      "Threshold: 0.55, Precision: 0.962963, Recall: 0.500000, F1-Score: 0.658228\n",
      "Threshold: 0.60, Precision: 0.960000, Recall: 0.461538, F1-Score: 0.623377\n",
      "Threshold: 0.65, Precision: 0.979167, Recall: 0.451923, F1-Score: 0.618421\n",
      "Threshold: 0.70, Precision: 0.978723, Recall: 0.442308, F1-Score: 0.609272\n",
      "Threshold: 0.75, Precision: 0.977778, Recall: 0.423077, F1-Score: 0.590604\n",
      "Threshold: 0.80, Precision: 0.975610, Recall: 0.384615, F1-Score: 0.551724\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.365385, F1-Score: 0.535211\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.317308, F1-Score: 0.481752\n",
      "\n",
      "XGBoost (SHAP Features):\n",
      "Best Threshold: 0.25\n",
      "Precision: 0.862500\n",
      "Recall: 0.663462\n",
      "F1-Score: 0.750000\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Train an XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "xgb_model.fit(X_indicators, Y)\n",
    "\n",
    "# Compute SHAP values\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_indicators)\n",
    "\n",
    "# Calculate average absolute SHAP values for each feature\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_importance_df = pd.DataFrame({\n",
    "    'feature': X_indicators.columns,\n",
    "    'shap_importance': shap_importance\n",
    "})\n",
    "shap_importance_df = shap_importance_df.sort_values(by='shap_importance', ascending=False)\n",
    "\n",
    "# Select top 100 features\n",
    "selected_features_shap = shap_importance_df['feature'].head(100).tolist()\n",
    "print(f\"Number of SHAP-Selected Features: {len(selected_features_shap)}\")\n",
    "print(\"SHAP-Selected Features:\", selected_features_shap)\n",
    "\n",
    "# Subset the data to the selected features\n",
    "X_shap = X_indicators[selected_features_shap]\n",
    "\n",
    "# Train XGBoost on the SHAP-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_shap = cross_val_predict(xgb_model, X_shap, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, SHAP Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_shap >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (SHAP Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0c7e29c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of L1-Selected Features: 100\n",
      "L1-Selected Features: ['missing_113', '248', '570', '15', 'missing_73', '34', '347', '154', '563', '60', '377', '300', '284', '569', '21', '391', '91', '188', '565', '73', '337', '74', '213', '113', '291', '346', '92', '68', '24', '552', '90', '125', '525', '524', '548', '131', '547', '556', '89', '490', '349', '130', '116', '65', '103', '341', '368', '121', '118', '575', '105', '469', '22', '581', '424', '564', '491', '582', '30', '474', '3', '189', '148', '299', '319', '486', '17', '339', '555', '566', '576', '476', '160', '63', '96', '317', '572', '278', '93', '23', '80', '206', '432', '567', '147', '59', '477', '431', '20', '85', '114', '38', '41', '332', '11', '478', '308', '151', '172', '511']\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, L1 Features):\n",
      "Threshold: 0.10, Precision: 0.772277, Recall: 0.750000, F1-Score: 0.760976\n",
      "Threshold: 0.15, Precision: 0.822222, Recall: 0.711538, F1-Score: 0.762887\n",
      "Threshold: 0.20, Precision: 0.829268, Recall: 0.653846, F1-Score: 0.731183\n",
      "Threshold: 0.25, Precision: 0.871795, Recall: 0.653846, F1-Score: 0.747253\n",
      "Threshold: 0.30, Precision: 0.917808, Recall: 0.644231, F1-Score: 0.757062\n",
      "Threshold: 0.35, Precision: 0.927536, Recall: 0.615385, F1-Score: 0.739884\n",
      "Threshold: 0.40, Precision: 0.955224, Recall: 0.615385, F1-Score: 0.748538\n",
      "Threshold: 0.45, Precision: 0.952381, Recall: 0.576923, F1-Score: 0.718563\n",
      "Threshold: 0.50, Precision: 0.950820, Recall: 0.557692, F1-Score: 0.703030\n",
      "Threshold: 0.55, Precision: 0.948276, Recall: 0.528846, F1-Score: 0.679012\n",
      "Threshold: 0.60, Precision: 0.981481, Recall: 0.509615, F1-Score: 0.670886\n",
      "Threshold: 0.65, Precision: 0.981132, Recall: 0.500000, F1-Score: 0.662420\n",
      "Threshold: 0.70, Precision: 0.980000, Recall: 0.471154, F1-Score: 0.636364\n",
      "Threshold: 0.75, Precision: 0.978723, Recall: 0.442308, F1-Score: 0.609272\n",
      "Threshold: 0.80, Precision: 0.977778, Recall: 0.423077, F1-Score: 0.590604\n",
      "Threshold: 0.85, Precision: 0.975610, Recall: 0.384615, F1-Score: 0.551724\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.346154, F1-Score: 0.514286\n",
      "\n",
      "XGBoost (L1 Features):\n",
      "Best Threshold: 0.15\n",
      "Precision: 0.822222\n",
      "Recall: 0.711538\n",
      "F1-Score: 0.762887\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost with L1 regularization\n",
    "xgb_l1 = XGBClassifier(\n",
    "    random_state=42,\n",
    "    eval_metric='logloss',\n",
    "    reg_alpha=1.0  # L1 regularization parameter\n",
    ")\n",
    "xgb_l1.fit(X_indicators, Y)\n",
    "\n",
    "# Get feature importance\n",
    "feature_importance = xgb_l1.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_indicators.columns,\n",
    "    'importance': feature_importance\n",
    "})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Select features with non-zero importance (or top 100 for consistency)\n",
    "selected_features_l1 = importance_df[importance_df['importance'] > 0]['feature'].head(100).tolist()\n",
    "print(f\"Number of L1-Selected Features: {len(selected_features_l1)}\")\n",
    "print(\"L1-Selected Features:\", selected_features_l1)\n",
    "\n",
    "# Subset the data to the selected features\n",
    "X_l1 = X_indicators[selected_features_l1]\n",
    "\n",
    "# Train XGBoost on the L1-selected features\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_l1 = cross_val_predict(xgb_model, X_l1, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning to maximize F1-score\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, L1 Features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_l1 >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (L1 Features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "48a41c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (L1 Features): {'gamma': 0.3, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250, 'reg_alpha': 0, 'reg_lambda': 0, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, L1 Features): 0.7894993894993895\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, L1 Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.590909, Recall: 0.875000, F1-Score: 0.705426\n",
      "Threshold: 0.15, Precision: 0.623188, Recall: 0.826923, F1-Score: 0.710744\n",
      "Threshold: 0.20, Precision: 0.698276, Recall: 0.778846, F1-Score: 0.736364\n",
      "Threshold: 0.25, Precision: 0.750000, Recall: 0.778846, F1-Score: 0.764151\n",
      "Threshold: 0.30, Precision: 0.752475, Recall: 0.730769, F1-Score: 0.741463\n",
      "Threshold: 0.35, Precision: 0.789474, Recall: 0.721154, F1-Score: 0.753769\n",
      "Threshold: 0.40, Precision: 0.840909, Recall: 0.711538, F1-Score: 0.770833\n",
      "Threshold: 0.45, Precision: 0.891566, Recall: 0.711538, F1-Score: 0.791444\n",
      "Threshold: 0.50, Precision: 0.902439, Recall: 0.711538, F1-Score: 0.795699\n",
      "Threshold: 0.55, Precision: 0.894737, Recall: 0.653846, F1-Score: 0.755556\n",
      "Threshold: 0.60, Precision: 0.891892, Recall: 0.634615, F1-Score: 0.741573\n",
      "Threshold: 0.65, Precision: 0.890411, Recall: 0.625000, F1-Score: 0.734463\n",
      "Threshold: 0.70, Precision: 0.911765, Recall: 0.596154, F1-Score: 0.720930\n",
      "Threshold: 0.75, Precision: 0.916667, Recall: 0.528846, F1-Score: 0.670732\n",
      "Threshold: 0.80, Precision: 0.947368, Recall: 0.519231, F1-Score: 0.670807\n",
      "Threshold: 0.85, Precision: 0.962963, Recall: 0.500000, F1-Score: 0.658228\n",
      "Threshold: 0.90, Precision: 0.957447, Recall: 0.432692, F1-Score: 0.596026\n",
      "\n",
      "XGBoost (L1 Features, Tuned):\n",
      "Best Threshold: 0.50\n",
      "Precision: 0.902439\n",
      "Recall: 0.711538\n",
      "F1-Score: 0.795699\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [250],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0],\n",
    "    'gamma': [0.3],\n",
    "    'subsample': [1.0]\n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_l1, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (L1 Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, L1 Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_l1_tuned = cross_val_predict(best_xgb, X_l1, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, L1 Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_l1_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (L1 Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "400c9931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (Boruta Features): {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 200, 'reg_alpha': 0, 'reg_lambda': 0, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, Boruta Features): 0.8039019116418498\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, Boruta Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.611111, Recall: 0.846154, F1-Score: 0.709677\n",
      "Threshold: 0.15, Precision: 0.677419, Recall: 0.807692, F1-Score: 0.736842\n",
      "Threshold: 0.20, Precision: 0.740741, Recall: 0.769231, F1-Score: 0.754717\n",
      "Threshold: 0.25, Precision: 0.782178, Recall: 0.759615, F1-Score: 0.770732\n",
      "Threshold: 0.30, Precision: 0.819149, Recall: 0.740385, F1-Score: 0.777778\n",
      "Threshold: 0.35, Precision: 0.844444, Recall: 0.730769, F1-Score: 0.783505\n",
      "Threshold: 0.40, Precision: 0.883721, Recall: 0.730769, F1-Score: 0.800000\n",
      "Threshold: 0.45, Precision: 0.882353, Recall: 0.721154, F1-Score: 0.793651\n",
      "Threshold: 0.50, Precision: 0.914634, Recall: 0.721154, F1-Score: 0.806452\n",
      "Threshold: 0.55, Precision: 0.925926, Recall: 0.721154, F1-Score: 0.810811\n",
      "Threshold: 0.60, Precision: 0.921053, Recall: 0.673077, F1-Score: 0.777778\n",
      "Threshold: 0.65, Precision: 0.920000, Recall: 0.663462, F1-Score: 0.770950\n",
      "Threshold: 0.70, Precision: 0.916667, Recall: 0.634615, F1-Score: 0.750000\n",
      "Threshold: 0.75, Precision: 0.927536, Recall: 0.615385, F1-Score: 0.739884\n",
      "Threshold: 0.80, Precision: 0.952381, Recall: 0.576923, F1-Score: 0.718563\n",
      "Threshold: 0.85, Precision: 0.966667, Recall: 0.557692, F1-Score: 0.707317\n",
      "Threshold: 0.90, Precision: 0.982456, Recall: 0.538462, F1-Score: 0.695652\n",
      "\n",
      "XGBoost (Boruta Features, Tuned):\n",
      "Best Threshold: 0.55\n",
      "Precision: 0.925926\n",
      "Recall: 0.721154\n",
      "F1-Score: 0.810811\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [200],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1.0]\n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_boruta, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (Boruta Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, Boruta Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_boruta_tuned = cross_val_predict(best_xgb, X_boruta, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, Boruta Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_boruta_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (Boruta Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b7095ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (MI Features): {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 120, 'reg_alpha': 0, 'reg_lambda': 0.1, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, MI Features): 0.7314159925924631\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, MI Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.637097, Recall: 0.759615, F1-Score: 0.692982\n",
      "Threshold: 0.15, Precision: 0.669492, Recall: 0.759615, F1-Score: 0.711712\n",
      "Threshold: 0.20, Precision: 0.735849, Recall: 0.750000, F1-Score: 0.742857\n",
      "Threshold: 0.25, Precision: 0.754902, Recall: 0.740385, F1-Score: 0.747573\n",
      "Threshold: 0.30, Precision: 0.789474, Recall: 0.721154, F1-Score: 0.753769\n",
      "Threshold: 0.35, Precision: 0.831461, Recall: 0.711538, F1-Score: 0.766839\n",
      "Threshold: 0.40, Precision: 0.833333, Recall: 0.673077, F1-Score: 0.744681\n",
      "Threshold: 0.45, Precision: 0.862500, Recall: 0.663462, F1-Score: 0.750000\n",
      "Threshold: 0.50, Precision: 0.868421, Recall: 0.634615, F1-Score: 0.733333\n",
      "Threshold: 0.55, Precision: 0.864865, Recall: 0.615385, F1-Score: 0.719101\n",
      "Threshold: 0.60, Precision: 0.873239, Recall: 0.596154, F1-Score: 0.708571\n",
      "Threshold: 0.65, Precision: 0.897059, Recall: 0.586538, F1-Score: 0.709302\n",
      "Threshold: 0.70, Precision: 0.901639, Recall: 0.528846, F1-Score: 0.666667\n",
      "Threshold: 0.75, Precision: 0.929825, Recall: 0.509615, F1-Score: 0.658385\n",
      "Threshold: 0.80, Precision: 0.942308, Recall: 0.471154, F1-Score: 0.628205\n",
      "Threshold: 0.85, Precision: 0.933333, Recall: 0.403846, F1-Score: 0.563758\n",
      "Threshold: 0.90, Precision: 0.953488, Recall: 0.394231, F1-Score: 0.557823\n",
      "\n",
      "XGBoost (MI Features, Tuned):\n",
      "Best Threshold: 0.35\n",
      "Precision: 0.831461\n",
      "Recall: 0.711538\n",
      "F1-Score: 0.766839\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [6],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [120],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0.1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1.0]   \n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_mi, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (MI Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, MI Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_mi_tuned = cross_val_predict(best_xgb, X_mi, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, MI Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_mi_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (MI Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5b9a2755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (SelectKBest Features): {'gamma': 0, 'learning_rate': 0.3, 'max_depth': 3, 'n_estimators': 50, 'reg_alpha': 0, 'reg_lambda': 0.1, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, SelectKBest Features): 0.7378947368421054\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, SelectKBest Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.426316, Recall: 0.778846, F1-Score: 0.551020\n",
      "Threshold: 0.15, Precision: 0.509677, Recall: 0.759615, F1-Score: 0.610039\n",
      "Threshold: 0.20, Precision: 0.571429, Recall: 0.730769, F1-Score: 0.641350\n",
      "Threshold: 0.25, Precision: 0.627119, Recall: 0.711538, F1-Score: 0.666667\n",
      "Threshold: 0.30, Precision: 0.682243, Recall: 0.701923, F1-Score: 0.691943\n",
      "Threshold: 0.35, Precision: 0.720000, Recall: 0.692308, F1-Score: 0.705882\n",
      "Threshold: 0.40, Precision: 0.755319, Recall: 0.682692, F1-Score: 0.717172\n",
      "Threshold: 0.45, Precision: 0.802326, Recall: 0.663462, F1-Score: 0.726316\n",
      "Threshold: 0.50, Precision: 0.831325, Recall: 0.663462, F1-Score: 0.737968\n",
      "Threshold: 0.55, Precision: 0.858974, Recall: 0.644231, F1-Score: 0.736264\n",
      "Threshold: 0.60, Precision: 0.873239, Recall: 0.596154, F1-Score: 0.708571\n",
      "Threshold: 0.65, Precision: 0.863636, Recall: 0.548077, F1-Score: 0.670588\n",
      "Threshold: 0.70, Precision: 0.901639, Recall: 0.528846, F1-Score: 0.666667\n",
      "Threshold: 0.75, Precision: 0.907407, Recall: 0.471154, F1-Score: 0.620253\n",
      "Threshold: 0.80, Precision: 0.938776, Recall: 0.442308, F1-Score: 0.601307\n",
      "Threshold: 0.85, Precision: 0.931818, Recall: 0.394231, F1-Score: 0.554054\n",
      "Threshold: 0.90, Precision: 0.916667, Recall: 0.317308, F1-Score: 0.471429\n",
      "\n",
      "XGBoost (SelectKBest Features, Tuned):\n",
      "Best Threshold: 0.50\n",
      "Precision: 0.831325\n",
      "Recall: 0.663462\n",
      "F1-Score: 0.737968\n"
     ]
    }
   ],
   "source": [
    "# Subset data to SelectKBest features\n",
    "X_selectkbest = X_indicators[selected_features_fclassif]\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.3],\n",
    "    'n_estimators': [50],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0.1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1.0]  \n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_selectkbest, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (SelectKBest Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, SelectKBest Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_selectkbest_tuned = cross_val_predict(best_xgb, X_selectkbest, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, SelectKBest Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_selectkbest_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (SelectKBest Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "61983051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (SHAP Features): {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 260, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, SHAP Features): 0.7924609861451967\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, SHAP Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.684615, Recall: 0.855769, F1-Score: 0.760684\n",
      "Threshold: 0.15, Precision: 0.714286, Recall: 0.817308, F1-Score: 0.762332\n",
      "Threshold: 0.20, Precision: 0.747748, Recall: 0.798077, F1-Score: 0.772093\n",
      "Threshold: 0.25, Precision: 0.774510, Recall: 0.759615, F1-Score: 0.766990\n",
      "Threshold: 0.30, Precision: 0.804124, Recall: 0.750000, F1-Score: 0.776119\n",
      "Threshold: 0.35, Precision: 0.819149, Recall: 0.740385, F1-Score: 0.777778\n",
      "Threshold: 0.40, Precision: 0.855556, Recall: 0.740385, F1-Score: 0.793814\n",
      "Threshold: 0.45, Precision: 0.863636, Recall: 0.730769, F1-Score: 0.791667\n",
      "Threshold: 0.50, Precision: 0.873563, Recall: 0.730769, F1-Score: 0.795812\n",
      "Threshold: 0.55, Precision: 0.884615, Recall: 0.663462, F1-Score: 0.758242\n",
      "Threshold: 0.60, Precision: 0.905405, Recall: 0.644231, F1-Score: 0.752809\n",
      "Threshold: 0.65, Precision: 0.914286, Recall: 0.615385, F1-Score: 0.735632\n",
      "Threshold: 0.70, Precision: 0.924242, Recall: 0.586538, F1-Score: 0.717647\n",
      "Threshold: 0.75, Precision: 0.934426, Recall: 0.548077, F1-Score: 0.690909\n",
      "Threshold: 0.80, Precision: 0.948276, Recall: 0.528846, F1-Score: 0.679012\n",
      "Threshold: 0.85, Precision: 0.980769, Recall: 0.490385, F1-Score: 0.653846\n",
      "Threshold: 0.90, Precision: 0.976190, Recall: 0.394231, F1-Score: 0.561644\n",
      "\n",
      "XGBoost (SHAP Features, Tuned):\n",
      "Best Threshold: 0.50\n",
      "Precision: 0.873563\n",
      "Recall: 0.730769\n",
      "F1-Score: 0.795812\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [260],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0.1],\n",
    "    'reg_lambda': [0.1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1.0]  \n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_shap, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (SHAP Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, SHAP Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_shap_tuned = cross_val_predict(best_xgb, X_shap, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, SHAP Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_shap_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (SHAP Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2d9f5f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters (RFE Features): {'gamma': 0, 'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 120, 'reg_alpha': 0.1, 'reg_lambda': 0.1, 'scale_pos_weight': 14, 'subsample': 1.0}\n",
      "Best F1-Score (Grid Search, RFE Features): 0.6798062015503876\n",
      "\n",
      "Threshold Tuning Metrics (XGBoost, RFE Features, Tuned):\n",
      "Threshold: 0.10, Precision: 0.327402, Recall: 0.884615, F1-Score: 0.477922\n",
      "Threshold: 0.15, Precision: 0.399083, Recall: 0.836538, F1-Score: 0.540373\n",
      "Threshold: 0.20, Precision: 0.465909, Recall: 0.788462, F1-Score: 0.585714\n",
      "Threshold: 0.25, Precision: 0.516340, Recall: 0.759615, F1-Score: 0.614786\n",
      "Threshold: 0.30, Precision: 0.579365, Recall: 0.701923, F1-Score: 0.634783\n",
      "Threshold: 0.35, Precision: 0.642202, Recall: 0.673077, F1-Score: 0.657277\n",
      "Threshold: 0.40, Precision: 0.670000, Recall: 0.644231, F1-Score: 0.656863\n",
      "Threshold: 0.45, Precision: 0.730337, Recall: 0.625000, F1-Score: 0.673575\n",
      "Threshold: 0.50, Precision: 0.787500, Recall: 0.605769, F1-Score: 0.684783\n",
      "Threshold: 0.55, Precision: 0.847222, Recall: 0.586538, F1-Score: 0.693182\n",
      "Threshold: 0.60, Precision: 0.861538, Recall: 0.538462, F1-Score: 0.662722\n",
      "Threshold: 0.65, Precision: 0.883333, Recall: 0.509615, F1-Score: 0.646341\n",
      "Threshold: 0.70, Precision: 0.924528, Recall: 0.471154, F1-Score: 0.624204\n",
      "Threshold: 0.75, Precision: 0.937500, Recall: 0.432692, F1-Score: 0.592105\n",
      "Threshold: 0.80, Precision: 1.000000, Recall: 0.394231, F1-Score: 0.565517\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.336538, F1-Score: 0.503597\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.298077, F1-Score: 0.459259\n",
      "\n",
      "XGBoost (RFE Features, Tuned):\n",
      "Best Threshold: 0.55\n",
      "Precision: 0.847222\n",
      "Recall: 0.586538\n",
      "F1-Score: 0.693182\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, eval_metric='logloss')\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],\n",
    "    'learning_rate': [0.1],\n",
    "    'n_estimators': [120],\n",
    "    'scale_pos_weight': [14],\n",
    "    'reg_alpha': [0.1],\n",
    "    'reg_lambda': [0.1],\n",
    "    'gamma': [0],\n",
    "    'subsample': [1.0]  \n",
    "}\n",
    "\n",
    "# Define F1-score as the scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='binary')\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(\n",
    "    xgb_model,\n",
    "    param_grid,\n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    scoring=f1_scorer,\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_rfe, Y)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters (RFE Features):\", grid_search.best_params_)\n",
    "print(\"Best F1-Score (Grid Search, RFE Features):\", grid_search.best_score_)\n",
    "\n",
    "# Train XGBoost with best parameters and perform threshold tuning\n",
    "best_xgb = grid_search.best_estimator_\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "y_scores_xgb_rfe_tuned = cross_val_predict(best_xgb, X_rfe, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "# Threshold tuning\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, RFE Features, Tuned):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores_xgb_rfe_tuned >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (RFE Features, Tuned):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6028e4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnwAAAKSCAYAAABIowakAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQABAABJREFUeJzs3Xd0VHX+//HnvdMnk0IaoabQS+i9FwEVsYuyigK6tt111+1fy6rbXNffrutWsaKogAI2ihJ6U+lFmrRQQ0kv0+fe3x9JRmISSCBhksn7cU5Okjt37v3cyWTmNZ+q6LquI4QQQgghwpYa6gIIIYQQQoj6JYFPCCGEECLMSeATQgghhAhzEviEEEIIIcKcBD4hhBBCiDAngU8IIYQQIsxJ4BNCCCGECHMS+IQQQgghwpwEPiGEEEKIMCeBTwjRYMyaNQtFUcjMzAx1UUQ9Wb16NYqiMH/+/Ho/17Rp00hJSan38wjRGEjgE2GtPEBU9fXb3/62Xs65ceNGnn32WfLz8+vl+HVhz5493HPPPbRq1QqLxULLli25++672bNnT6iLFnKBQICWLVuiKApLly4NdXHqRXnoUhSFd999t8p9hg4diqIodO/e/bLO8f777/OPf/zjCkophKhLxlAXQIir4fe//z2pqakVtl3uG9mlbNy4keeee45p06YRExNTL+e4EgsXLmTKlCnExsZy//33k5qaSmZmJm+88Qbz589n7ty53HLLLaEuZsisXLmSrKwsUlJSeO+997juuutCXaR6Y7Vaef/997nnnnsqbM/MzGTjxo1YrdbLPvb777/PN998w89+9rMrLKUQoi5I4BNNwnXXXUe/fv1CXYwrUlJSQkRExBUd4/Dhw0ydOpW0tDTWrl1LQkJC8Laf/vSnDB8+nKlTp7Jr1y7S0tKutMg1VhfXVlfeffdd+vTpw3333ccTTzxRp2VzOp3Y7fY6OVZduP766/n000/Jzs4mPj4+uP3999+nefPmdOjQgby8vBCWUAhRV6RJVwhg6dKlDB8+nIiICCIjI5k4cWKl5s1du3Yxbdo00tLSsFqtJCUlMWPGDHJycoL7PPvss/zqV78CIDU1NdhslpmZSWZmJoqiMGvWrErnVxSFZ599tsJxFEVh7969/OAHP6BZs2YMGzYsePu7775L3759sdlsxMbGctddd3HixIlLXueLL76I0+nk1VdfrRD2AOLj45k5cyYlJSX89a9/BWD+/PkoisKaNWsqHWvmzJkoisI333wT3LZ//35uv/12YmNjsVqt9OvXj08//bTC/cqb2desWcOjjz5KYmIirVu3rrbMn3zyCRMnTqRly5ZYLBbatWvHH/7wBwKBQIX9Ro0aRffu3dm6dStDhgzBZrORmprKK6+8csnHpZzL5eKjjz7irrvuYvLkybhcLj755JMq9126dCkjR44kMjKSqKgo+vfvz/vvv19leUaMGIHdbueJJ54A4Ny5c9x///00b94cq9VKz549efvttyudY+7cufTt2zd4jvT0dF5++eXg7T6fj+eee44OHTpgtVqJi4tj2LBhZGRk1Oh6b7rpJiwWCx9++GGF7e+//z6TJ0/GYDBUeb9LPf9GjRrF4sWLOXbsWPB/4Pt96TRN409/+hOtW7fGarUyduxYDh06VOlcH374YfBc8fHx3HPPPZw6darSfh9//DHdu3fHarXSvXt3PvrooyrLfqnHVIhwJTV8okkoKCggOzu7wrbyGo3Zs2dz3333MWHCBF544QWcTif/+9//GDZsGNu3bw++UWVkZHDkyBGmT59OUlISe/bs4dVXX2XPnj189dVXKIrCrbfeyrfffsucOXN46aWXgudISEjg/PnztS73HXfcQYcOHfjzn/+MrusA/OlPf+Lpp59m8uTJPPDAA5w/f55//etfjBgxgu3bt1+0Gfmzzz4jJSWF4cOHV3n7iBEjSElJYfHixQBMnDgRh8PBBx98wMiRIyvsO2/ePLp16xZsGt+zZw9Dhw6lVatW/Pa3vyUiIoIPPviAm2++mQULFlRqJn700UdJSEjgd7/7HSUlJdWWedasWTgcDn7+85/jcDhYuXIlv/vd7ygsLOTFF1+ssG9eXh7XX389kydPZsqUKXzwwQc88sgjmM1mZsyYUe05yn366acUFxdz1113kZSUxKhRo3jvvff4wQ9+UKlMM2bMoFu3bvzf//0fMTExbN++nc8//7zCvjk5OVx33XXcdddd3HPPPTRv3hyXy8WoUaM4dOgQP/7xj0lNTeXDDz9k2rRp5Ofn89Of/hQofb5NmTKFsWPH8sILLwCwb98+NmzYENzn2Wef5fnnn+eBBx5gwIABFBYWsmXLFrZt28a4ceMueb12u52bbrqJOXPm8MgjjwCwc+dO9uzZw+uvv86uXbsq3acmz78nn3ySgoICTp48yUsvvQSAw+GocJy//OUvqKrKL3/5SwoKCvjrX//K3Xffzddff13hcZ4+fTr9+/fn+eef5+zZs7z88sts2LChwnN92bJl3HbbbXTt2pXnn3+enJwcpk+fXumDRE0eUyHCli5EGHvrrbd0oMovXdf1oqIiPSYmRv/hD39Y4X5nzpzRo6OjK2x3Op2Vjj9nzhwd0NeuXRvc9uKLL+qAfvTo0Qr7Hj16VAf0t956q9JxAP2ZZ54J/v7MM8/ogD5lypQK+2VmZuoGg0H/05/+VGH77t27daPRWGn7hfLz83VAv+mmm6rdR9d1/cYbb9QBvbCwUNd1XZ8yZYqemJio+/3+4D5ZWVm6qqr673//++C2sWPH6unp6brb7Q5u0zRNHzJkiN6hQ4fgtvK/ybBhwyoc88LbLnzsqnrcH3roId1ut1c418iRI3VA/9vf/hbc5vF49F69eumJiYm61+u96HXruq7fcMMN+tChQ4O/v/rqq7rRaNTPnTsX3Jafn69HRkbqAwcO1F0uV4X7a5pWqTyvvPJKhX3+8Y9/6ID+7rvvBrd5vV598ODBusPhCD7uP/3pT/WoqKhKj9GFevbsqU+cOPGS1/V9q1at0gH9ww8/1BctWqQriqIfP35c13Vd/9WvfqWnpaUFr6Fbt27B+9Xm+Tdx4kQ9OTm52nN36dJF93g8we0vv/yyDui7d+/Wdb30MUlMTNS7d+9e4XFetGiRDui/+93vgtt69eqlt2jRQs/Pzw9uW7ZsmQ5UKENNHlMhwpU06Yom4T//+Q8ZGRkVvqD0E39+fj5TpkwhOzs7+GUwGBg4cCCrVq0KHsNmswV/drvdZGdnM2jQIAC2bdtWL+V++OGHK/y+cOFCNE1j8uTJFcqblJREhw4dKpT3+4qKigCIjIy86DnLby8sLATgzjvv5Ny5c6xevTq4z/z589E0jTvvvBOA3NxcVq5cyeTJkykqKgqWKycnhwkTJnDw4MFKzXA//OEPq20yvNCFj3v5sYcPH47T6WT//v0V9jUajTz00EPB381mMw899BDnzp1j69atFz1PTk4OX3zxBVOmTAluu+2221AUhQ8++CC4LSMjg6KiIn77299WGtSgKEqF3y0WC9OnT6+wbcmSJSQlJVU4j8lk4rHHHqO4uDjYfB4TE0NJSclFm2djYmLYs2cPBw8evOi1Xcz48eOJjY1l7ty56LrO3LlzK5TtQlfy/Pu+6dOnYzabg7+X1zofOXIEgC1btnDu3DkeffTRCo/zxIkT6dy5c7AWOisrix07dnDfffcRHR0d3G/cuHF07dq1wjlr8pgKEa6kSVc0CQMGDKhy0Eb5G+WYMWOqvF9UVFTw59zcXJ577jnmzp3LuXPnKuxXUFBQh6X9zvdHFh88eBBd1+nQoUOV+5tMpmqPVR7kyoNfdb4fDK+99lqio6OZN28eY8eOBUqbc3v16kXHjh0BOHToELqu8/TTT/P0009Xedxz587RqlWraq+tOnv27OGpp55i5cqVwRBa7vuPe8uWLSsNsCgvY2ZmZjCgV2XevHn4fD569+5doS/ZwIEDee+99/jRj34ElA58gZqN8m7VqlWFUANw7NgxOnTogKpW/LzdpUuX4O1Q2uT9wQcfcN1119GqVSvGjx/P5MmTufbaa4P3+f3vf89NN91Ex44d6d69O9deey1Tp06lR48elyxbOZPJxB133MH777/PgAEDOHHiRKUm7HJX8vz7vrZt21b4vVmzZgDBQSLlj0OnTp0q3bdz586sX7++wn5VlalTp04VPozV5DEVIlxJ4BNNmqZpQGk/vqSkpEq3G43f/YtMnjyZjRs38qtf/YpevXrhcDjQNI1rr702eJyL+X7tT7nvDz640IW1W+XlLZ8frqrase/3k7pQdHQ0LVq0qLJf1oV27dpFq1atgmHXYrFw880389FHH/Hf//6Xs2fPsmHDBv785z9XKBfAL3/5SyZMmFDlcdu3b3/Ra6tKfn4+I0eOJCoqit///ve0a9cOq9XKtm3b+M1vflOjx72m3nvvPaB0/rmqHDlypNYjl2tyjdVJTExkx44dfPHFFyxdupSlS5fy1ltvce+99wYHeIwYMYLDhw/zySefsGzZMl5//XVeeuklXnnlFR544IEan+sHP/gBr7zyCs8++yw9e/asVDNW7kqef99XXe2uXtZXtT7U5DEVIlxJ4BNNWrt27YDSN4Jrrrmm2v3y8vJYsWIFzz33HL/73e+C26tqSqsu2JXXYHx/QubyGoqallfXdVJTU4M1V7Vxww038Nprr7F+/foKo37LrVu3jszMzArNolDarPv222+zYsUK9u3bh67rweZcIBiETCbTRR/H2lq9ejU5OTksXLiQESNGBLcfPXq0yv1Pnz5daRqVb7/9FuCiKy4cPXqUjRs38uMf/7jS4BRN05g6dSrvv/8+Tz31VPA5880331QKsTWRnJzMrl270DStQi1fefN0cnJycJvZbGbSpElMmjQJTdN49NFHmTlzJk8//XTw3LGxsUyfPp3p06dTXFzMiBEjePbZZ2sV+IYNG0bbtm1ZvXp1cDBDVWrz/Kvu/6Cmyh+HAwcOVKqBP3DgQPD28u9V/S8eOHCg0raaPKZChCPpwyeatAkTJhAVFcWf//xnfD5fpdvLR9aW10Z8v/ahqpUEysPG94NdVFQU8fHxrF27tsL2//73vzUu76233orBYOC5556rVBZd1ytMEVOVX/3qV9hsNh566KFK++bm5vLwww9jt9uDU8uUu+aaa4iNjWXevHnMmzePAQMGVGiSTUxMZNSoUcycOZOsrKxK572cEcpQ9ePu9Xqrfcz8fj8zZ86ssO/MmTNJSEigb9++1Z6nvHbv17/+NbfffnuFr8mTJzNy5MjgPuPHjycyMpLnn38et9td4Tg1qZ26/vrrOXPmDPPmzatQ7n/96184HI5g4Pz+30dV1WBTrcfjqXIfh8NB+/btg7fXlKIo/POf/+SZZ55h6tSp1e5Xm+dfRETEFXV16NevH4mJibzyyisVrmfp0qXs27ePiRMnAtCiRQt69erF22+/XeF8GRkZ7N27t8Ixa/KYChGupIZPNGlRUVH873//Y+rUqfTp04e77rqLhIQEjh8/zuLFixk6dCj//ve/iYqKYsSIEfz1r3/F5/PRqlUrli1bVmVNU3mwePLJJ7nrrrswmUxMmjSJiIgIHnjgAf7yl7/wwAMP0K9fP9auXRusgaqJdu3a8cc//pH/+7//IzMzk5tvvpnIyEiOHj3KRx99xIMPPsgvf/nLau/foUMH3n77be6++27S09MrrbSRnZ3NnDlzgrVY5UwmE7feeitz586lpKSE//f//l+lY//nP/9h2LBhpKen88Mf/pC0tDTOnj3Ll19+ycmTJ9m5c2eNr7PckCFDaNasGffddx+PPfYYiqIwe/bsaoNVy5YteeGFF8jMzKRjx47MmzePHTt28Oqrr160f9l7771Hr169aNOmTZW333jjjfzkJz9h27Zt9OnTh5deeokHHniA/v37B+dJ3LlzJ06n85JNgw8++CAzZ85k2rRpbN26lZSUFObPn8+GDRv4xz/+Eew7+cADD5Cbm8uYMWNo3bo1x44d41//+he9evUK9vfr2rUro0aNom/fvsTGxrJlyxbmz5/Pj3/845o8vBXcdNNN3HTTTRfdpzbPv759+zJv3jx+/vOf079/fxwOB5MmTapxeUwmEy+88ALTp09n5MiRTJkyJTgtS0pKCo8//nhw3+eff56JEycybNgwZsyYQW5uLv/617/o1q0bxcXFwf1q8pgKEbau/sBgIa6e8mk+Nm/efNH9Vq1apU+YMEGPjo7WrVar3q5dO33atGn6li1bgvucPHlSv+WWW/SYmBg9Ojpav+OOO/TTp09XmlJF13X9D3/4g96qVStdVdUK04w4nU79/vvv16Ojo/XIyEh98uTJ+rlz56qdluX8+fNVlnfBggX6sGHD9IiICD0iIkLv3Lmz/qMf/Ug/cOBAjR6XXbt26VOmTNFbtGihm0wmPSkpSZ8yZUpwSoyqZGRk6ICuKIp+4sSJKvc5fPiwfu+99+pJSUm6yWTSW7Vqpd9www36/Pnzg/tc7G9S1bQsGzZs0AcNGqTbbDa9ZcuW+q9//Wv9iy++0AF91apVwf3KpxDZsmWLPnjwYN1qterJycn6v//974s+Flu3btUB/emnn652n8zMTB3QH3/88eC2Tz/9VB8yZIhus9n0qKgofcCAAfqcOXMqlacqZ8+e1adPn67Hx8frZrNZT09PrzRdz/z58/Xx48friYmJutls1tu2bas/9NBDelZWVnCfP/7xj/qAAQP0mJgY3Waz6Z07d9b/9Kc/XXIKmgunZbmY6q6hJs+/4uJi/Qc/+IEeExNTYXqU6s5d3bRF8+bN03v37q1bLBY9NjZWv/vuu/WTJ09WWaYuXbroFotF79q1q75w4UL9vvvuqzAtS00eUyHClaLr9dhDVgghrpJRo0aRnZ1dYeUPIYQQpaQPnxBCCCFEmJPAJ4QQQggR5iTwCSGEEEKEOenDJ4QQQggR5qSGTwghhBAizEngE0IIIYQIcxL4hBBCCCHCnAQ+IYQQQogwJ4FPCCGEECLMSeATQgghhAhzEviEEEIIIcKcBD4hhBBCiDAngU8IIYQQIsxJ4BNCCCGECHMS+IQQQgghwpwx1AUQQgghrrZAIIDP5wt1MYSoxGAwYDKZ6vy4EviEEEI0Gbquc+bMGQoKCtB1PdTFEaJKFouF+Ph4oqKi6uyYEviEEEI0GQUFBeTn55OQkEBERASKooS6SEIE6bqOz+ejoKCAU6dOAdRZ6JPAJ4QQoknQdZ1z584RFRVFfHx8qIsjRJVsNhuRkZGcPHmS7OzsOgt8MmhDCCFEkxAIBAgEAnXaTCZEfVAUhejoaDweT531NZXAJ4QQoknw+/0AGI3SuCUavvKBG4FAoE6OJ4FPCCFEkyL99kRjUNfPUwl8QgghhBBhTgKfEEIIIUSYk44MQgghBJB/vgBnoSvUxQiyR9mISYgOdTFEmJDAJ4QQosnLP1/Ai9P/i7PQGeqiBNmj7PzqrUcvO/StXr2a0aNHV9hmsVho2bIlI0eO5Ne//jVdunSpi6JetszMTGbNmsXNN99Mr169QlqWcCeBTwghRJPnLHThLHTSqX97ImMdoS4ORbnFHNh8CGeh64pr+aZMmcL1118PgMvlYteuXbz++ussWLCA3bt3k5ycXBdFviyZmZk899xzpKSkSOCrZxL4hBBCiDKRsQ5iEsJrnr4+ffpwzz33VNjWoUMHfvrTn7Jw4UIef/zxOjlPUVERkZGRdXIsUfdk0IYQQgjRxLRs2RIAs9kc3Ob3+3nhhRfo2rUrVquVuLg4brnlFnbv3l3hvpmZmSiKwrPPPsu8efPo27cvNpuNn/zkJwCMGjWKlJSUSue88H4As2bNCjY5T58+HUVRUBSFUaNGAaBpGn/6058YMWIESUlJmM1m2rZtyyOPPEJOTk4dPyLhT2r4hBBCiDDmdDrJzs4GSpt0v/nmG5588kni4+O57bbbgvvdfffdfPDBB4wbN45HHnmEM2fO8J///IfBgwezbt06evfuXeG4H3/8Mf/85z955JFHePjhh2u9gsmIESN44okn+POf/8yDDz7I8OHDAWjevDkAXq+XF198kdtuu42bbrqJiIgINm/ezBtvvMH69evZunVrhcAqLk4CnxBCCBHGnnnmGZ555pkK27p27cq6detISkoCICMjgw8++IDJkyczd+7c4KS/kydPpm/fvjz22GOsW7euwjH27NnDrl27LnvgR1paGuPGjePPf/4zgwcPrtTsbLFYyMrKwmazBbc9/PDDDBkyhAceeICPP/6YyZMnX9a5myJp0hVCCCHC2IMPPkhGRgYZGRl89tlnvPDCC2RnZ3P99ddz7NgxAD766CMAnnzyyQorPPTs2ZNJkyaxfv16zp8/X+G4EydOrNdRvoqiBMNeIBAgPz+f7OxsxowZA8DXX39db+cORxL4hBBCiDDWoUMHrrnmGq655hpuuOEGfv3rX/Ppp59y9OhRfvOb3wBw9OhRVFWtMsB169YtuM+FOnbsWO9l/+CDDxg4cCA2m41mzZqRkJBAWloaAHl5efV+/nAiTbpCCCFEEzNw4ECio6NZuXLlZR/DbrdXub26NWD9fn+tjr9w4ULuvPNOBgwYwMsvv0ybNm2wWq0EAgGuvfZaNE2rdZmbMgl8QgghRBPk9/vxeDxAaX86TdPYt28fPXr0qLDf3r17AUhNTa3RcWNjY9m6dWul7UeOHKm0rbpwCDB79mysViurVq2qEC73799fo3KIiqRJVwghhGhiMjIyKCkpoW/fvgDcfPPNADz//PPouh7c75tvvuHTTz9l2LBhJCQk1OjYHTt2pKioiE2bNgW3aZrGSy+9VGlfh6N0kuvc3NxKtxkMBhRFqVCTp+s6f/zjH2tUDlGR1PAJIYQQZYpyi0NdBKBuy7Ft2zbeffddADweD3v27OHVV1/FZDIFw9O4ceOCI3Tz8vK44YYbgtOyWK1W/vnPf9b4fA8++CB/+9vfuOWWW/jpT3+K2Wxm/vz5VTbpdu3alcjISP773/9it9uJiYkhMTGRMWPGcPvtt7NgwQLGjBnDvffei8/n4+OPP8bpbDjL3zUmEviEEEI0efYoG/YoOwc2Hwp1UYLsUXbsUbZL73gJc+bMYc6cOQCoqkpcXBzjx4/n//7v/+jfv39wv/fee48+ffowa9YsfvGLXxAREcHIkSP5wx/+QHp6eo3Pl5qayscff8wTTzzB008/TVxcHFOnTmXGjBl07ty5wr42m425c+fy1FNP8bOf/QyPx8PIkSMZM2YMd911F0VFRbz00kv88pe/pFmzZkyaNIm//OUvxMXFXfHj0tQo+oV1t0IIIUSYcrvdHD16lNTUVKxWa6Xb888X4Cx0haBkVbNH2a54HV3ReF3q+VpbUsMnhBBCADEJ0RKwRNiSQRtCCCGEEGFOAp8QQgghRJiTwCeEEEIIEeYk8AkhhBBChDkJfEIIIYQQYU4CnxBCCCFEmJPAJ4QQQggR5iTwCSGEEEKEOQl8QgghhBBhTgKfEEIIIUSYk8AnhBBCCBHmZC1dIYQQAsjPd1Li9IS6GEERdgsxMfZQF0OECQl8Qgghmrz8fCcvvLgYZwMKfHa7hd/8auIVhb4jR47wl7/8hbVr13L8+HEsFgtJSUkMGDCAadOmMXr0aABSUlJwOBx88803VR5n2rRpvP3225w/f574+PhKt+/bt4+uXbsCsHbtWoYPH17lcRRFqfC7xWKhTZs2TJo0iaeeeorY2NjLvlZxcRL4hBBCNHklTg9Op4fOnVoQGWkLdXEoKnKx/0AWJU7PZQe+LVu2MHLkSEwmE/feey/dunXD5XJx8OBBli1bRmRkZDDwXak33niDyMhIbDYbb775ZrWBD6BXr1784he/ACA3N5clS5bw0ksvkZGRwdatWzGbzXVSJlGRBD4hhBCiTGSkLWyaUZ977jmcTic7duygZ8+elW4/c+ZMnZzH5/Mxe/Zs7rjjDqKjo3n11Vf55z//SWRkZJX7t2rVinvuuSf4+2OPPcakSZNYtGgRn3zyCXfccUedlEtUJIM2hBBCiDB08OBB4uLiqgx7AElJSXVyns8++4xz585x3333MW3aNEpKSpg3b16tjjFhwgQADh06VCdlEpVJ4BNCCCHCULt27cjJyWHhwoU12j8QCJCdnV3ll8dTfd/GN954g9TUVIYPH06PHj3o3bs3b775Zq3KevDgQYAq+weKuiFNukIIIUQYeuqpp8jIyOC2226jQ4cODBs2jP79+zNq1Ci6dOlSaf/9+/eTkJBQq3OcPn2aL774gqeeeio4IOO+++7jZz/7Gfv27avyPD6fj+zsbADy8vJYtGgR//vf/4iOjuamm266jCsVNSE1fEIIIUQYGjx4MFu3buW+++6joKCAt956i0cffZSuXbsyYsQIjhw5UmH/lJQUMjIyqvwaP358leeYNWsWmqZx7733BrfdfffdmEymamv5li1bRkJCAgkJCXTs2JGf//zndO3alWXLlpGYmFh3D4CoQGr4hBBCiDCVnp7OrFmzADh27Bhr1qzh9ddfZ926ddx0000VRsVGRERwzTXXVHmcd999t9I2Xdd588036dGjB5qmVeh/N3ToUGbPns3zzz+P0VgxagwcOJA//vGPQOm0LMnJybRt27YuLldchAQ+IYQQoglITk7m3nvvZerUqQwfPpwNGzawadMmhg0bdlnHW7NmDYcPHwagQ4cOVe6zaNEibr755grb4uPjqw2Wov5I4BNCCCGaEEVRGDhwIBs2bODUqVOXfZw333wTi8XCO++8g6pW7iH20EMP8cYbb1QKfCI0JPAJIYQQYSgjI4PRo0dXalJ1uVwsW7YMILg6Rm0VFBQwf/58xo8fz+TJk6vcZ/HixcyePZusrCxatGhxWecRdUcCnxBCCFGmqMgV6iIAdVOOxx9/nJycHG688UbS09Ox2+2cOHGC999/n2+//ZZ7772X9PT0yzr2nDlzcLlc3HbbbdXuc9tttzFr1izefvttfvvb317uZYg6IoFPCCFEkxdht2C3W9h/ICvURQmy2y1E2C2Xff+///3vfPLJJ6xfv54FCxaQn59PdHQ0PXr04De/+Q3Tpk277GO/8cYbGI1Gbrzxxmr3GTduHJGRkbz11lsS+BoARdd1PdSFEEIIIeqb2+3m6NGjpKamYrVaK92en++kxFn9BMNXW4TdEjbLvInau9Tztbakhk8IIYQAYmLsErBE2JKJl4UQQgghwpwEPiGEEEKIMCeBTwghhBAizEngE0IIIYQIcxL4hBBCCCHCnAQ+IYQQQogwJ4FPCCGEECLMSeATQgghhAhzEviEEEIIIcKcBD4hhBBCiDAngU8IIYQQIszJWrpCCCGqFfAHOH34DK07tkRRlFAXp17lFjsp9nhDXYwgh8VMrEPW9hV1QwKfEEKISnRdZ/+mQyx+NYOsI2e5+6nb6TM2PdTFqje5xU6enL+MYo8n1EUJclgs/On28Zcd+lavXs3o0aN58cUX+eUvf1ntfs8//zzbtm1j69atHD16lOTkZDIzMy+z1KKhksAnhBCigqyjZ1n86nL2bNhPRLQdn8eHu9gd6mLVq2KPl2KPh/TWSUTZLKEuDoUuD7tPnqHY4633Wr4nnniC2NhY+vTpQ35+fr2eS4SOBD4hhBAAFOUVkzF7DV9+shlQ6D6sM82TE1jx3joCAS3UxbsqomyWJteMevjwYdLS0gDo3r07xcXFIS6RqA8S+IQQoonzeX1s/HgzGbPXUFLgJK1HMind2qAaSsf1KYqC1kQCX1NUHvZEeJPAJ4QQTZSu6+xet48lry0n6+g5WqY1p++4npitpgr7KaoEPiEaOwl8QgjRBJ04cIpFMzM4sPkQ0fFRDLmxH46YiCr3VRSFgD9wlUsohKhLEviEEKIJyT9fwBdvrWLT0u2YzEZ6je5OQuu4i95HaviEaPwk8AkhRBPgdXtZ8+GXrJqzHneJh/a9U2nbuRWKeum59aQPnxCNnwQ+IYQIY5qmsX3Fbpa+sYLzJ3Np07EFHfqkYTTX/OVfUaVJV4jGTgKfEEKEqSO7jrFo5jIO78wkrkUsw24ZgD3SVuvjSA2fEI2fBD4hhAgz2adzWfrGCrav2I3FbqHf+F7EJsVc9vEUBQJ+CXxCNGYS+IQQIky4il2smruRNR9sxO/z06l/e1q1T7riNXAVVW0yNXyFroaxtFpdlmPFihW43ZVXSomPj+fhhx9m9uzZHDt2DIDz58/j9Xr54x//CEBycjJTp06ts7KI0JHAJ4QQjVzAH2DT0u18MWsVeWcLSOnWhnY9kzEYDXVyfEUh7FfacFjMOCwWdp88E+qiBDksFhwW8xUf5/PPP+fzzz+vtL1Tp048/PDDvPHGG6xZs6bCbU8//TQAI0eOlMAXJiTwCSFEI6XrOt9uOcyiVzM4vvckiW3jGXHbIKwRdbsWbFOYhy/WYedPt4+n2OMNdVGCHBbzFS3zNmrUKHRdv+R+q1evvuxziMZDAp8QQjRCZzLPseS15exet4+IKDsDJ/YhOj6qXs7VVAZtxDrsTW4dXdF0SOATQohGpDi/hBXvrWX9wk3o6HQb2pmklIQr7qd3MU0l8AkRziTwCSFEI+Dz+vjy0y1kzF5DUW4JaT3aktq9LapBrfdzK4bwb9IVItxJ4BNCiAZM13X2bNjPoleXk3X4DC3SmtNnbDpm65V35q8pVVHw+yTwCdGYSeATQogG6uTBLBbNXMb+rw8RFRfJkBv742gWcdXLoaiq1PAJ0chJ4BNCiAamILuQZW+v5qtFWzEaDfQc1ZXENvEhK4+iKmgBCXxCNGYS+IQQooHwur2sW/A1K95fh6vIRfveqSR3aY2i1t+AjJpQVWnSFaKxk8AnhBAhpus621d+w9I3VnDu2Hlad2rJgOt6YzI3jJdoRVXRZGk1IRq1hvFqIoQQTdTRb46zeGYGB7cfJTYphmG3DMQeZQt1sSpQVQW/9OETolGTwCeEECGQk5XHF2+tZMuynZitZvqN60Fsi2ahLlaVVFVBk8AnRKMmgU8IIa4iV4mbNfM2svqDjfjcPjr2bUfrji3qdeLkK6WoativpStEuJPAJ4QQV0EgEGDLFzv5/M2V5J3Jp23X1rTrmYLRZAh10S5JVRV8HqnhE6Ixk8AnhBD17Nuth1k0M4Nje06Q0DqO4bcNxBphDXWxakxRlSZRw5fjLqHY7w11MYIcRjNx1qs/76IITxL4hBCinpw7fp4lr61g59q92B1WBlzfh5iEqFAXq9ZKR+mGdw1fjruEX2/6jGJfAwp8JjN/HTDpskPf6tWrGT16NAA/+tGP+Pe//11pn3PnztG6dWt8Ph8jR45k9erVAIwaNYotW7ZQXFx82eUXDYsEPiGEqGMlBSWseG896z/6Ci2g021wR5JSExt0P72LUZtADV+x30uxz0uP2BZEm0Nf+1rgdbMrN4tiv5c4rqyWz2q18v777/O3v/0Ni8VS4bbZs2ej6zpGo8SBcCd/YSGEqCN+n58vP9tKxjurKcwpJjW9LWnpbVENaqiLdkUUVWkyS6tFm61h14x6yy23MGfOHD755BMmT55c4ba33nqL66+/nhUrVoSodOJqadyvQkII0QDous6ejQf4+w9fYf7fPsXmsDLi9kG075XS6MMegKqqaGFewxfO+vTpQ48ePXjrrbcqbN+0aRN79uxh+vTpISqZuJqkhk8IIa7AqUNZLH51Ofu+/JbIWAeDJ/UjMtYR6mLVqdK1dCXwNWYzZszg5z//OadOnaJVq1YAvPnmmyQmJnLDDTeEuHTiamj8Hz2FECIECnOKWPCPRbz00EwObT9Kj5FdGTixT9iFPSibeFnT0TQJfY3VPffcg9Fo5O233wbA5XIxd+5cpk6dKv33mgj5KwshRC14PT7WL/yaFe+txVnool2vFJK7tkZVw/fzs6KqoEPAH0A1h+91hrO4uDhuvPFGZs2axRNPPMHChQspKChgxowZoS6auEok8AkhRA3ous7O1XtY8vpyzmaep1WHFvSf0AuTxRTqotU7RVXQ0aVZt5GbPn06EydOZP369bz55psMGDCArl27hrpY4iqRwCeEEJdwbO8JFs3M4ODWI8Q0j2HozQOIiLaHulhXjaoqoCOBr5GbMGECrVq14rnnnmPVqlX873//C3WRxFUkgU8IIaqRdzafz99axZbPt2OymulzTTpxLWNDXayrTilrrpbA17gZDAbuvfdenn/+eWw2G1OmTAl1kcRVJIFPCCG+x1XiZu2HX7Jq7gZ8bi8d+rajdccWjXbi5Culqgq6rhPwS+Br7B5++GHMZjNpaWlERTW+VV/E5ZPAJ4QQZQKBAFuX7eKLt1aSfTqPtl1a0b5XCkZT036pVMqadMN9tQ0oXeGiIaivcrRt25Znn322Xo4tGram/SomhBBlDm0/yqKZGRzdfYz4VrEMv3UgNkfol9hqCFRVDftBGw6jGYfJzK7crFAXJchhMuMwmkNdDBEmJPAJIZq0cyeyWfr6Cnau3oPVYaX/tb1p1jw61MVqUMoHbYTz8mpx1gj+OmASxX5vqIsS5DCar2iZt1GjRqHreo32LS4urvD76tWrL/u8omGSwCeEaJJKCp2smrOBtfO/RAtodBnUgRZpzZtsP72LUhR0wn/QRpw1gjjCax1dIcpJ4BNCNCkBf4CvFm1l2durKThfSGp6W1J7JGMIgzVv64tqUEHXw7qGT4hwJ4FPCNEk6LrOvq8PsvjVDE4cOE1SSiIj7hiMxSZ9pC6lvNYz3Gv4hAhnEviEEGEv68hZFr+awZ6NB3DERDB4Uj+iwnDN2/qiGhR0mXhZiEZNAp8QImwV5hax/N21bPxkM6qikj68C82TE0JdrEandJ1gmYdPiMZMAp8QIuz4vD42fLSJ5e+upSTfSbteKSR3a10WXERtKYrU8AnR2EngE0KEDV3X2bV2L0teW86ZzPO0atecfuN7YrKYQl20Rk01KKWDNiTwCdFoSeATQoSFY/tOsvjV5Xy75RAxCdEMvak/EdH2UBcrLCiq2iSmZREinEngE0I0annnCvjirZVsXroDo9lI7zHpxLeKDXWxwoqihP/Ey0KEOwl8QohGye30sG7+V6ycsw6Py0uHPmm06dRSJk6uB6pBgTBfWk2IcCeBTwjRqGiaxtaMXXz+xgqyT+fRtlNL2vdJxWiSl7P6oqgqyKANIRo1eYUUQjQah3dmsmjmMo7sPEZ8qziG3zoQm8Ma6mKFPVUtXVpNmnSFaLwk8AkhGrzsUzkseWMFO1Z8gyXCQv9re9GseUyoi9VkNJWVNkr8BbgDrlAXI8hqsBFhjA51MSoZNWoUmZmZZGZmhrooohYk8AkhGixnkYvVczew5sMv8fv8dB7YgZbtmks/vRBQFCWsJ14u8Rcw/8S/8QScoS5KkMVg5/Y2P76i0HfkyBH+8pe/sHbtWo4fP47FYiEpKYkBAwYwbdo0Ro8eXYclrluzZs0iPz+fn/3sZ5VumzZtGm+//Tbnz58nPj4+uF3XdR5//HFefvllbrnlFubMmYPFYiElJYVjx44F91MUhcTERDp16sQjjzzCXXfdVa/XkpmZyaxZs7j55pvp1atXvZ6rOhL4hBANTsAf4Osl21g2azX55wpI6daGtJ7JGIyGUBetyVJVJaxr+NwBF56Ak9b29tgMoV92zxUo5qTzEO6A67ID35YtWxg5ciQmk4l7772Xbt264XK5OHjwIMuWLSMyMrLBB77MzMwqA19VAoEA999/P2+//TbTp0/ntddew2D47jWjdevWPP/888F9T506xdtvv82UKVPIysri8ccfr4/LAEoD33PPPUdKSooEPiGE0HWdA5sPsfjV5Rzfd5LmyQkMv30QVrsl1EVr8hRVaRITL9sMjgbZjHo5nnvuOZxOJzt27KBnz56Vbj9z5kwISlU/PB4PU6ZM4aOPPuLxxx/nb3/7W6WWgOjoaO65554K2x566CFatGjBrFmz6jXwNQSyzpAQokE4k3mON598n5m/fIf8cwUMnNiXnqO6Neqwp2kaHy9fwMO/m8EtP5rItN/+gNc/nInb03D6idWUoihoMmijUTl48CBxcXFVhj2ApKSkCr8vX76c8ePHExMTg9VqpUePHrzyyiu1Ot/UqVNp0aIFZrOZlJQUfvWrX1FSUlJp3zNnzvDYY4+RlpaGxWIhMTGRcePGkZGRAUBKSgpr1qzh2LFjKIoS/Fq9enWlY5WUlHDDDTfw0Ucf8fvf/56///3vNe720axZM6xWK2az+bKv58SJE8yYMYPk5OTgtQwZMoS3334bKK2pLK9JnT59evBaRo0aVaMy1hWp4RNChFRRXjHL313Lxo83A9B9WGeaJyeERT+91z58hc9WfszgXkO5edztnMg6zmcrP+bIiUP88WcvNKq1fZtKDV84adeuHQcOHGDhwoXceuutF9331Vdf5eGHH2bQoEE8+eSTREREkJGRwSOPPMLhw4d58cUXL3r/rVu3MmbMGGJiYnjooYdo1aoVO3fu5J///CcbNmxgzZo1mEylSxxmZmYydOhQzp49y7333ku/fv0oKSnhq6++Yvny5YwbN45//OMf/N///R/Z2dm89NJLwfN06dKlwnnz8vKYNGkSX3/9Nf/+97/50Y9+VG0ZA4EA2dnZwZ+zsrJ4+eWXKSoq4qGHHrqs6/H7/YwbN45Tp07x6KOP0rFjRwoKCti1axfr1q3jvvvuY8SIETzxxBP8+c9/5sEHH2T48OEANG/e/KKPaV2TwCeECAmf18fGT7aQ8c5qSgqcpPVIJqVbG1RD4wlBF3PsdCaLVn3CkN7DeOLh3wW3J8UnMXPef1m7ZTWjBowJYQlrR1HCuw9fOHrqqafIyMjgtttuo0OHDgwbNoz+/fszatSoCsEpKyuLxx57jLvuuov3338/uP3RRx/lpz/9KX//+9955JFHSEtLq/ZcM2bMoEWLFmzevJnIyMjg9rFjx3Lrrbfy3nvvMW3atOBxT58+zeeff86ECRMqHEfTSp9jN998M//4xz9wuVyVmmEvdO2113L8+HFmz57N3XfffdHHY//+/SQkJFTYZrVamTlzJg888MBlXc/evXs5cOAAL7zwAr/+9a+rPG9aWhrjxo3jz3/+M4MHD77o9dSn8HhlFUI0Grqus2vtXv72wCss/MciIps5GHH7INJ6JIdN2ANYu2kVuq5z49hbKmyfMPx6LGYLq75eEaKSXZ5wH7QRjgYPHszWrVu57777KCgo4K233uLRRx+la9eujBgxgiNHjgAwf/58PB4P999/P9nZ2RW+Jk2ahKZpLF++vNrz7N69m127dvGDH/wAj8dT4f7Dhg0jIiKCZcuWAZCbm8vnn3/OtddeWynsAbWu9c7KysJqtZKamnrJfVNSUsjIyCAjI4Nly5Yxa9YsBg0axCOPPMJbb711WdcTHV3a33PVqlWcO3euVmW/2qSGTwhx1Zw4cIrFr2awf9MhouOjGHJjfxzNIkJdrHrx7bFvURWVTimdKmw3m8yktWnHwcxvQ1Syy6QoMvFyI5Sens6sWbMAOHbsGGvWrOH1119n3bp13HTTTWzdupV9+/YBcM0111R7nLNnz1Z7W/n9n3nmGZ555pmL3v/QoUPouk7v3r0v53Iq+fDDD7n77ru59tprWbp0KUOHDq1234iIiErXePfdd9O7d29+8pOfcOONNxIXF1er60lOTubJJ5/k+eefp0WLFvTq1YuxY8dyxx130L9//zq5xroigU8IUe/yzxfwxazVbFqyDaPJSK/R3UloHRfqYtWr3PwcohxRmEyVO4PHxcSz7/BefH4fJqMpBKWrPUVq+Bq95ORk7r33XqZOncrw4cPZsGEDmzZtQtd1AN555x1atGhR5X0v1pxbfv9f/OIXXHvttVXu06xZsyssfdUGDhzIsmXLGD9+fDD0DRs2rMb3NxqNjB07lpdffpmvv/6a66+/vtbX88c//pEZM2awePFi1q1bx+uvv86LL77Ir3/9a1544YUru8A6JIFPCFFvvG4va+d/xcr31+Eq8dChdyptO7dCURv/gIxL8Xg91Ya58hB4sX0aGunDFz4URWHgwIFs2LCBU6dO0aFDBwDi4+MvWstXnfL7GwyGS96/ffv2KIrCjh07alTOmhgwYAAZGRmMHz+e6667jiVLlgQHRtSEz+cDoKioCKjd9ZRLS0vjJz/5CT/5yU9wu91MmDCBv/71r/ziF78gMTGxQQxCC58OM0KIBkPTNLZm7OSv0/7DZ//7gtikGEbdMZjkrq2bRNgDsJgt+Py+Km/z+bzBfRoLRQ3vlTbCUUZGBn6/v9J2l8sV7IPWtWtXJk+ejMVi4ZlnnsHlqjxlUEFBAR6Pp9rz9O7dm+7du/PKK68E+wVeyO/3k5ubC0BsbCzXXXcdS5curbJfYHntGoDD4SAvL6/Ctur079+fjIwMjEYj1113HWvXrr3kfQDcbjeff/45AH369Kn19RQUFAQDYzmr1RocFJOXlxe8FiB4v1CQGj4hRJ06uvsYi2ZmcGjHUeKSmjHsloHYo2yhLtZVFxsTx4ms4/h83krNujn52UQ5ohtN7R40nRo+V6A41EUA6qYcjz/+ODk5Odx4442kp6djt9s5ceIE77//Pt9++y333nsv6enpAPzvf//jgQceoEuXLkydOpXk5GTOnz/P7t27+fjjj9m7dy8pKSlVnkdRFGbPns2YMWPo0aMHM2bMoFu3bjidTg4dOsTChQt5/vnng6N0//3vfzNkyBCuu+467rvvPvr27YvL5eLrr78mJSUl2Aw6aNAgFi1axI9//GOGDBmCwWBgzJgxJCYmVlmOfv36Bad1uf7661m8eDEjR44M3l5QUMC7774LlAbL06dP8+6773LkyBF++MMfBmv2anM9q1at4sEHH+S2226jU6dOOBwOtm7dyuuvv87AgQPp1Km0D2/Xrl2JjIzkv//9L3a7nZiYGBITExkz5uqN1JfAJ4SoEzlZeSx9YwXblu/CYjPTb3wvYpNiQl2skOmY3JHte7dyIPMA3TukB7d7fV6OnDhcYVtjoCiE9Tx8VoMNi8HOSeehUBclyGKwYzVc/oelv//973zyySesX7+eBQsWkJ+fT3R0ND169OA3v/lNMIBB6YTAHTt25P/9v//HzJkzyc/PJz4+nk6dOvGHP/yh0iTN39erVy+2b9/O888/z6effsorr7xCZGQkKSkpTJs2jbFjxwb3TU1NZcuWLfzhD39gyZIlvPPOOzRr1oyePXvy4IMPBvd7/PHHOXLkCPPnz+eVV15B0zRWrVpVbeAD6Nu3L8uXL+eaa67h+uuvZ9GiRcFJj0+ePMnUqVOD+9rtdrp27cp///vfSvPw1fR6evbsya233srq1at57733CAQCtG3blieeeIJf/OIXwePZbDbmzp3LU089xc9+9jM8Hg8jR468qoFP0WtSVyqEENVwFbtYNXcjaz/ciM/rp2O/drRqn9Qg+qyEUuapo/zkDw8zuNfQCvPwfbbyY2bO+y+/mP5rRg+qfX+pUPnysy10H9aFu35zc6iLctncbjdHjx4lNTUVq9Va6fYSfwHuQMNZBcVqsIXNMm+i9i71fK0tqeETQlyWgD/A5s938MWsVeSdySe5a2va9UrBYDRc+s5NQEqrVCaOvJFFqz/hT/97jn7pA4IrbXTv2IORjWjSZWgaTboRxmgJWCJsSeATQtSKrut8u/UIi2dmcGzvCRLaxDP8tkFYIxrPAISr5Yd3PkxifHO+WLeEzd9sIsoRxQ2jb+KeG+9rVMuqQWngk3n4hGi8JPAJIWrs7LHzLH41g93r9xMRaWPg9X2ITogKdbEaLINq4NZxt3PruNtDXZQrphjCv4ZPiHAmgU8IcUklBSUsf3ct6xduAqDbkE4kpSQ0+X56TYmqKPh9UsMnRGMlgU8IUS2f18dXn20l4501FOYWkZaeTGp627Ba81bUjGpQpUlXiEZMAp8QohJd19mzYT+LX1vB6UNZtEhrTu+x3TFbKy8TJpoI6cMnRKMmgU8IUcHJg1ksfjWD/V8dJDLOweAb+xHZzBHqYokQU1UJfEI0ZhL4hBAAFGQXsuzt1Xy1aCtGo4Eeo7qS2CY+1MUSDYSiqmiytJoQjZYEPiGaOK/by7oFX7Pi/XW4ily0751Kcpems+atqBlVVfBLDZ8QjZYEPiGaKF3X2b7yG5a+sYJzx8/TukNLBlzXG5NZXhZEZdKkK0TjJq/sQjRBR785zuJXMzi47SixzWMYevMAIqLsoS6WaMCkSVeIxk0CnxBNSE5WHl+8tZKty3ZisprpN64HsS2ahbpYohFQVQWfR2r4hGisJPAJ0QS4StysmbeR1R9sxOf20aFvO1p3bCETJ4saU1SFQJivtKFruaAVh7oY31EdKGpsqEshwoQEPiHCWCAQYMsXO/nirVXkZOWR3KUV7XqlYjQZQl000cgoanhPvKxruej5vwG9JNRF+Y4SATEvXHboW716NaNHj66wLSIigo4dOzJ16lR+8pOfYDR+FwNGjRrFmjVrqjzWhAkT+PzzzwF49tlnee6556o979ixY1m+fPlllVnUHwl8QoSpg9uOsGhmBpl7jhPfKo4Rtw3EGmENdbFEI6WqYb6WrlZcGvZM6aBGh7o0oBWAb3dpua6wlm/KlClcf/316LrOmTNneOedd/j5z3/Ovn37ePXVVyvsa7FYeP311ysdo2XLlpW2/f73vyc1NbXS9hYtWlxReUX9kMAnRJg5dyKbJa8tZ9eavdgcVgZc14eYhKhQF0s0ckpTGaWrRjeIZlS9Do/Vp08f7rnnnuDvjz76KJ07d+b111/nT3/6EwkJCcHbjEZjhX0v5rrrrqNfv351WFJRnyTwCREmSgqdrHxvHesWfoWu6XQZ3JEWqYnST0/UCVVVw7uGrwmJiIhg0KBBzJ8/n8OHD1cIfKIiTdPx+wMYDCqGRr6GuAQ+IRo5v8/PV4u2kvHOGgqyi0jt3obUHsmN/sVJNCyKqhDwBdB1XT5EhIHDhw8DEBtbuTYzOzu70rZmzZphMFTs+1tQUFDlvhEREdhstjoq6dVRHux8vkDwe/nPOuCIsBAb27iXmJTAJ0Qjpes6e7/8liWvL+fkgdMkpSbSc1Q3LDZzqIsmwpCqKuggga8RcjqdZGdnB/vwvfLKK2zfvp0BAwbQsWPHCvuWlJRUWeO3b98+OnfuXGHbNddcU+X5XnzxRX75y1/W3QXUoUsFO13XURUFVVVAUQhoOmaTAYej8fd/lsAnRCN0+vAZFr+awd6N3+KIjWDwpH5ENvJPn6JhU1QVdAj4A6hmqT1uTJ555hmeeeaZCttuvfVW/vOf/1Ta12q18tlnn1Xa3rZt20rb/vOf/1QKjECV26622gQ71aBisZgwGFRUg4JC6RREJU4PFrOR+PhITGEws4EEPiEakcLcIjLeWcOXn21BURR6jOxKYtv4UBdLNAGlNXy69ONrhB588EHuuOMOfD4fu3fv5oUXXuDkyZNYrZVrrQwGQ7U1d983YMCAkA/auNJgVxW/P4DT6cViNRIfFxk23WMk8AnRCHg9PjZ8tInl767FWeikXa8Ukru2RlXD44VINHyKqpTV8Enga2w6dOgQDHHXXXcdw4YNY9iwYTz88MPMnTs3xKWrmfoIdlXx+vy4XT5sdjNxsRFh9RorgU+IBkzXdXau3sPSN1Zw5ug5WnVoQf8JPTFZTKEummhilLI3Pl2TwNfYDRkyhKlTp/LOO+/w2GOPMWTIkFAXKehqBbuqeDw+PB4fDoeVZs0iwq6vqgQ+IRqoY/tOsuiVZRzceoSY5tEMvXkAEdH2UBdLNFGqqqDrutTwhYmnn36a9957j9/97nchWRUjlMGuKi63F58vQHS0nagoW9iFPZDAJ0SDk3c2n8/fWsWWz3dgsproPTad+FahnwhWNG3BJt1w78OnFdTppMeXTSuo18O3b9+eu+66i/fee49169YxfPjwWh9j6dKl7N+/v9L2iIgIbrnlFqDhBbvKdJxOL4GARmyziLAYjVsdCXxCNBBup4e1H37Jqrnr8Ti9dOybRutOLcPyk6ZofFRVRUcP39U2VEfp2rW+3aEuyXeUiNJy1ZMnn3ySOXPm8Lvf/Y5Vq1bV+v6/+93vqtzesmVLhg+/poEFu8p0Xcfp9KDrEBcfiT3Mp7RSdF1vEB9mhGiqAoEA2zJ28fmbK8k+lUvbLq1p3zsFo0k+j4mGIzcrj+0rv+G37z5GYpvGOTLc7XZz9OhRUlNTqxyhqmu5pWvXNhSqo0Es83ah2tbYGVQ15MGuKpquU1LiQVUgPj4SSwPsF32p52ttyTuKECF0aMdRFs3M4OiuYyS0jmP4bYOwhXGTgmi8lLKJl8N5WhZFjYUGFrBCpeE3xV6+gKbhLPFgMBpIiHdgaiIfrpvGVQrRwJw/mcPSN1awY+U3WCOs9L+2N82aR4e6WEJUS1VV0MO4SbeJCudgVxV/oHSOPbPJSHy8A6Ox8U+oXFMS+IS4ipxFLla+v561879E8wfoMrADLdo1l356ZY67nWwtzqdHRBTtbLJySIOilA7aCOcavnDW1IJdVXy+AC6XF6vVRFycI2wmVK4pCXxCXAUBf4CvF2/ji1mrKDhfSEr3NqT1SMbQhD5dXopX01hXmEOe30uW1003j4shUXGYwmji08ZMNchKG42BBLuqeb1+3G4vERGlc+ypavhea3Uk8AlRj3RdZ/+mQyx+NYPj+0+RlJzAiDsGYwnz0WCXY0dJPvl+L+n2KAoDfr5xFnLW52F0TAIJJkuoi9fkla84IPPwNQwS7GrO7fbh9fqJirIRHW1vsi0qEviEqCdZR86y+LXl7Nmwn4iYCAbf0JeouMhQF6tByvN52VFcQJLJikU1kKAacBiMHHaV8HH2aQZGxZJuj2qyL9QNgaIq6NKke9VJsLsyTpcXvz9As2Z2HA5rk34NkcAnRB0rzC1i+btr+fKTzaAopA/vQmLb+Cb9QnMxuq6zsSgXHWhh/m6Esk010NUeyQmvi3UFOZz0uBgZHU+EQV62QkFVlbAZtNEQZyOTYFe3dHScJV40XSMu1kFERONrJajr56m8cgpRR3xeHxs/3kzG7DWUFDhp1zOF5K6tUZtYx+DaOuwu4ZjbSZo1AvV7oVhVFJItdmIMJo64S1jgO82o6HjaWmWJuatNUdXSaVm0hheWaspoLH3L8/v9ISuDBLv6p5fNsUfZHHs2a+PsQuPz+QAwGOqmr7cEPiGukK7r7F63jyWvLSfr6DlatmtO33E9MVsb3kSeDY1X0/iqMBeHwUCMsfrHK9poontEFIfdJSzOO0NPezQDopphVCRMXy1q+dJqjbiGz2AwYDAYKCwsJDKyfrtXSLALDU3TKCnxYDCoxMdHYjY3zpij6zoFBQVYLBZMprp5L2mcj4QQDcTx/adY/GoGBzYfIiYhmiE39sMRExHqYjUa24rzKQj46G6PuuS+JkWlk9XBWZ+H7SUFZPncjIlOoJmpcX56b2wUVYFGPkpXURQSExPJysrCYrEQERFxxV0tyoOd368RCJR+9/kCBALlwQ5UpTQwK2rpyhPlIe+7YKcDpfcJNN48HXKBgIbL7cVoMBAZaUXT/LjdoavNvRy6ruPz+SgoKKC4uJhWrVrV2bEl8AlxGfLOFbBs1io2Ld2OyWyk95h04lvJDP21kevzsrPku4EaNaEoCklmK1EGI4fcJSzIOc3QqDg62xzSR7KeKaqK3shr+ACio6NxuVxkZ2dz/vz5Gt9P13UCAR1N0wgEvvvSNP27vlaKgqooKGU1d999r6eLEUGBgIbX68doMuCIsFJSkhPqIl0Ri8VCq1atiIq69IfhmpLAJ0QteFwe1n74FSvnrMPj9NKhTyptOrUqq/0QNaXrOhsKS1+QLxyoUVN2g5Hu9igyPU5W5p/npMfF8Og4rDUMjqL2ygdtNOYaPij90NCiRQsSExODfaQu5PX6yc0tJienhNy8YrKzizl3rpD8/BL8ZSHPbDJisZqIiDDjiLASEWEhIsJSxUS+etmXqE9nzxZw6NBZOnduwY2TemM2N+7uNAaDoc6acS8kgU+IGtA0jW3Ld7P0jRVkn8qlTaeWdOiTirGJrMFY1w65SzjucdG+ioEaNaUqCmnWCHJ9Xg66ijnn8zAmJuGyAqS4NEVRQFEafeArFwjoZGc7OXeukHPnCzl7tpBTp/PIySkubaINaFjMRmw2M5GRVhISmhEZaSUy0lrtclzSHHv1HTp8lkOHzzJiWCduu7V/k1s9ozbk3UqISziy6xiLZi7j8M5M4lvGMuyWAdgjbaEuVqPl0QJ8VZhLlMFI9EUGatRUrMlcOmefu4RPc7Lo64ihtyMGg7Sj1TlVVRrdxMter7801NUw2LVLS7xksBOhp+s6e/aeIisrnxuu78WE8enSreMSJPAJUY3sUzksfXMl21fsxmK30G98L2KTYkJdrEZva1E+hQE/6TUYqFFTZlWls83Baa+br4tyOeV1MTo6gag6CJTiO0oDruGTYNd0aJrGtu3HKCx0cefkgQwZ3CHURWoUJPAJ8T3OIher525gzYdf4vf56dy/PS3bJ8mnxzqQ7fOw21lIC7MFcx2vkasoCq0sNqKNptIBHdmnGB4dT3ubo07P05QpqkIgxIFPgl3T5vMF2LL1CD6fxvRpw0nv3ibURWo0JPAJUSbgD7Bp6Xa+mLWK/LMFpHRrQ1rPZAzyJlEndF1nY2EOCtDCVH/97BwGI+n2KI66S1iWd46THhdDouLqPGA2RYqqoF2lUboS7MT3ud0+vt58GIvZyMMPjSYtNTHURWpUJPCJJk/Xdb7dcphFMzM4vu8kiW3jGX77IKz2xrcUT0N20FXMCY+LDtaaT6GSc+40a5cv4MypoxQX5hEIBIiOiad9514MGjWJyKhmVd7PoCi0tzk47/Ow11nEGZ+bMdGJJJrlb3olFKXua/gk2ImaKCp2s2nTYWJjHfzw/pEkSfeaWpPAJ5q0M5nnWPLacnav20dEtJ2BE/sSHV+/M/BfDQHVR17MMRwliVg9dddX7nK5tQBfFeURZTDVql9dYUEOxYX5dOren6joOFRV5dyZE2z7egV7dn7JDx//CxGO6Grvn2CyEFk2Z9/HOacZGBlLekTUZY8MbuoURbnsefgk2InLlZtbzJZtmSS3ieP+GSOJiZGlFS+HBD7RJBXnl7D83bVs+GgTKNB9WGeaJyeETT+9EnsOWUm7MWgmWpxJJya/bUiXa9palEfRZQzUSO2QTmqH9Erb26Z2YeG7/2Dn5jUMGX3jRY9hVQ10tUVy0utifWE2p7wuRkbHE2GQl7/aUlUF/RJr6UqwE3XpzJkCdu46TtcuLbnv3mHYpeXlsskrnmhSfF4fX366hYzZayjOLSGtZzIp3dqghtncTX6jG13RMfqsnGy5jRJbDi3OpmPQrv6o1fM+D984i2hpttZZP7roZvEAuF0lNdpfVRTaWuxEG0wcdZewwHeakdHxJFulpqBWLqjhk2An6lvmsWz27z/NwAHtuOvOQZhM8ry5EhL4RJOg6zrfrN/P4teWk3X4DC3SmtNnbDpma3iuw+o3elA1I1FFLXF7C8hrdgyXLZ82p/pd1SZeTdfZWJCDCiSZLv+Tud/nxet14/f5OH/2JCuXzAGgfedetTpOtNFEt4gojrhLWJJ7hh4RUQyMisWohFfgr0v+gEaR00uh00uOorJ+10m2/+UzCXai3ui6zoFvz3D8eDbjrunOpBt6l670Iq6IBD4R9k5+e5pFr2aw/+tDRMdHMuTG/jiaRYS6WPXKb3SjaqUhxuqJxui3UhB9kiOpa69qE+9BVzEnvbUbqFGV7ZtW8sXHs4K/RzdL4KYpP6ZtWpdaH8ukqHS0Ojjr87CjpJAsb+kKHbGm8Az/NXVhsCtyeSks8ZJf7KHY5UPTNTQNinUVJbeE9q3iJNiJeqFpOrt2Hyc7u5hbb+nHyBGdw6arTahJ4BNhqyC7kC9mreLrxdswGg30Gt2NhNZxoS7WVeEzulH07/69jQELsbmpFEWeKW3itefQ4kz9NvG6tABfFeUSXcuBGlXp1K0/cQmt8HndnDmVybd7t+IqKbzs4ymKQpLZSpTBxCF3MQtzTjMkKpYutsiwf3OpSbAzGhRMRgNWs4GEGBtWswGr2cjR/CJSEqPo3q11qC9DhCG/X2PrtqO4XF7uuXsI/fqmhrpIYUUCnwg7XreXtfO/YuWc9biK3XTonUrbzq1QmlCTgM/kwhCo+O+toH7XxBtzDJe1fpt4txblURII1MmKGlExcUTFlIb1Tt370zl9AG/+80l8Pi9Dx9x82ce1Gwx0t0dxzONkVX42Jz0uhkfHY1Mbf43VlQQ7Q3X/K0ppM70Qdc3r9fP1psOoqsoD94+ic6cWoS5S2JHAJ8KGrutsX7GbpW+u5Nyx87Tu1JKB1/XGaG56T3O/yYXRV3lAgqZprP1wO2uWbSDnfC4R0TaGDBjMjAk/xmapu/WBz3k9fOMspKXZiqkeJjxu3jKZpFYpbNm47IoCH5QO6Ei1RhDt93LIVcI5r5cxMfG0rMPHoz7VS7CrhqIo6A10aTXReDmdHr7edITISCv3zxhJm9axoS5SWGp674QiLB395jiLXlnGoR2ZxCXFMOyWgdijGscbdl3TlAB+gw+Lu/K/97w3P2bF4rX0HtiD8TeO4vi5o3zx8QqOnDjMX370MiaufMoDTdfZUJiDAYXmVzBQ41J8Pi9uZ81G6dZErNGMw27kkLuYT3Oz6OtoRh9HDIYG0sR7NYPdxWhl07KcPJnJ3DmvcvjwfnJzz+MP+ElISKJf36Hcett9xMYm1Nk5RfgqKHCyectRkppHc//9I0kIg3lQGyoJfKJRy8nK4/M3V7I1YycWm5l+43sS28RnYPcb3YCG+r3+eaeOZ7FyyTr6DOrBo7+ZUbZ1KAkJCcx/ZSkLMl/l5k73YvVUP5FxTRxwFXPa66aj7coGagAUF+bjiIqptD3z0B7OnzlBcruuV3T87zOrKl1skZz2utlUlMspj4vRMQlEX2EfxNqoSbBzlpwn8/gWss7sp7DoPIGAn9iYRNI7D2Rov2sxm+tn6TpFUdD10hq+nOyz5OVlM3jwaOLim2MwGDiWeZAvvljIunVf8PI/5xETIzU1onrnzxexfUcm7ds3Z/p9I4iMrL8lF4UEPtFIuUrcrJ67gTUffonP7aNT//a0ap8U9h3ua8Jv9KAreqU+fJvWbUPXda6ZNLLC9rGjx/DpW8vZvHIX6deuo+UVjOJ1BQJsKsolxmAisg4mNl668A2Ki/JIbt+N6JgEAn4vWSePsmfnRswWG9fcMPWKz/F9iqLQymIj2mjisLuEBdmnGB4dT3trRJ0+v66kxi5jbQZ7D6yiS/s+9O85HINq4MjxvWSs+5Dd+7/mkanPYaqHUceKAlqgtIavZ6+B9Ow1sNI+3br35YW//JqPMz4k/eYb6G5sjkWRtxpR0cmTuezZe4pePdtyz91DsFiu/hyhTY38F4pGJRAIsOXzHXz+1iryzuST3LU17XqlYJBpIYLKJ13+fg1f5qHjKKpCaofkCttNZhNtUltzat85TOUTNV/mKN7NwYEadTPtTbfeQ9i1dS27t67DWVKEQumULH0GXcPgkZOCEzDXB4fBSHd7FEfdJWTkneOkPZKhUXG1njy6Pppiu3cawKjBN2K1fNdPc2Dva4hr9gGrv/yELbtWM7jv+Cu6/iopCppWfR++gK7hiSst09aCI+T5MrEqJroZZZF78Z1Dh89y6PBZRgzrxG239scQZhPfN1QS+ESj8e3WwyyamcGxPSdIaBPP8NsGYo2QJoDv8xs9KDooesUX0fzcQhyREZhMlf/tm8VGc3j/Uey5iZgdEcFRvK1P9cVWwybes143e12FtKrDgRpdew6ma8/BdXKsy2FQFNrbHGT7POxzFnHW62Z0TALNq2gyvZp97Fq3SKtye48ug1j95SeczT5xWdd7KaU1fBUDn9frId9ZxD53Fl9l7mDHux8AkN53MHXXw1KEA13X2bP3FFlZ+dxwfS8mjE+XVpmrSAKfaPDOHjvP0tdXsHPtXuyRVgZe34fohKu3WkRj4zO6UTVjpSZZr8dbZdgDMJWNZPZ6vdg90Rj9NgqiT3AkdR0tzqTT7BJNvOUDNYyoJNbjQI1QiTdZcBiMHHaX8HF2Ft3NDlr5zRS7fSEbPFGVgsJcABz2y++HqekaG7d8waYdK8gvyCbCHkl654FcM+x2VFUNDtoAyNdcvL30LZa/9lpwW1RiApN+9jjdu/fha1/9BE/R+GiaxrbtxygsdHHn5IEMGdwh1EVqciTwiQarpKCE5e+tY/3Cr9E1nW5DOpGUkiCfCC+htIavchO32WKmsKCoyvv4vP7Sfcyl/b6MAXNwouZTLbfhvEQT7z5nEVleN53CaOJiTddxe/24vYHS7x4/msdHvuJnuakEu1uhZZEBu2q86sGuyvJqGqs2foyqGujZdchlH2fxinf5cusXdO3Yj+H9r+dczmk2bl3G6bPHGNfzXjQtwMlAATv9ZzgSyMXdJ4Uxv/s1Do9CztFMDm3ehKuw6ueZaJp8vgBbth7B59OYPm046d3bhLpITZIEPtHg+H1+vvxsKxnvrKYwp5i09LakprdFlX4eNeIzulC1yv/aMbFRnD55Bp/PX6mmLy+3AEdUBMYLtlc1UXN5E6+maXy68iM+X7uYszlnMdodJHfvT/rEH0AdDNa4mqoKdk6PH48vgK7raDoYVAWDqmAyqiQZrPgNOrnRAXKiVZKwkUDol2VbtGI2x08fZPyIySTEtbysY5w9f5Kvti6jW8f+3H3Lz4Lbm8UksGj5O+zosAP7sN7s9OzBpBhobYgmoXlL1KSygDtoCJ0HD2HWr36Bz+vBcGPlQR2iaXG7fXy9+TAWs5GHHxpNWqr05wyVxvXKLMKaruvs2XiAxa8u5/ShLJJSE+k9pjtma+jfTBsTn6nqwJfSvi17dhzg6MFjdOza7rv9vT5OHD1Fx65V9wuzVmjiLV2L98OZS/hs5ccM7jWUrkMn8O3pTL79agXzzp7ingefRKmHyZZrK+f8aXZvW8/Rb3eRl3MWv89HdGwiqV360an3WAK64aLBzmEzYzKWNs1WVWEXiZGzeNlOMW2x0AE7hquwPnFVMtZ+yFfbltG/5xhGDb7pso+zc99GdHSG9Ls2uM1tCNBsZH/UtXPZl7WN7qae9DTEE61YqqzNTUxJoXlaGtuWLqG/BL4mrajYzaZNh4mNdfDD+0eS1MSnzAo1CXyiQTh5MIslr2Ww78uDRMY5GHxjPyKbOUJdrEZHR8dvcmP2VH7s+g/rzZIFy1n+2ZoKgW9txpd4PV4GjuhX7XG/a+I9yzbPChat+oTBfYZw//Tf8HHOacb1Hc7x5m344pNZ7Nm5ke69h9XL9dVEeY3dV+uXs3vLClql9aBd215oukLO6YNsWrWQfTu/YtgtP8dqtV4y2FXHgEILzBQSIBMP+QTohp3Iq/yyunz9AlZ9+TF900dy84QZl77DRZzKOoKiKLRp0Y4Cs49j0SWcinLhU3WiWrek5OgpEs4aiUm9+GApv9eLu7j4ispy5XQMqhuTwYnRUILJ4MQfsFPkbhvicjUNubnFbNmWSXKbOO6fMZKYmMor/4irSwKfCKmC7EIyZq/hq8+2ohpUeozqSmKb+ptqI9xpqp+A6quyhq91cktGXzeMlUvW8Z+/vEl63y5knTzLysVr6ditHQNH9LnosUubeFuwb/GX6LpO33tS+Mp/DJNiIdFkIW7gGFYumcPubeuvSuC7VFOsOb4zo6aMwGq1YzKqGA0GTAPHsvvLT9n95RJyj22la98xV1QGBYVojFhROYuPTRTRERutsVzWPIa1tXz9AlZuWEif7sO55boHrrj/ZGFxHlZ7JFvbFnLe7kUBYtwmYtwmshyx5LsyCfh9ABTn5eFo1qzSMY7t3sX548dp2637FZWlKqriw2hwYjKUlH13Yrwg0BmNxZgNxZiMxZgMJahKABQNRdFQ0PH5HWw78nM0XVoN6tOZMwXs3HWcrl1act+9w7Dbw28gV2MkgU+EhNftZf3Cr1nx3jqcRS7a906lbZdWqA2gKbAx8xs9oOgYAlUPrrhrxi3EJcaydtlGdm/dgyPKwZjrh3PTlOtr/NifPHAWRVWI7OoAdSdpJztCTiRGk5nmLZPJOnG4Li+p1n3symvsmsd2qbLGLq1Lf3Z/uYT87FN1VkYLKq0xk42PPTjJwUdXIjBTf8/nFRsWsnLDQnp3G8at1z+Iqlz+ufyKxqkoF4W4CFhU8qw+EkvMRHq/G+1tKFttxO/xAvDFzP9RnJdHcnoPohMS8Ht9nDlyiL3r12O22hg7fTqZlzyzViG4XVgbV/7dZCzGZCzCZCjBoHqC4Q1FQ0FD001ouhFNM6LrJgKaGX/AhtcXTUA3E9DM6LqRhOjt5BZ3krBXzzKPZbN//2kGDmjHXXcOwmSSOVIbCgl84qrSdZ0dq75h6RsrOJt5ntYdW9L/2l6YZJb1OuE3etDRq6zhA1ANKhNuGs2Em0YHt2maxvJFa1m7bCPZ53KJjHLQb2gvbp5yHRZr5U/m5fP5nT2TQER8AZa0g+hRxXC8I5HRsZw89i0Bvx+DsXYvL5cb7GrbFOsszAPAZq/bqX1UFBIxE0GAs/gopJBuRBDH5T23V3/5CafPZnLqzFHyCs4TExXPrx95GYAvty1jxfoFxETF0S6lOzv3bqxwX4c9mg6p6Zc8h9Po53i0k+NRTtxGDdVsQiv0klJQufmtvGZPLfu7dh0+gt2rVvHN6lU4CwtRFIWohAT6TriG0beNJ6mlHbt+jFSrlxbmgxfUyhVjNpbWwhkNThQ0FOW7AAcKAc2EppvQdQMBzUJAM+PyxqNppQEuoJuDP1ODUB1tP0QgYOVkzuhL7isuj67rHPj2DMePZzPumu5MuqE3aghGq4vqSeATV03mnhMsfjWDg1uP0CwphqG3DCAiSvp11KXqVtm4mHlvfsyKxWvpPbAH428cTdbJM6xcvJYTR07x8+ceqVTz5/V4wWjAp0NEYRL4SiA+C91eiLGsa5fP56k28F2tYFfluTWNHRsXo6gG0rrWz4CCCAy0ReUMXrZSRBo20rCi1rKJd9naD7BZHbRqnoLb46xw26msIwDkF+Ywf/Erle6b2qZLtYFPRyfPWto/73SkG03RiXabaFFs5ZijGWeyzhLw+4I1egY1gN3kwVecjS3CRq/ks3SJ1elzsxnrbSOwGftjN5ZgM7qwGl0YFT+K8gUKGiW4iVGsWLGjacbSmjjdRECz4PFH4fLGlwY47bsAp9fx25JBdeOwneTE+Wvw+mPq9NiilKbp7Np9nOzsYm69pR8jR3QOm+mZwokEPlHvcs/k8fmbK9m6bCcmq5k+1/QgrmXlvj/iyvmNbhRdQdVr1rx36ngWK5eso8+gHjz6m+86/Mc3j2PO6wvZvH47A0f0rXAfg9mEz+UmxmguHZXqcqB7LRB/Bp/tHABGkzmkwa46m1bM5fzpw/QdcQvRcUn1cxLAiEIrzOTh5xAucvHRnQjs1Lx565cPvURsTOkUFv944zd4ve7gbbdPfJjbJz5cqzJp6JxxuMmMKcYdWUK0wUtvTaO5HsAe68Vu8lDc08pn3+r0tH3MwP6ROMxuzEYfPq+fN0+fpG+faO4ZvgVLZCR+3UhAM+DXjfg1E17NTIkrAp9mwqeZ8GsmDvqK6aC2oKXh8ieCvlLNIr7F440lKy90K7aEM79fY+u2o7hcXu65ewj9+qaGukiiGhL4RL1xlbhZ88GXrJ63AZ/bS4e+7WjdsYV88qtHfqOn2ubcqmxatw1d17lm0sgK20eMG8yC2Yv4cs2WCoFP13VM0RF4Tp7FpulgKK0x8nlUfMebU3DWiS3aQnb8DrK2tSLgU0MS7Kqybe3H7Nu2io49R9Bj8PX1fj4FhVhM2DFwBi9fUUgX7CRhrtGAjvKwd2k6RqMPi9mD1eLGEvzyYDG7MVpd+KKK0KOKGGRxE2XwYtHBBKiKjqLo6Dr4A0bibrOwaD4s+WQvA/sPJNfpwBsw8tGCw7jcGp16dyVjdxqtO7auUcl8uhtCNFUNgNlYgMWcy6Gs29F0GThQ17xeP19vOoyqqjxw/yg6d2oR6iKJi5DAJ+pcIBBg67JdfPHWSrJP55HcpRXteqVilM679c5n9KBqNX+cMw8dR1EVUjskV9huMptok9qKzEPHg9s0TSPT6SQyuTn6N4fZt/MwcamtyTl9nt1fbCDnxFmKzueiKAoLnnuNtr1S6dDxZiKMra9qsKvK9vWfsvPLxXRIH8qQCfdc1XNbUWmLhXP42EUJOfjoTATGiwQhVQ1gMZeFNosbs8mLrvtI77q9LMy5sFlLv6xWNwaDH0XRURQNtXQhZbyaSrGqUKyAWzcQ8BnRiu3k+GPwBoz4/Ea8ASNev5GArgIKWGD0dcWsXPINj/9iF+l925J1Mp8Vi3fTqVtLeg3sSMAXuHoP3hXRaeY4QJEzmezCHqEuTNhxOj18vekIkZFW7p8xkjatY0NdJHEJEvhEnTq47QiLZmaQ+c1x4lvHMfzWgdgcF5+zS9Qdn9GJotf837p8AMaFK29omobbGyAi0kFx4VEOHs/GEwCn3092RIBWPTqxd8lGdi7/mrE/bI2/2Im32El0fDRF53PpNLgHJpuRvct3cmjjQW7/xWNQmEaoanq2r/+UHRs+o333wQy97t6Q1DCrQIpZRzO78FiyKbT46WpRiDX7gjVypQHOicXqxmzylNa+qaU1cC/8txgdja6ddxEIGAgEDPj9Jnx+I678Zvj9Rnx+Ez6fiXwVzpo1Ci2ly+XZfQbsPkON+xBOmTGU+MRI1izby66tx3BE2RhzfXdunjIARdcbTeCzW85iUD0czx5HTQZ2iJorKHCyectRkppHc//9I0mIjwx1kUQNSOATdeLciWyWvLacXWv2You0MuC63sQkhq7fTlPlM7kwBGr+b+31eDEYjZzOKf6uj503gIaOy68BkFvgJMJhx+tQMBgMpKW2In1Uf3av2szqNxeS3L0DCckt2L1iEy07JjP63kkoqkKLdskse3UhRzOX03P4GEoOpKL7r+5Lzo4Nn7Fjw2e06zaIYddPQ7mCqUsq0jEaAtgtbmwWD3aLB5u57LvFg93ixm7xEGF14bC6sFs8qKoWrIXzKxo6pRNaGwMmtIABf8CI32+kpNhBvr80xJWGOhMez058Ph/7v+1WZWk0RafA6iM7wovLGEDVIdJrxOpXaz0fYOlI7l5MuKlXpds8Ti+6roGuQwPumqEQIMZ+iJyi7hS5UkJdnLBy/nwR23dk0r59c6bfN4LISPlA31hI4BNXpKTQycr317NuwVdoAY0ugzrQIq259NMLgfJVNizumk83YraYcToLOXm+GLNRxWhQcdhNmIwqprJslBQficeo4MJPJAYMKAy9czyRcdHsXbeNY7sPYXPYSR/TnwE3jkIpa7+NjIsBoOSMirlFNoZIJ8V72hEoujorqOzbtort6z8lIiqWlildOLJ3U4XbrfYoWqV2Df6uKho2c2lgKw9tNosXu9mDrSzAOWwuIixu7FYXJkMAVdVRlfIgp5eGtoCKP2DAHzDg9Ztweqzklzjw+o34fKVNqF6/kQINXIADAy0xY7pYMNOr3uxTdXJtXnJtXryqhllTiXGbMGlK/Uz8XPZ/raNflYmlL1ek/RiabuBE9thQFyWsnDyZy569p+jVsy333D0Ei0yn1ahI4BOXRdM0Nn6ymYx31lBwvpDU9Lak9kjGYJCmk1DRVB+aEqh20uWqxMRGcfrEGYyqTkK0rcJtRflF2B12DEYDZ3CjomAte5NXVZVe4wfTa/x3Ix/9Pj8+txePy03u6fN8tXAFAG06d8F9ojmWFtlE99tLybfJeE4lUt9NvNlZRwEoKcxl3eK3Kt3etWsSP506BofNVRruzF4URUdVykKcqqNpKr6yEBfQDHh9pX3fsguiy4JbabOq12fEFzDW6pocgBmdQgIcxU0LzETWcBSv0xgg1+Yl3+pFU8DqV4nzmDHq1Z9f03SWL9rFmmV7yT5XRGSUlf5D23PzlP5YrLV449bLvhpo3lMVD5HWE5zKHYnHJ/3K6sqhw2c5dPgsI4Z14rZb+8trfSMkgU9cloLsIj7+91IUYMQdg7HYZPb6UPOZaj8HX0r7tuzZcYDsE1k0b9bhu2P5fJw5kUVyhxRy8eNCoxkXDzT71m1n3dzPg79HxsVwzf0307JDW3Q/uE80x5yQh6PrEUwxhZfVxGs0+L/XdFrelOoO1so5bG7sFhc/u8WBUR0TDHGKUlpNVl775g8Y8AUK8PlNFDojyC6ILg1vFwQ5rYbT21wuMwqxGCnEzwk8NMNI84usz1Fo8ZNt81Bs9qNQ2j/PVsP+eXPf3MCKxbvpMzCV8Tf2JOtkHisW7+b4kfP84rkbazVJrq7XXd4rKHDyxtsrWLV2D2fPFRBht9AuLYlHHxxPn15ptT5eM8dBvP5oTucOraMSNm26rrNn7ymysvK54fpeTBifLi04jZQEPnFZouIcjLhtMKvmrufQ9qN0HdxRXgRCzG/wAFqt+vD1HdyTxfMz+Oi1uSwMaNgj7XTr1x1HdCQ+r49uA3twHh9W1IuOKgVI7d2JmKQ4fB4v2SfOkLnzW1zFF04YrOA9H0vA6Sxr4i2hZG8aZo/5uybUSmHOTYTVHayFMxv9qOp3o1EVRScQMOArG8jgCxjw+Y24vWYKnXZ8flOwCdXnM+LXDDS0qikViMaIG60sXAdoiQVrWewLKDp+VcenahyNLsGoK0R5jFgCNe+fd+p4LiuX7KbPoFR+9Jtrg9sTmkfx/uvr2bT+IINGdLzkcb77F6+mjbmWTmfl8cCj/8Pl8nLzpAEkt42nqNjNwUNZnDtfWOvjmQxFWM3ZHDlzMwHNduk7iIvSNI1t249RWOjizskDGTK4w6XvJBosCXzishgMBiY9Mp741rF89PISNhfsoM816RhN8pQKFb/RU+savrXLvwbA5/GS1CYJuyOCr5Z/ia7rJHdIJmFAZ4oI4KhBU6OjWRSOZpGYDQFsg9tSMKwNM5+aS6wpl5vu6orN5MNm9BFRViMX2boYa7dN6C4LeE2lNXGqjqYp39XCaWpZrZuJ3KIovL6KNXC+gAG9nmvhrgYFsKFiQqGAAJm42bfxMNkFBbiMAfKLnGh+ja/e3o2KQlyCgyGjOtX4+F+vO4iuw7hJFacnGTGuC/Nnf8VXa2oY+MoCpl43eY8nn51DIKDxwbs/JyH+Spe6K52GpcTdivMFveukfE2Zzxdgy9Yj+Hwa06cNJ717m1AXSVwheXcWl01RFIbc2J/YpBje/cN8vvxsK/0n9MQaIaO2QqF0lQ1DLWp9sljz+XpSunWkfecUtq/fyrGDmZitZjwuD50H96RIDRCtQKTJg83kw2r0YTN5g+HNZvJiM3qJMJd+2UxejGrp4vZqH50N79vZvHwXf/2NGb9mIKAp+DUD3oCBgvwoci1+AkYNl8dBSVYiXq8JrRbzCF6ORZ9u5VjmeY5lnuf8+ULi4iP5fy/dW6/nrCkDChFmhVybl4wNuzm262yF2z99fzMAnbq1rFXgyzx0vmy+xeYVtpvMRtqmxnP00LmaHagOa/i2bj/C9p1H+c3PbyIhPgqfP4DfH8BmvbzuITbzeUwGJ4eybkevxYomojK328fXmw9jMRt5+KHRpKXWdBJw0ZBJ4BNXrPOADvzo5Rm89fQcNn6yhb7jexB9xZ/WRW35Te4aTbqsoGEy+NixsbQm77pJHRg+qCXW+1tiMXpRdRe33PQp+Yc28KfH87Abvms+DTajaqW1b35NJaCpeAMGPH4DRR5HabOqZsQbMJBXvJe8vABbTyVXWx7V4sUY6URpmQ2nE8Bdv2/WCz78igiHheTkBJxOT72eq6Z0BZwWP8U2Px6ThqrDQ3+aSMAPBlRiMGC5grnk8nNLiIy0Yqpi8vOY2AgO7T+D3xeo8eTodVHDt37jfgCSkmJ47BdvsuGrAwQCGm3bxPPQjGuYeF3fSxzhQhoxEQfJLe5MgbPdlReuCSsqdrNp02FiYx388P6RJCXFhLpIoo5I4BN1okVac378z/t5+5l5fL1kOz1GdCEpRT4VXh06BoMXky2XeJOTeMcZzAYPZqP3gu9uLCY3FoMHk8GLqmi8cWo3qgo/uu0cDnse/oCBgF4a4Dp2dLB/byGFrghyLwhwvrIv/YJaxPxcFzGxlftL7d15lhOZhXTpcfHngeYx4/MbMUYXY0s+jedsPP78+pvI9YW/3UNi2RyRT/12Dm6Pr97OdSkBVafEWhr0/AYdgwZ2jwFjoHz6E3Cjk4ufCFQiL9mTsmpej7/aMGcyl273eqvfJ0gpG6RbB4kv83hpreLvn59P29bx/P7pO/H5A8x+fw1PPjcXX0Dj5hv61+hYkbbSFWFOZF9DQ+uj2Zjk5hazZVsmyW3iuH/GSGJi7KEukqhDEvhEnYmKi+Sh/3cv8178hC2f78BZ6CKtR/U1O6J6iuLHaHJjMjkxGsu+m9yYjKXfjSYnZnMJJlMJJpMTVQ3gtuWBEsColc7oFtAMaHppiNM0A37NiNtvpcTrwK8ZOZm1g6goC7tOpeCwWih/o/SjY4o+REF+Acdzoi4ZAt769xbyc1107dmc+MQIfL4AmQdz+XLNcWw2I3f/8NL9qfSAii83CmOkE0vL8xgiXHjOxEOg7vvnJTaACcG9Ro1imx+n1Y+mgCmg4HAZUL83rUpp3z4FL1CMhhcfMRhrHfvMFiNFBa4qb/N5S1fOMJsv/XZwYYvuwnc2cuTbsxw+cIZzp/NJSIrmlQWP1rhMzpLS2tUIu4XX//twcLWXMSO6MfG2v/Dv/y3lxuv7oqoXfw6oipco23Gy8obg8sqHzMt15kwBO3cdp2uXltx37zDsdll7ONxI4BN1ymw1c/eTtxHfMpZl76yhOL+E7sM6X/JFO/xpGI0ejCYXJpMLo7Hse/BnJyazMxjgDEZPcFUGRdEAHV03oGlGNM2ArhkJBEwEAmZ83ggCmpmzOgQ0M0ZXDAH90qNRXS6tLMxV3LeIACZz6d/L47l0M9/gUcmsX36UDSsyKSpwg6IQn2hnzPXtmHh7F+ITI2r8KPmL7KheI8aoYlSLF8/pBDR3eLzx6IDbHKDY5sdtKQtZPgWzz3DJ+GZGwYiCC41sfERhxFbjxdJKm21Pn8zD5wtUatbNzy3BEWWtWXNucJiuznsz1+CIspLWMQlnkbuGJflO+aS9147rXWFpv6goOyOHd2XRkq1kHjtPWmrz6g4BQHTEYXyBCE7lDq91GUSpzGPZ7N9/moED2nHXnYOqbPoXjZ8EPlHnVFXluvvHEtcylvl//5RNS7fTd1xPTDWoQWg8dFTV973Q5qoY6MoDnLkEk8mFQll4KwtyoKAFjKUhTjeglQU4tzsaTSv9uXxbIGDiYuuB6ugURWZh8llrvJauxWqkpMSLesF0Oh40XAQIeEuXVbNYLv3CP2hEWwaNaFujc9ZEpSbec3H48xpvn1BNAafVT7HNh9eoo+pg9aqY/LX7EKQCEai40cnHjxeVKGo2B19K+wT27DjB0YNn6di1ZXC7z+vn+NFsOnZtUcNSlLbp6hr854OHSWrVDICf3fMablftmsabl9W0xsdVbr5PiCv9excWVV0rWc5oKMFuOUvm2RvwB2r+wUKU0nWdA9+e4fjxbMZd051JN/Su1XyMonEJp3dg0cAMuK43sUkxvPPcB3z5yWb6XdsLe2TDnRtLUQIYja4qmlLLQ1xpLZzZXILR5MSg+kH5bn1U0Mtq4IzoWmltXCBgxuez43FHVwpweg1q4WpKUwKly13pNf9kHh9vJzMzn0CgNNzp6BQQwIBCfo6LyGhLjTvx17VgE6/DiaVFNga7u96aeOuL31DeP89HQNUxBlQi3AYM2pX9za0o+FFwouFFJwYj5ks8jwYMa8+SBdvI+GxXhcC3NmMfXo+/RlOyVKQHw97l6tatLR9+9BVnzxVUuu3suXwAYptdfBm+ZhEHcHqac7ag3xWVpSnSNJ1du4+TnV3Mrbf0Y+SIzjKXapiTwCfqVfveqfz4nzN486m5fPnpFvpc04Nmza9WHyodg8FzQc1bxf5wRqMLs7k4WBNXfTOqoSzElTejmvD54oM/a2UBTtNMhKrDuGbwgaLXaJRuuU6d4tm86RRHDubSs2cSJWj40TB64fjhPDqlh74/lL/YjuprPE28OuA1aRTbfLgsATQFzH4Fm6dy/7wrYQTsqLjRyMGHAwMOqm8abp0cx+jrurNyyTf85y+fk963LVkn81mxeDedurVk4IiaTairUPrBoC5G6Y4Z0Y0X7RYWf76NH04fG+wzdj67kFVr95DcNoG2beKrvb/VlI3ZWMTRUzei17BWW5Ty+zW2bjuKy+XlnruH0K9vaqiLJK4C+S8R9S6xbQI//ucMZv/+QzZ/vp3uwzrTsl3SZR1LVX2l4c1Y1pR6wc+lwc5ZGuLKauZUJXBBM2rpu5QWMKLppX3htICJgGbC44km4KzYjKppxrJauIYvoPprXcM3YlQK7727k8Wf7Kdbz+alffdQWbP0IB5PgCGjG8aAm8bQxKsDLkuAYpsPj1lDAcxlzbb19RFApTT0edEpwl9W22eoNvZNmTGU+MRI1izby66tx3BE2RhzfXdunjKg5s14ZRWseh3MwxcVZefxx27gj39ZwNQH/sXNN/TH5w/w4cIv8fkC/OYXN13k3jrNIg6SX9KRvJKaz0coSkdjf73pMKqq8sD9o+jcqabN+aKxk8AnrgpHTAQP/OVu5v99EV99toWSAifte6eiqhpGo7v6fnAmZ9mAhhJMZicG1VdlM6peNpihvBnV77fh8URdEOBMBDQzegNcWqsuBAw+QEfVVTRNZ/4H3/DpJ/s4c6aY6Bgro8ekcf8DfbHZvluFIzmlGWOvbc/ypYf40+9X07V/Etknilj2ybd0Tk9kyOiUkF3P9zXUJt6AouO0lQY9X9m0KlaPAVPg6j3HzCgYMOBGIxuNaIzBZdkupBpUJtzUiwk39brsc9XxymrcfvMgmkVHMOvd1fzn1S9QVZUe3dvy5+d+QO+e1dc6OawnUdQAx2UallpxOj18vekIkZFW7p8xkjatY0NdJHEVSeATdUrXddCdoBeBVlT6XS8ErRijXsidPy5i4MhCzh5dSEw8xCWZUMrWRkXRUdDQdbVCX7iAVhraXM7YigGurBbuYoMZmgpN9ZXV7in8658bWfDhHoaPSOHOKT04lpnHgg+/4eC32bz08sRgbY5f07hnei9atI5i2ZKD7Nx8msgoC+Nu7Mjt96bXqvN21slCNqzMZPe2M5zLKsbnDZDYwsHA4W2ZcEsnrNa6eamp1MSblYDmuvpNvD6DTonNR4nNT6BsWpUIlwFDHTbb1oaB0gEdru/N2Vdf/xl1MQ9fubGj0xk7Or3G+yuKj2j7Uc7m98fpkdqpmioocLJ5y1GSmkdz//0jSYivv7kuRcMkgU9ckq57vxfeiioEOl0rAC0f9ALQCkH3ARoQKPsOYAbFDBhJ6WwhqllbDm47Tc5ZI3GtWqIoVgKaCS04mEHUhqb6UVA4eiSXhfP3MGJkCn/887jg7S1aRPLyP75kxfLDjBvfHgB/QMdoVBl5W0dG3NYB2xXEgzXLjrD8s4P0GdSKoaNTMBgV9u48x4dv7+Lrtcd59h/jMFvq5uWmQhNv29o38W5cf4Ds7CIAiopc+P0an368BYD4+EiGDKu6iVAHPGXTqrjKplUx+RVsXkOD+chhQ8EHlBAIDugw1WkNWNmx6i7v1Vq0/Qh+zcrJnJGhK0Qjc/58Edt3ZNK+fXOm3zeCyEhZ/rIpksDXBOl6APSSsnBWBHrxdz9rhehaEej5pSFOKwDdzXfhrfzLVBbgzKAYQbGCYgdDbNnP5V8WwFxp9FdcW9ANuXz56RayjheS0i0ai+3y1tAUpU26iqayfPlhdB3umNy9wu033NiZma9sZtkXh4KBLxDQ8JrAj4b9CtceHTCsDTfe2RV7xHd/w7ETO5DUMpJP5u5h9RdHGH9jbUeCVu9KmnjXrtnLgf2nK2z7aMHXAHTq3LJS4Ctf9qzI7sdrLF32zOJVMddyWpWrxVTWxHvhnH32WszZdzFKiPOeUXUSYc3i+Llr8QUaVj/OhurkyVz27D1Fr55tuefuIcH5D0XTI4EvDJQ2o7rKwtsFTalaYTDM6VpBWYgrKNvvwgAXANSycGYCxQSUhTVDiyoCnAVFufJauPhWsYyeMpQNn2zmyM5jtO3SiohoWcrncgQMpU26+/edR1UVunStOMLWYjHSvkMc+/efD25zaxoeg46ZK6+hSusYV+X2QSPb8sncPZzMrDz1Rl0obeI11aqJ97dP3lKjYwdUnWJb6bJnAbXysmcNWfmcfR50CvDjQSX6IgM6aq0Om3RrI8bxLW5vPGfyB4Tk/I3NocNnOXT4LCOGdeK2W/tjMDTMDyni6pDA10DpurcsrF0Y3i5sRi0ELe+CZlQvFQMclNa+mQjWxik2UGNASSoNb3wX4hQlNJ/6HDERjL5zKF8t3saxvSdpkdb8Kk7bEj4CBi9qwER2tpPoaCtmc+VAnpBg55vdZ0tXWzCqlCh+UBQs9djpPTfbCUB0s/prQtI8Jnz+qGATr/dcHL4rGMVbadmzephW5WqxlK3QUTqgo3QUr+VK4n1ZFZ+u66z+fDfZZwoBKMx34vdrzJ+1AYD4pChGXVvzfnk1YTHlYjEVcPD0nWi6tAZcjK7r7Nl7iqysfG64vhcTxqfLHHtCAt/VoutaaTNqFeENrSzA6Rf2hXNSsR+cBhgvaEY1fVfrZoj5Xi2claqaURsqs9XE0Jv7s2PlNxzafhSPy0Pz5IRGU/5Q09EJqH4Mfgsetz+4LNr3lYdAt9uP26biVcGqK/U2yFELaHz8/h4MBoXBo+p3ipfyJl6Dw4m5RTZqLUfx6oDbUrbsmbl2y541dAa+m7PvwgEdV3RdOqxctIs9249X2DzntbUAdOvdto4Dn06ziG8pLEkjp6hbHR43/GiaxrbtxygsdHHn5IEMGVyzORZF+JPAd5lKR6l5LghvxRcMaChtStW1/Av6wRWB7qdiPziCfdxQjIClrBYuEQwXhrfyWrjwHcxgMKj0uaYHjmYR7FqzF6/LR+tOLWWZnxrQVD8AqmbAYjXiyqt6XVOvtyzIWAyc8rpRdDDVY6iePXMbB/dlM3laD1q2uTr9rQLFdvRaNPGWL3tWZPPhM+oY9Ks/rcrVoAC2sjn7itHw4iMGY61j34V9+H7/77vrvJzVibCeRlV9Mg3LJfh8AbZsPYLPpzF92nDSu7cJdZFEAyKB7wK67qsivH1vNGp5LVywGfX7gxnKR6OWN6NaQY0GJZHvmlDLg5xRarEuoCjQqV87IqLtbF66nSO7jpHctXWYrcFb9wLlcxPqBuLj7RzLzMfrDVRq1j1/3kl0jJUSJYBH0zAEQDXXz/Pvw7d3kfHpQUZf144b77q6NTI1aeL1G3SKL5hWxRhQ6mTZs4bOXNbEe+GADtvlDOi4in34FPxE249wvqAXxW4JMNVxu318vfkwFrORhx8aTVpq6FfKEQ1LWL+Tlg5mqK4ZtbAsxOWXNaMWglZCxX5wZc2owSZU03c1bobW1TSjSqfYK9W6QwvskTa+/LR0MEdy19ZYIxruclqhFjD40QFVU+ncJYHNm06xb+85evb6bo4yj8fPoYM5pPdsTq7fhxkFfz2VZ8Hs3XwyZw8jxqcy47H+9XSWi6u6iTcOj0rptCrWQOnHM7+CzWdADeE0I1db+YAONzr5FwzoqFXsu4qPV5Q9E00zczJndJ0f+9SpY6xatZgd278iK+sEPp+XpKTWDB02jptuuhurtXTtb13XWb16CZs3reXQoX3k5J4nKiqGtNSOTL7zATp1qtv+irVVVOxm06bDxMY6+OH9I0lKiglpeUTD1GgDn657wbevYpi7cE64Cs2oF9bC8V0zKsbSnxUrqAllIc5Wdnt5M2qjfYgatdikGEbdNYyNn2zmyK5jtOncishmEaEuVoOkqT4AFFTGjG3Hu+/s4MMPvqkQ+BZ9uh+328+AMcnogFlXCNRD09iC2bv56L1vGH5NKg/8bGDIa7ADxXY0rxE1pggl0kV+UTQ+rxlzA55W5WqxouAvq+3zoRNdwwEdiqKgXaXEZ1DdOGwnOXH+Grz+mDo/fkbGxyxZ/AEDBoxk5KjrMBqM7Nq9hXdn/4f165bx//72DhaLFZ/Py9//9hRpaZ0YPmICzZu3JC83m6VL5/OrX97H4z//A6NHT6zz8tVEbm4xW7ZlktwmjvtnjCQmRmY6EFVT9LqcMv0q0n170QufB9yUBjcrwSlFykNbhZGo5VOKmEL+JiRqzuvxs3npNk5+m0VSSgKxLZqFukgNTqHjLEWOMzhKSptw/vH3DSxcsJfhI1IYPLgNmcfyWfDhN3RNb85PXhhJlMmE1xPA6w9gNdVdv9CP3vuGBbN3M2xsCg/+YlDI+1/qCnhMATzmABg0Iq1uVB2Kc6NxFckqA+U0wF0W4RwYcFxi8paC7CKaNY8hKu7Sj+HRQC5dDYm0NFxeH874yF0A7Mz8EZpe97X8Bw/uoWXLtkREVLyW2bP/wwfzXuehh37DDZPuIhDws3fvDtLT+1XYLy8vhx/96HYMqsrb72Sgqlf3Q8SZMwXs3HWcrl1act+9w7DbpSVEVK/xVl8ZUlBsE9HdSwEdTOkohpahLpWoY2aLkcE39mfXmr0c2HIYt8tLi9RECe0XKF1W7bvH4yc/HUxSi0g++2Q/X315nOhoK7fe3o1xU7tgNqiYFRW35ketw8cw49NvWTB7N3GJdrr1TmLjqswKt0c3s5Le5+osg6UZ9NKgZ9LQFVA1MHqNeNwOLDY30XF5mK1eCrOboetNu5YPSpt47WUDOorw4y1bj7e6AR0KXJU+fGZjARZzLoeybq+XsAfQoUPV/UuHDx/PB/Ne59ixQwAYDMZKYQ+gWbM4unfvw5cbV1JQkEuzZvH1Us6qZB7LZv/+0wwc0I677hyEqQ4/vInw1GgDn6LawX4XmIehO98H72Z0NQZMfVFUR6iLJ+qQqir0HNUNR4ydHav24HX5aNu5JapMIgqUDtpQLliOzmBQuWtKD+6a0iO4LcfnIcfnJcJQ+i+vaRpKHdbAHfk2t/Q855zM/H9fVbq9c3pivQc+n7E06PmMpV03DAGlbLTtd9fpcdkI+I1Y7U6MLb0Uno/D55U53aB0QIcBA240ci4Y0FGV+s97Os0cByhyJpNd2OPSu9ex7OyzAMQ0q3pC8QvlZJ/DaDRVqiWsL7quc+DbMxw/ns24a7oz6YbeIa9NF41Dow185RRja4j8Ffi2oDvngCcD3ZgGxm7S/y6MKAq0752KPdrOpsXbOLzzGCnd28gIXkpX2VAvUlPl1QLk+X1Y1e9WWtDqeEqWh345iId+OajOjldjCnhNGm5TgIBBL51qJqBiCJTdWAW/z0SgyIDN4aRZi3MU50bjlCZeoHTOvghUXOjklQ3oiMJYMfaVVvHVaznslrMYVA/Hs8fBVV6pOBAIMG/uaxgMRkaOvO6i+27ZvI5vv/2G0aMnYjbXf3Oqpuns2n2c7Oxibr2lHyNHdJbWDlFjYVFFoigKirk/SvTzKPa7IHAW3J+j+4/TSLsoimq0TGvOyDuHYI2wcHhHJq7iqueca0o0gxdFq6Y5R9c57/OiA3a1dJ+AXtpfqzG/TWhq6UTJBRFeiq1+dEXH7FOxeFUM36vVq4quqTgLIwh4jUTF5RGdkI2iaFen8I2AjdIVWJxltX2+CgFPqdcaPoUAMfZD5BR1p8iVUn8nqsbrr73I/v27uPvuh2nduvrznz51jL///Wni4hK5/4Gf13u5/H6NzVuOkJ/v5J67hzBqZBcJe6JWwqp6RFEsYLsFzEPRnXPB+xX4D6Ob+6CoslxXuGiWGM3ou4ay8dMtHN19nNadWhIVG9pmfE3Tee/jr5m/ZCunz+bTLDqC8SO68ui9o7Bb66/JUEMjoAYw+qsOfMUBPyUBPw7Dd//qugag12mT7tUSMOi4zQG8Rg2U0mZbi1e5zGXPlApNvKaWPgoaQRPv8gU7OHk4mxNHssk9W0SzBAe/e3VKnZ/HhILhe3P22VHrvYYv0n4MTTdwIntsvZ2jOu/O/g+LFs1jwrW3ccfk+6vd78yZUzz51MOgKDz73L+Jjo6t13J5vX6+3nQYVVV54P5RdO50dfrDivASFjV836cYElEjH0OJ+i2ozcCzEt27tXQqFxEWbA4rI+4YRNuurTl54DTZp3JDWp4XZ37B/3t1GWltE/jto9cxbngX5nyyiceemYum1d+bo2bwAXqVNXxaWe2eSVEwXzA/ZEDXQW9cNXw+o0ax3U9hhA+vUcMYULF6VEx+9YrXuPX7TDiLIlFVjWYtzmGPLKqjUtePxe9u5uDu08QnRWF31G8zYvmcfUYUCvCThx9NUco+NNTD+RQPkdYTnMkfjMdXvyHq+95/7xXmzXuda665iR/96Mlq9zt79jRPPvFD3C4nf/jD/0hJqd+ly5xODxs2HsRmM/PoI2Ml7InLFlY1fN+nmHpAdGdwL0d3fVTazGvqDoZUqQoPAyaTkUET++CItrPvq4N4XF5apjW/6jVXhzLPMefTTYwd2pm/Pz05uL1VUjNe+N/nfL7mG64fXT8Ts2qqHxS9yj58uT4vPl0j2miqeB9NbxTPf10Bb9m0KoH/z955x8dxXWf7uTOzs72gV4Jg772JnRRVLMmS1SzJli3Jllzi7iTOF5ckLomd2E4c9x43SZas3iWSokRV9t5JgOhEr9t3Z+73x4IgIYIkCKKS8/x+kMBpe2exO/POOfe8RyE1Py+hoJrQ33JVmgrhDjd2ZxRfRgs2R4z2pnSkOfyeib/+yzvJzE3ZnPzXFx4nFk0M+GvaOzt0RDEx3QIHA6P40jxHiSf91DQvHZDjn42HH/oVf/3rr7lyzY18/gv/etbvR11dDV/76gOEQkH+/T9+xbhxkwd0XG1tYbZuO05ujp/7719JVqY119Si7wy/q1k/I4SOcF6PCPwX2FdCYj/E1iPNoY0IWfQPQghmLJ/C/GtnE2wJUba/CsMY3LlYL7++DynhI7cs6rb8tuvm4rDbeGHD3gF7bUNJza56b4Qvbhq0GN0LNU5iyuEd3jMVScRu0OaJE3YYKaPohII9frL12UANPpXijYZcOFxhMvLqsOnDLytwUuwNNicLOgCaRYKGcAizH0N9NrUDh95IVdMqDNPZb8c9H3/966/5619/zerVN/DFL37zrF569fU1fO1rnyAU6uDb3/kl48dPHdBxNTR0sHlLCWPHZvG5z15liT2Li+aSjvCdjlDSEZ5PIe0rkeGHILYRqeaDbXZq7p/FiGbszCJcfiebnt9O6e4yRk8dhe6wnX/HfmDfkRoURTB9YkG35XZdY/K4HPYfqRmw1zbVJCkJlBJBD/55F0cON3LgcAP1J4Jk5bj59SO3dd/HlMPySS/Zaaty+vw8LSm6zm3QxpGwYXR4cVhVvD1iS4CiKbREo0SSSXLdHnT1Yj3gUjYsoWgBDW1z+mWcveGF5x/l4Yd+RVZWLrNnL2Ljxpe6rQ8EMpgz5wrC4RBf++onqa+r4f033kV1dRnV1WXdtp09+wrSemHj0huqqprZf6Ca2bOK+MjdS7DbB+daZnFpc9kIvpMI22TwfTMl+CKPQfQlpDYVtPFWH9wRTu7orFQxx9NbKdldRvHUUTi9jgF/3YbmIAGfC70Hi5jsDB+7DlSRSBgDYoya8uBTOBn1+s2vt+L12SkcHyAU7Dk6ZQynlK6AuGYSs5kkNTPloX4eW5XBQJoKkfaRkeIdbASgSYHTbqcjEaeio40spwu/bk/5J/UBp96ATQ1z7MTtSAbPQPjI0f0ANDTU8qMf/esZ66dPn8ecOVfQ0dFGXV01AM8/90iPx/rud3/bL4LvWEkdx0rqWLFsErfdugDV8hu16CcuO8EHIIQKjitBX4CMPAnRdWCUIm1zEWr2UA/P4iLwZ3hZdddSNj2/neP7KiiYkId/gFMh0VgC/Sxi7qQIjMQSAyP41CScNn/v4UfvIJGRes1vPPAi0ciZ87tMU6IN8U3EFBDXDWI2A1NJdcPQEwrKAMzP6zupFG8yqeFwRbDpdSOiindwkKiKgt9uJxRPUBcKEU4myXa6UC+4vZhJwH2U5uBk2sLjBmS0Z+PLX/42X/7yt8+7XU5OPs89v3NAxyKlZP+Bak6caOX918/m2mtmDJ8HM4tLgstS8J1EKF6E+16kfXlnt453kEo26LMRwmpAPVJxuu0sv20RO9bt4fjeCuKROJmF6QN28XTYbTRHeo6mxePJ1JgGKCVjKN1Nlx3ZDsLJOAGtZ1FiSpma8zdE9xGjs+1ZvLPtmWqcFHrD98ZmJGyEO1QrxXsScfqvAo+uEzcMOuJxYskkOW43Ts1GTUUTL7+8jd9uraO+ppVYPElhQTpXXzmLj9y1HKcz9Rn1OisAqGy8iuEj9gcX0zTZsbOc9vYId96xiCWLB7by1+LyxIoVA0Ibi/B+HeH5fGpBdC0ycQApjaEdmEWf0TSVBe+bw/Rlk2msaabqyIkBs0fJSvfQ2h7uEnenU9/UTprfNWB9Lg013tVWLdZZqOFU1bN+sVPvweCndBOaSdCZpN2VIGYzUQ3RaauiDmuxd5KTKd5uRs3KZWzU/J6vkq6qBOx2DCmpCnbQHInw6vO7efNve8krSOOTH7+KL3/uBoqLsvn5r1/m3k/8jGg0gSLi+JwV1LUuJBK/PLMriYTB5i0lhMNxPnbfckvsWQwYl3WE73SEEGBfCrY5EH0WGX0JjDKkbTZCzR/q4Vn0ASFg2pJJuANutq/dzfF9FRRPKUTtZ/E1fWI+7+4oZd+RauZOH921PBZPcqikjnkzivr19U4ikZhqEi1p7+yoEUMATnH28zM7PfgGxblGQMxmELOZp9qeJQfGVmVw6CHF25hOInZ5FX0JQY8djBQh8Nt1wskkDZEw4xYV8bUPT2FBYBT5aqqy+IO3LqZoVCa/++OrPP3cFv7uY+kkDDfVzSsG+zSGBdFogs1bS7DrGp/+1GrGjrk8Ra/F4GBF+N6DUFwI110I33+ANh3iW5GxjUgzONRDs+gjxVMLWXH7FSiKQsmecuI9zGu7GK5dOQ0h4MGnNndb/sRLO4jGEgPmwSeFiSkMhFToMJKEDQO3cm4xa5hywPO5XbYq7k5bla62ZwNtqzI4pFK8HoRqkpbbMOyNmvsfwdk7bQhcmg2/3U7WuHRMu6A9Huu2xbVXzQKg5HglLnsd1U0rSRrugR3yMKQjGOXtd47g8zr53GeussSexYBjRfjOgtAKwfsVSGxDhv8KsXVIbSxo0xDCettGGlmFGay6cwnvPruNkt1lFE0pwO3vn3maE8bkcOeNC3jk2a18+dt/Y/mC8ZRWNvLXZ7Ywf8boARN8RqfpsjQVGhNxbELBdp5KcynlgMmts7U9ExfZCWM4MpRVvFtfP0pLQ0pkBtujGEmDtY/tACAty8uCVYOQEjzP7AhNUQjodqLEqehoR4mqjPUH0BSFuvo2AApzYoRjOdS1zR/48Q4zmpuDbNtRxuhRGdz/8ZUEAtaccYuBx1Iu50AIAfoCsM2E6IvIyHOQrETaZoI6yqqgGmH40j2sunMJm17YTvmBKvLG5ZKW3T8mtv/0qWvJzwnwxIs7eHPrUQI+F3fdtIDP3rMaZYDyp6aaBCRtcZOENM9aqHE6KUuW/h1H3NbdVkUzFLQhtlUZHIYmxbt5/WFK9p/otuylh7cDMG5a3qAIvt7MhhVCoKkqHl2nJthBeyzKeH86v/nDejRV4Y4PZFLZcDVSXl63odraNnbvqWDqlHzuvWcZLtflNSXAYugQsqfJGBY9Io16ZPgRiG8C4QV9LkLxD/WwLC4QI2mw49W9lO4uJyM/neyijBEp3iOOVhrSSqlv8OBQ1DPm7n3xY88SjSS6GS+3hVLpNZt2cZEoeXJ+np6yVRFmSuiN3Pl5F4dQTByeMELIS76KNx5JoCgCX+b5H5Ya1BAFSR/upJ2GcIhHf/c6b7y0h699eRofu+daDlV/hMvp81JW3sihQzUsWjiOu+68YsCKuSwseuLyerS6SISajfB+AZlYhQw9DLENSLUIbDMQwvLmGimomsr8a2bjCXjY99YBYuEYhZPyBywSN1AklQQRw0QgzlmocTqGKdHUvp+nqchOoZeyVTnlnzey3rv+5nIyahZC9Bjhi0cTfOuBB2msbWfVB2byoc+v7lpnUxRee2I7b7y0h6tvGMtH7yvgcM1KLhexJ6Xk8JFaKioaufqq6dz4/jkj7npjMfKxBF8fELaZ4J8M0fXIyFMQfRlpmw7qmBEZKbocEQKmLBqPJ+Bi68u7OL6nnNHTCtFsI+cr0SojJAy6FWq8vraEhroQAO1tUZIJk8f+sgeAzBw3MxcX9Okz2q3tGaCaQ9P2bHhzGVXx9qD4nv3TJjraIj1u/shf3uDxh99mzbUz+Nl3c9hYVsy6I22sLGhjtO/SzpKYpmTP3goaG4Pcest8Vq6YbN0nLIaEkXN3G2YIoYPzerBfgQw/BrE3IFmC1OchlPShHp5FLxk1KR+n18m7z22jZFdK9DlGwJyauGnSLMPY6F6o8eqLx9i/u67btn/9v10ATJ2VzczF+b2/2YjU/LyozcRQzZStyjBoezbc6WbUnNtAsMVPuP3STfECVByt59UndnLrJ5fx+K/e7Lbu6T+/wzMPbmL11TP53remYxLkRMtqkAleKithZmY2C3Pz0S64Q8fwJ5k02b7jOJFInI/cvYT588YM9ZAsLmOsOXz9hEwcQoYfguQRUPPBNhshhr9wsEgRbAvz7jNbaalrY9TkfDyB4W0TcSwSJJhZhsduIkK9KzyJJ006wjGc9nM/553e9sxQQO2cnze82p6NBCS6M4pujxMNuWi7RFK8iWgSKSWB7FRkzjRMvve5R/Gnu7jr86v5+kf+0JXSffTBN9nwxx2sWjODf/znq5iWv48t5cvZWzMPKSV14TDVwXZyXG5WjxpNusM5xGfXf8TjSTZvKUFRFO69ZxmTJ+UN9ZAsLnOsCF8/IWyTwfdNiG1ERh6D6EtIbSpo4xHnscqwGHo8fhcr71jClpd2UnGwmtwx2aTnBoZ6WEAqJfTQ05t5/MXt1NS1EvC7mLZoLPf+4yiErfdzR00pz6nXzrRVAXtcQbkEbVUGB0E84sRI2nC4wmRcKine99jwrX9iJ7WVzXzq327ottlrz+xmwx93kJHtZeacYvZseovdUmFbZTOmuQG3L42x0+bg03WOtbbw1LEjLM4rYEr6yCyiOp1wOMbmLaV4vQ7u//hKRhVaWR+LoccSfP2IECo4rgR9ATLyJETXgVGKtM1FqJap5nBHd9hY8oEF7HptH0d3lBKPxMkpzhrym88Pfv0KDz+zhSuXTOajt17BltJq1r20h4a6Kr7549W9dk83TbPHOXcJzSSmmyRSJbaXka3K4GAkNMIdHhzuSyjF25kYajzRxnN/3sQNH1lEZq6Pxtr2rk3KD6emFjTVd/CTHz532s4pg/KiiTMYO20OLpuN6ZlZlLe38XpVOdXBDpYVjMKpjczbU1tbmK3bjpOb4+f++1eSlTnC/9YWlwwj8xs1zBGKF+G+F2lfjgw/DPF3kEo26LMRwjLYHM4oimDOlTPwBNzsfn0/sUicUZPyUdShidIeK6vnr89uYc3SyfzPv9zBiXiUjPBocnP9/Ol3G3lrbRUrVk3s1bHM0z34umxVTAzlUmh7NryRpkKkw43ujOJLb0G3x2hvSsM0R54thwBOdhF+6H83kJnr5+rb55yx3X3/dA03fHUpBUkvi7OP0hTK4aUDt9DT50sRgjH+AAG7g2NtLdRHQqwuHE2+Z2SJpYaGDnbuKmP8+Bw+ft8KPB7HUA/JwqILK9c4gAhtLML7dYTn86kF0bXIxAGkNIZ2YBbnRAiYOG8sSz6wgEQ8SemeChLx5JCM5eXX9yElfOSWRcRNk7JoCJeict31U7E7VN54pbzXxzKkRKoQsRu0ejrbnnFptT0b3qRSvJGQG7srQnpePbo9dv7dhhtCgIRN6w9xcEcFd39xNap2duGa625CU5Nsq1jM+T5faQ4H0zOyiBsmz5UeY2vtCQxpnnOf4UJVVTM7dpYxa2YRn/rEakvsWQw7LME3wAghEPalCP9/Ipw3g1EOsVeQRs1QD83iPBSMz2XlBxejO2yU7iojGhr8m/O+IzUoimD6xALKomFipiTdZkN3Soon+Th2sLlXx4krJmFnkrDPIKobKKbAHlPQE+pl76E32JxM8QrFJJDbgMs38nrxJhJJHv/VG0xfWIwv3U19dSv11a0016VSupFQnPrqVuLBCKP9NZQ0TKYh2LuiBV1VmZKeQZbLxda6Gp4vPUZ7bHgL42Mldew7UMXyZRO5797l2O22oR6ShcUZWCndQUIoLnDdBfqyzjTvVqQSANs8hOIZ6uFZnIX03ACrPrSUd5/ZRuneCgon5uFLH7y/V0NzkIDPRVSRnEhECWgaKgKpGmRk2zm8u4VEwujRsV8CUdUgrCaIKyZGUqIZCjYrkjfknJHidURpb0wfMSneRMygozXC3s1l7N1cdsb6zesPsXn9Ie754lQS9xexs2rRBR1fCEGBx4tft1PS1sITxw6zvKCQcf60IZ9TezpSSvYfqObEiVbef/1srr1mxrAan4XF6ViCb5ARWiF4vwKJbcjwXyG2DqmNBW0aQozsP8fhIzV853tPsmP3cWpOpIRI0ahMrrtmNl/50vvJy0vrtv27m4/wnz98hh27ymhq7iA/L43VK6fx1X/8AGPH5AzRWZyJ2+tkxQcXs+3lXVQeriZ7dBaZ+Wnn37EfiMYS6DaVY9EQKgKv2hk5UJPY9JQ4iMe6Cz5TSMJqkrCWxBASVQrshkIsKlFVQb8307XoI92reNPz6mlvTCc+zKt4BaDbNT75L9ed8VkKtkZ4+CevMW3BaFbdMImrFzaxpXYGwVjfelZ7dJ3pGVkcb29jXUUZVWkdLMkvRFeHXhibpsmOneW0t0e4845FLFk88D2MLSwuhpGtMEYoQgjQF4BtJkRfREaeg2Ql0jYT1FEj9gmxqrqZE7Ut3HzjAgoL0tE0lb37K/jtH17l0cffYee7/0l2p3fXy2t38f7bvs+4sTl89lPXkJnpZf+BKn77hw08+cwW9mz5PgX5w8fKQLdrXHHjPNx+F4e3HiMeiZM3NnvA/1YOu43GcIy2ZIJcm+NUXE41SMTMzrGlbn5JxSSkJomoBlJINFPgMjQUBIZhIkhNjrcYXpxexRsYIVW8qqYwd8WEMz7/J6t0s/L9fPgDdkIik227p5F1Ma+lKIwPpNEYCXOwpYnacIjVo0aT4xo6r8xEwmDb9lISCZOP3becGdNHDdlYLCx6iyX4hhAh7OC8BfSlyPAjEN/U2a1jLkIZee2G1qyezprV089YvmLpFO786I/544Mb+ae/vwmA//3ZS6iqwtuvfovM05qwT5tayCc/+1see3ITX/rc9YM29t6gKIKZK6fiCbjZuWEv8UicUVMKUAewgjcz3UNJeQO6IbDbT3sdNUFTfRSf347pgBY1RkwxkIBuKtik2s2CxUT22P/UYngwolK8vXho0JUk6fYOHimfTdzsnz7jmU4XHpvOsbYWni05woLcfGZmZg/6Q0w0mmDz1hLsusanP7WasWMsyy2LkYFVtDEMEGo2ivcLCN8/g5IGsQ3I+HakjA/10PqF0UWZALS0hrqWtXdEcDh00tK6z4fLz02lSt3u4ZnWEgLGzR7NslsWYhgmpbvLScQSA/Z6+WMzkVLSWNbYbXnMiFF2pJ1RU9Jo1mPEFRPdVPAYGvp7xB6ANK1Ze8Of06p4ndHhX8XbwxNEZq6PX6//At/71igqOnLY3Ny/kS+HpjEtPZM0h5O3a6p4qayEYGLwrpMdwShvv3MEn9fJ5z5zlSX2LEYUluAbRgjbTIT/2wjXvWC2QvRlZLKUkdb9LhqN09jYTlV1E2vX7+HTn/8dANdfe8qr65qrZtLREeG+T/yC3XvKqa5p5pV1u/nHrz3IlMkF3HX7kqEafq/IHZPN6juX4vDYKdldTiQYvaD9TVPylyc38YEHfs6CG/+Daz7yv/zwN2sJR0/dvNqSCSYuGoMQ8OJzOwFICJMGe5QXXjxELGqw8NoinEkVl6Fhkypnk3USrLl7I4SRU8Xb83Upx9WCXU2woWoucgAeM4QQjPL6mBhIp7KjnSeOHqasva3fX+e9NDcHeXfTMQoL0vn8Z68md5h04rGw6C1WL91hijSbkeHHIPYGCCfocxHK8JnTdi5+9qtX+MI//LHr38Wjs/jOv97B3Xct61oWiyX48v/7M3/480Zip0XIrr92Ng//8fP4fCPDoDoSivHus1tpqGxKVfBm9G7u1X/98uWu7hnLFoyntKKBR57dypzpRfzmex8FAbuCbYRNg+f/9DYvvbiLWUvGMO6KAuoqWtj09AEmzEzn//3v1SjK+W+q0XgSaZIq2rAYIZzWizfsHDYpXiNhkoglCGT7zzAkV4TJFTkHONAymqdKVlJia2J6PJcCY2CmqCRNk9K2FkKJBDMys1iYm49N6f/3qLa2jd17Kpg6JZ9771mGyzU8MxAWFufCmsM3TBFKOsLzKaR9JTL8UKpHr5oPttmpuX/DmJtvnM/kifkEQ1F27i7juRe209jUPUqhqgoF+elctXo6N9+0gPQ0D2+/e5if/eoVPnTvT3n6b/+AzTb8P55Ot50Vty9m29pdlO+vImtUnKzCjHPu897uGScpyE3jv375Mi9v3MesJRNoNeJ4XRpXf24BjLKx9YUj7Ntajtfn4Nq7RnPrx2f1SuwBmKYV4Bt5vKeKN7+e9oahr+I9WajRU6SgyFuHIVVerzqz88ZAoCkKEwLp1EfC7G6o50QoVdCR4XD222uUlTdy6FANixaO4647r+jRAsnCYiRgRfhGAFIaKcEXeQzMFtCmgjYeIUZGRn7P3nIWrvgG//a12/jqV24G4KMf/xnvbD7Kvm0/wOk8Nan7V79bx2e++H/85uef4IH7rhyiEV84Ukr2v32YA5uO4E3zkD8+96xi7Gd/3MBvH3mLP/zwXuZOH921PBZPsuKDP2DOjCJu+8Yaoo4kQgNNKjgNFXtndEe1xfFkVROPeJBGShSbpmTtY4d5/ZljNNaG8AYcLFw9ilsfmIndoRGKxlFVxarSHaEIxcThDiMUSbDVT7ht6Kp4zaQkFo0TyPKjaqeuQbqSYFHuQd6smclrVXMBBjzCdzrhZIKS1hYAFucVMDU986Kq6KWUHD5SS0VFI2uunMaN75/T6wcsC4vhyMhQDJc5QqgIx5UI//fBcR0kj0BsLdKoH+qh9YqZM0YzZ1Yxv/ztOgAqKht56NG3uf59s7uJPYAP3nIFAG+8eXDQx3kxCCGYvmwyC943h3BHmLL9lRjJnltCnd4943SSOowel8meo9W0u+PoikIgoRNI6F1iD0BRjFS07rQOGQ//ZAd//elO8ov9fORL81iwehTrHj/Cj/7fRpLGyGhNZXF2TlbxJuMavrQWAtkNKMoQtWjs+th1jxWMD1TTHnfz7olpgz4kAJdmY1pGFh6bzsaqCtZWHCeS7FtBlWlKdu+poKqqmVtvmc9NN1piz2LkM/xzZhZdCMWLcN+LtC/v7NbxDlLJBn02QgzvOW+RSJzmllSVbnVNqh2Y0YMQSSaNbv8fCXzvB0+zc3cZ23ce53hZPYX56fzsyzdRsruM4mmj0B0ps+RYPMnzr+5hz8EqkLDs9u+Tme5h0uQ8rv3oXPQxLuxZDkIHovhCGl69ZzsLoRiARMrU81pVaRvrnzjC/JWFfP4/lndtl5Xn4cH/3c7mV8uZvaLAiu6NeIZnihfAYwuT6WjjueNLiRr9P55ELMqv/+0ztDbWMX/1+3nf3X/X43aKEIzxBwjYHZS2tdIYDrNq1GgKPL2PiCaTJtt3HCcSifORu5cwf96Y/joNC4shxYrwjUCENhbh/TrC8/nUguhaZOJAKvU7hNTWtva4/LWN+9l3oJIrFowHYNKEfFRV4ZnnttF6mlULwB8ffAOA+fPGDehY+5Ovf/NRNmzcz7gx2aSluVE1hVV3LsXtd1Gyu4xQewSAmrpWvv3j50kkDVxOnb/77BpmXTmWd7eX8rVP/5UTO+rRldQzmBo7+0wLRTWQ8lR7tE3ry5ESrrljUrftVt44Dt2h8u7acsuD7xKiq4pXpKp43f4hquKVp36ZGKiiOpTJrsbxA/JSG595kHBHe6+3T3M4mJ6RRcI0ea70GFtqazDM80e64/Ek7246SjJp8sD9qyyxZ3FJYUX4RihCCLAvBdsciD6LjL4ERhnSNhuh5g/JmD7zpd9zoraV1SunMbook2g0wY6dx3nk8Xfwep388HsfASA93cMXP3sd//OTF5i75Ks88LErSU9z8867R3jo0bcZNzaHB+5bPSTn0BeO7fvfrlZwM+Z/hWAoii/Dw6o7l7L5he2U768kf3wuaX4XD/78Ab72g6doaQtTeGMR42UxK6+cwtf/7hEe/fU7+LJTvoS64+xfTaEaIE9F644fakIogrFTuheL6HaVoglpHD/UbHnwXWKcMmqO4U1rwWaP0t6UjmkMXkHByenfWc42XFqMp0qXY8r+jyGcKD/G5vXPsOb2j7P+b7/r9X66qjI5PYMToSBb605QE+pgdWExfnvPEchwOMbmLaV4vQ7u//hKRhWODFcEC4veYgm+EY5QXOC6C/RlnWnerUglALZ5CMVz3v37k7s+uIS/PPwmD/71TRoaOxACRhdl8cn71/CVL91I0ajMrm1/8N27mTQxj9//8TW+94OnicWSFOSn8elPXMU3v3b7iLFlAc7a99fh0ll2y0K2r9/LoaMVGOPcdExRcGQ6CFY0kxOx4dLtMNrFqNEZVJY1ITQFj8+Bdo5KQKEku9K5AK2NEbx+vau37umkZTo5treRZFIyAoqeLS4IQTziwEhqp3rxDkqKV5z2m8l4fw2HW0dR2tb/D5qmafDCn37CuOnzmDx3yQUJPkg9GOd7vPjtdo61tvDEsUMsyx/FhEBat4KOtrYwW7cdJzfHz/33ryQrc3i3trOw6AvWLeASQWiF4P0KJLal2rTF1iG1saBNQ4jB+TPfcdti7rhtca+2FULwiY+t4RMfWzPAoxo6pISqaAdNUx20pHsJtYXxNmtMn5DPkW3VlB1uYOqMQkxT0twcxO13UFPaxPhpeax/ajdvvLSfproOvH4H85aP56aPLsTusKGoSaR5SvDFosZZBeJJEZiIGTjPETW0GLm8txdvqNVPaACreE+PFo/yNgCSDZVz37Omf9i87mmaaqu4/TNfv6jjuG060zOyKGtv49XKMqqDHSzJL8CuajQ0dLBzVxnjx+fw8ftW4PE4+mn0FhbDC+sOcAkhhAB9AdhmQvQlZORZSFYibTNBHXVRFgUWF0ZDNMjrNSW0xMPYFRuT8vOQjjiVByqZPiWXpwQ8/+ROps4o5Plnd9HWHGby3FEc2lGJRPLYb99m9pIxXH3rbE5UtLDh2b1UljType/ehKIa3Qx47Q6V9paeqxETcaNzG+urfikzqCnezstIY3UDL/58Izv2xmlseBvTMHClZ5A/ayaTr7seZyBwUS/T0lDLG888xPIbP0QgM4fWxrqLOp6qKIwLpNEUCXOwuYm6cIiptgAnjjUxe1YRH7l7CXa77aJew8JiOGPdBS5BhLCD82bQl6SiffFNkCxB6nMRysD7YVlAXThIYzTEKE+AbGdnaj3Phd2po2hlrFg9iY0bDvMv//A3Du2vwe1zcGR3FaMnZHFoZxVzlozl0994X9fxMnO9PPqrt9i28QhX321HJk/dmAKZTqrL2knEjTPSui2NETx+HZtu1Wdd+gxOilcASHBFK6mtT5I1axH5gWyEqtBWWUXJ669Tvnkz7/v2d+DcHuTn5KUHf04gK5dFV9/SX0MHIMPpwqPr7Kmp44VwE1dOHcNHP7oU3ZrzYHGJY90FLmGEmo3i/QLC98+gpEFsAzK+HSkHr9n45crkQA6jPAFOhDtIGKeqpz0BF5MWjOej9y9h9ZpJHNxbjWlIVEWw+saZTJpZgJSw5uaZ3Y63/H1T0e0am18/AshuRRtjJmcgTUnpwaZu+8RjBhVHWyiamDag52oxvDizirf31a29xWOPcd0qnbu//Qmm3vYhJqxZw/hVq5n30Y+y8P4HiLa2cvytN/t8/L3vbqD0wE6u+8hnUbX+FWJSShrrOnAFJZMKsikxg/zv2rdpCob79XUsLIYbluC7DBC2mQj/txGue8FshejLyGQpVpOVgUNTBKvzx+PRdI61N2Ge9l7bXTqKy8WWzWX4M9x89Vd38IOHP8Ydn1xK1fFUxW3xpO6FIDZdY9TYTMqONABgnjaHb9GaIoSAtX873G2fjc+VEI8aLLiyyGqrdplxyqjZhjetNWXUrPafbdPM/HpqQ3621088Y507MxXWi4dCZ6zrDclEgnV/+x3jZ8zH40ujua6G5roa2ppSRvPRSIjmuhqi4eAFH1uaJhUVTXS0R1g4byzvWzyFpRNGc6C6nm8+tZ7tZdV9GrOFxUjAimFfJgihg/N6sF+BDD8GsTcgWdqZ5rXsBwYCt6ZzZcEEXqw4QEWwhWJv6n0+tK+aL933fzg8dj7+resIOG1Eg1EcHgetTSE8PkeP/ToDGW5KDtZiJA04TfCNGhdgza0TWP/EUX7ytTeZuTifE2VtrHv8CJNmZzH/ylEIy5jlMmRgUryZ7g58zjiPHJ6JIVWMeJxkLIaRSNBWU83uv/0NgLyZs+iLQ2AyESPc0caxPVs5tmfrGev3bXqNfZteY80HP87ia2/r9XENw6S8vAHDkCxZMoGCglTkO8vn5qrp49laWsWPX3mbq6eP546FM7FbKV6LSwzrE32ZIZR0hOdTSPtKZPihVI9eNR9ssxDCqk7rb3KcHpbmjuH1EyU0RII0l7TxhY/8DrtT55Pfv5Hx43IwgnFa6tswjQjxWPK8FbexiIn6npDd3V+YS2aum9efLWH3uzV4/Hauun0iN9471WoJdZlzZhWvj1Cbr49Hk0zOqmN/dSYHm/Jx+6DkjY3sePDBri3cmZlc8clPkT1pEh00neNYPWPTHdz26a+esTzc0cZLD/2CcdPnMXvZNWQX9t4UOZEwOF7WgKYprFgxkcz32K7omsqSCUUcb2jhlb1HOVLbyCdWLaQoI3DB47ewGK5Ygu8yRdgmg++bKcEXeSyV5tWmgjYeIaxM/4Xwl4ffpLwylWptaGwnnjD49/96EoDRo7L46IeX0xQNs2H/IX5wz9/oaIuw4paZVByso62sDYC6qha2bDhMS2MQI2nyuVt+Q3qWh+nzR3Pt7bPxp7u7Km51h4YR6y7iFFXhug9N4boPTem2PJ4wus0htLg86V7F24rNHutTFe9ofzOaYvL0nonITs1UOHcevrx8ktEoLRXlVO/cSSx44enWk6iaxpT5y85YfrJKNy0rr8f1ZyMaTVBW1oDLbWf50on4/M4etxNCMDY7nUyvi83HKvnOM69y56JZrJk6znI4sLgksATfZYwQKjiuBH0BMvIkRNeBUYq0zUWo2UM9vBHD//35NTa+ebDbsn/99mMArFw+hY9+eDkLs4vY+OZB2ltSE8Of/8OmHo8VyHDR2hzm1vuuoK6mjbde3s+2N47yjZ/dQWtTCG/Ajqqq9FbCmVJa6VygtrKDzevK2b+tloaaEIm4QVa+h/mrCrnqtonYnZfDpbBvKd4v3/LbHpZuRNXtfPA3v8GVno4rPTVdoXDePEbNn8/ab30LIx7DfsuSATiP3hMKRSmvaCI9zcOypRNwunruT306PqeDNdPGsbuilj+8sY39VXXct2IefqeVAbEY2VwOVzmL8yAUL8J9L9K+vLNbxztIJRv02QgxcjpeDBWvvfyv591GFYIv3ryGwl1ZVDS3kqF6yHCfGWl4+Gev8/Qf3yU9w8WK66YxcXoev/neWt566SCVpY1MnpPBhRjcSokl+IC3XzrOa08fY9aSfBZdNRpVExze2cDTv9/Httcq+eov1qDbL4/LYV9SvGOn5nLbrXm49DgvHZlOU7OBL7PnfQKjikgrGs3RDRuY3o+CL5CZwzd+90Kvt29ri1BV1Uxenp/Fi8ej673/+yqKwpzifHIDXrYfr6assYX7V85nemFuX4ZuYTEsuDyucBa9Qmhjwfv1lOALPwrRtUhtImiTUtFAi4vCqdlIi7spp5WkbqTE2Hu02LL3TeWZP73LOxuOMGZSNl5/KqpwZG818ViSpdcVduuycT5MKc+Y73c5Mm9lIdd9eDIuz6kIz6qbxpNd6OHFBw/y1ovHufKWCUM4wsHlQlO82Xlu7vmgh+cPzyDNMQmlsYOMvLPb/RiJOPGLSOteLE1NQWprWxlTnMX8BWNR1b59B/ICXq6aPp4tJZX84MU3eP/sKdw8byo21boeWow8LMFn0Q0hBNiXgm0ORJ9FRl8Cowxpm41Q+79X5uVEVXM7dU1BJgQyqTJbaTei+LXuaaLcUemsvmkWG57ZTbAtSmNdykPt8J5qJs7IZ9kNub0WfFLKVIjPKtqgeFLPlegLVo/ixQcPUn28bZBHNBzofYo3YI9Q3epm49FC6PT8jra14U07s1933cGDtFVVkTV5yhnrBhopJXV17TQ3B5kyJZ+ZM4q42CnJDpvG8knFHKlt5Knt+zlUU88DqxaSF7D67VqMLCzBZ9EjQnGB6y7Ql3WmebcilQDY5iGUMy/yFucmaUq2lFbisNkY481AiQrKYy3oiopTOdU1Y8PTu9jwzG4ASg7WAuB06SxcNZ5b71+Mqp0gGeut4Ov/87jUaGmIAOBLu3znZ52e4k3LrSfY6j8jxbvxtVpeefkE0nwZ3e0hbdJ0zEgHZixK9tQpuDMyMRIJWsqOU7F5M5rDyZy77qJ5EM9DmpKq6maCwRhz5oxm4oTcfmvvK4RgUl4W2T4PW0oq+dZT67l7yRyWTRxtFXRYjBgswWdxToRWCN6vQGJbqk1bbB1SGwvaNISwPj695UBVHS2hCJPyshAIihzphMw4DYkgebofW2cYYsGqieQXZxCNJCg7XMu2jcdYevVk5i4pxkxGAXlBET4JKNYNqUdMw+SFvxxAVQWL1hQN9XCGlLOneBWmTfMxb9lo9kQWkIzFaDx0gNodm3AE0ggUFlD2zjvE2jsQAlwZmYxbtZrJ11+POyOD5j7YsvQFw0gZKscTSRYtGsfo0RfR0+0cpLmdrJk2nh3l1fz6tc0crKnj7iVzcNvPXwxiYTHUCGm1W7DoJVLGIPoSMvIsyDjYZoI6ynrCPQ8d0RhPbz+A12GnMP1UL+OENNgdrCZmJsnTfT0Ks/Kj9Xz1nj9y68eXsOK6ItIKG0hGfXTl1c5BImkQSxjomjXfqCce/skOXnvqGLc8MIPr7x789ONwRbUlcbjCGIaKJ6pR7GnlF1tWUtF2SkQdeP55Kt9Yx4xbb2PaTTed9Vgltiamx3MpMAauh3cymfLYE0Kw+Irx5OYOTr/wyqZWdpafID/NxydWLmBCbuagvK6FRV+xDNcseo0QdoTzZoT/P0FfCIldEHsdaV6O8596h5SwtbQawzTJT+ueJrMJlSmuHISAhmSoxxTs6AnZjJmUw6tP7yKzwIeiCKKhBEbi/MYsqQpdi554+v/28tpTx1jx/rGW2HsPJ1O8qjCYml/FwWA6FW3d50AWLFyOUDVq9uweolGmiMcTlJTUo9s0Vq2cMmhiD2BURoCrpo2jPRLlu8+9znM7D2KY5qC9voXFhWIJPosLRqjZKN4vIHz/DEoaxDYg49uRMj7UQxt2VDW3UdbYTEG6v8cInlu1M9GVTcxM0GZEejxGPJYk2BZFtZnYHDZsDjvxWJxkPHnO15ZSWtHXHnj2j/t44S8HWfq+Yj7y9/OGejjDEmkq5IgksYTGfhwUTCxBtZ36fiuqisPnI97Rl+Zp/UMkEqekpAGPx8GqVVNISxt8CymXXWf1lLEUZfh5ZNNufvjimzR29K2HsIXFQGMJPos+I2wzEf5vI1z3gtma6taRLMWaJZAiYZhsOV6F02Yj4Dp7UUCm5sEX1GkzooTNRLd1+7aVU1HSwIQZ+SASCBS8ATdOj5NkPEk8GoezvN2WB9+ZPPvHfTz3pwMsvraYe76ywBLEZ8GuJsl3d/B2zRiq6nJw+zsomnoEly8l8EwjQbStFbt/8CJqp9PREeX48QYyszysXjUFj+fi+gNfDEIIphXmsHTiaA7U1PPNp9az7XjVkI3HwuJsWLPuLS4KIXRwXg/2K5DhxyG2EZKlSH0uQunZCuNyYX91Ha2hCJPys84rvB76r1eprG+meE4uxYVZmHGT0oO1vL32AE6Xzj1fWgMkkVIBBC6vE1VVCLWFiZkx7E79DFM/swefv8uZ5/60n+f+dIArrh7Nff+0wOoxfA7G+5ppiTl5oyQdxW2npS4Tb3orhZOO0Vidx94n3kSaJgWzZw/62FpaQtTUtDJqVDqLFo49a+/pwSbT6+bq6ePZVlrFT9a+zVXTxnPHolk4bNZt1mJ4YBVtWPQrMnEIGX4IkkdAzQfbLIS4/Cwv2iMxntlxAJ/TTkHa+aMg617exQvPbuPgoSo6WiMIIcjK9TFz0RhuumcRmbl+hL0cIaJII9C1XyKWoKMllUKyO3XESREjIRSNo6qKVaULvPbUUR7+yU7Sc1x84GPTzxB7vjQ7U+dbXRQAfLYoszLreLpsMn/88X4aj9aQM7UIV6YXRQap3nWcqp21BIqKuOob/4Kmn71Ctb+LNuob2mmob2fChBzmzik+9XkfRkgpKWtsYU9FLcWZaXxy9UJGZ57dpNrCYrCwBJ9FvyOlAbGNyMhjYLaANhW08YiLdUAdIUgJrx4ooaq5lakFORckuMJmnN3BahQhyNa83SJ0iuMYEgmGj6d+v5vSg02UHmikvjpIRo6L7/7pWnSnjqIqmKYkEktYgq+T//vPLbz7StlZ10+clcVX/nf14A1o2CKZl1VDY9TNn4/MpmLbMY6u20FrVSOxYAShKPjy/ExcXcjMW+cQC88mHgmc9Wj9JfiklNScaKWtLcz0aYVMm1ow7CuSOqIxNh2rRErJHYtmcvW08dYUAoshxRJ8FgOGNDuQkSchug6EBra5CDV7qIc14JQ3tfLqvmMUZQYIuM7sl3s+mpNh9odO4FHtpGmn9lecB5GmDqaHD876Pzx+O2OmZFB6oBGX28Z/PnQ9iXgC3a6DIojGk9gsSxaLCyDHGWScv5kHj86iIhg463bxaJTMghAON3Q0jybYXEhPU8L7Q/BJ06SisplIJM68ucWMHTdyriGmabKnso7yxhYWjB3FfcvnnXM+r4XFQGJNLrAYMITiRbjvRdqXd3breAepZIM+GyEGv6JuMEgYJltLq3DabX0SewDpmotiRzql0SZ0RcWt6IAJwgBSAu5nL9xOTmHK5uXvb32SaCSJN92TmtMXiSE09Wy1HBYWPaJgMtbXwoGW7HOKPUhV8bbWp5Gel8SXcRzd0UZr3URMo3+LJwzDpLy8AcOQLFkygYKCkZUaVRSF2aPzyA142F5aTVlDM/evXMCMUdb0AYvB5/LIsVkMKUIbi/B+HeH5fGpBdC0ysT+V+r3E2FdVR1s4SlFG4KKOU2gPkGPz0pQIETcNEMlUBkumvrInxd7pCCHwBNy4PE6MpJmq2rCw6CVF3jYMqbCxpriXewjikQDh9hzsrlYyR+1Cd7b023gSCYOS0noQghUrJo04sXc6uX4va6aPJ2ma/PClN/jblj3Ek5fe9c9ieGNF+CwGBSEE2JeCbQ5En0VGXwKjHGmbjVDzh3p4/UJbOMbeylqyfG7s2sV9tQSC8c5MImaCukQHBZqGigR5/hSt0+skbphEIwmS8SSabn3NLc6NriQpdLfzVu1oWuK9i0zLzhiykXQQas3H6W0go2BftxRvMhhiy5MPUbNjB+0tjdgdTrIKRrPyAx+haOL0sx47Gk1QVtaAy21n+dKJ+Px9i5YPJxw2jeWTijla18Qz2w9wqKaBT6xaSF7AO9RDs7hMsO4EFoOKUFzgugv0ZZ1p3q1IJQC2eQjFM9TD6zNSwpbjlZjSJNffPxdwrbMTx65QNa1GB1lITqZ0z4dQFGx2DTOeJBFLYtNVy6PF4qyM87XQnnCwqb6w9zudFkCWUiXcnoPubOtK8Vbvz2D/d3+EiCSYt+xa0nMLiIVD1FWV0dF69h67oVCU8oom0tM8LFs6Aafr0ulTK4RgYm4m2V43m0sq+dZT67l7yWyWTSy2CjosBhxL8FkMCUIrBO9XILENGX4EYuuQ2ljQpiHEyPtYlje18uufv0JDVQslh09QU9VMXn4az6z9+ln32bennF/+5GX276lACJgxu5jPffl6Jk4u6NrGodiY7MymNNFCwpSovSxNNE2JqiroHgexcIxEPImqa1bFrsUZeGwxMp1hni+fSNQ4f49m4CwVsqkUr5Fw4PQ2sPX/HkKYSW7+zn8ywVvcq8O2tUWoqmomL8/P4sXj0S/R6HTA7eSq6ePZUVbDr1/bwr6qOu5ZNhe3/dIRtxbDD2sOn8WQIYRA6AsQ/u8iXB8Co66zW0fFiOrWEU8abCut5Mk/vcXubccpLMzA5zt3Cmrv7nI+fd8vqalq5pOfu5ZPfPZaKssb+eQ9v+DYkRPdtg1oLrLtTpISYua526mlkJimiRACRVFwuB2omooRT2IaI+d9tRgMJBP8zVSHvOxu7n0hgUCc9TtqJB0ceytBzd4TzP/oZDLGtpFMxknEouc8ZlNTkKqqJsYUZ7Js2aRLVuydRFUUFowtZG5xPu8cLeebT63n8ImGoR6WxSXMpf2NshgRCGEH582gL0lF++KbIFnS2a1jaFo3XQj7qupoi8R49PmvMKY4B4C7bv4BkfDZewv/9/eexmZT+fWfPkN2Tuocr7p2Fnfe9H1+/IPn+OlvP9lte5+mkjRttJgxVCHQxNlTu2ZnscbJFJEQAofbTiyS6r8rpYJq2bVYAJmOMC4twTNlkzHlBT7/dwq+aEeYXX97k/LNhwg1tmNz6qj2VKTQlpHOy9//JcffrUKakvScfJa//0PMWHzlaYeR1NW109wcZMqUfGbOKOIysewEoDDdT7rbxeaSSv7z+de5Zd40rp81GU29jN4Ei0HBEnwWwwahZiO8X0AmViFDD0NsA1ItAtuMVAu3YUhrOMreqlqyfW7yAmdWzvZEZUUjB/ZVcuMtC7vEHkB2jp8rr5nF809vpbGxnczMU8czRRyP6iSqaLQbMQKaE+Us6V3DlKk+ut3StyLVfg145ZFDvPXicZrqwngDduavGsUHPjYdu9O6HFxOCCTjfC0cbsugtKNvFbAd9a08/9U/kIzGmXT1XPwFGcRDUfY+uwmAd374Bun56dz49SsxYipbHt7PM7//bwzDYPayq5GmpKq6mWAwxpw5o5k4IXfYGyoPBC67jVVTxnCwpp5HN+/hQE09969cQJbXPdRDs7iEsB4hLIYdwjYT4f82wnUvmK2dad7SYZfmlRK2llaC5IIKNQ7sqwRgxqzRZ6ybMasIKSWH9ndvvm4QQ0ElV/ehCYWOZBTO4rSXSttKlDO+3YLHfrWHJ36zl9zRPu78zCzmrRzFhieP8tOvv9UVGbS4PCj0tCOQvF4zhj6pLAmv/fcTSNPktp9+hoX3Xc2kq+cy4+Yl+PMzANCcNj7w7w8wfcE1zLp+Ch/5zbU43E5ee+pPJBNJysobCUfiLFo0jokTL0+xdxIhBFMLclg2sZhDJxr45pPr2VJaOdTDsriEsB7pLYYlQujgvB7sVyDDj0NsIyRLO9O86UM9PADKGluoaGpjTFbaBVXYNda3AZCdc2ZEMCs7FfFrqG/vWiaRmMTRcGMTKnm6j8ef3ElLbQS70GhviZJMmDzxm10A+DMdzFlVyHvvnlWlrax/7DDzV43iM/++jFg4hjQhM9fFIz/bxdYNFSy66kwRanHpoSkGRZ42NtcX0BjtQxRJQN3hKuoOVLD4k9fhSvdiJg3MpIHm0LusgApXTkG1aQipIdtG4XQ1M2H5KPa+fIQ9e7fh849h2dKJ5OYO/6kbg0WG18XV08az7Xg1P133DldOGc9dV8zEqfeyoMbC4ixYgs9iWCOUdITnk0j7CmT4oVSPXjUfbLMQYuhaFMWTBluPV+Fx6PicFzaOaDQBgK2HSem6Xevc5tT8P4mBxER0WrI4FRubny1n/7babvs+8vMdAEyak83cVWfaa2xaV4aUcM2dU1BVFYc7VcG7+JoinvzdXjatL7cE32XCOG8LoaSNd+qK+ngEwYndpQB4sgK88u2HqNx+DGma+PIz8HROR3Cku0/bQ0A4A4c9Ff178RffYfkHriP35v+8qHO5FLFpKosnFHG8oZlXDxzjaF0jn1y1kOKskWs+bTH0WILPYkQgbJPB982U4Is8lkrzalNBG48Yghneeypr6YjEmJJ/4X09HY7Uk3oifmbFbTyW7Nzm1JxFU8QBiXKa6fJP//xBGuIdtBoRfKqjWxFHR0eEpGGecezjB5sQimDs1NQN92QFrxAKhWP9lB1qvuBzsRh5uLQ4Wa4Qr1ROIJzs+9zYjtrU5+XNnz2LPz+dVV++BSNpsPepd6jZcxyASGNHt33i8SRHNh899W97DU3qDtKNWV0PNBanGJOVTqbXzeZjlfz7sxv44MIZXDN9guXZZ9EnLMFnMWIQQgXHlaAvQEaehOg6MEqRtrkIdfAaqreEIuyvqiPb70HvQ7VrZmfatr6u/Yx1DZ3p3qzs0wo2SAm+02+IAvAYdj530+M0VAe59s7JPPC1JUCqaKOnG0JrQwSv354yYT55HCFwuHXSslyUHmgmFklgd1qpo0uZCf4m6iNuvnT1gz2u1+w27vzD35/7IAKSJyPVTp0b/uM+VFvqdlJ8xWQeeeBHxEMxKl7dT+L2a8HhJBpNsOet3bTVNuIJeAi2BtFw0KzuICrqyUkuR8MqUngvXoedK6eOZW9VHX9+awcHquv52PJ5BNwjv/uIxeBiCT6LEYdQvAj3vafSvPF3kEo26LMRwjWgry0lbCmtAgG5vr51Bpk6fRSQ8uK7+fZF3dbt3V2BEILJ006lZE2RACER72mr9qefbiHYEgMgLg0kEoHANMwebVdisSSa3lM0VHSJvGg4gaoJNJt1abgUSbeH8dgSvFQ5EYCsyYVMuHJ2t21EL+1A1M4pCeNWzOgSewB2j5PRiyZzdMNuYs0hnvjKr5m+fD7BYJhdz7+JEILbvnwnf/q336NJN15jPB1KCbXa6xQmb+ifE73EUBSFWUV55Po9bC2tpqyxhftXzGdmUd5QD81iBGFV6VqMWIQ2BuH9OsLz+dSC6FpkYj9SDlxT8uMNzVQ1tzEqPdDntMqookymTCtkw9rdXRE9SEX3NqzdzfxF47tbshAHqSJO+7oeOVDP43/ZxX2fOyUYQ0a8q9JW6WFodrtGMn5mqhcgEU+9Z740J1L2nG62GOlIxvubKWlP40hbKq3vzQ4wZtm0bj/Fi6ec90gCcAZSDzyutDMffFxpqar1KfcuR9VtvPPYOna/8CZSmtz/vU8zfs7Erm1PPtA4ZE4/nOOlTY7fy9XTx2NKk/9+6U3++u4u4smBu95ZXFpYj/EWIxohBNiXgm0ORJ9FRl8Coxxpm41Q8/v1tWJJg23Hq/E47Pic9jPWv/jsdk6caAGgtTlEImnw+1+vByAvL43rb5rXte0//PPN/N3Hf8kn7/kFd9y9FIC/PfQ2pin54j/e2O24JoluYs8wTH74rxtYuGw0K64exy+//xZOxUZcGggzjuS9HnwpAllOqsvaSMSNbmldgJaGCN6AHbvLjs0wiIXjJGJJNF215gtdIhS4O1CF5LWasZxewW10VtfaHL2fzyeA9DG5lG7cQ6jxzKkJoabUstzFE5h95VLSYwq//cIPufGBm5l71XyaahqB1MNMSKnAZ0wiw5h7Ued3uWC3aSybWMyxuiae33WIw7Wpgo78tN75gFpcvliCz+KSQCgucN0F+jJk+GGIb0UqAbDNQyh9S72+l90VJ+iIxpha0PN8wWef3MyObaXdlv36py8DMHf+2G6Cb+acYn75h7/jVz99mV/95GWEEMyYPZrv/c9HmTi5u1A1iXcTfI//aReVx1v49o+v71qmKyppmpMHf72Nyv0tVB1tpaEmSGaum/9+6hYAxkzJYN/mE5QeaGLS7FPnEI8ZVBxtZtLsVIRFUVXsbjuxcIxkPIlq01B6ChlajBhUYTLa28qOxjzqIqe+DxWbD3P8rf1IU2L3uRh9xWRm3bEC3XXmA003BOTNGYvtUTvHXt/DnDtXYOt8CAo3d1C26SDevHRceQGydT9rv/UgmQVZrPnINd0OExfteMzRZBtLun3GLc6NEIIJuZlk+zxsKqnkm0+v5+7Fs1kxaYz1gGZxVizBZ3FJIbRC8H4FEttSbdpi65DaWNCmIUTfP+5NwQgHquvJ8XuwqT0Xavzqj5+5oGPOnF3ML37/6fNuZ4hYV8HGiao2/vjzzdzzdwvJLfBRW30qupJhc/PsL/bi8umMmZROuKN7a7dFa0bz/J/2sfbRg90E38ZnjxKPGiy+trhr2ckK3lgkjpFIgqagnOW8LYY/Y7wtxAyNt2pP2e5kjMujaNEkvDlpJCJxanaVcGTtDuoPVnLNtz5y3oif7nKw6OPX8NbPn+OZf/wdE6+eg5kwOPjSVsyEwdx7r8Zut1H+zn4ObT7Al3/7/1C11HfQIDX3VMNNTnKlVaHbR/wuB1dNG8fO8hp++/pW9lfVcc+yuXgc5xHsFpclluCzuOQQQoC+AGwzIfoSMvIsJCuRtpmgjrrgJ+CTHTWEEOT4+ydaeCGYxFE6b4j/863XySv088F7Z5+xnYLgZ3+7HfJB1RT+/cOvEAufmos3anwaa26bxPrHD/OTf97IzCX5nChrZ93fDjF5TjaLrxnT7XhCCBwunXhUkIglkBKrB+8IxKEmyHMHWV81jmDilBB433fu6bbd2BXTCRRls/tvb3D45W1Mv3nJuQ8sU310sycX0lhygs2/fwUE5EwvZtFnb2LasilUx1tY+5OnmbZ0Br4MH/WVdUgSVDfuA0C0+6mvPIE3EMDttVKSfUFVFOaPKaTa38a7xyoobWjmgVULmZyXNdRDsxhmWILP4pJFCDs4bwZ9SSraF98EyWNIfR5C6b2zf0l9M9Ut7YzNTk+Zxw4iJ7tsKHhY9+whtr9TwY//fBuarWfhlZ3jIRSNEfWkqnbfy91fmkdmnpvXnznK7neq8fjtXPXBSdz6iVlnSdsKdIcNoQjikQTSTHZ1UbAYGUzwN9MYdbGt8fxzWqe+fyF7n3yb6p0l5xF8Agls+/Or2L1O8qaNpuFYDZpDZ803Pkx2lhebTcMIJgm3Btn31h72vbXnjKO89fLLvPXyy3zk83/PjR+5t+8naUFBup90j4vNJZX81/Ov84G5U3n/7Clovay6trj0sa7cFpc8Qs1GeL+ATKxChh6G2AakWgS2GakWbudASthRVo0pJcFojHjSQNdU7JqGTVMGXABKEkgkiTj84vtvsWhFMemZLqrLWwFoqA8BEOqIU13eSjiRwOXS0KWNVEfd7qJPURWu+/BUrvvw1AsYhcCm21AUhVg4RiKe7Cz6sOYKDXcCegS/HuWJ41NJmOePziqaijPNQ6wj0qvj3/GbL+DPyyCZNHji87/AiCXIzfGhdqb/NYeN2//9PgKaE4kkotQgUNEai/nT93/E7MVLWX3jLRRNmHBR52mRwqnbWDl5DAdrGnhsy14O1NTziZULyOqjhZTFpYUl+CwuG4RtJvgnQ3Q9MvJUqluHbTqoZ5/oLARMyM2kqrmNYDRGLBnGlCClREqwaQqaomDT1E4hmBKDuqahayrKRU6gNkUCkCQj0NocYdPGMjZtLDtju3XPHWbdc4e564E53PThGTikDQWBFGAIA1VefCpW1U61Y0vEDKuCd9gjmeBv5nhHGgdbe5feM+JJws0dZI4/TzSw8+/uy00jkTAIhWKpz7qqdIk9SAnIyVfOJkf30KEcQ+IlP3kt7dVx/sSPyCko5Io1V/f5DC3ORAjB1IJscvwetpRU8m9PreeeZXNZNPbCp7NYXFpYgs/iskIIHZzXg/0KZPhxiG2EZClSn4tQ0nvcZ87oPOaMThmcxpMGoXicUDRBOB4nFEsQisUJRmO0R2O0haOYUmKaElNKNFXBpipoqopdVdE1DbstJQ51TUNTzp1uOdllw+m0880fXXfG+taWCP/77ddZuKyI9906FZfvlMhUpEBIQYQkLhSUfojIKerJYo6oVcE7zMlzBdEUk9drxvDeaGysI4Lde2anht2PvYk0TArmju/Va8RiSaLRBB6PA1VTMBNnbiORhJQyTOLkJa/CIbNop7ovp2RxAWR4XFw1fTzbj9fw8/XvcmByPR9aPAunbnXSuVyxBJ/FZYlQ0hGeT57q1hHbiFTzwTYLIRxn3S8l1JykuXpua2SYspsQPP2nIxKjORTGMM0uQSiEQFdVVFXpShXrmordlvrd1FKmtDabzsprz7wJn6zSzS/ys3TNWGpqWlDUUzd3BYGGQkTEcUud/kjDCkXgcDuIRuIYcauCdziiCpMxvhb2NOVQHT6zGGLf0+/QeLSGnKlFuDJ9JKNxanaVUneggozxeUy69vyeeCYQjSbw+12kpbnP+smSWj0JIuQYK3HJAgCy8wt4dPPuizjDi6Omooy3XnqB3Zvfpa66ikQ8Rk7BKK5YczXX33U3Duepjj3PPfQndrz1BjXlZQTb2/D4/OSPLua6Oz/MwlVrhuwceoNNVbli/CjKG1vYcLCEo3WNfHL1QsZk9fxwa3FpYwk+i8saYZsMvm+mBF/ksVSaV5sK2niEuPDJzqoi8DrseM9iiyAlRBLJbkIwHIsTjMUJRuN0RKPEkwZmZ8pY9zTh9Bt0hEOoikBTUtFCTU39bspTc/SMpAEyVbV3OmmmkyYlTEQkcEob/TP3TuBw6sSFVcE7HBntbSVuqLxZW9zj+uwpRbRVNVL65j5iwQhCUfDlpjHrjhVMuX5BV9u0HpGQNE1MIOB3EUg7e/9bt96BVAyyjJV4zbEXd1L9yOvPPc0rjz/KvOWrWPa+69E0jf3bt/Lor37Gu+vX8h+//wu6I/XgV3JgH1l5+cxesgyfP0CwvZ1NG9by3//v77njk5/htvs/NcRnc35GZ6aR4XGx6Vgl//7MBj64cAbXTJ9oRecvM4SU8sxSPguLyxBpdiAjT0J0HQgNbHMRas8mywM2BgkJw+gSg5WRA7SZJ1ATWcQSSaLxJHHDACkxO7dXFYEiBMmESSQcw+OyoyqCr9zzDNFIkl8/eSdRkaBViWCTKrrs3+e8ZDxJLBJHCKwK3mGAXU2yILua12vGdPPd6xckxBIGhilRk0mKxud29dH9wz0/JB6J86nHvgaAUNsIK1WkG3OZpq3s33FcJCUH95M3qgiXx9tt+SO/+hlP/eG3fOwf/5n3ffBDZ93fSCb553s/RH1NFX9Y/9aIiXCbpsm+qjqON7Qwt7iAj6+YT5q752yFxaWHdXW2sOhEKF6E+95Tad74O0glG/TZCOE6/wH6YwzitLSx20lHuw0Rd5GTntG1jWlK4oZBPGEQTyaJJZPEkwaNLUGiQhAzDaQBhkyljZujERQhUDWViC2BYUpsKAiREoqpZ/y+P+lruoZQRGcxR2cFrzU5fMgY72umJeZkS31Bn48hTcmhl7dx7NVdBBvbcHhdFC2azKSbF6PZddx2lUQyiSnp2TJZCaLYGmhszyFTmTTs7jTjpkzrcfmSq67lqT/8lsqSY+fcX9U00rOyqSw5SjKZRB8hgk9RFGYW5ZEb8LK1tIp/e3IdH18xn9mj+7cNpcXwZJh9DS0shh6hjQHv11OCL/woRNcitQmgTUaIwb2wx8wI6ns6hCiKwKFoOGwacCp1LEIGbz9XQjiYwJSScHucZMLgtUcOY0pJWrabWdfnE1OSmEmV0x1bhACBSIlABEKQEoPiZMOrcws4VVNxeBxdti2qrl10hbLFheOzRUl3RHi6bDJxs++X9+1/eZXDr2xn1IKJTLlhAa3VTRxeu52msjpu+ObdCCnpoT4jhRJG1U9gxgs40Z7BlMDI+Rw01dcB4D/tAeskwbY2TNOgvbWVTa+uZfemt5k2bwG6feR1tcj2ebhq2ni2Hq/if15+i/fNmMjtC6eja5YkuJSx/roWFj0ghAD7UrDNgeizyOhLYJQjbbMR6uA9DceNCJro3Q0lFjPY8Mox9u+u7bb8od9sB2D2vHzuuG0m5UYrcc3AK3UkqUKTk9FAQ5qp302T0yd7CFLvyUkBKIRA0Bkh7BSLXe3YwrHOYg6tWwGJxUAjmRBooiLoZ19zTp+P0lrVwOG1KbG34su3YEpJLG7gzPCz68FXKX/3IMVXTOl8yffOCJKoeg1mIgcjPBOo6fM4BhvTMHjy/36Dqmosu/b6M9Z/6YM30dHWCoCqaixcfRUP/NPXBnmU/YfdprF0wmhK6pt5YfchDtc28MnVCylI670pvcXIwhJ8FhbnQCgucN0F+jJk+GGIb0UqAbDNQygDa2ZqSkncjOGwnX1S/OnE4wn+6yc34OvBbuN0ClQf5UYrYZHALxw95uSkBLNL/IHBKSGYlBJpmikvwtP2UTpFIA4N4oJEwkBFoKqqZdE8COQ4Q9hVg9dqxiAv4h0ve+cgSJh83XwMQxJPGthUhVnXz2ffY29wbONeihdPAQkH1u4g3NwBQLg1iJFM8PZvD2DG2/FlB2Fx7+xdhgN//NH3ObJ3N3f93RfIH118xvq//6//IRGL0dxQz6ZX15GIRYmEw/jSRm7FqxCC8TkZZHndbCqp5FtPv8qHrpjFqsljLc++SxBL8FlY9AKhFYL3K5DYlmrTFluH1MaCNg0hBuZrlJBRJBJNnN83yzQliaSJ3XH+bXVU8hUvVWY7IZnA3cPxhQBVdHbwPfWfU0g6I4ISg5QQ7Pq3KZE2BVOkClAS0uhKFUNntPCMH+vmcjEomIz1tXCgJZuKYOCijtVccgIhBP7i3K7OMh6njhCQUZxDQ8mJrr/W/pe3U73neLf93/zNmwAUzpzOuBEi+B791c945bFHWHPzbdxy3/09bjN1zryu31ffeDM//sb/418/cS///chTeHwjuw+w3+Xgqmnj2FV+gt9v3MaB6nruWTb3rG4DFiMTS/BZWPQSIQToC8A2E6IvISPPQrISaZsJav+72MeNGFJKKkra+dWP1rF/7wnqaoMkkwb5BX5WrRnPJz63mOwcL4m4gTRN1F72zXQLnSzFRZ0ZQpMK9gudm9g5x09BYAN476T1TkEYi8YJBSNIIVDtGmZnszcTME5tCph0xgfP+AH6xTT6UqbI24YhFTbWFF/0scKtQXSvE1MIHLqGy27rqsFxpXupO1yFkTRoq23Gk+EjfXQWwcY2zKSJNzubsQsXMP+Dt+DJSGdbsPKixzPQPPbbX/LkH37Lqvd/gE/887/0er+VN9zEO+teZsvr67nyplsHcISDg6oozBtTQF7Ay6aSCkobmnlg5QKm5A+uU4HFwGEJPguLC0QIOzhvBn1JKtoX3wTJY0h9HkLpv/kvcTOKxKS+Nkx9fZBrrp9Mbp4PVVM4fLCeR/6yg+ef3s9zGz6J3a5iStAuoFowXbiICoM2GUXDgdoH38Gz0ikInS47Nk2loyWEDCVwuvSuYg4TOsWfRJKymTE7/22eth5AniYIOw+fmkvY+e/LWRDqSpJCdztv1Y6mJX6RFhsSkrFEqqeu3YbL3v0WcdKfz4gnibQECTW3M3F1Ed5sLyTG0VBay54XXuHQa29yz6/+F4Z5U4fHfvtLHv/dr1h5w0186uvfvKCHtngsCkCwrX2ghjck5Kf5SHM72Xyskv96fiM3zZ3CTXOmovXyYdJi+GIJPguLPiLUbIT3C8jEKmToYYhtQKpFYJuRauF2kSRkDIDlK8azfOWZqbGFi4v4/ANP8MQju/ngh2YCoGkXJnxyFQ9xw6BNxgjgZCB8WDVdw5fhIdgSIhaKoTt1VFXh5O1D7RbLO4XZ9X/ZJQTlaf82OF0Qpn47eczLKW08ztdCe8LBpvrCizuQhFjCRLXZMGLhM8QepIQegGa3kT2lkPm3FaDZJUZwAdJIzWUrnDGN5//9++xb+yrKDQsvbkwDyOO/+xWP/+5XLL/u/Xz6G99C6aHNYTQSBgkOV3dbJtMweOXxRwGYMH3moIx3MHHqNlZOGcOhEw08sXUfB2saeGDlAnL8Aztv2WJgsQSfhcVFImwzwT8ZouuRkadS3Tps00Edc1Fp3rgZRRXqWR1RCgpT0cT2tgixhNHpmn9hr6cgKFC9lButdMgofhz904jjPaiaijfdS7AtRCwSx2a3YbOdOxqpdP3/9LjeKWTXj3xPZPDySRt7bDEynWGeL59I1Oh7OE1KiCcMJBJPhpcTJ5owEskuU+WThJs7cPhcqDYVX3YIoWoYoVNiD8CXk0oBxoJBhqul7yuPPcJjv/0lmbl5zFh4BW+/8mK39f70DGYuWkxtZQXf/PT9XHHlVeSNLsbj89PcUM87a1+ipryMlTfcxJQ5529DNxIRQjAlP5scn4fNJZV86+n13LN0LovG9f/0FYvBwRJ8Fhb9gBA6OK8H+xXI8OMQ2wjJUqQ+F6H0rYovbsRQTiuWiEWThEJxYrEkxw438P3vvArAqqsmpLzv+hies6GSp/ioMtsIyTjufohO9oSiCnxpHkLtYaKhGNI0sdltfZZZp8Sa6HyXeo4SXrppY8kEfzPVIS+7m3P7fhQJsXgSIQRel53sCfnU7DlO/dEa8qYWIU3Jvhe2cOCV7bSfaEa1qWz764ssuncy4cYF2NQ04pF2jHicpvJK3vjdHwEYs3A+ted+6SGj5OB+ABprT/CLb33jjPVT585n5qLFpGfnsOK6Gzi4aydbNm4gGgrj8ngonjSZWz/+yR7tWy410j0urp4+nm3Hq/n5+nfZX13HhxbPxqUP83y9xRlYrdUsLAYAmTiU6taRPAJqPthmIYTjgo5xoG0L7clmchxFAPz591v51ldf7lpfWBTg7/95FR+4fQZHj9YSjSRIT+97yqVFRqg1g3iEHcdAGkxLiISiRNojKKqCzakzVLODzpc27i4IUwyXtHGmI8SkQBN/PTaD0o6+PVSc9NhTFQWPS0dTBM3l9TzxD7+heOFkrv6n23nn96+w/8WtpI/Oprm8nqJ5o6naVUHG2EJu/Ld/p+zdt9nw8990HdOXm82y+z7ClDWr2Bas5MrABCa7+u4LaDF8KG9sYU9FLQVpPj6xeiHjss80qLYYvlgRPguLAUDYJoPvmxDbiIw8lkrzalNBG4/oZXFEzIh0s2S5+rpJjBufQSgU58DeWl595QjNzeHUtrFkryt0z0aacBIVSVo7izi0/iziOB0BTo8DVVVTKd5wDLvTPiDzB89Hb9PGZwrBoU0bCyTjfC0cbsugtCOtT8cwTEk8YaCqCr7TimnSR2cz9X3zOfDSNl745oPU7C3Dl5tGS1Uj+TMKue1/VrPpTzW8+39rKXlnE5OWXUF6USHxSJT6Y6WUvLuZyCVWyGCRYnRmGpleN5uOVfLdZ1/jtgUzeN+MiZ3TSSyGO5bgs7AYIIRQwXEl6AuQkSchug6MUqRtLkI9v9VBTEZwKKdmQeXl+8jLT/l9XXP9ZN73/inccu3viYYTLFoxCrf74j2zchQPCcOgXcYI4BjQ9mi604ZP9dLREiQWjqZE3zC7cZwUa2cThDA0aeNCTzsCyes1Y3oc0/noMlTWVLxO2xlzshZ/7Bq82QF2PfEWALFQlBnvn8OKz0wh3JJDwcx5KNprHHvrbebfch3erEwAJiy9gonLF/PQ5/6BRCyGeuMVFzw2i+GN266zeupY9lfV8dA7O9lfXcf9K+aT7hmcfuMWfceqs7awGGCE4kVx34vwfwe08akevbF3kDJ81n0MaZA046jnMF2ePC2HqTNyefAP25Cm2S+2CQqCPNWHikK7jHXrtzsQaLqKP9OLqqlEwzEMwzz/TsMMhZQttQ2BjoIDBRcKHlR8qHg7/+9BwY2KA4GOQCMl1Qwg0fkTRRJFEu/8SSBJ0mlu3SkoNcWgyNPGtsZ8GqO968JyOknDJJY0sNtUvE69xwn4iqow86YryByXj1AEH/3jp7nqH2dixHNoqylG03U82bk0Hj9+xr5ZY8eQPW4su5576YLHZjEyUIRgxqhcFk8oYl9VLf/21Hp2lo+cNnqXK1aEz8JikBDaGPB+PSX4wo9CdC1SmwDa5FQ08DTiZgyJRFXO/RWNRhK0tkaRgKr1z/ObDYUCxUuF2UZQxvEMUBHHSRRVwZvhJdTa+wrekcSFpI1T0cDuaeOT9jOmKXntiaNse/EodTURNK+b/EXlTL99KTaHvStSqJxtJqGEuGFiGCYuXcPVi64s4ZYOHF4n3qw2oh1+WqvGdZ2R7vHRXlOJkUig2rofKxmPE+3o6M3bYzGCyfZ5uHr6eLaWVvOjl9/i2hkTuH3BDOw2S1oMR6y/ioXFICKEAPtSsM2B6LPI6EtglCNtsxFqftd2CTOKlBINGw11QbJyzizGePetMo4camDewkKk5KLn8J2OU9jIUTycMDvQpIJjgNrHnURRBN40D6H2CNFQ9KIreEcSvU0b/+XnO1j35BGuvjqNJbfMYNchgyOv7KCtrJ4lX7uDk5Mg5XvmESqAIgVJw0SaErfdhlPvnaBOxhJodkEi4qalcgJSnr5fKvybiMW6Cb6KXXtoLKtg1MzpF/hOWIxEdE1jyYQiShuaeXH3YQ6faOQTqxcwKj0w1EOzeA+W4LOwGAKE4gLXXaAvQ4YfhvhWpBIA2zyE4jkV4RMa//JPz9BQH2TxsmLyC/3EYkn27T7BC0/vx+3R+ew/LEPp7GzRnwSEg5hI0iwjqCjYBqqI4yQC3D4nqqYQbg9jmhJ9CCt4hxM1x9tY/9QRrlyTybe+P5uHD83nygUqWdnpvPWntXS8e5RxS6d1pX4NwOhMBSelJGoaSAGaXSGimEQMMyUEEShCoAqRSk13tstTEAjFQHcJwi0mzeUTkUb320V7Tapt2uaHHyeQn4sRj1N3tIRDr7+J7nSy8lMfp2LQ3ymLoUAIwbjsDLI6Czq+/fSrfOiK2ayeMtby7BtGWILPwmIIEVoheL8CiW2pNm2xdUhtLHHDk4rOCJUbb53GU3/by1OP7aW5KYQQgoJCP3fdM49PfnYxhjRoaBiY9FmW4iZmJOkYhCIOAAQ43HYUVUmleMMx7E594F93mPPuhnKkhPvvy+HtmrEYnZG2KavnsOmR1zj21j4mLZ2O9p7ooCklwUgCBwrpfic2m0JCmiQlJKVJQsrUv00TozOFLAEhTDLTm3Gmu2gqa6W+I4FuE2iKgoZAEwqqTUdRVQ69/gaR1raUj19OFjNvuJYFd9yKLzuLihHQS9ei//A5HayZNo7dFSf4/cat7K+q497lc/E5L8ySymJgsASfhcUQI4QAfQHYZkL0JWTkWTzGHvyKCkhu+MA0bvjAtLPuX3KsHlW9MEH04P9t48ihBg4fbOBEdTu5eV7+9sK9Z2ynIMhXfZQbrbTJGGkD1InjvegOG0qGl2BzSvTpTh21h9ZXlwulh5pRFPAXj+Ktysyu5ZqukTk6h/rSE2fsY5iSYDSBKgRZaU7sWkok2s8SMzU7RWBSmiiBOiQavtzJIKtpLi/HNWYM0kgJwmQiTizYjn/iRFZ8/Z/QNRVdU7HbtNTvqtrV7s7i8kJVFOYWF5Dr97LleCXHG1t4YOV8phZYXoxDjSX4LCyGCULYwXkz6EtoafwmOeoe7HIfHYzFEGevxozFEmjahRU5/OZnm/D57UycnE2wI3bObTUU8lUvlUYbHTKOd4CLOLpe16biy/R02rbE0R22Cz7PS4VgS5C0NBtbGifyXsXtTvNSe6QKI2mgdr4/ScMkFEmi2QQ5AVevKrgVAboQ6N7mVNlx9UwWLxrDgRc3EHr7Ha6ePZ+4aRKTJjvWr8NMJJi0eAmZXhcd0RgtoQhJw8SUEilBCGhzRNjXUUeHLYHbbsNt13HpOm67jlPXrHTfJUx+mo90t5PNJVV8/4WN3DhnKjfNnYJNvTy/w8MBS/BZWAwzhJrNIfMKZNLNPK2OgLmPmMgkJEYjeyieiMeTOJwXJsIeefaj5Hf24r33gw8TCSfOub2TU0UcESlwnsMupj9RVAVfupdQW5hYJI7UwdbLgoNLBVWYJOMJUDUaIt4z13dWRCZjCVRNJZE0CUUTOHSNrIDzwlruuZtBi8GJqRAJkF0YYN6VV7H91XU89bMfM27mLJpqatj26loy80dz1ao1TJ02PrWvhEgiSSgWJxyPE4rGeaOphCJ7AHdMp7EjRHljG4ZpYpgmUoLdpuKwaTh1Gy67nhKFnYLQZbdd1lHdSwGHbmPF5GIOn2jkyW37OFhTzydWLSTH3/eOQBZ9xxJ8FhbDkGCyDSlGUa6uJiD3kGFuJl3uJEQRUbJT4RMgmTAwTHnBHnwnxd6F4BcOYsKgSYbRULANZPu10xCKwBNwI1SFWDCKKU30QazgNU3JK08cYcNzx2isDeEN2Fm0qojbPjYDh3PgL6FF3hbsDpWmszSvMBJJADS7jVjCIBJL4nLYyPQ7L6x7ibMV7GGomwDBrK7FV33oo/gzsti1cQMle3bh9HiZt+Zq8kfP676/AKeu4dQ1IGXCeyBeyw1TJ7M6PyUKw/EELaEwzcEIzZ3/bwqFaWwP0dARoqalvVMQSgzTRFdVHLqG3abh1lMi0G3Xu35sqmJFCYc5Qggm52eR4/ew+Vgl33xqPR9dOofF44usv90gYwk+C4thhpSScLIdvy0TKTRaxFw6xEQyzXfwywM4qCXIWJLCSyJuIKXsV0uWc5GluIiZJ4s4nINXTHGygldNVfDGBrGC98Gf72Ttk0eYv7yQ6++YTHV5O2ufPEL5sRb++YerB7Q7iENNkO3uwO7zEzlWi5FIdkX0ThJq6cDhdRE3U31xvS6ddK+DC/rTONrB1Q4NY6Etv9sqRVFY9L7rWfS+67stP7qjFNO8sHl6Lt2GS/dTkNbzA0fCMGgNRWgOpQRhUzBCczBMUzBMQ0eQyqY24kmjK0qoKAoOW0oQunRbV9r45I/DZqWNhwtpbidXTR/H9rIafvnqJvZX1/HhxbNx2wdnioiFJfgsLIYdCRknYcawKadapSWFh1r1GtrkNLKMNwiYB4iJdDoS2UjJoM1tEwjyFS/lRhttMkoA54UJi4vE4bajqgrB1jCxUAy7a2AreKuOt7HuqSMsWF7IF7+9rGt5dp6bP/90B5s2lLPkquIBe/1ifxPNURfOojHI7SeoK6khf3JR1/pkPEljeR3ZEwuJxQ0CHjsBzwW22NND4G6B5lGpn14iBMgLFHznw6aqZPk8ZPl6TvlJKWmLxDqjhGGaQqcLwlBn2ri1K0IoJakIoZaKPJ5MFZ8UhC7dShsPJpqqsmjcKCqaWtl46DjH6pr4xKqFjM/JGOqhXRZYgs/CYpgRSXZgYqIrZ1oZREQBFeqd+OV+Ms13yLXto9XvQlEuPEXbV9TOIo4Ko40OGcOHfVAqd09ic9jwZXjoaAkRC3VW8A5QhPOkHcq1t0/stnzV+8fx6G938/b6gRN8Pj2CT4/ywvHpjF0k2Pb0O+x5aUs3wXdgw06SsQSjF04hw+/Ae4FzObFFwNsIbXnQMI4L+kMKBXOQW+EJIQi4HARcDsZkpfe4TTieoDkYTkUJg+FU6jgUobE9RH1HkOr3po01FYfNht2mpoSgrp8qMLHr6JdpodBAUpQRIMPjYvOxSv7j2de4bcF0rps50RLfA4wl+CwshhkRI5iap6acJVIjFNrEDIJiPLG2l8n378PhiFEXKSCSHJzJ0A40chUPJ8x2IjIxaEUcJ1FtKr4MD8GWVDHHQFXwlh5qRiiCcZO7RyB0XaVoXBqlh5r7/TVTSMb6m6jsSONIazYZRYLpV89n39ptvPyjxymaPY6Wqkb2rt1G9sRRLLhmHm7nBf4N1Bh4G6AjE2rPrP49H4JUxG244dJtuNL9FKb3/BAUTxq0hiM0BcO0dInCCA0dqXmEFU2tZ6SNnTYbuk3tKio5PUpopY37htuus3rqWPZX1/Hwu7s4UF3H/SsXkOFxDfXQLlkswWdhMcwIGx1IaXZL6faEIZxsr55JoiPA1TPKKHCXEU56qA/nk5QDPy/GJ+zEFRcNZhhVKuiDVMRxEkVV8Ka7T6vglWj6e62HL46Wpghev95jZXBappOj+xtJJgy0fu79m+PqQFNM3qo5FXVbes/VeLP8HNiwk/Kdx7B7nEy6cg5X3XPNhYs9JQH+eoj44cQU6MtsSMEFz+EbDuiaSrbPQ/ZZ0samKWmPRGkKpQThSWHY1BGmviNIXXuQSDzRFSXsKW3s7qw4dtl13LoNxYpc9YgQgumFueT4vGwtreKbT63nvuXzmFdcMNRDuySxBJ+FxTAjbARRhIrai/61wWCUWCKdLfXjyHeVMTGwm2LfEVpimTRFs+nTjfwCyBRuosLoKuJQBznScbKCV9VUIh0RTFNic9j67azjsSS2s4g5vVMExmL9K/gUYTLK18LBpjxqw6eiVIqiMPuGK5jxvoUEo0k0VZAVcKFrF3i2Ign+Ooi5oXo6yL7dBoQQyEFO6Q4GiiIIuJ0E3M4e10spU2njUJiWzirjk8KwobPauLqlHcMwMeTJtLGGQ1NxnFZYclIMuu06tss8bZzlc3PV9PFsLa3ix6+8zdXTx3PHwpnYbZZE6U+sd9PCYpgRTnag9TJFGgzFcLvtgKAmPIb6SAFjfQcY4zuET2+hPlxAKOkb0PHmKR4qDIP2ISjiAFJ2IF5Hqh1bW5h4OI7u6p8KXt2u0R6J9rguHjcAsNv792Y9yttKwlB5t7b4jHUJwyQUSWC3qWQFXGgX2GEFYabEXtKeEnvmxaXizWGY0h1ohBBdUbxR6YEet4knjVS6uNN6piWUKjBp7AhS3x6irLGFRNLsShurndXGuk07VWl8Wur4ckgb65rKkglFHG9o4ZW9RzlS28gnVi2kKCMw1EO7ZLAEn4XFMCNiBNGU89+IkwmDeDxJWuDUnJek1DnSNpua8BgmBXaQ664gZrioCxeQME+liF95/hC1tan+u60tERIJkz/9bisAublern3/5F6Pd6iLOE5id+koqkKwNdRvFbxpGU6qy9tJxI0z0rotjRG8fnu/Rvd0NUmuu523a8bSEe8eYYonDcLRJE67Rpbf2Qc7GBN8dWCqKbGXvLj+pgNRpXupoGsqOX7PWQ2GTVPSFol2CcIuT8JgiPr2EHVt70kbA06bhq5pnSLQ1tmxxNZVbXwppI2FEIzNTifTmyro+M4zr3LnolmsmTrukhe8g4El+CwshhmhZBs25fxz8MKRGKaU2PQzv8bBhJ/tDavIcVYxKbCL0d4jtMUyaIzmIlF44ZkD7Npe022f3/9iMwCz5+VfkOADsKORp3ioNjsIywSufiriqClvY+OLJezeVENdVTvxuEFuoY/FVxXz/g9PxfGeuWs2u4Yv3UNHa/9U8I6dnM7ebbWUHGpi8szsruXxuEFFSQuTTlvWH4zxNdEac7Kjvrs9SjRhEI0l8ThtZPj6EkWV4GsAIaF6BsTP3qqv1wiBaV56Kd3BQFEEaW4naW4n43r4CJ2eNk4JwZNp4xCNnXMJq5q7p43tNg17Z8XxyTmEp9vQjKSWZj6ngzXTxrG7opY/vLGN/VV13LdiHn7nxT2kXO5Ygs/CYpgRTLZhE+e/sIUjCaQp0XsQfCkEdZFRNEbzKPYeYqxvP2P0Vhoiefzkt7fQ32E4j7CToRg0mCG0firiePWZo7z82EEWrChixXVjUTWFfdtO8Ndf7OCddcf53h/fj93R/fxVm4ov3ZPy6rvICt4rVhfx7EMHeOXxI90E3+vPlxCLGiy5avRFnd/peGxRAo4IL5VNJWGmzsk0JDte2MSh13YRbm7D5XczZel0lt99Jbqjt4U5EjyNoCZSkb1o/6T4hRCDbstyuXChaeNTgjBMY2e18fGGFpLGaWljNdXGzq6p3f0IOyOF9mGWNlYUhTnF+eQGvGw/Xk1ZYwv3r5zP9MLcoR7aiMUSfBYWw4iTXTbS9fNf1CLhGFJy3gnfhtQoaZ9OTaiYSYFd5LnLCRhN1IULiJs9T0zvK5nCRUwkaZcx0nCgiotLMy2+qphbPzYTt/eUuLn29snkFW3nid/v4dVnjnD9nVPP2E9RFXxpHkLtYaLhGKbNxNaHdmyjxga46uYJrHvqKP/7r28xa1EeNZ2dNibPymLJmv4SfJKxgSaqg34ONeeklkjY+KdXOLR+O2MXTGLS7ctorGpg2/ObqCs9wYe+cy+iN2k8dwvo0VQ1brhn77q+IIQYkVW6lwq9TRs3nfQiPM2bsL49SG1rkGgiQbIzbSwgNY/wtLTx6QbVQ5U2zgt4uWr6eLaUVPKDF9/g/bOncPO8qSMqYjlcsASfhcUwIm5GScpkj6bL7yUcjmOzKb1O70UMD7uallEVGsfkwA6KvCW0xwM0RvIw6b+LZ57iJWEYtHW1X+v7scZPzexx+dKrx/DE7/dQWdJ69p0VcPtdKJpCpD2CNCW2PrRj++hn55CV62bD8yXs2lSD12/n6lsmcvvHp/dbW7VMZwhdTfJWzTgkCqaE6pIaDr26nXELJ3PHNz7ctW0gJ411v3mRA2/uY9rKmec+sLMN7EGonwAd/Zt+FgpWhG8Yc3raGM7sZHEybdwUPE0MdgrD+s5q46rm9q4ooSllZ9pYw2HTuqWNT/6uDZAIc9g0lk8q5khtI09t38+hmnoeWLWQvIB3QF7vUsUSfBYWw4iw0YHEOK8HH6QEX1/mpzVG83in9n0UeY8w3r+PMf5DNEZyaYun0x9pXgVBvuqj3GilQ0bx4+j3Io6m+jAA/vTzRCgFOD2OVDu2tjDxcAzdab8gEaqoCtffMZnr77iweY29Pj4mo33NHGnJpjqYhiklwUiC45sPgoTFtyzptv3sa+bx+p/Wse/13ecWfPYOcLVC4xho7X9fMyGUS9KW5XLh9LTx2Sph48lkV1SwqbPauMukuv1k2tjAMCVJ08SmqilRaFM7U8Wn+RHadeya2ue0sRCCSXlZZPs8bCmp5FtPrefuJXNYNnH0sEpFD2cswWdhMYwIJzs6u2ycP8IXCsXQ++hTZaJS1jGFE+HRTPDvYZSnlIC9ibpwIVHj4p3udVTyFS9VZjshmcDdj504DMPk8d/tQlUFy983tnfjcer4VJWOliCxcBS7yz6gPXgvhAJvG4ZUePfEWAxTEowmUIUgWN2AUAR5Ewu7ba/pNrLH5HLiaPXZD6qHwdMMLYXQ1H/zDE9HCKyijUscXdPI9XvJ9fccSTNMk7ZwtHMe4SlB2NgpCGta24kmkl3VxifTxnabhlO3nREldOq2834v09xO1kwbz47yan792mYO1tRx95I5uO0DbzY/0rEEn4XFMCJipARfbyJ8oVCsxwrdCyFmuNjXfAXVobFMDuyg0FNKMOGjIZKP0UdD3pO4hU6W4qbODKJJBXs/deL4w39v4fCeBu7+7DwKinvfQ1jTVXwZXoItwQHvwdtbbEqSPHcbW2qLaQw7CEUSaDZBTsBFuKUDp9eF1oOo92b4qD5UiZFIor53vRZNtUxrz4H68QyUR06qaMOaw3c5oyoK6R4X6R4X43POXC+lJBiLd1nPnCwsaQ6dMqmubGrrKiw5mTZ2aBqO07qWuE4Th5qqoqkKC8eOorKplTcOl3GsvplPrFzAhNyep4BYpLAEn4XFMCLc6cGnnKfYwTRNItFEp+nyxdMSy2ZT3TUUekqZ4N/DGN8hmqI5tMQyuRjBkC6cREWSNhlF64cijr/+YgcvPXqQq2+dyK0fP8/8tR5QNQVvhpdQa4hYJI7NbjtrJ43BoNjXTEfCwbs1hQQjCRy6RlbAiaoIErFEj2IPQOsU+olYorvgU+Pgq4dgBtROYiANEYUQmNKK8FmcHSEEXocdr8PO6MxAj9vEEsnT5g+emkfY2HGyuKQZwzRImin7GdvJamObhku3UZjup6Suie8+9zq3zp/G9bMmoV4CnoQDgSX4LCyGEREj2KsuG5FIAtM0z2HJcuFIFCqD46kNj2K8fy+jPUfw683URQqIJHuuBOwNuYqHeD8UcTz66508/vvdXHnTBD71tSXn3+EsKIrAe7KCNxRDmn2r4L1Y3LYY6c4wL5ZOpDkELoeNTP+p98dmtxGKhHrcNxlPdm3ThdLZMi3igxNTQQ6wkFUE0orwWVwkdptGXsB71gKMpGGmTKrfU1jS1Flt3NgRxu3QSZoGT2/fz8xRuYzOTBvksxgZWILPwmIYEUp2oPZC8IXDKdNlXe//m3rCtHOwZT7VwbFMTttBgbuMcNJDfTifpLzweTIKggLVe1FFHI/+eid/+80uVr1/PH/3L0svfpK2ALfPhaqqhDvCmKZE70MFb9+RjPE3UdXhYUt1Jl6XTrrX0a3i2pPupbGygWQieUakr6OpHafPdSq6Jwzw10LcBTXTwRz4S7siUvMpLSwGEk1VyPC4yPD0PLf4VNo4TFs4arViOwdW3NPCYhiR6rLRuwpdaYJtAJuLtyfS2VK/hl2NSwEo9h0hw1ELXPhN3oZKnuLDQBKS8Qva92+/2cXffrOLlTeM47P/tqzfrFAQ4PDY8aR5kFISC8cYLFu5NEcYXYmz/vhofG4nGT7HGfY6eRMKkKbkxJGqbsuT8QT1x2vJG5/fueRkf1w9ZaxsDNLkdWsOn8Uw4GTaeHRmGjOL8qyK3XNgRfgsLIYRoWQbei8EXyQSR1VF/4mfsyKoCY+hPlLAWN8BxvgO4dNbqA8XEEpeWMcGt7CRrbipNYOoUsXRiyKOl/52kEd/vZPMXDczF+bz5sul3dYH0h3MuuLiLEd0hw1femcxRziK3akPqMGsRFLkaeRwUzptMpeAp+e/95RlM3jnsTfZ+uwmRk0r7lq+a+12ErEE01bOAmRK7EklJfYS/WukfS6EEEhrDp+FxYjBEnwWFsMEU5pEjCBu7fyVp+FIvM/twvpCUuocaZtNTXgMk/w7yXVXEDNc1IULSJi9LxxJE05iIklLZxGHdp4ijmP7GwForA3x039784z10+blXrTgg84K3kwvHc1BouE49gGq4DWBHEcLALtbJ+N1nv29yy7OYd71C9n+wmae+O5fGTd/Ik2VqU4bRdOLmbZyemd/XBOqZ0K87/Ms+4JVpWthMbKwBJ+FxTAhZoQxZLL3lixDUF0aTPjZ3riSHGcVkwK7GO09Qlssg8ZoLrKXM0SyO4s42mWMAI5z+m59/lvL+fy3lvfX8M+Jor6ngle3YevHOZImYCTjjPK1caBlDDEC593nqgeuw58dYNfabZRsO4LT52Le+xex4sOrEb6WVFVuzTSI9N6epr8QirB8+CwsRhCW4LOwGCaEjd6bLgeDMfQhsxMR1EVG0RjNo9h7iLG+/YzRW2mI5NGRCHC+igwFQZ7qo8Jo7RJ9g14iexa6Kng7IsSCUaTsnwpeU0piCZPJGS0kcbK/bXzvxqMqLLplKYtuWdp9hasF9EjKeiV0ZtuswUAIkFYvXQuLEYMl+CwshglhI4iJed45fFJKwuEYaWmDm8J7L4bUKGmfTk2omEmBXeS5ywkYTdSFC4ib555LZkMhX/FSYbYRlHE8Yhi55Atwe52oqkK4vXcVvKYpeeWJI2x47hiNtSG8ATuLVhVx28dmYHOoxOIGPnuSfG+Ud+unEr2YwgpHOzg7oH4ctOf2/TgXixBIK8JnYTFisKp0LSyGCRGjA4mJTZxb8EWjCQzDHBBLlr4QMTzsalrG1voriRlOirwlZDurUDDOuZ9T2MhRPERJEpXJQRptLxHgcNvxpnlA0lnBe/Zo1oM/38lDv9hJQbGfe74wj4Uri1j75BH++2tvEIkmsWkK07JbaYl7OdRaeNbjnBd7ENwt0FSUaps2hChCYJoSrCCfhcWIwIrwWVgME8LJDmxCP6+tQCQSR0r61XS5P2iM5vFO7fso8h5hvH8fY/yHaIzk0hZP52w524BwEBNJmmUEFQXbRXbi6G9sDhu+DA8dLaFUOzaXfoaLf9XxNtY9dYQFywv54reXdS3PyHHx0M93suvtam6/JQ+XLc7bVdMx+mqIbIuApwla86FxDEOeBxcCZCq6qajDJCdvYWFxVobX1dXC4jKmt102wuF4Ks04hC3BzoaJSlnHFN48cQOVwXFkOusY7T2KQw2fdZ8sxY0bGx3y3FG0oUK1qfgyPGi6jVg4TjLZPXL57oZypIRrb58IpAJe8aTJkmuL0R0qmzdUMMrTSFUok/JgVt8GocU6++NmQd0EhlzsASe1uWXNYmExMrAEn4XFMKH3XTbiKIoYENuQ/iJmuNjXfAWb6q4imPBR6Ckl11WBKs5M3SoI8lUfGgptMjYsU4SKquBNd2N36sSjCRLxU6Kv9FAzQhGMm5zRKfYMDEMS8DsYMyGd0kONKEKytX4ifRJqagJ8dRAOQO0UhstlWwgFiUyldS0sLIY9w+PKYWFhQcjonelyOBJD00bGV7clls2mumvY23wFmpJkjO8QafYG3qvqNBTyVS8g6bjAThyDhVAEnoAbl8dBMp4gFksggZamCF6/jqqrxBIGpglulw2HrpKR7aCtJc6+hlyaYhdmVA2k+uP6aiHqTdmvDHR/3Avg5MwDq1LXwmJkMDLuGhYWlwGptmrnt2QJhwfXdPlikShUBsfz5okbON4xmTR7A8XeIzi1YLftnNjIUbzESRKRiSEa7XkQ4PQ5cQfcmEmDWCROPJZEs6UqcaUEr8uG3Za6tPqdqUjg1pqiPryWkYrsJZyd/XHPH/0dTETXHD4rpWthMRIYXrO+LSwuUwxpEEmG8DnO76kWDMaGXcFGb0iYdg62zKc6OJbJaTsocJcRTnqoD+eTlCmbEr+wExMummQYDQVbL9qvDQV2l46iCoKtYWw2lUg4hhDgdelonQUMdiWBMGIAJFUPF3Ymnf1xTS3VMi3Z+24m0jTZuv4Vdr6+gbbGRlxeL1MWLGL5Lbeh28//QNFbThYXWRE+C4uRgRXhs7AYBkS7PPjOd0NOefANx4KN3tKeSGdL/Rp2NabMhIt9R8hw1JLqRQFZiguP0IdtEcdJbHYbbr8Lf4aDYHsMXYgusQcwytNA9YkkLp8L1XYhAl2Crx6k6OyP67qgca1/5EFefeQhMvMLuObue5i8YCHbXl3L4z/+n371zROKACktwWdhMUIYeWECC4tLkHAyiJTnN12Oxw0SCWNERvi6I6gJj6E+UsBY3wHG+A7h01uoDxcQSvrIV7yUG220ySgBnJzHqWZISCQMIrEkE6ZncWBHPQe21TD7ikJsDg2vLYxuhjl4KETh1OILOKpMVeMqRiqNG/Ne0JgaqqvY9uo6Js1bwK2f/WLX8kBmNuse/jMHtmxi2hVLLuiY5xmtldK1sBghWBE+C4thQNjowMQ47xy+cKTTkmXEC74USalzpG02b9deR2Mkj1x3BYWeUhxKorOIQ9AxDCt343GDcDiOy6Vzw53TEQI2vFROuCNCNBSlyN3Ir/7SQTyaZNrKWb0/sLsZbDGonQzhtAse14HN74KULLj62m7LZ69chU23s+/dty/4mGdDKFZK18JiJHFp3DUsLEY4ESNVwHA+H77ISQ++YdJlo78IJvxsb1xJjrOKSYFdjPYeoS2WQTKSRrUZJCITOHthWTMYRKMJorEEPo+D9HQPWVlePvCRGTz9l70IIVi0wM26+lb+/FAdRdOLmbZyRu8O7GwFewjqJkIf/fpOHC9FCEHemHHdlms2neyiIk4cL+3TcXtCCJGK8BmW4LOwGAlYgs/CYhgQMXrXZSPcWRygqpeW4EshqIuMojGaR7H3EGN9+5mpt+IMp3EkJlGlgj6URRwy1eUknjBI87vxB071C/7Mvywjp9DLi4/s51fvVOL26UxdNYur778OofQikeLoAFcbNIyFtvw+DzHY2oLT60WznSmOvYE0qo8dxUgmUbWLv/R3VelaxssWFiMCS/BZWAwDwslg70yXIwk0TR2Wc9r6C0NqlLRPpyZUzKTALia6y0m3w75QAMP0ow7FyUtobgrxtfteoOFEiA98dAZf+OaKrtWqqnDHA3P43Kdy8SgdPH9iFXt3NtLWFESz29DOVWSjh1Kp3OZR0NwH+5bTSMTjaFrPnyPNpnduE+sfwWeldC0sRhTWHD4Li2FAKNmOpvSmy8bIMV2+WCKGh11Ny9hafyU20898Xy05zioUjPPv3I9IE4KhKI//fg/BtrObQtuIk6Y2cSwxBeHLZtKCcTjcdhqqmohFzrKfFgVvI7TlQsM4LrZlmk3XSSZ79jBMJuKd2/Te4uVcdMo9TMOK8FlYjAQujzuHhcUwJ5hs7YUlC4RCMWwXZPEx8mmM5rGp9jqOtizEq8WZ5DtGpt7KYFRymKakIxil7EgL6544zL1fWnjWbXO0E4RMLyXxyQDYXXYmzh+LP9NL84kWwh2R7juo8ZT9Skcm1E2iP/rjegJpRDo6SCbOFH0drS04Pd5+ie7BSVsWkMPYOsfCwuIUluCzsBgGhI32XrVVS5kuX4rz986NiUptcAZvnLienR255Djqmeotx6VGzr9zHzEMk46OKAL4y4+2sWBFEcuvHdvjtk4RwqN0cCg+gySnIrWaTWP87DFkFWXS1tBBe1NndxElkTJWjvhT/XFl/1yK88aMRUrJieMl3ZYnE3HqKyrIKx7TL68DJ4s2LB8+C4uRgiX4LCyGmKSZIGZEzmvJYhgmsXjikrFk6QsemcmB5sX8vmYmDXEXkzyVjHHVoIlkv75OMmF0dTR5+6Uyqo63dpuz1x1JrlZDk5FFVbL4jLVCEYyeUsioSfmE2yO0NDSDvxZi7lR/XLP//p5TFl4BQrB13Svdlu/a+DqJeIxpi/vPg4/Oog1L8FlYjAwu3zuHhcUwIeXBd37T5XA4jjTliO6y0R8UK2nsixfyy2o/a/xtLEo7ynTf/2/vzgPbOs8z0T8HKwGS4E6CO0WRFDftCyWukqzF8RrbsS3LUuI4q+stnXvnTjK3aZYm0yaZ9k6d9E47nWmmdVIn6U2a2o2zeIt3x7Z2S7YWkuAq7iQI4Bzg4Cz3D0qKFYogJQE4APj8/rMInPOSMsWH33e+9+3F+WAeRkM5AAT4vCH87B+O4e3f9mNyTITDaUH5yhzse2g9Gte7I14/LKsQJRmONCvCQR1PfvcdHHh0M9xlLowMzs57fZZpBlYhjJPyOugRfocuqiqA1WHGZOAk/BMWOGbqYeHeENMAADXOSURBVL4wUi5aCsvKsXHnLhx64Tn89Hv/DSvXrMXk8DDefeE3qFhVj6aW6AW+i4eP2XiZKDkw8BEZTFIvTNkQIq/w/b4H3/L+thUEoMFcAEkP4yWvGX2BLmzKOYPGzD7k27x4pzsd/9enXkZQDGPn7XUoqXRB9IfRd3YKU2NixGuHQgqCwTDSnXbk5Wfgi598GsXlLtz94JWbJwvQUGgewVC4ApNqUcRr67oGa94EcjPy0fPvLgyNjaCqsRx2Z3QOUVy0676DyMorwNGXX0T38aNwZGRi4w270fnRjy2tRcxSCXPX4gofUXJY3j85iBKAdGGOrnWRFb6AFIKuY9kd2rgSi2BCs6UI7ypDGFBlhCdX46y/HC25p/DXX34RghbGEz++GZn5WUu7oA5IwTBkWUGWy4HsHCee+/lpHHptAP/Pj+5YsK1KvnkMGkx4X448TUPXdfi1XujQUO7ajZJbM/Dm0++i50Q/yleVICM7/Wq/BAsymUxoufEmtNx4U9SueSWCgLk+fAx8REmBz/ARGUxU/BAgwCxEDnKSKMNiMaV0D76r4RSsaDIXQtZVTOsSJuVs/O1vivDuu7N44FMr0FkzhkLzGGTpym1KLtGBgChDlhXk5qQjO8cJOaTib7/5OrZsr0RuvhNDnhkMeWYwOuQDAAR8IYx4JmD2jaA7vAoBfeGZt7quQ9T6oekhFNq6kGYqRHqWE133bENpjRv9HwxhamQmil+Z+Lg0aYNbukRJgUsFRAYTVR8sS5qyIS+bHnxLlWdyosachzPqBGy6Gd2/Ow0A8Ng34u4HXsHRNwagqkBpZQbu+vQGdN10+cgxXQMCYgiapiE/LxPpGXPP1MkhBTNTEn73Uh9+91LfvPs+//MzeP7nZ/Cf/nM9qg82RKxR0oYR1n0otLXDaS699OdWuxXbbtuMYy+/h7OHeiBLIbhXFCIa7VniYe5/V57SJUoWDHxEBpMU35KaLgfEEKzL/MDGlZSbsuDTQziv+TAxOAYAeOavfo6c0nzc/aWPosI2iH998hSe+PIrEFQZnbfOBTRN0xHwhwABKCxwIc3x+7+DNIcFf/q9vfPu5Z0K4q//9GVs7SrFA/dlQahuRRgLb8UHtTHI+hTyrFuQYZ7f0sVkFrB+x2pk5mTg6IvvISSFUV5fAlM0n7WLGQGCILDxMlGSYOAjMlhAnYVFWPy0ZiAQgo3P780jCMAqcz5EPQxfYO5Qhs1hx/3/9XMwWy0AdHx+iwdfvefv8c9/cwj7PpaLYTEP/kAYZpMJBYWueb0NLVYzuj5SM+9eF0/p1laZ0X5jPV6WNi9YV0ibQlAbQbZlLVzm+gifAFCzfgWcLifefvYweo73obKxHNYkOZzDxstEySEZfo0kSml+xbvolA1d0yGK8rI/obuQuUMchbDa51bpGnesuxD2AEDAhGUFVrSsxvh4GBP946hP70Zxhogi9/ywtxQmqDgVoQ1LWJuFqA0i01yPHMu6RbfrAaBkZRG67m2F3WFHzzEPgmLoquuKN8EkcEuXKEkw8BEZSNd1iMriUzakoAxN0xj4InAIVpQVXmiNkjP/6+nMnTux+0/vVqNXLEZj4SxqHL2wC8El36O4LAN9/Tfg4a/uxJh65X5+ih6AX/Mgw7wC+dYtSwp7F+UUZmHHvlbkFmej93gffFP+Jb/XCNzSJUoeDHxEBgrrMsJaaNGWLKIoQ9d1WJfhWLWrUd8wtw07PT4Nny5f9rHpkWkAQLq7FD3OPXgztBOS5kSV9Rzc5kGYoC56/VzzBABcaMMyP8ipugS/2gOnqRQF1jYIwtX/E+vIdKDjY9tQXl+KgdNDmDw/fdXXiBcBArijS5QcGPiIDCQpF6dsRN7SFUUZmgY+w7eI9e3rkeZMw8CLp3A+MI2QPhfiRvrG0f3WKeSU5mNjez1MJhPG1WK8Iu3FidAmOExB1NhOI9s0CeDKCcYMBfnmcfSGa+HT5vf3U3UZPrUbdlM+Cm2dMC3SZicSq82Clls2omFrHcb6JzDcPYKETFYmQGdbFqKkwJ8eRAaSVD80ffGxapIkw2wWYDIlR8sOo6RnpuPuz9+NJ//qSbz4+JOo2rsa1gBw8lfvQFM17P/agcu2WDWY0R2ux5BSgXrbcVRYepFjnsJ5pRRB3XnZtQvNIwhqaTgTbpp3X01X4FfPwSq4UGTbAbNw/dMzTCYBa7oakZGTjiPPn4Dn1CAq6kthMifW7+lsvEyUHBj4iAwkqr65kVtL2NK1WLiduxRdt3YhIysDv/zRL3H8H1+BIAAVzStw+x/fgZUb5p+8BYCg7sTR0Fb0h6vRbD+CSmsPfJoLo0oJVFhgE4JwmWdwPLQZsn75aqyuq/Cr3TALaXDbdsIiOK94j2tVvaYS6S4n3vr3Q+g+5kFVU/mlwylG4zN8RMmDgY/IQKLqh0kwLzplIyCGGPiuwrq29cgpK4fPFIavUkVBRgYqM3MWfd+UVogXprrwF7f9CUYHvfjEJ0rwha+2I9M0i1k1G57w5YFR1zX41B4AAopsO2E1uWLy+RRVFWD7vja8+W/voOeYBxWN5XBkRH4MIB4EQWBbFqIkkVh7A0TLjKj4YBEWX63x+4PX1D5kOQqHVXT3jAEmATdua8KOihpMySImgoElvf+Z7z6Dmam5k7uzWg7yzBOwm0I4Ja+Dht//Hei6joDmgQ4VRbbtsJtyY/L5XJSVn4nt+9qQV5YPz3v9mJ30xfR+SyEIbMtClCwY+IgMJKn+JUzZYA++pQqGwjjXPYq0NBt2dNUjPz8T9dmFaM5xo98/DX84cm+7gZN9+O0/PY+bHr0NADCpFuJlaS9+J3ViRP39WLS5+biDUHUJhbZ2OMxXbtESbWnpdnTe1YLKpnIMnhnG+ODCh0ziglu6REmDgY/IQAHFC6sp8pSNYFCBomo8obuIQCCEnp5x5OSkY+eOBrhcF5+lE9BSUImy9Gx0z04irF25/YqmavjnP/0nNLQ3Yd3uDZf+3KvlYkQtw4fbsAT1EYT1GeRbtyHdXBHDz2o+s9WMlps2oLm9AZPDUxg8O2LYtqoATtogShb8CUJkIL/ihVVYpCWLJEPXYt+DzzvlxdP/+2kcf+s4ZqdnkZWbhfXt63H7J2+HMyO6BxGizTsrYXBwCsXF2di2tWbeaqjZZMKOkho803cSZ70TqM8phOkP+ui9+I/PYbR3BJ9+4qGI9wpq4whp48i1bkKm5cqHQGJOAJraViE924lDvzkOz3v9qGgshzneJ3gFntIlShZc4SMyyJKnbIgyNE2P6Qrf7PQs/ssf/Re89svXsL59PfY/uh/r2tbht0//Ft/54+8gFEzcMV+Tk34MDk5hRVUB2tvqFtz6dlps2FlaCx06+nxTl31sYnAcz37vaXzkoVuQV5q/4L1kbRqSNowsczOyzPPbs8RbVVM5Oj+2FYLJhO6jHsjBcFzvLwgCdG7pEiUFBj4ig8haEIquLKHpcgiCCTFdvXn2h89icnQSD37xQex/bD+6buvC/sf248EvPoiBcwN47l+ei9m9r5WuAyOjXoyOedFQX4ItW6oX/RoVpGWgrWgFvHIIY9LvDz386Ks/QF5ZAXY+sHvB94Y1HwLaADLNdci1bryqkWmxVFCeh+33tiIjy4nu4x6IPjGu99e4pUuUFBj4iAwiqj7oUBftwSdJYVgtZsQyX3xw5APY7DZs2bnlsj/fvGMzrDYrXv/V67G7+TXQdWBwaArT0wGsX1eJtWvLlxzAarMKsCa3GEOBWfjCIbz99Fs4/cb72PeV+2FeYBVV0UUEtF6kmypRYN2aMGHvoszcDOy4rw1FFQXwnByEd3w2LvcVBEBXGfiIkgGf4SMyiKj4LkzZiLzCF48efEpYgcVmmRdkTCYTrHYrxofH4fP6kJmVGdM6lkLVdPT3T0CWVbRsWYnKyoW3YBeyuaACUyERZyZG8bNv/RiNnc1w5WdhvG8MADAzOje/VvJJGPEMQHeNIie7EoW2dghCYrbHsTlsaL9zCw4/fwI9xzwISTIKK/JwpZm/0SIIAjSdW7pEyYCBj8ggkjoX+BZb4QsEQrBZYxsySqpKMPLqCPrP9aOi5venTvvP9V/aIpwanTI88CmKhl7POARBQEd7HYqK5s+0XQqTIGBHSQ1+MjUD/5QfJ18+gZMvn5j3uneeeQvvPPMWbv4/d2Pf5z4O0xJ6JhrJbDFj8951yMzJwIlXTyEkySirK47diqSJbVmIkgUDH5FBxAs9+ExC5CcrAv4Q0jOufzZrJLs+tgtHXj+Cv/va3+Heh+9F6YpSDHuG8aO/+RHMFjNURYUckmNaw2JkWUGvZwJpdgva2uqQk5N+XddLM1uxt7oR576xDxaTCUXOTAgXVsP8Uz78+Os/RG1bFbbctQFrmu+CeZHT1AlDAOpbapCe5cC7vz6G3hP9qGwsgzkGq8QCBJ7SJUoSDHxEBpFU/6JTNsJhBXJYQY4ttm1R6tbU4XNf/hye+u5TeOJLTwCY287tuLkDs1WzOPLqEaQ5jQs8kiTD0zcBl8uB9rY6ZERprFhRZhYe2H8bXhg+C7cjA27n3Gi0icG5rd28ilzsvuUR2EzXtpJopPL6UjgyHXjrmUPoPjo3g9fmiNzz8WoJJnCFjyhJMPARGSSg+GBeJPCJcWjJctGm7ZuwoWMDBnsHERSDcJe74cpx4RsPfQNmsxmFpYUxr+FKfL4gBgYnUVDgQltrLez26G6rVrvyMBUScWhiAE6LDZlWO0RtAADgNJUmZdi7KL8099IM3u7jfaioL0V6VvR+eRAEE9uyECUJBj4ig8xN2VjshK4MTddhjdNYNZPZdNkzfN4pLwbODqBubR3sabHdVr6S6ZkAhodnUF6Wi5aWlTE7vLIxvwxTsoge3ySqXEFklTjwv04/A4e5JCb3i6eMbCe67m3F2784jL5TAyiuLkJOUXZUri0IgKYx8BElA7ZlITJIQPEu2nRZFGVAB6wxPqV7JZqm4aknnoKmabj5wM1xv//Y+CyGhmdQW1OE1m21MT2pLAgCutwrkZsWwJQ8iVxLa0qEvYtsaVa0fnQLajeuxIhnDCOeMURjBq8gCNDYloUoKXCFj8gAmq5BUv1It0TeLpQkGVZrbHvwAUBQCuKbD30T69vXI784H1JAwtsvvI2+M32441N3oH59fWwL+BBdB4bPT8PrlbCmuQxNTaWIZWuRi6bkAZRnCjg1sQonggLaivSE67d3PUxmARtuWI2M7HQcf/kkZFFGWX0JTKZr/71fEASu8BElCQY+IgOEVBGqrizekkWUYbHEfiHeYrGgfGU53n7hbcxMzsCWZsOKVSvwhW99Ac1bmmN+/4t0XUd//ySkYBibN61AdXV8nhscDw1hJOhBa/6N2JpTg7888VucmhlFU447LvePGwGo21SNjGwnfvfsEfQc70dVUxks1/qMqEmAzlO6REmBgY/IAKK6xKbL/hCsMe7BBwAWqwWf/fJnY36fSFRVQ1/fBBRVR2trLUpLcuJy32l5DIPiGazOasP6nO0QBAF3r1iLp7oPI8fmQEl68h7aWEhJjRvb79mGN55+B93HPKhsLEea8+qf0TQJc39vRJT4+AwfkQFE1Q8N2qLP8AUCwbic0DVaOKyiu2cMMAno6lwVt7DnC0/DEziJ2sz12Jr/kUtbuB+tWo22ohX43Xg/fHIwLrXEW447Gzv2tSO7IAu9J/rgnwlc/UX4DB9R0mDgIzKApPqgQ4NVWDjwqaqGYEiBLU4ndI0SDIVxrnsUaWk27OiqR35+fKZ5iIoP3f5jqExvQGfBHTB9aGSaSRDw2YZtqM7MwysjvQhralxqijeny4Gue1pRWluM/vcHMXV++qrez2f4iJIHAx+RAUTFB6tgi3goQJJkaJoWly1dowQCIfT0jCMnJx07dzTA5Yptg+mLQqqEs/4jKEqrws6ie2Axze/t57TY8FhzJ5wWK94Y8UDXU3Mly2q3YNutm1G/pRaj/RM43zOKpZ7gFQQ+w0eULBj4iAywlCkb4oUefKm6wuedleDpm4DbnYWuzno4ojwFYiFhLYQzvkPIsRZij3t/xOcoS5wu/FFjG2bDQZyYPh+X+oxgMgtYu70JG3evgW/KB8+pwSVN0BBMXOEjShYMfEQGWMqUDUmUoWuAzZZ6K3yTk34MDk5hRVUB2tvq4hZqVV3BGd9hOC0u7Ck+AIclY9H3rMsrxX01G3DOO4EB/0zsizSKAKxcV4W2O1qgKhp6jvchLIcjv0UAV/iIkgQDH5EBAurSmi6bLUJK9YLTdWBk1IvRMS8a6kuwZUs1zOb4/DOk6SrO+o7AItiw130ALmvukt97S3kjthfX4N2JAczIUgyrNJ57RSF23NsKe7odPUf7IPkjHFoRBM7SJUoSDHxEBpgbqxa5JYsoyYZM2IgVXQcGh6YwPR3A+nWVWLu2PG5hVtd19PhPQNNV7HbvR6796vrrCYKAT61qQa2rAK+N9EJWlRhVmhiyClzYsa8NuSU58LzXj9kp3xVfZxKEuWcbuchHlPAY+IjiTNVVSEpg8RW+QCim48TiSdV0ePrGIYoyWrasRF1dMeIxPQOYC3se8RQk1Y+dRffC7ai8puukWax4vLkDmVY7Xh/1QEvRQxwXOTLS0HX3NlQ0lGHw9DAmhqbmvUYQBEAHNG7rEiU8Bj6iOAte6sEXeYXP7w+mxIENRdHQ0zMGVdXR0V6Hysr8uN5/SOrGbHgSnYV3oiJ91XVdq9CRiUca2yEqMo5NDkepwsRltprRcssGNLXVY3xoEkPnzs8t1V5kAnTo0HVu6xIlOgY+ojgTFT90PXLTZV3TIUpy0jddlmUF3T1jsFnN2N5Vj6Ki+E6tGA32YSI0iK15N6E2c11UrtmcW4yDtZvQ65+Cxzd/1SvVCIKA5vZ6bN67DgGviN6TA5ee2xOEuR8hbL5MlPgY+IjiTFR90KBGfIYvGApDVbWkPqErSTK6e8aQkWHH9u0NyMlJj+v9J0PnMSz1YH3ODjRnbYvqtfeUrsKukjocnhjCVEiM6rUT1YrVFWi/swXQdXQf8yAcCuPiI5ip2qOQKJUw8BHFmaT6ASBiHz5RDEHXkbRbuj5fEL2eceTnZ2LH9gZkZETevo42rzyBfvEDNLpasCl3V9QPhwiCgAfqNqMxpwivj/QiqEZuX5IqiioLsH1fG5wuB7qPeRAMBC88w8ctXaJEx8BHFGeSuviUDVGUoWk6rEm4wjc9E0D/wCTKSnPR2bEKdnvkfoPR5ldm0BM4geqMNWgtuDVmJ4HtZgsebepAjt2J10d6U/4Qx0WuvExs39eGgvJ8jA/ObWmzFx9R4mPgI4ozUfEv3nRZkmEyCTCbkutbdGx8FkPDM6itKULrttq4nzKWVD/O+Y6hzFGL7YV3wSzE9v75ael4rKkDIU3F4YnBmN4rkaQ57ei4qwU166qQle+COUVOkxOlsuTcLyJKYgFl9oqzWz9MFGVYLMkT9nQdGD4/Da9XwprmMjQ1lSJebVcuktUgzvqOoCCtFLvc98Fqis+otlXZhXigdjP+xwdvIsfuxEpXXlzuazSzxYzNN66f+4/U6Q1OlLIY+IjizK/MLNqSJSCGYLUmx6qJruvo75+EFAxj86YVqK4ujHsNYU3GGf8huKy52OM+ALvZEdf77yypRb9/Gs8OvI8sWxry0+J7QMUwDHpESSN5lhCIUoSozi7adDngD8GaBC1ZVFVDb+84QrKK1tZaQ8KeemFkmt2Ujr3ug0i3uOJegyAIuL9mE1bnluCNUQ8kZXkc4iCi5JH4P1GIUoiihRFSJeTYIo320hEQQ3C54rtKdbXCYRW9nnFYrGZ0tdUhPz8z7jVouoZzvqMwCSbsLT6ALFt8mzp/mM1sxqNN7fjKoV/htZFe7CytgVmI/e/Uwz0e/Ox7fw/PyQ8wPToOVVGQV+LGuq523PKZg8gpLFjwvc/98F/w/a/8BQDgb99+Hq7cnJjXS0TGYOAjiqO5HnyRmy6HZAVhRUvopsvBUBi9veNIT09DR3stXC5n3GvQdR29/veg6DL2Fn8c+faSuNfwh3LsTjzW3Ik/O/wbvDs+gJbCaxvjdjWmRsYwMzaBTbt3IM9dCJPFgoHT5/Dij3+GN3/xa/z5M08hKy933vumR8fxo+98F2npTgQDy6OXINFylrg/UYhSkKRemLIhLPwMnxSQoWt6wvbgCwRC6OufRG5uOtrb6uBwxOdwxIfpuo5+8QME1Fnscu9DiWNF3GtYSI0rH5+u34rvnXwNZ7zjqMtaeIUtGppbt6C5dcu8P6/fvB5PPPZFvPLTZ3DrZz8x7+Pf/+pfoKiiDGW1K/Havz0b0xqJyHiJ+ROFKEVJF+boWiOs8InSXA++RAx83lkJxw+dwbGX3sD0+VH8+egUFEWFu7QArbu24MAjH0O++/enVP/+W0/if37nB1e81qNf/TQOPHL3NdVxPtiLmfAYOgruQFV64zVdI5Y63NXo803h3/pOItvmQKEjI+415JcWAwAC3tl5H3vnNy/i0Auv4Gv/8n08/8N/iXdpRGSAxPuJQpTCRMUPAQLMwsLfeqIkQxAAszmxzlRNTvoxMupFhl2ASVOw/ZY2FJYUwGw2o/v9Xvz8yWfx3L/+Fk/+9r8jtyD7svd+4RufQ3bu5XN069fVXlMdY8EBjAb70ZK3F/WuTdf66cTcvpUbMBDw4o1RD/aU1cFpie1KqBwKIRgQEQ7JGDrXg6e+/V0AwLrt7Ze9TvT58b+/9m3csO9O1KxtZuAjWiYY+IjiSFR9sCwyZUMSZVgsZsRoQMRV03VgdMyLqSk/GupLsGZNCz7+uZvnvW79ttX4z5/6Jn7x1G9w8LF7LvtY102tKKmIdFBlaabkUQxK57AuuxNrszuv+3qxZDGZ8HBjG75y6Fd45XwPdpXWwRLDRtov/fjn+Mevf/vSfxeUleCP/vLPUL95/WWve+o7T0DXNOz7j4/ErBYiSjwMfERxJCm+JTVdtibI5AJdBwaHpuD3B7F+XSXq6txYqPmau3yuJcus13/Fj/t9AaQ50q55+sZseAp9gVOoz9yELXl7YzYyLZpctjQ83tyJrx/+Dd4e78O2wqqY1b1p93aUrKxCUBTRd/I0Dr3wCnzTM5e95vSho3jxqZ/h4b/6BpyZ8T9VTUTGYeAjiqOAOguLEHlrzx8IJsQMXVXT0d8/AVlWsbWlBhUVl0+QCAVlSAEJoZCM3tP9+Juv/y8AQOuuzfOudX/nQxD9IsxmExo3rMKD/8f9V3zdQgLKLHr8x1GV3oiOwtthikO7k2ipyszFZ+q34q/fewWnvWOozy6KyX3yiouQVzx37c27d2DLjTfgT+44CFkK4vaHHoQih/E//+9vorl1C1pvvTEmNRBR4mLgI4ojv+JdfMpGIASnI3Jj5lhTFA29nnEIgoCO9joUFWXNe83TP/gl/usX/99L/11cUYSv/ff/hPXbVl/6s4ysdHz04zdhzZZGZGZloO/cIH78d/+K/3Dfl/EnT/wH3HLfnkVrCaoizvmOwu1YgR1F90R8/jFRbSuqwkBgBj/pOYpsmwNuZ+ybQ1fU16KqcRWe++H/h9sfehC/+cFPMNzjwYEv/TFGPAOXXiddaMkyPjgMyR9AUUVZzGsjovhLvn85iZKUrusQlVlkWRduDqwoKkIhBVlZ8e9rd5EsK+j1TCDNbkFbWx1ycq48JqzzplZU1pZDCgRx+vg5vPrrtzAz5b3sNfd9/s5577vt/r24r/1z+G9/8nfYeWsHnBkLN5gOayGc8R1Grr0Ie9z7F51QksjuqlqDfv803hrrx67SWmRYY/+5yKEQAt65v5OJ4fPQNQ3f+tSjV3ztl+/8OOxOB75//LWY10VE8cfARxQnYV1GWAtFbskiytB141qySJIMT98EXC4H2tvqkJGx8GpkUUkBikrmesx13dSKHbe245O7H0NQCuGBL+xb8H1ZuS7c+cDN+PtvP4nj75zC1h0br/g6RQvjtO8wMizZ2Os+iDRzcs+nNZtM+HxDK0bEX+PVkV7sLq2FxXT9W/cz4xPILpj/S8TJN9/BwJluNLbMfX277roNqzaum/e6537wE5z63SF89i++gvQsPtdHlKoY+IjiRFIuTtlYOEQZ2YPP5wtiYHASBQUutLXWwm6PfLjkD9U2VaNu9Ur89B+eiRj4gLntXwDwTnqv+HFNV3HWfwQ2kx17iw8gw5p9VbUkqgyrHV9Y3YmvHPoV3hztQ7t7xXUf4viHP/1zzIxNoHHbZhSUFkMOhdD73gd48xe/hiPdifu/9McAgMqGOlQ21M17/5GXXgUAbNjZwdFqRCmMgY8oTiTVD02PPFZNEmXoOuJ+Snd6JoDh4RmUl+WipWXlNZ+kDQVDmJ3xLfq6gZ4hAEBu4fyAoes6uv3HAV3HnuL7kWMrvKZaElVZejYeamjDX574LU7NjKIp5/ra1bTeshev/vwXeO3nz8I3NQ0IAvJL3bhh31245TMHkV9SHJ3CiSipMfARxYmo+qDri0zZEEOwWExx7cE3Nj6LsXEf6mqKsGF9FQRT5JtPjk4hr2j+bNZ3Xz2Knvf7sKFtDYC55xGDYhAZrsu3YkeHxvDT7/87snJdWL358ikZuq6jN3ASIU3EbvcBFKaVX+dnl5g2FZTjnup1+Odzh5Btc6A0ff6hmKXaevMebL158cMvC/n8t7+Gz3/7a9f8fiJKDgx8RHEiKj6YBHPEU6aSJMNiiU/LEV0Hhs9Pw+uVsKa5DE1NpViox96Hfes/fhcTo1PY1LEW7rIiyCEZHxw7i+f+9WU4Mxx4/OufBQBIAQl3bPgEOm/ahhW1FcjMnjul+/QPfgUpIOHP/seXkPYHp5EHpbPwKVPYWXgPyp3XNokjWdxe2Yw+3xTeGPNgl7UWmbbIp7eJiK4HAx9RnIiqHxYh8nNxfn8IVmvsvy11XUd//ySkYBibN61AdfXSt0333Lkdz/74efzyJy9gZtILQRDgLivEHZ+4CQceuRvusrlr2dNs2HFLO04e/gCvPPsmxICE7NwsbO5aj4OP3o2mDfWXXXdE8mAydB5tBbdiZeaaqH6+icgkCPhswzaMSD68MtKLPWV1sEbhEAcR0ZUIuq7rRhdBtBy8PPYz9AXeR71r4YbDv3j2KKxWC4rd177FtxhV1dDXNwFF1bF160qUlhj/oP5EaBgD4mlsyt2Fjbk3GF1OXA2Ls/jqoV/BBAGdxdVJMUFkqX7aexwPN7ZjR0mN0aUQLXvJ066eKMkFFC+spoWnbGiaBkkKw2aN3SpPOKyiu2cMMAno6lyVEGFvRh7HgHgazVmt2JCz0+hy4q7E6cLDjW2YDQdxYvq80eUQUYpi4COKE7/ihVVY+DktSQpD07SYtWQJhsI41z2KtDQbdnTVIz/f+J5rvvAMegPvoSZjLbbl35RSq1tXY21eKe6r2YBz3gkM+GeMLoeIUhCf4SOKg4tTNnJtC7fgEKWLTZejv8IXCITQ1z+J3Nx0tLfVweGIPM83HiTFj27/MZQ7V6Gr8C6YhOX9/Not5Y0Y8M/gpeFzyLTZkW1beAIJEdHV4gofURzIWhCKrkRsuiyJMjQNUT+04Z2V4OmbgNudha7O+oQIeyFVwhn/ERSlleOGonthMV1dk+dUJAgCHqzbgrqsArw20gtZVYwuiYhSCAMfURyIqg861EV78JnNAkyL9MG7GpOTfgwOTmFFVQHa2+oMG9n2YWFNxhnfYWRb87HbfT/sZq5kXZRmseLx5g5kWu14fdQDjWfqiChKGPiI4kBUfBembEQeq2aN0oENXQdGRr0YHfOiob4EW7ZUw2w2/ttd1RWc9R2G05KBvcUH4bQY/xxhoil0ZOLRpg6Iioyjk0NGl0NEKcL4nwBEy4CkzgW+yCt8Mszm6w98ug4MDk1hejqA9esqsXZteUIchtB0Ded8R2EWLNjjPgiXdf60DprTlOPGwdpN8Pin4fFNGV0OEaUA4/d3iJYBUfXDYrLCJCz8O5bfH7zuAxuqpqO/fwKyrGJrSw0qKvKu63rRous6evzHoehhfKT4E8izX9/82OVgT+kqeHzTeH7oDFy2NOTanUaXRERJjCt8RHEgLTplQ4coytf1jJ2iaOjpGYOq6uhor0uosNcnvg9J9WNn0b1wO6qMLikpCIKAB+o2ozGnCK+P9CKoho0uiYiSGAMfURwEFB/MEQKfFFSgKOo1Bz5ZVtDdMwab1YztXfUoKordpI6rNSx1wxueQEfhR1GZXr/4G+gSu9mCR5s6kGN34vWRXh7iIKJrxsBHFAdzUzYWfn5PEkPQdVzTlA1JktHdM4aMDDu2b29ATk769ZQaVaPBfoyFBtGSdyPqMjcYXU5Syk9Lx2NNHQhpKg5PDBpdDhElKQY+ojgIKF7YIh3YkGRomn7VK3w+XxC9nnHk52dix/YGZGQsfAo43qZCIxiWurE+pwurs9qMLieprcouxCfrtmAwMIPu2UmjyyGiJMRDG0QxpukaJNWPdMvC26ySKMNkEq6qdcr0TADDwzMoL8tFS8tKWCyJM6nCG56ER3wfja4t2Jy7JyFOCSe7HcU16PdN4xcDp5BlS0N+WuKs5BJR4uMKH1GMBdUAVF1ZtCWLxbL0b8ex8VkMDc+gtqYIrdtqEyrsBRQvevwnUJ3RjLaC2xj2okQQBOyv2Yg1uSV4fbQXksJDHES0dAx8RDEmqf5Fmy4HxNCSQpuuA0PD05iY8GNNcxk2bqyCEMXJHNdLUgM46zuKUsdK7Ci8G+ZlPh832mxmMx5t6kBhWgZeG+mBqmtGl0RESYKBjyjGRNUPDVrEZ/gCgdCiBzZ0fa7Hns8XxOZNK9DUVAYgccKerAVx1ncY+Wml2OW+D1aT8TN7U1G23YHHmzuh6jreHR8wuhwiShIMfEQxJqk+6NBgFRYKfPpc4ItwYENVNfT2jiMkq2htrUV1dWFsir1GihbGGd9huKy52Ou+H2lmNgmOpZWufHy6fivOiz6c8Y4bXQ4RJQEe2iCKMVHxwSrYFnyWTZZVhMML9+ALh1X0esZhsZrR1VaH/PzEmj+r6SrO+g7DbnJij/tgxMMpFD0d7mr0+abwb30nkW1zoNCRYXRJRJTAuMJHFGOLTdmQIrRkCYbCONc9irQ0G3Z01Sdc2NN1DWd9RwHBhD3u+5Ftyze6pGVl38oN2JhfhjfHPAiEZaPLIaIExsBHFGOLTdkQxbnAZ/2DZ/gCgRB6esaRk5OOnTsa4HIl1japruvoCbyHsB7CrqJ9KEgrNbqkZcdiMuHhxjYUO1x4daQHisZDHER0ZQx8RDG2aNNlUYYg4LJTut5ZCZ6+CbjdWejqrIfDkXgHIAbE0wgoXuwovBulzpVGl7NsZdrS8HhzJ8yCCW+P90Hn+DUiugIGPqIYC6heWCO0ZBElGRaLGRcf8Zuc9GNwcAorqgrQ3lZ3zfN1Y2lY6sWUPIq2gtuwIqPJ6HKWvarMXHymfivGpQBOe8eMLoeIEhADH1EMqboKSQlEXOGTxBAsFhN0HRgZ9WJ0zIuG+hJs2VJ9VZM34mU8OIjRoAeb83ajwbXZ6HLogm1FVbhzxRqcnB7FiDhrdDlElGAS76cJUQoJXurBt/AKnz8QgtVixuDQFKanA1i/rhJr15Yn5ISKaXkMg9JZrMlqx7rsLqPLoT9wV9UabC2sxFtj/fCHQ0aXQ0QJhIGPKIZExQ9dX7zp8vSMCFGUsbWlBnV1xUikhsoXzYan4AmcRF3mRrTkfyQhA+lyZzaZ8PmGVpSnZ+PVkV6ENdXokogoQTDwEcWQqPqgQV3wGT5V1RAKKUhPt6OjvQ4VFXlxrnBpRGUWPf7jqExvQEfB7TAJ/KcjUWVY7fjC6k5YTSa8NcpDHEQ0h/9qE8WQpPoBYME+fGazgK7OVdjeVY+iosRsWBxSJZz1H4XbUYWdRffCYlq4xQwlhrL0bDzU0IZpWcLJ6RGjyyGiBMDARxRDkhp5ygYgoKDAhZyc9LjWtVRhLYQzvkPItRVht/v+iFvTlFg2FZTjnup1OOMdx1DAa3Q5RGQwBj6iGBIVf8Smy4ns4nzcdEsW9rgPwGFOzFBKC/toZTPa3dV4e7wfs3LQ6HKIyEAMfEQxFFBmk3ILVNNVnPUfhdVkx97ig8i05hhdEl0DQRDwmfqtqM7M4yEOomWOgY8ohvzKTMSWLIlI13V0+49D1zXsdu9Hjq3Q6JLoOjgtNjze3AmnxYo3Rjw8xEG0TDHwEcWQqM4m1XNvuq6jN3ASIU3EDe57UZRWYXRJFAXFThcebmyDTwnh+NR5o8shIgMw8BHFiKKFEVKliGPVEs2gdBY+ZQqdBXei3FlndDkURWvzSnHfyg3onp3AgH/G6HKIKM4Sb0gnUYqY68EXuelyIhmR+jAZOo/WgltQk7nW6HIoBm4ub0C/fxovDZ9Dps2ObJvD6JKIKE64wkcUI5J6YcqGkPgrfBOhYZwP9mBD7k40ubYaXQ7FiCAI+NSqFqzKKsBr53shq4rRJRFRnDDwEcWIdGGOrjXBV/hm5AkMiKfRlLUVG3N2cmRairObLXisuQMumx2vj3qg8RAH0bLAwEcUI6LihwABZiFxn5zwh2fQG3gPNRlrsS3/Foa9ZaLQkYlHmjogKjKOTg4ZXQ4RxQEDH1GMiKoPlohTNowlqX6c8x9DubMWXYV3wSyYjS6J4qgpx42DtZvQ55+GxzdldDlEFGOJu/RAlOQkxZewTZdDqoQzviMoTCvHDUX7ErZOiq09pavQ55vGc0On4bKlIdfuNLokIooRrvARxUhAnYVFsBldxjxhTcYZ32FkW/Oxx30/7Gae1FyuBEHAJ+o2ozHHjddHehFUw0aXREQxwsBHFCN+xZtwUzZUXcFZ32E4zBnYU3wATkum0SWRwexmCx5r6kCu3YnXR3p5iIMoRTHwEcWArusQlcSasqHpGs75jsIkmLGn+H5kWfOMLokSRF5aOh5t6oCsaTg0MWh0OUQUAwx8RDEQ1mWEtVDCtGTRdR09/hNQ9DB2u/cj315idEmUYFZlF+KTdZsxFJhB9+yE0eUQUZQx8BHFgKRcnLJh/JauruvoE9+HpPqwo+geFDtWGF0SJajtxTX4SFkDjk0OYyIYMLocIooiBj6iGJBUPzQ9McaqDQd74A1PoL3gdlSlNxhdDiUwQRBwf+1GrM0txRujHkgKD3EQpQoGPqIYEFUfdN34KRtjwQGMBQfQkncjVrk2GloLJQeryYxHmtpRmJaB10Z6oOqa0SURURQw8BHFgKj4YBLMhk7ZmAqNYEg6h3XZnVid1WZYHZR8su0OPN7cCVXX8c74gNHlEFEUMPARxYCo+mERjGtmPBueRJ/4Pupdm7Elb2/CTvugxFXtysOn67diRPThjHfc6HKI6Dox8BHFgKT6DZteEVBm0e0/gRXpzWgvuJ1hj65Zh7sat1Y04r2pEYxJfqPLIaLrwMBHFAMBxQurKf5TNoJqAGd9R1DqWIkdRR/jfFy6bvtWbsDGgjK8OeZBICwbXQ4RXSMGPqIY8CteWIX4tmSRtSDO+I4g316CXe59hh8YodRgMZnwcEMbih0uvDrSA0XjIQ6iZMTARxRlRkzZULQwzvoOI9OajT3F9yPNnB63e1Pqy7Sl4fHmTpgFE94e74fO8WtESYeBjyjKZC0IRVfi1nRZ01Wc9R+BzeTAXvdBZFiy43JfWl6qMnPx2YZtGJf8+MA7ZnQ5RHSVGPiIokxUfdChxmVLVdc1nPMdA3Qdu933I9tWEPN70vK1tbASd61Yg1PToxgRZ40uh4iuAgMfUZSJiu/ClI3YrvDpuo6ewEnIehA3uO9DYVpZTO9HBAB3Vq3B1sJKvDXWD384ZHQ5RLREDHxEUSapc4Ev1it8A+IZBJQZbC+8C2XOmpjei+gis8mEzze0oiIjG6+O9CKsqUaXRERLwMBHFGXihR58JiF2317npV5MySNozb8V1RmrY3YfoivJsNrxeHMnbCYz3hrt4yEOoiTAwEcUZVKMp2yMh4YwEvRgU+4uNGZtidl9iCIpS8/GQ41tmJYlnJweMbocIloEAx9RlAUUH8wxCnzT8hgGxDNYndWG9TnbY3IPoqXamF+Ge6rX4Yx3HEMBr9HlEFEEDHxEUTY3ZSP6z+/5wtPwBE6iLnM9tuZ/hCPTKCF8tLIZ7e5qvD3ej1k5aHQ5RLQABj6iKAso3qg3XRYVH875j6EyvQGdBXfAxJFplCAEQcBn6reiOjMPr470QlZ5iIMoETHwEUWRpmuQVH9UW7KEVAln/UfgTqvCzqJ7YDHF7vlAomvhtNjwheZOOC1WvDHq4SEOogTEwEcURUE1AFVXoralG9ZCOOM7hBxrIfa498dtegfR1XI7XXiksR1+JYTjU+eNLoeI/gADH1EUSao/ak2XVV3BGd9hOC0u7Ck+AIclIwoVEsXOmrwS7F+5Ad2zkxjwTxtdDhF9iMXoAohSiaj6oUG77mf4NF3FWd9RWAQb9roPwGXNjVKFRLF1U3kD+vzTeGn4HDRu7RIlDK7wEUWRpPqgQ4NVuPbAp+s6evwnoOkKdrv3I9fujmKFRLElCAI+taoFq7IKYOZJcqKEwcBHFEWi4oNVsF1zyxRd1+ERT0FS/dhZdC/cjsooV0gUe3azBY81d2B7cS1qs/KNLoeIwC1doqi63ikbQ1I3ZsOT2F74MVSkr4piZUTxVejIxMNNbUaXQUQXcIWPKIquZ8rGaLAPE6EhbM27CbWZ66JbGBERLWsMfERRdK1NlydD5zEs9WB9znY0Z22LfmFERLSsMfARRVFA9cJ6lS1ZvPIE+sUP0OhqwabcXRyZRkREUcfARxQlqq5CUgJXtcLnV2bQEziB6ow1aC24lWGPiIhigoGPKEqCl3rwLW2FT1L9OOc7hjJHLbYX3gUz5+MSEVGMMPARRYmo+KHrS2u6LKtBnPUdQUFaKXa574PVZItDhUREtFwx8BFFiaj6oEFd9Bm+sCbjjP8QXNZc7HEfgN3siFOFRES0XDHwEUWJpPoBIGIfPlVXcdZ3BHZTOva6DyLd4opXeUREtIwx8BFFiaT6YIkwZUPTNZzzHYVJMGFv8QFk2TiBgIiI4oOBjyhKRGXhKRu6rqPX/x4UXcYu937k20viXB0RES1nDHxEURJQZmExzQ98uq6jX/wAojqLHUV3o8SxwoDqiIhoOWPgI4oSvzJzxZYs54O9mAmPoa3gNlSlNxpQGRERLXcMfERRIqqz81qyjAUHMBrsx+bcPah3bTKoMiIiWu4Y+IiiQNHCCKnSZS1ZpuRRDErnsDa7A2uzOw2sjoiIljsGPqIomOvB9/umy7PhKfQF3kd95iZsydvLkWlERGQoBj6iKJDUC1M2hDQElFn0+I+jKr0BHYW3wyTw24yIiIzFn0REUSBdmKOrYa7XXrFjBXYW3QOzYDG6NCIiIgY+omiYm6Oro8d/HLl2N3a798O6hJm6RERE8cDARxQFouqDDg3plmzsLT6ANHO60SURERFdwsBHFAUOczpybEXYW3wAGZZso8shIiK6jKDrum50EUSpQNVVmAWz0WUQERHNw8BHRERElOK4pUtERESU4hj4iIiIiFIcAx8RERFRimPgIyIiIkpxDHxEREREKY6Bj4iIiCjFMfARERERpTgGPiIiIqIUx8BHRERElOIY+IiIiIhSHAMfERERUYpj4CMiIiJKcQx8RERERCmOgY+IiIgoxTHwEREREaU4Bj4iIiKiFMfAR0RERJTiGPiIiIiIUhwDHxEREVGKY+AjIiIiSnEMfEREREQpjoGPiIiIKMUx8BERERGlOAY+IiIiohTHwEdERESU4hj4iIiIiFIcAx8RERFRimPgIyIiIkpxDHxEREREKY6Bj4iIiCjFMfARERERpTgGPiIiIqIUx8BHRERElOIY+IiIiIhSHAMfERERUYpj4CMiIiJKcQx8RERERCmOgY+IiIgoxTHwEREREaU4Bj4iIiKiFMfAR0RERJTiGPiIiIiIUhwDHxEREVGKY+AjIiIiSnEMfEREREQpjoGPiIiIKMUx8BERERGlOAY+IiIiohTHwEdERESU4hj4iIiIiFIcAx8RERFRimPgIyIiIkpxDHxEREREKY6Bj4iIiCjFMfARERERpTgGPiIiIqIUx8BHRERElOIY+IiIiIhSHAMfERERUYpj4CMiIiJKcQx8RERERCmOgY+IiIgoxTHwEREREaU4Bj4iIiKiFMfAR0RERJTiGPiIiIiIUhwDHxEREVGKY+AjIiIiSnEMfEREREQpjoGPiIiIKMUx8BERERGlOAY+IiIiohTHwEdERESU4hj4iIiIiFIcAx8RERFRimPgIyIiIkpxDHxEREREKY6Bj4iIiCjFMfARERERpTgGPiIiIqIUx8BHRERElOL+fzC6RvsD2eAmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Features (selected by all methods): 2 features\n",
      "{'125', '248'}\n",
      "Features selected by at least two methods: 131 features\n",
      "Features unique to Boruta: 0 features\n",
      "Features unique to SHAP: 22 features\n",
      "Features unique to L1: 23 features\n",
      "Features unique to MI: 34 features\n",
      "Features unique to SelectKBest: 35 features\n",
      "Features unique to RFE: 38 features\n",
      "Overlap between Boruta and SHAP: 24 features\n",
      "Overlap between Boruta and L1: 25 features\n",
      "Overlap between Boruta and MI: 13 features\n",
      "Overlap between Boruta and SelectKBest: 13 features\n",
      "Overlap between Boruta and RFE: 8 features\n",
      "Overlap between SHAP and SelectKBest: 43 features\n",
      "Overlap between L1 and SHAP: 47 features\n",
      "Overlap between L1 and MI: 34 features\n",
      "Overlap between L1 and SelectKBest: 36 features\n",
      "Overlap between L1 and RFE: 32 features\n",
      "Overlap between MI and SHAP: 38 features\n",
      "Overlap between MI and SelectKBest: 31 features\n",
      "Overlap between MI and RFE: 30 features\n",
      "Overlap between RFE and SHAP: 24 features\n",
      "Overlap between RFE and SelectKBest: 21 features\n"
     ]
    }
   ],
   "source": [
    "from venn import venn\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"# Replace these with the actual feature lists\n",
    "selected_features_boruta = X_indicators[selected_features_boruta]    # List of 28 features\n",
    "selected_features_shap = X_indicators[selected_features_shap]    # List of 100 features\n",
    "selected_features_l1 = X_indicators[selected_features_l1]    # List of 100 features\n",
    "selected_features_mi = X_indicators[selected_features_mi]    # List of 100 features\n",
    "selected_features_selectkbest = X_indicators[selected_features_fclassif]    # List of 100 features\n",
    "selected_features_rfe = X_indicators[selected_features_rfe]    # List of 100 features\"\"\"\n",
    "\n",
    "# Create a dictionary of feature sets for the Venn diagram\n",
    "feature_sets = {\n",
    "    'Boruta': set(selected_features_boruta),\n",
    "    'SHAP': set(selected_features_shap),\n",
    "    'L1': set(selected_features_l1),\n",
    "    'MI': set(selected_features_mi),\n",
    "    'SelectKBest': set(selected_features_fclassif),\n",
    "    'RFE': set(selected_features_rfe)\n",
    "}\n",
    "\n",
    "# Generate the Venn diagram\n",
    "plt.figure(figsize=(10, 10))\n",
    "venn(feature_sets)\n",
    "plt.title(\"Feature Overlap Across Methods\")\n",
    "plt.show()\n",
    "\n",
    "# Analyze overlap numerically\n",
    "# 1. Features selected by all methods (core features)\n",
    "core_features = set.intersection(*feature_sets.values())\n",
    "print(f\"Core Features (selected by all methods): {len(core_features)} features\")\n",
    "print(core_features)\n",
    "\n",
    "# 2. Features selected by at least two methods\n",
    "all_features = set.union(*feature_sets.values())\n",
    "features_at_least_two = set()\n",
    "for feature in all_features:\n",
    "    count = sum(1 for feature_set in feature_sets.values() if feature in feature_set)\n",
    "    if count >= 2:\n",
    "        features_at_least_two.add(feature)\n",
    "print(f\"Features selected by at least two methods: {len(features_at_least_two)} features\")\n",
    "\n",
    "# 3. Features unique to each method\n",
    "for method, features in feature_sets.items():\n",
    "    unique_features = features - set.union(*(other_features for other_method, other_features in feature_sets.items() if other_method != method))\n",
    "    print(f\"Features unique to {method}: {len(unique_features)} features\")\n",
    "\n",
    "# 4. Pairwise overlap (optional, for deeper analysis)\n",
    "for method1, features1 in feature_sets.items():\n",
    "    for method2, features2 in feature_sets.items():\n",
    "        if method1 < method2:  # Avoid duplicate pairs\n",
    "            overlap = len(features1.intersection(features2))\n",
    "            print(f\"Overlap between {method1} and {method2}: {overlap} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "05a86a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Threshold Tuning Metrics (XGBoost, Features Selected by At Least Two Methods, 131 features):\n",
      "Threshold: 0.10, Precision: 0.695312, Recall: 0.855769, F1-Score: 0.767241\n",
      "Threshold: 0.15, Precision: 0.735043, Recall: 0.826923, F1-Score: 0.778281\n",
      "Threshold: 0.20, Precision: 0.775701, Recall: 0.798077, F1-Score: 0.786730\n",
      "Threshold: 0.25, Precision: 0.805825, Recall: 0.798077, F1-Score: 0.801932\n",
      "Threshold: 0.30, Precision: 0.816327, Recall: 0.769231, F1-Score: 0.792079\n",
      "Threshold: 0.35, Precision: 0.821053, Recall: 0.750000, F1-Score: 0.783920\n",
      "Threshold: 0.40, Precision: 0.824176, Recall: 0.721154, F1-Score: 0.769231\n",
      "Threshold: 0.45, Precision: 0.825581, Recall: 0.682692, F1-Score: 0.747368\n",
      "Threshold: 0.50, Precision: 0.855422, Recall: 0.682692, F1-Score: 0.759358\n",
      "Threshold: 0.55, Precision: 0.864198, Recall: 0.673077, F1-Score: 0.756757\n",
      "Threshold: 0.60, Precision: 0.894737, Recall: 0.653846, F1-Score: 0.755556\n",
      "Threshold: 0.65, Precision: 0.927536, Recall: 0.615385, F1-Score: 0.739884\n",
      "Threshold: 0.70, Precision: 0.937500, Recall: 0.576923, F1-Score: 0.714286\n",
      "Threshold: 0.75, Precision: 0.950820, Recall: 0.557692, F1-Score: 0.703030\n",
      "Threshold: 0.80, Precision: 0.962264, Recall: 0.490385, F1-Score: 0.649682\n",
      "Threshold: 0.85, Precision: 0.960000, Recall: 0.461538, F1-Score: 0.623377\n",
      "Threshold: 0.90, Precision: 0.957447, Recall: 0.432692, F1-Score: 0.596026\n",
      "\n",
      "XGBoost (Features Selected by At Least Two Methods, 131 features):\n",
      "Best Threshold: 0.25\n",
      "Precision: 0.805825\n",
      "Recall: 0.798077\n",
      "F1-Score: 0.801932\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Subset data to the 131 features selected by at least two methods\n",
    "# (Replace 'features_at_least_two' with the actual list of 131 features from your analysis)\n",
    "features_at_least_two = list(features_at_least_two)\n",
    "X_at_least_two = X_indicators[features_at_least_two]\n",
    "\n",
    "# Perform cross-validation and threshold tuning\n",
    "y_scores = cross_val_predict(best_xgb, X_at_least_two, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "best_f1 = 0\n",
    "best_recall = 0\n",
    "best_precision = 0\n",
    "best_threshold = 0.5\n",
    "\n",
    "print(\"\\nThreshold Tuning Metrics (XGBoost, Features Selected by At Least Two Methods, 131 features):\")\n",
    "for threshold in thresholds:\n",
    "    y_pred = (y_scores >= threshold).astype(int)\n",
    "    recall = recall_score(Y, y_pred, zero_division=0)\n",
    "    precision = precision_score(Y, y_pred, zero_division=0)\n",
    "    f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "    print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "    if f1_val > best_f1:\n",
    "        best_f1 = f1_val\n",
    "        best_recall = recall\n",
    "        best_precision = precision\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"\\nXGBoost (Features Selected by At Least Two Methods, 131 features):\")\n",
    "print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "print(f\"Precision: {best_precision:.6f}\")\n",
    "print(f\"Recall: {best_recall:.6f}\")\n",
    "print(f\"F1-Score: {best_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3f1418c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Boruta + SHAP + L1 (Union, 153 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.651163, Recall: 0.807692, F1-Score: 0.721030\n",
      "Threshold: 0.15, Precision: 0.704348, Recall: 0.778846, F1-Score: 0.739726\n",
      "Threshold: 0.20, Precision: 0.733333, Recall: 0.740385, F1-Score: 0.736842\n",
      "Threshold: 0.25, Precision: 0.733333, Recall: 0.740385, F1-Score: 0.736842\n",
      "Threshold: 0.30, Precision: 0.770000, Recall: 0.740385, F1-Score: 0.754902\n",
      "Threshold: 0.35, Precision: 0.846154, Recall: 0.740385, F1-Score: 0.789744\n",
      "Threshold: 0.40, Precision: 0.857143, Recall: 0.692308, F1-Score: 0.765957\n",
      "Threshold: 0.45, Precision: 0.875000, Recall: 0.673077, F1-Score: 0.760870\n",
      "Threshold: 0.50, Precision: 0.905405, Recall: 0.644231, F1-Score: 0.752809\n",
      "Threshold: 0.55, Precision: 0.929577, Recall: 0.634615, F1-Score: 0.754286\n",
      "Threshold: 0.60, Precision: 0.942029, Recall: 0.625000, F1-Score: 0.751445\n",
      "Threshold: 0.65, Precision: 0.940299, Recall: 0.605769, F1-Score: 0.736842\n",
      "Threshold: 0.70, Precision: 0.938462, Recall: 0.586538, F1-Score: 0.721893\n",
      "Threshold: 0.75, Precision: 0.934426, Recall: 0.548077, F1-Score: 0.690909\n",
      "Threshold: 0.80, Precision: 0.962963, Recall: 0.500000, F1-Score: 0.658228\n",
      "Threshold: 0.85, Precision: 0.960000, Recall: 0.461538, F1-Score: 0.623377\n",
      "Threshold: 0.90, Precision: 0.975000, Recall: 0.375000, F1-Score: 0.541667\n",
      "\n",
      "Best Threshold: 0.35\n",
      "Precision: 0.846154\n",
      "Recall: 0.740385\n",
      "F1-Score: 0.789744\n",
      "\n",
      "Testing Boruta + Unique SHAP + Unique L1 (127 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.622222, Recall: 0.807692, F1-Score: 0.702929\n",
      "Threshold: 0.15, Precision: 0.656000, Recall: 0.788462, F1-Score: 0.716157\n",
      "Threshold: 0.20, Precision: 0.704348, Recall: 0.778846, F1-Score: 0.739726\n",
      "Threshold: 0.25, Precision: 0.742857, Recall: 0.750000, F1-Score: 0.746411\n",
      "Threshold: 0.30, Precision: 0.770000, Recall: 0.740385, F1-Score: 0.754902\n",
      "Threshold: 0.35, Precision: 0.808511, Recall: 0.730769, F1-Score: 0.767677\n",
      "Threshold: 0.40, Precision: 0.839080, Recall: 0.701923, F1-Score: 0.764398\n",
      "Threshold: 0.45, Precision: 0.853659, Recall: 0.673077, F1-Score: 0.752688\n",
      "Threshold: 0.50, Precision: 0.851852, Recall: 0.663462, F1-Score: 0.745946\n",
      "Threshold: 0.55, Precision: 0.870130, Recall: 0.644231, F1-Score: 0.740331\n",
      "Threshold: 0.60, Precision: 0.876712, Recall: 0.615385, F1-Score: 0.723164\n",
      "Threshold: 0.65, Precision: 0.900000, Recall: 0.605769, F1-Score: 0.724138\n",
      "Threshold: 0.70, Precision: 0.924242, Recall: 0.586538, F1-Score: 0.717647\n",
      "Threshold: 0.75, Precision: 0.938462, Recall: 0.586538, F1-Score: 0.721893\n",
      "Threshold: 0.80, Precision: 0.950820, Recall: 0.557692, F1-Score: 0.703030\n",
      "Threshold: 0.85, Precision: 1.000000, Recall: 0.471154, F1-Score: 0.640523\n",
      "Threshold: 0.90, Precision: 1.000000, Recall: 0.442308, F1-Score: 0.613333\n",
      "\n",
      "Best Threshold: 0.35\n",
      "Precision: 0.808511\n",
      "Recall: 0.730769\n",
      "F1-Score: 0.767677\n",
      "\n",
      "Summary of F1-Scores:\n",
      "Features Selected by At Least Two Methods (131 features): 0.801932\n",
      "Boruta + SHAP + L1 (153 features): 0.789744\n",
      "Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform threshold tuning\n",
    "def threshold_tuning(X, Y, model, cv, thresholds=np.arange(0.1, 0.95, 0.05)):\n",
    "    y_scores = cross_val_predict(model, X, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "    best_f1 = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"\\nThreshold Tuning Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "        print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_recall = recall\n",
    "            best_precision = precision\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {best_precision:.6f}\")\n",
    "    print(f\"Recall: {best_recall:.6f}\")\n",
    "    print(f\"F1-Score: {best_f1:.6f}\")\n",
    "    return best_f1\n",
    "\n",
    "# 1. Boruta + SHAP + L1 (153 features)\n",
    "print(\"\\nTesting Boruta + SHAP + L1 (Union, 153 features)\")\n",
    "combined_features_boruta_shap_l1 = list(set(selected_features_boruta).union(selected_features_shap, selected_features_l1))\n",
    "X_combined_boruta_shap_l1 = X_indicators[combined_features_boruta_shap_l1]\n",
    "f1_boruta_shap_l1 = threshold_tuning(X_combined_boruta_shap_l1, Y, best_xgb, cv)\n",
    "\n",
    "# 2. Boruta + Unique SHAP + Unique L1 (127 features)\n",
    "print(\"\\nTesting Boruta + Unique SHAP + Unique L1 (127 features)\")\n",
    "unique_shap = list(set(selected_features_shap) - set(selected_features_boruta).union(selected_features_l1, selected_features_mi, selected_features_k, selected_features_rfe))\n",
    "unique_l1 = list(set(selected_features_l1) - set(selected_features_boruta).union(selected_features_shap, selected_features_mi, selected_features_k, selected_features_rfe))\n",
    "combined_features_boruta_shap_l1_unique = list(set(selected_features_boruta).union(unique_shap, unique_l1))\n",
    "X_boruta_shap_l1_unique = X_indicators[combined_features_boruta_shap_l1_unique]\n",
    "f1_boruta_shap_l1_unique = threshold_tuning(X_boruta_shap_l1_unique, Y, best_xgb, cv)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of F1-Scores:\")\n",
    "print(f\"Features Selected by At Least Two Methods (131 features): 0.801932\")\n",
    "print(f\"Boruta + SHAP + L1 (153 features): {f1_boruta_shap_l1:.6f}\")\n",
    "print(f\"Boruta + Unique SHAP + Unique L1 (127 features): {f1_boruta_shap_l1_unique:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "983e197a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert lists to sets for Venn diagram\n",
    "set_boruta = set(map(str, selected_features_boruta))\n",
    "set_l1 = set(map(str, selected_features_l1))\n",
    "set_shap = set(map(str, selected_features_shap))\n",
    "\n",
    "# Create Venn diagram\n",
    "plt.figure(figsize=(8, 8))\n",
    "venn3(\n",
    "    [set_boruta, set_l1, set_shap],\n",
    "    set_labels=('Boruta', 'L1', 'SHAP')\n",
    ")\n",
    "plt.title(\"Feature Overlap Between Boruta, L1, and SHAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2d8929e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Boruta + SHAP  L1 (Excluding Unique Features, ~54 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.644444, Recall: 0.836538, F1-Score: 0.728033\n",
      "Threshold: 0.15, Precision: 0.726496, Recall: 0.817308, F1-Score: 0.769231\n",
      "Threshold: 0.20, Precision: 0.773585, Recall: 0.788462, F1-Score: 0.780952\n",
      "Threshold: 0.25, Precision: 0.797980, Recall: 0.759615, F1-Score: 0.778325\n",
      "Threshold: 0.30, Precision: 0.812500, Recall: 0.750000, F1-Score: 0.780000\n",
      "Threshold: 0.35, Precision: 0.855556, Recall: 0.740385, F1-Score: 0.793814\n",
      "Threshold: 0.40, Precision: 0.875000, Recall: 0.740385, F1-Score: 0.802083\n",
      "Threshold: 0.45, Precision: 0.891566, Recall: 0.711538, F1-Score: 0.791444\n",
      "Threshold: 0.50, Precision: 0.891566, Recall: 0.711538, F1-Score: 0.791444\n",
      "Threshold: 0.55, Precision: 0.912500, Recall: 0.701923, F1-Score: 0.793478\n",
      "Threshold: 0.60, Precision: 0.923077, Recall: 0.692308, F1-Score: 0.791209\n",
      "Threshold: 0.65, Precision: 0.946667, Recall: 0.682692, F1-Score: 0.793296\n",
      "Threshold: 0.70, Precision: 0.945946, Recall: 0.673077, F1-Score: 0.786517\n",
      "Threshold: 0.75, Precision: 0.943662, Recall: 0.644231, F1-Score: 0.765714\n",
      "Threshold: 0.80, Precision: 0.939394, Recall: 0.596154, F1-Score: 0.729412\n",
      "Threshold: 0.85, Precision: 0.951613, Recall: 0.567308, F1-Score: 0.710843\n",
      "Threshold: 0.90, Precision: 0.962963, Recall: 0.500000, F1-Score: 0.658228\n",
      "\n",
      "Best Threshold: 0.40\n",
      "Precision: 0.875000\n",
      "Recall: 0.740385\n",
      "F1-Score: 0.802083\n",
      "\n",
      "Summary of F1-Scores:\n",
      "Boruta (28 features): 0.810811\n",
      "Features Selected by At Least Two Methods (131 features): 0.801932\n",
      "Boruta + SHAP + L1 (153 features): 0.789744\n",
      "Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\n",
      "Boruta + SHAP  L1 (~54 features): 0.802083\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform threshold tuning\n",
    "def threshold_tuning(X, Y, model, cv, thresholds=np.arange(0.1, 0.95, 0.05)):\n",
    "    y_scores = cross_val_predict(model, X, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "    best_f1 = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"\\nThreshold Tuning Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "        print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_recall = recall\n",
    "            best_precision = precision\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {best_precision:.6f}\")\n",
    "    print(f\"Recall: {best_recall:.6f}\")\n",
    "    print(f\"F1-Score: {best_f1:.6f}\")\n",
    "    return best_f1\n",
    "\n",
    "# 1. Boruta + SHAP  L1 (Excluding Unique Features, ~54 features)\n",
    "print(\"\\nTesting Boruta + SHAP  L1 (Excluding Unique Features, ~54 features)\")\n",
    "shared_shap_l1 = list(set(selected_features_shap).intersection(selected_features_l1))\n",
    "combined_features_boruta_shap_l1_shared = list(set(selected_features_boruta).union(shared_shap_l1))\n",
    "X_boruta_shap_l1_shared = X_indicators[combined_features_boruta_shap_l1_shared]\n",
    "f1_boruta_shap_l1_shared = threshold_tuning(X_boruta_shap_l1_shared, Y, best_xgb, cv)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of F1-Scores:\")\n",
    "print(f\"Boruta (28 features): 0.810811\")\n",
    "print(f\"Features Selected by At Least Two Methods (131 features): 0.801932\")\n",
    "print(f\"Boruta + SHAP + L1 (153 features): 0.789744\")\n",
    "print(f\"Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\")\n",
    "print(f\"Boruta + SHAP  L1 (~54 features): {f1_boruta_shap_l1_shared:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc2700d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features selected by at least 3 methods: 68\n",
      "Number of features (Boruta + At Least 3 Methods): 73\n",
      "\n",
      "Testing Boruta + Features Selected by At Least 3 Methods (73 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.641791, Recall: 0.826923, F1-Score: 0.722689\n",
      "Threshold: 0.15, Precision: 0.703390, Recall: 0.798077, F1-Score: 0.747748\n",
      "Threshold: 0.20, Precision: 0.747664, Recall: 0.769231, F1-Score: 0.758294\n",
      "Threshold: 0.25, Precision: 0.804124, Recall: 0.750000, F1-Score: 0.776119\n",
      "Threshold: 0.30, Precision: 0.815217, Recall: 0.721154, F1-Score: 0.765306\n",
      "Threshold: 0.35, Precision: 0.824176, Recall: 0.721154, F1-Score: 0.769231\n",
      "Threshold: 0.40, Precision: 0.831461, Recall: 0.711538, F1-Score: 0.766839\n",
      "Threshold: 0.45, Precision: 0.847059, Recall: 0.692308, F1-Score: 0.761905\n",
      "Threshold: 0.50, Precision: 0.857143, Recall: 0.692308, F1-Score: 0.765957\n",
      "Threshold: 0.55, Precision: 0.898734, Recall: 0.682692, F1-Score: 0.775956\n",
      "Threshold: 0.60, Precision: 0.898734, Recall: 0.682692, F1-Score: 0.775956\n",
      "Threshold: 0.65, Precision: 0.943662, Recall: 0.644231, F1-Score: 0.765714\n",
      "Threshold: 0.70, Precision: 0.939394, Recall: 0.596154, F1-Score: 0.729412\n",
      "Threshold: 0.75, Precision: 0.937500, Recall: 0.576923, F1-Score: 0.714286\n",
      "Threshold: 0.80, Precision: 0.949153, Recall: 0.538462, F1-Score: 0.687117\n",
      "Threshold: 0.85, Precision: 0.963636, Recall: 0.509615, F1-Score: 0.666667\n",
      "Threshold: 0.90, Precision: 0.959184, Recall: 0.451923, F1-Score: 0.614379\n",
      "\n",
      "Best Threshold: 0.25\n",
      "Precision: 0.804124\n",
      "Recall: 0.750000\n",
      "F1-Score: 0.776119\n",
      "\n",
      "Summary of F1-Scores:\n",
      "Boruta (28 features): 0.810811\n",
      "Features Selected by At Least Two Methods (131 features): 0.801932\n",
      "Boruta + SHAP + L1 (153 features): 0.789744\n",
      "Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\n",
      "Boruta + SHAP  L1 (~54 features): 0.802083\n",
      "Boruta + Features Selected by At Least 3 Methods (73 features): 0.776119\n"
     ]
    }
   ],
   "source": [
    "# Combine all feature sets into a list of sets\n",
    "feature_sets = {\n",
    "    'Boruta': set(selected_features_boruta),\n",
    "    'SHAP': set(selected_features_shap),\n",
    "    'L1': set(selected_features_l1),\n",
    "    'MI': set(selected_features_mi),\n",
    "    'SelectKBest': set(selected_features_k),\n",
    "    'RFE': set(selected_features_rfe)\n",
    "}\n",
    "\n",
    "# Count how many methods each feature appears in\n",
    "feature_counts = {}\n",
    "for feature in set.union(*feature_sets.values()):\n",
    "    count = sum(1 for feature_set in feature_sets.values() if feature in feature_set)\n",
    "    feature_counts[feature] = count\n",
    "\n",
    "# Identify features selected by at least 3 methods\n",
    "features_at_least_three = [feature for feature, count in feature_counts.items() if count >= 3]\n",
    "print(f\"Number of features selected by at least 3 methods: {len(features_at_least_three)}\")\n",
    "\n",
    "# Combine with Boruta's features\n",
    "combined_features_boruta_at_least_three = list(set(features_at_least_three).union(selected_features_boruta))\n",
    "print(f\"Number of features (Boruta + At Least 3 Methods): {len(combined_features_boruta_at_least_three)}\")\n",
    "\n",
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform threshold tuning\n",
    "def threshold_tuning(X, Y, model, cv, thresholds=np.arange(0.1, 0.95, 0.05)):\n",
    "    y_scores = cross_val_predict(model, X, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "    best_f1 = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"\\nThreshold Tuning Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "        print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_recall = recall\n",
    "            best_precision = precision\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {best_precision:.6f}\")\n",
    "    print(f\"Recall: {best_recall:.6f}\")\n",
    "    print(f\"F1-Score: {best_f1:.6f}\")\n",
    "    return best_f1\n",
    "\n",
    "# Test Boruta + Features Selected by At Least 3 Methods\n",
    "print(f\"\\nTesting Boruta + Features Selected by At Least 3 Methods ({len(combined_features_boruta_at_least_three)} features)\")\n",
    "X_boruta_at_least_three = X_indicators[combined_features_boruta_at_least_three]\n",
    "f1_boruta_at_least_three = threshold_tuning(X_boruta_at_least_three, Y, best_xgb, cv)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of F1-Scores:\")\n",
    "print(f\"Boruta (28 features): 0.810811\")\n",
    "print(f\"Features Selected by At Least Two Methods (131 features): 0.801932\")\n",
    "print(f\"Boruta + SHAP + L1 (153 features): 0.789744\")\n",
    "print(f\"Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\")\n",
    "print(f\"Boruta + SHAP  L1 (~54 features): 0.802083\")\n",
    "print(f\"Boruta + Features Selected by At Least 3 Methods ({len(combined_features_boruta_at_least_three)} features): {f1_boruta_at_least_three:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "141cd07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in Boruta + SHAP  L1: 54\n",
      "Number of features selected by at least 4 methods: 34\n",
      "Number of features (Boruta + SHAP  L1 + At Least 4 Methods): 57\n"
     ]
    }
   ],
   "source": [
    "# Compute Boruta + SHAP  L1 (~54 features)\n",
    "shared_shap_l1 = list(set(selected_features_shap).intersection(selected_features_l1))\n",
    "combined_features_boruta_shap_l1 = list(set(selected_features_boruta).union(shared_shap_l1))\n",
    "print(f\"Number of features in Boruta + SHAP  L1: {len(combined_features_boruta_shap_l1)}\")\n",
    "\n",
    "# Combine all feature sets into a list of sets for \"At Least 4 Methods\"\n",
    "feature_sets = {\n",
    "    'Boruta': set(selected_features_boruta),\n",
    "    'SHAP': set(selected_features_shap),\n",
    "    'L1': set(selected_features_l1),\n",
    "    'MI': set(selected_features_mi),\n",
    "    'SelectKBest': set(selected_features_k),\n",
    "    'RFE': set(selected_features_rfe)\n",
    "}\n",
    "\n",
    "# Count how many methods each feature appears in\n",
    "feature_counts = {}\n",
    "for feature in set.union(*feature_sets.values()):\n",
    "    count = sum(1 for feature_set in feature_sets.values() if feature in feature_set)\n",
    "    feature_counts[feature] = count\n",
    "\n",
    "# Identify features selected by at least 4 methods\n",
    "features_at_least_four = [feature for feature, count in feature_counts.items() if count >= 4]\n",
    "print(f\"Number of features selected by at least 4 methods: {len(features_at_least_four)}\")\n",
    "\n",
    "# Combine Boruta + SHAP  L1 + At Least 4 Methods\n",
    "combined_features = list(set(combined_features_boruta_shap_l1).union(features_at_least_four))\n",
    "print(f\"Number of features (Boruta + SHAP  L1 + At Least 4 Methods): {len(combined_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "726bd6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Boruta + SHAP  L1 + At Least 4 Methods (57 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.610294, Recall: 0.798077, F1-Score: 0.691667\n",
      "Threshold: 0.15, Precision: 0.675000, Recall: 0.778846, F1-Score: 0.723214\n",
      "Threshold: 0.20, Precision: 0.727273, Recall: 0.769231, F1-Score: 0.747664\n",
      "Threshold: 0.25, Precision: 0.772277, Recall: 0.750000, F1-Score: 0.760976\n",
      "Threshold: 0.30, Precision: 0.785714, Recall: 0.740385, F1-Score: 0.762376\n",
      "Threshold: 0.35, Precision: 0.800000, Recall: 0.730769, F1-Score: 0.763819\n",
      "Threshold: 0.40, Precision: 0.815217, Recall: 0.721154, F1-Score: 0.765306\n",
      "Threshold: 0.45, Precision: 0.833333, Recall: 0.721154, F1-Score: 0.773196\n",
      "Threshold: 0.50, Precision: 0.848837, Recall: 0.701923, F1-Score: 0.768421\n",
      "Threshold: 0.55, Precision: 0.867470, Recall: 0.692308, F1-Score: 0.770053\n",
      "Threshold: 0.60, Precision: 0.898734, Recall: 0.682692, F1-Score: 0.775956\n",
      "Threshold: 0.65, Precision: 0.909091, Recall: 0.673077, F1-Score: 0.773481\n",
      "Threshold: 0.70, Precision: 0.905405, Recall: 0.644231, F1-Score: 0.752809\n",
      "Threshold: 0.75, Precision: 0.914286, Recall: 0.615385, F1-Score: 0.735632\n",
      "Threshold: 0.80, Precision: 0.952381, Recall: 0.576923, F1-Score: 0.718563\n",
      "Threshold: 0.85, Precision: 0.952381, Recall: 0.576923, F1-Score: 0.718563\n",
      "Threshold: 0.90, Precision: 0.964286, Recall: 0.519231, F1-Score: 0.675000\n",
      "\n",
      "Best Threshold: 0.60\n",
      "Precision: 0.898734\n",
      "Recall: 0.682692\n",
      "F1-Score: 0.775956\n",
      "\n",
      "Summary of F1-Scores:\n",
      "Boruta (28 features): 0.810811\n",
      "Features Selected by At Least Two Methods (131 features): 0.801932\n",
      "Boruta + SHAP + L1 (153 features): 0.789744\n",
      "Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\n",
      "Boruta + SHAP  L1 (~54 features): 0.802083\n",
      "Boruta + Features Selected by At Least 3 Methods (70 features): 0.776119\n",
      "Boruta + SHAP  L1 + At Least 4 Methods (57 features): 0.775956\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform threshold tuning\n",
    "def threshold_tuning(X, Y, model, cv, thresholds=np.arange(0.1, 0.95, 0.05)):\n",
    "    y_scores = cross_val_predict(model, X, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "    best_f1 = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"\\nThreshold Tuning Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "        print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_recall = recall\n",
    "            best_precision = precision\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {best_precision:.6f}\")\n",
    "    print(f\"Recall: {best_recall:.6f}\")\n",
    "    print(f\"F1-Score: {best_f1:.6f}\")\n",
    "    return best_f1\n",
    "\n",
    "# Test Boruta + SHAP  L1 + At Least 4 Methods\n",
    "print(f\"\\nTesting Boruta + SHAP  L1 + At Least 4 Methods ({len(combined_features)} features)\")\n",
    "X_combined = X_indicators[combined_features]\n",
    "f1_combined = threshold_tuning(X_combined, Y, best_xgb, cv)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of F1-Scores:\")\n",
    "print(f\"Boruta (28 features): 0.810811\")\n",
    "print(f\"Features Selected by At Least Two Methods (131 features): 0.801932\")\n",
    "print(f\"Boruta + SHAP + L1 (153 features): 0.789744\")\n",
    "print(f\"Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\")\n",
    "print(f\"Boruta + SHAP  L1 (~54 features): 0.802083\")\n",
    "print(f\"Boruta + Features Selected by At Least 3 Methods (70 features): 0.776119\")\n",
    "print(f\"Boruta + SHAP  L1 + At Least 4 Methods ({len(combined_features)} features): {f1_combined:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b8293aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected_features_boruta: ['17', '22', '34', '60', '65', '68', '74', '96', '113', '125', '141', '151', '240', '248', '284', '346', '347', '424', '427', '524', '555', '563', '565', '569', '570', '581', 'missing_73', 'missing_113']\n",
      "selected_features_shap: ['113', '346', '248', '60', '74', '73', '581', '569', '563', 'missing_73', '556', '564', '11', '582', '558', '386', '1', '347', '548', '570', '22', 'missing_113', '470', '567', '164', '478', '299', '91', '520', '512', '308', '461', '572', '160', '547', '249', '576', '492', '103', '313', '568', '555', '134', '554', '46', '301', '163', '487', '566', '206', '182', '432', '549', '352', '173', '580', '426', '185', '288', '128', '552', '15', '87', '272', '284', '114', '17', '557', '172', '297', '485', '104', '96', '292', '41', '117', '427', '80', '565', '447', '141', '153', '551', '588', '589', '489', '161', '123', '147', '438', '151', '125', '34', '146', '19', '333', '5', '181', '420', '240']\n",
      "selected_features_l1: ['missing_113', '248', '570', '15', 'missing_73', '34', '347', '154', '563', '60', '377', '300', '284', '569', '21', '391', '91', '188', '565', '73', '337', '74', '213', '113', '291', '346', '92', '68', '24', '552', '90', '125', '525', '524', '548', '131', '547', '556', '89', '490', '349', '130', '116', '65', '103', '341', '368', '121', '118', '575', '105', '469', '22', '581', '424', '564', '491', '582', '30', '474', '3', '189', '148', '299', '319', '486', '17', '339', '555', '566', '576', '476', '160', '63', '96', '317', '572', '278', '93', '23', '80', '206', '432', '567', '147', '59', '477', '431', '20', '85', '114', '38', '41', '332', '11', '478', '308', '151', '172', '511']\n",
      "Number of features in Boruta + SHAP  L1 (before adjustment): 54\n",
      "Features in combined_features_boruta_shap_l1 (before adjustment): ['141', '427', '284', '432', '113', '125', '91', '347', '570', '582', '308', '60', '34', '11', '565', '15', '569', '68', '548', '581', '572', '147', '566', '478', '240', '424', '80', '73', '524', '547', '576', '552', '160', 'missing_73', '563', '41', '65', '206', '96', '556', '103', '151', 'missing_113', '567', '22', '17', '346', '172', '555', '114', '564', '74', '248', '299']\n",
      "Columns in X_indicators: ['1', '2', '3', '4', '5', '7', '8', '9', '10', '11', '12', '13', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '44', '45', '46', '47', '48', '49', '51', '52', '54', '55', '56', '57', '58', '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '71', '72', '73', '74', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '87', '88', '89', '90', '91', '92', '93', '94', '95', '96', '97', '99', '100', '101', '102', '103', '104', '105', '106', '107', '108', '109', '110', '111', '112', '113', '114', '116', '117', '118', '119', '120', '121', '122', '123', '124', '125', '126', '127', '128', '129', '130', '131', '132', '133', '134', '135', '136', '137', '138', '139', '140', '141', '143', '144', '145', '146', '147', '148', '149', '151', '152', '153', '154', '155', '156', '157', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '176', '177', '178', '181', '182', '183', '184', '185', '186', '188', '189', '196', '197', '198', '199', '200', '201', '202', '203', '204', '205', '206', '208', '209', '211', '212', '213', '214', '215', '216', '217', '218', '219', '220', '222', '223', '224', '225', '226', '228', '229', '239', '240', '245', '246', '247', '248', '249', '251', '252', '253', '254', '255', '256', '268', '269', '270', '271', '272', '273', '274', '275', '276', '278', '279', '280', '281', '282', '283', '284', '286', '287', '288', '289', '290', '291', '292', '295', '296', '297', '298', '299', '300', '301', '302', '303', '304', '305', '306', '307', '308', '309', '310', '311', '312', '313', '317', '318', '319', '320', '321', '322', '324', '325', '332', '333', '334', '335', '336', '337', '338', '339', '340', '341', '342', '344', '345', '346', '347', '349', '350', '351', '352', '353', '354', '355', '356', '357', '358', '360', '361', '362', '363', '364', '366', '367', '368', '369', '377', '378', '383', '384', '385', '386', '387', '389', '390', '391', '392', '393', '394', '406', '407', '408', '409', '410', '411', '412', '413', '414', '416', '417', '418', '419', '420', '421', '422', '424', '425', '426', '427', '428', '429', '430', '431', '432', '433', '434', '435', '436', '437', '438', '439', '440', '441', '442', '443', '444', '445', '446', '447', '448', '449', '453', '454', '455', '456', '457', '458', '460', '461', '468', '469', '470', '471', '472', '473', '474', '475', '476', '477', '478', '480', '481', '483', '484', '485', '486', '487', '488', '489', '490', '491', '492', '494', '495', '496', '497', '498', '500', '501', '511', '512', '517', '518', '519', '520', '521', '523', '524', '525', '526', '527', '528', '540', '541', '542', '543', '544', '545', '546', '547', '548', '549', '550', '551', '552', '553', '554', '555', '556', '557', '558', '559', '560', '561', '562', '563', '564', '565', '566', '567', '568', '569', '570', '571', '572', '573', '574', '575', '576', '577', '578', '579', '580', '581', '582', '583', '584', '585', '586', '587', '588', '589', '590', 'missing_73', 'missing_74', 'missing_113', 'missing_248', 'missing_346', 'missing_347', 'missing_386', 'missing_520', 'missing_late_utilization', 'proportion_na_features']\n",
      "Number of columns in X_indicators: 466\n",
      "Number of features in Boruta + SHAP  L1 (after adjustment): 54\n",
      "Features in combined_features_boruta_shap_l1 (after adjustment): ['141', '427', '284', '432', '113', '125', '91', '347', '570', '582', '308', '60', '34', '11', '565', '15', '569', '68', '548', '581', '572', '147', '566', '478', '240', '424', '80', '73', '524', '547', '576', '552', '160', 'missing_73', '563', '41', '65', '206', '96', '556', '103', '151', 'missing_113', '567', '22', '17', '346', '172', '555', '114', '564', '74', '248', '299']\n",
      "Number of non-Boruta features in SHAP  L1: 26\n",
      "Number of features (Boruta + Top 11 from SHAP  L1): 39\n",
      "\n",
      "Testing Boruta + Top 11 from SHAP  L1 (39 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.651515, Recall: 0.826923, F1-Score: 0.728814\n",
      "Threshold: 0.15, Precision: 0.703390, Recall: 0.798077, F1-Score: 0.747748\n",
      "Threshold: 0.20, Precision: 0.738739, Recall: 0.788462, F1-Score: 0.762791\n",
      "Threshold: 0.25, Precision: 0.778846, Recall: 0.778846, F1-Score: 0.778846\n",
      "Threshold: 0.30, Precision: 0.792079, Recall: 0.769231, F1-Score: 0.780488\n",
      "Threshold: 0.35, Precision: 0.831579, Recall: 0.759615, F1-Score: 0.793970\n",
      "Threshold: 0.40, Precision: 0.857143, Recall: 0.750000, F1-Score: 0.800000\n",
      "Threshold: 0.45, Precision: 0.873563, Recall: 0.730769, F1-Score: 0.795812\n",
      "Threshold: 0.50, Precision: 0.892857, Recall: 0.721154, F1-Score: 0.797872\n",
      "Threshold: 0.55, Precision: 0.888889, Recall: 0.692308, F1-Score: 0.778378\n",
      "Threshold: 0.60, Precision: 0.921053, Recall: 0.673077, F1-Score: 0.777778\n",
      "Threshold: 0.65, Precision: 0.930556, Recall: 0.644231, F1-Score: 0.761364\n",
      "Threshold: 0.70, Precision: 0.942857, Recall: 0.634615, F1-Score: 0.758621\n",
      "Threshold: 0.75, Precision: 0.955224, Recall: 0.615385, F1-Score: 0.748538\n",
      "Threshold: 0.80, Precision: 0.953125, Recall: 0.586538, F1-Score: 0.726190\n",
      "Threshold: 0.85, Precision: 0.967213, Recall: 0.567308, F1-Score: 0.715152\n",
      "Threshold: 0.90, Precision: 0.963636, Recall: 0.509615, F1-Score: 0.666667\n",
      "\n",
      "Best Threshold: 0.40\n",
      "Precision: 0.857143\n",
      "Recall: 0.750000\n",
      "F1-Score: 0.800000\n",
      "\n",
      "Summary of F1-Scores:\n",
      "Boruta (28 features): 0.810811\n",
      "Features Selected by At Least Two Methods (131 features): 0.801932\n",
      "Boruta + SHAP + L1 (153 features): 0.789744\n",
      "Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\n",
      "Boruta + SHAP  L1 (~54 features): 0.802083\n",
      "Boruta + Features Selected by At Least 3 Methods (70 features): 0.776119\n",
      "Boruta + SHAP  L1 + At Least 4 Methods (57 features): 0.775956\n",
      "Boruta + Top 11 from SHAP  L1 (39 features): 0.8\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost model with Boruta's best hyperparameters\n",
    "best_xgb = XGBClassifier(\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=200,\n",
    "    scale_pos_weight=14,\n",
    "    reg_alpha=0,\n",
    "    reg_lambda=0,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "# Define cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Function to perform threshold tuning\n",
    "def threshold_tuning(X, Y, model, cv, thresholds=np.arange(0.1, 0.95, 0.05)):\n",
    "    y_scores = cross_val_predict(model, X, Y, cv=cv, method='predict_proba')[:, 1]\n",
    "    best_f1 = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    best_threshold = 0.5\n",
    "\n",
    "    print(f\"\\nThreshold Tuning Metrics:\")\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        recall = recall_score(Y, y_pred, zero_division=0)\n",
    "        precision = precision_score(Y, y_pred, zero_division=0)\n",
    "        f1_val = f1_score(Y, y_pred, zero_division=0)\n",
    "        print(f\"Threshold: {threshold:.2f}, Precision: {precision:.6f}, Recall: {recall:.6f}, F1-Score: {f1_val:.6f}\")\n",
    "        if f1_val > best_f1:\n",
    "            best_f1 = f1_val\n",
    "            best_recall = recall\n",
    "            best_precision = precision\n",
    "            best_threshold = threshold\n",
    "\n",
    "    print(f\"\\nBest Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"Precision: {best_precision:.6f}\")\n",
    "    print(f\"Recall: {best_recall:.6f}\")\n",
    "    print(f\"F1-Score: {best_f1:.6f}\")\n",
    "    return best_f1\n",
    "\n",
    "\n",
    "# Debug: Inspect feature lists\n",
    "print(\"selected_features_boruta:\", selected_features_boruta)\n",
    "print(\"selected_features_shap:\", selected_features_shap)\n",
    "print(\"selected_features_l1:\", selected_features_l1)\n",
    "\n",
    "# Ensure feature names are strings\n",
    "selected_features_boruta = [str(feat) for feat in selected_features_boruta]\n",
    "selected_features_shap = [str(feat) for feat in selected_features_shap]\n",
    "selected_features_l1 = [str(feat) for feat in selected_features_l1]\n",
    "\n",
    "# Compute Boruta + SHAP  L1 (~51 features)\n",
    "shared_shap_l1 = list(set(selected_features_shap).intersection(selected_features_l1))\n",
    "combined_features_boruta_shap_l1 = list(set(selected_features_boruta).union(shared_shap_l1))\n",
    "print(f\"Number of features in Boruta + SHAP  L1 (before adjustment): {len(combined_features_boruta_shap_l1)}\")\n",
    "print(\"Features in combined_features_boruta_shap_l1 (before adjustment):\", combined_features_boruta_shap_l1)\n",
    "\n",
    "# Debug: Inspect X_indicators columns\n",
    "print(\"Columns in X_indicators:\", X_indicators.columns.tolist())\n",
    "print(\"Number of columns in X_indicators:\", len(X_indicators.columns))\n",
    "\n",
    "# Ensure X_indicators columns are strings\n",
    "X_indicators.columns = X_indicators.columns.astype(str)\n",
    "\n",
    "# Filter features that exist in X_indicators\n",
    "combined_features_boruta_shap_l1 = [feat for feat in combined_features_boruta_shap_l1 if feat in X_indicators.columns]\n",
    "print(f\"Number of features in Boruta + SHAP  L1 (after adjustment): {len(combined_features_boruta_shap_l1)}\")\n",
    "print(\"Features in combined_features_boruta_shap_l1 (after adjustment):\", combined_features_boruta_shap_l1)\n",
    "\n",
    "# Check if the list is empty\n",
    "if not combined_features_boruta_shap_l1:\n",
    "    raise ValueError(\"No features remaining after adjustment. Check feature name alignment.\")\n",
    "\n",
    "# Train XGBoost to get feature importances\n",
    "X_boruta_shap_l1 = X_indicators[combined_features_boruta_shap_l1]\n",
    "best_xgb.fit(X_boruta_shap_l1, Y)\n",
    "importances = best_xgb.feature_importances_\n",
    "feature_importance_dict = dict(zip(combined_features_boruta_shap_l1, importances))\n",
    "\n",
    "# Identify non-Boruta features in SHAP  L1\n",
    "boruta_features = set(selected_features_boruta)\n",
    "non_boruta_features = [feat for feat in shared_shap_l1 if feat not in boruta_features]\n",
    "non_boruta_importances = {feat: feature_importance_dict[feat] for feat in non_boruta_features}\n",
    "print(f\"Number of non-Boruta features in SHAP  L1: {len(non_boruta_features)}\")\n",
    "\n",
    "# Select top 11 non-Boruta features from SHAP  L1 (to get total ~39 features: 28 Boruta + 11 others)\n",
    "top_11_non_boruta = sorted(non_boruta_importances, key=non_boruta_importances.get, reverse=True)[:11]\n",
    "reduced_features = list(set(selected_features_boruta).union(top_11_non_boruta))\n",
    "print(f\"Number of features (Boruta + Top 11 from SHAP  L1): {len(reduced_features)}\")\n",
    "\n",
    "# Test the reduced feature set\n",
    "print(f\"\\nTesting Boruta + Top 11 from SHAP  L1 ({len(reduced_features)} features)\")\n",
    "X_reduced = X_indicators[reduced_features]\n",
    "f1_reduced = threshold_tuning(X_reduced, Y, best_xgb, cv)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of F1-Scores:\")\n",
    "print(f\"Boruta (28 features): 0.810811\")\n",
    "print(f\"Features Selected by At Least Two Methods (131 features): 0.801932\")\n",
    "print(f\"Boruta + SHAP + L1 (153 features): 0.789744\")\n",
    "print(f\"Boruta + Unique SHAP + Unique L1 (127 features): 0.767677\")\n",
    "print(f\"Boruta + SHAP  L1 (~54 features): 0.802083\")\n",
    "print(f\"Boruta + Features Selected by At Least 3 Methods (70 features): 0.776119\")\n",
    "print(f\"Boruta + SHAP  L1 + At Least 4 Methods (57 features): 0.775956\")\n",
    "print(f\"Boruta + Top 11 from SHAP  L1 ({len(reduced_features)} features): {f1_reduced}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "338569c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Boruta features in SHAP  L1 with importance scores:\n",
      "Feature: 564, Importance: 0.048140\n",
      "Feature: 41, Importance: 0.032679\n",
      "Feature: 548, Importance: 0.024367\n",
      "Feature: 547, Importance: 0.017568\n",
      "Feature: 206, Importance: 0.015771\n",
      "Feature: 147, Importance: 0.013302\n",
      "Feature: 556, Importance: 0.012581\n",
      "Feature: 582, Importance: 0.009973\n",
      "Feature: 432, Importance: 0.008827\n",
      "Feature: 299, Importance: 0.008369\n",
      "Feature: 572, Importance: 0.008363\n",
      "Feature: 478, Importance: 0.007946\n",
      "Feature: 552, Importance: 0.007813\n",
      "Feature: 73, Importance: 0.007668\n",
      "Feature: 91, Importance: 0.007273\n",
      "Feature: 308, Importance: 0.006724\n",
      "Feature: 567, Importance: 0.006480\n",
      "Feature: 114, Importance: 0.006454\n",
      "Feature: 11, Importance: 0.006368\n",
      "Feature: 15, Importance: 0.006199\n",
      "Feature: 160, Importance: 0.005815\n",
      "Feature: 566, Importance: 0.005336\n",
      "Feature: 80, Importance: 0.004970\n",
      "Feature: 103, Importance: 0.003602\n",
      "Feature: 576, Importance: 0.003346\n",
      "Feature: 172, Importance: 0.001432\n",
      "11th feature: 572\n",
      "SHAP values for 572: [-0.01071335 -0.17573264  0.00418493 ... -0.04258944 -0.01554144\n",
      " -0.02362737]\n",
      "Correlation of feature 572 with target (Y): -0.019353\n"
     ]
    }
   ],
   "source": [
    "non_boruta_importances = {feat: feature_importance_dict[feat] for feat in non_boruta_features}\n",
    "top_11_non_boruta = sorted(non_boruta_importances, key=non_boruta_importances.get, reverse=True)[:11]\n",
    "\n",
    "print(\"Non-Boruta features in SHAP  L1 with importance scores:\")\n",
    "for feat, importance in sorted(non_boruta_importances.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"Feature: {feat}, Importance: {importance:.6f}\")\n",
    "\n",
    "eleventh_feature = top_11_non_boruta[-1]\n",
    "print(f\"11th feature: {eleventh_feature}\")\n",
    "\n",
    "import shap\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "shap_values = explainer.shap_values(X_boruta_shap_l1)\n",
    "feature_idx = combined_features_boruta_shap_l1.index(eleventh_feature)\n",
    "print(f\"SHAP values for {eleventh_feature}:\", shap_values[:, feature_idx])\n",
    "\n",
    "correlation = X_boruta_shap_l1['572'].corr(Y)\n",
    "print(f\"Correlation of feature 572 with target (Y): {correlation:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "441d49be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features (Boruta + 572): 29\n",
      "\n",
      "Testing Boruta + Feature 572 (29 features)\n",
      "\n",
      "Threshold Tuning Metrics:\n",
      "Threshold: 0.10, Precision: 0.608108, Recall: 0.865385, F1-Score: 0.714286\n",
      "Threshold: 0.15, Precision: 0.671875, Recall: 0.826923, F1-Score: 0.741379\n",
      "Threshold: 0.20, Precision: 0.714286, Recall: 0.769231, F1-Score: 0.740741\n",
      "Threshold: 0.25, Precision: 0.757282, Recall: 0.750000, F1-Score: 0.753623\n",
      "Threshold: 0.30, Precision: 0.802083, Recall: 0.740385, F1-Score: 0.770000\n",
      "Threshold: 0.35, Precision: 0.810526, Recall: 0.740385, F1-Score: 0.773869\n",
      "Threshold: 0.40, Precision: 0.822222, Recall: 0.711538, F1-Score: 0.762887\n",
      "Threshold: 0.45, Precision: 0.848837, Recall: 0.701923, F1-Score: 0.768421\n",
      "Threshold: 0.50, Precision: 0.876543, Recall: 0.682692, F1-Score: 0.767568\n",
      "Threshold: 0.55, Precision: 0.873418, Recall: 0.663462, F1-Score: 0.754098\n",
      "Threshold: 0.60, Precision: 0.881579, Recall: 0.644231, F1-Score: 0.744444\n",
      "Threshold: 0.65, Precision: 0.878378, Recall: 0.625000, F1-Score: 0.730337\n",
      "Threshold: 0.70, Precision: 0.875000, Recall: 0.605769, F1-Score: 0.715909\n",
      "Threshold: 0.75, Precision: 0.913043, Recall: 0.605769, F1-Score: 0.728324\n",
      "Threshold: 0.80, Precision: 0.940299, Recall: 0.605769, F1-Score: 0.736842\n",
      "Threshold: 0.85, Precision: 0.983607, Recall: 0.576923, F1-Score: 0.727273\n",
      "Threshold: 0.90, Precision: 0.982456, Recall: 0.538462, F1-Score: 0.695652\n",
      "\n",
      "Best Threshold: 0.35\n",
      "Precision: 0.810526\n",
      "Recall: 0.740385\n",
      "F1-Score: 0.773869\n"
     ]
    }
   ],
   "source": [
    "# Add feature '572' to Boruta's features\n",
    "boruta_plus_572 = list(set(selected_features_boruta).union(['572']))\n",
    "print(f\"Number of features (Boruta + 572): {len(boruta_plus_572)}\")\n",
    "\n",
    "# Test the new feature set\n",
    "print(f\"\\nTesting Boruta + Feature 572 ({len(boruta_plus_572)} features)\")\n",
    "X_reduced = X_indicators[boruta_plus_572]\n",
    "f1_reduced = threshold_tuning(X_reduced, Y, best_xgb, cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "1ef754de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boruta features: ['17', '22', '34', '60', '65', '68', '74', '96', '113', '125', '141', '151', '240', '248', '284', '346', '347', '424', '427', '524', '555', '563', '565', '569', '570', '581', 'missing_73', 'missing_113']\n"
     ]
    }
   ],
   "source": [
    "print(\"Boruta features:\", selected_features_boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5869b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def backward_elimination(X, y, p_threshold=0.05):\n",
    "\n",
    "    Perform backward elimination to select features with p-values below the threshold.\n",
    "   \n",
    "    Parameters:\n",
    "    - X: pandas DataFrame of features\n",
    "    - y: pandas Series or numpy array of target variable\n",
    "    - p_threshold: float, p-value threshold for feature significance (default: 0.05)\n",
    "   \n",
    "    Returns:\n",
    "    - X_selected: pandas DataFrame with selected features\n",
    "    - selected_features: list of selected feature names\n",
    "    \n",
    "    # Ensure X is a DataFrame and y is a Series or array\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(Y) if isinstance(Y, (pd.Series, np.ndarray)) else pd.Series(Y.values)\n",
    "   \n",
    "    # Add a constant term for the intercept in statsmodels\n",
    "    X_with_const = sm.add_constant(X)\n",
    "   \n",
    "    # Initialize selected features\n",
    "    selected_features = list(X.columns)\n",
    "   \n",
    "    while len(selected_features) > 0:\n",
    "        # Fit logistic regression model using statsmodels\n",
    "        model = sm.Logit(y, X_with_const).fit(disp=0)  # disp=0 suppresses convergence messages\n",
    "       \n",
    "        # Get p-values (excluding the constant term)\n",
    "        p_values = model.pvalues[1:]  # Skip the 'const' term\n",
    "        max_p_value = p_values.max()\n",
    "        max_p_feature = p_values.idxmax()\n",
    "       \n",
    "        if max_p_value > p_threshold:\n",
    "            # Remove the feature with the highest p-value\n",
    "            selected_features.remove(max_p_feature)\n",
    "            X_with_const = X_with_const.drop(columns=max_p_feature)\n",
    "        else:\n",
    "            break\n",
    "   \n",
    "    # Return the selected features and the corresponding DataFrame\n",
    "    X_selected = X[selected_features]\n",
    "    return X_selected, selected_features\n",
    "\n",
    "# Example usage (will be integrated into the pipeline):\n",
    "# X_selected, selected_features = backward_elimination(X_vif_filtered, Y, p_threshold=0.05) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
